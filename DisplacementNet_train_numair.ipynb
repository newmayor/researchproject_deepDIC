{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.nn import init\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from torchvision import models, transforms\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "def default_loader(path):\n",
    "    return Image.open(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torch.nn import init\n",
    "\n",
    "# custom convolutional layer that accounts for transposed image planes as well as conventional 2D conv layers\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, bias=False, transposed=False):\n",
    "    if transposed:\n",
    "        layer = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=1, output_padding=1,\n",
    "                                   dilation=dilation, bias=bias)\n",
    "    else:\n",
    "        padding = (kernel_size + 2 * (dilation - 1)) // 2\n",
    "        layer = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n",
    "    if bias:\n",
    "        init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "# Returns 2D batch normalisation layer\n",
    "# the range of activation values for each layer is \"forced\" to a normalized distribution of a static mean and cov value, mu & beta\n",
    "# When the activations of a previous layer are forced to a normalized distribution, it makes training of subsequent layers much more efficient\n",
    "def bn(planes):\n",
    "    layer = nn.BatchNorm2d(planes)\n",
    "    # Use mean 0, standard deviation 1 init\n",
    "    init.constant(layer.weight, 1)\n",
    "    init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# feature extraction using pretrained residual network - this performs as the decoder component of the architecture\n",
    "# Resnet addresses the vanishing gradient problem for very deep networks, using skip connections between layers\n",
    "class FeatureResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(BasicBlock, [3, 14, 16, 3], 1000)\n",
    "        self.conv_f = conv(2,64, kernel_size=3,stride = 1)\n",
    "        self.ReLu_1 = nn.ReLU(inplace=True)\n",
    "        self.conv_pre = conv(512, 1024, stride=2, transposed=False)\n",
    "        self.bn_pre = bn(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_f(x) #upsample 8 to 64. changed from Ru's original model bc my image set has 8 feature channels for some reason?\n",
    "        x = self.bn1(x1)\n",
    "        x = self.relu(x)\n",
    "        x2 = self.maxpool(x) #maxpool with kernel size of 3 and add padding of 1\n",
    "        x = self.layer1(x2) #identity sample 64 to 64\n",
    "        x3 = self.layer2(x) #upsample 64 to 128\n",
    "        x4 = self.layer3(x3) #upsample 128 to 256\n",
    "        x5 = self.layer4(x4) #upsample 256 to 512\n",
    "        x6 = self.ReLu_1(self.bn_pre(self.conv_pre(x5))) #upsample 512 to 1024\n",
    "        return x1, x2, x3, x4, x5,x6\n",
    "\n",
    "\n",
    "class SegResNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_net):\n",
    "        super().__init__()\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = conv(1024, 512, stride=1, transposed=False)\n",
    "        self.bn3_2 = bn(512)\n",
    "        self.conv4 = conv(512,512, stride=2, transposed=True)\n",
    "        self.bn4 = bn(512)\n",
    "        self.conv5 = conv(512, 256, stride=2, transposed=True)\n",
    "        self.bn5 = bn(256)\n",
    "        self.conv6 = conv(256, 128, stride=2, transposed=True)\n",
    "        self.bn6 = bn(128)\n",
    "        self.conv7 = conv(128, 64, stride=2, transposed=True)\n",
    "        self.bn7 = bn(64)\n",
    "        self.conv8 = conv(64, 64, stride=2, transposed=True)\n",
    "        self.bn8 = bn(64)\n",
    "        self.conv9 = conv(64, 32, stride=2, transposed=True)\n",
    "        self.bn9 = bn(32)\n",
    "        self.convadd = conv(32, 16, stride=1, transposed=False)\n",
    "        self.bnadd = bn(16)\n",
    "        self.conv10 = conv(16, num_classes,stride=2, kernel_size=5)\n",
    "        init.constant(self.conv10.weight, 0)  # Zero init\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        At init, the FeatureResNet() method (aka \"decoder\") is used to extract features \\\n",
    "        from the input space and then those layer activation values are passed into the \\\n",
    "        encoder's conv layers to reduce dimensionality. Then this is done recursively \\\n",
    "        via gradient descent.\n",
    "        '''\n",
    "        x1, x2, x3, x4, x5, x6 = self.pretrained_net(x) #at init, this is used as feature extraction. Then, it's subsequently used as a decoder\n",
    "        \n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x6)))\n",
    "        \n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn6(self.conv6(x+x4 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn7(self.conv7(x+x3 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn8(self.conv8(x+x2 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn9(self.conv9(x+x1 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bnadd(self.convadd(x)))\n",
    "        x = self.conv10(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-09500fd8f94c>:22: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(layer.weight, 1)\n",
      "<ipython-input-3-09500fd8f94c>:23: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(layer.bias, 0)\n",
      "<ipython-input-3-09500fd8f94c>:72: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.conv10.weight, 0)  # Zero init\n"
     ]
    }
   ],
   "source": [
    "fnet = FeatureResNet()\n",
    "fcn = SegResNet(2,fnet) #changed num_classes from 2 to 8 due to the same change in definition of self.conv_f\n",
    "fcn = fcn.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/numairahmed/deepDIC/images/image_sample_pairs/'\n",
    "filename = \"validation_dataset.txt\"\n",
    "mynumbers = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        item = line.strip().split('\\n')\n",
    "        for subitem in item:\n",
    "            mynumbers.append(subitem)\n",
    "            \n",
    "test_set = []\n",
    "for z in range(317):\n",
    "    test_set.append((dataset_path+'/imgs5_crack/test/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'/imgs5_crack/test/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'/gts5_crack/test/train_image_'+str(z+1)+'_.mat'))\n",
    "\n",
    "# dataset_path = '/home/numairahmed/Deep-Dic-deep-learning-based-digital-image-correlation/image_data_numair'\n",
    "filename = \"train_dataset.txt\"\n",
    "mynumbers = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        item = line.strip().split('\\n')\n",
    "        for subitem in item:\n",
    "            mynumbers.append(subitem)\n",
    "            \n",
    "train_set = []\n",
    "for z in range(3013):\n",
    "    train_set.append((dataset_path+'/imgs5_crack/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'/imgs5_crack/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'/gts5_crack/train_image_'+str(z+1)+'_.mat'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/numairahmed/deepDIC/images/image_sample_pairs//imgs4/train_image_1_1.png', '/home/numairahmed/deepDIC/images/image_sample_pairs//imgs4/train_image_1_2.png', '/home/numairahmed/deepDIC/images/image_sample_pairs//gts4/train_image_1_.mat')\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "#light_index_2 = [2,7,15,8,4,22,13,57,54,40,91,21,29,84,71,25,28,51,67,62,34,46,93,87]\n",
    "class MyDataset(data_utils.Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None, loader=default_loader):\n",
    "        '''\n",
    "        fh = open(txt, 'r')\n",
    "        imgs = []\n",
    "        for line in fh:\n",
    "            line = line.strip('\\n')\n",
    "            line = line.rstrip()\n",
    "            words = line.split()\n",
    "            imgs.append((words[0],int(words[1])))\n",
    "            \n",
    "        '''\n",
    " \n",
    "        self.imgs = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_x, label_y, label_z = self.imgs[index]\n",
    "        img1 = self.loader(label_x)\n",
    "        img_1 = ToTensor()(img1.resize((128,128)))\n",
    "        img_1 = img_1[::4,:,:]\n",
    "        img2 = self.loader(label_y)\n",
    "        img_2 = ToTensor()(img2.resize((128,128)))\n",
    "        img_2 = img_2[::4,:,:]\n",
    "        imgs = torch.cat((img_1, img_2), 0)\n",
    "        try:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_1'].astype(float)\n",
    "            \n",
    "        except KeyError:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_2'].astype(float)\n",
    "            \n",
    "        # print(f\"gt size before {gt.shape} \")\n",
    "        gt = gt[:,::2,::2]\n",
    "        # print(f\"gt size after {gt.shape} \")\n",
    "        # gt = np.moveaxis(gt, -1, 0)\n",
    "        # print(f\"img_1 {img_1.shape}\\n img_2 {img_2.shape}\\n gt {gt.shape}\\n  imgs {imgs.shape}\\n\")\n",
    "        # print(f\"img1 {img1.size}\\n img2 {img2.size}\\n\")\n",
    "        \n",
    "\n",
    "        return imgs,gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TROUBLESHOOTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE =  12\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 300              # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 12\n",
    "print('BATCH_SIZE = ',BATCH_SIZE)\n",
    "LR = 0.001              # learning rate\n",
    "#root = './gdrive_northwestern/My Drive/dl_encoder/data/orig/orig'\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "#optimizer = torch.optim.SGD(cnn.parameters(), lr=LR, momentum=0.9)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_data=MyDataset(dataset=train_set)\n",
    "train_loader = data_utils.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_data=MyDataset(dataset=test_set)\n",
    "test_loader = data_utils.DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7137, 0.6157, 0.4627,  ..., 0.7137, 0.7059, 0.6980],\n",
      "         [0.7333, 0.6431, 0.5059,  ..., 0.7059, 0.7020, 0.7020],\n",
      "         [0.7647, 0.6941, 0.5804,  ..., 0.6980, 0.7020, 0.7020],\n",
      "         ...,\n",
      "         [0.1373, 0.0863, 0.0588,  ..., 0.5216, 0.4627, 0.4745],\n",
      "         [0.1294, 0.0980, 0.0745,  ..., 0.6157, 0.6078, 0.6353],\n",
      "         [0.1255, 0.1020, 0.0863,  ..., 0.6902, 0.7098, 0.7451]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0118,  ..., 0.5647, 0.5569, 0.5569],\n",
      "         [0.0000, 0.0000, 0.0039,  ..., 0.5647, 0.5647, 0.5608],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5686, 0.5647, 0.5647],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9WahubbvnB/3ubjRPM7vVve/Xf3tX7araVSmSkESSQiwUIWgwJxI0EiME9pGgqJiKRx4oJCdqjpQNESIIpaKghICIocCAWKlKKlSzu2/v/e3v+95urTXbpxnN3Xlw3WM8z1xrrubtqtam3gvmO9815zPHM54xxn3dV/O//n+Vc+Y7+86+s398Tf+jPoHv7Dv7zv7R2ndO4Dv7zv4xt++cwHf2nf1jbt85ge/sO/vH3L5zAt/Zd/aPuX3nBL6z7+wfc/vWnIBS6l9USv2eUupnSqm/9m29z3f2nX1nX8/Ut4ETUEoZ4PeB/yrwK+A/Af7bOed/8I2/2Xf2nX1nX8vst3Tcfw74Wc75jwCUUn8d+JeBB51AZRe5dadko0FB1opsFFlDNhy+myyxizpyXFlBPvr+qqny+vJdqfu/zhmICrJCRSCDStMvp78/nAMmo3RGqYyezuOVY8qP5PdGJRoTsCpRK3943/JHioxGXmdJaKXeOzzLQMqZAcOQLNvY0EVHjJqcFKiMNQlnIgszUquAUQktF+wNx1QkNDErulQRsqaPjpQUOT/wQV+9lq9eBwWqnIdVkVpHnI5oEiGX8x5r6DVmALsLECM5BEDJVbIWtCJXVu6DVuRyXx68KPOblx+p+/9+p033W0OuMsYmFnZkZQZq5alVxCiFfsMBI5mYM322jNmyiQ19sMTBYnrQHrRPcsGUIltFsorQAC5z2nYs9MhSD1glp51yJqIIaGLWxPI9IfdF7qiSrwwJRcqakDU+anJW9H/42cuc85NXz/fbcgLfB3559O9fAf+F4xcopX4L+C2Axqz553/03yW3FalxpNoyXDhCq+keafwK/CrjTxO5yqgmAsiD7hUkhfIKle47glwcRnYZdAabUTahdJYL5zUMGrsxmF5R3YIZwe4zKoKOmWQVycFwqvAnmfE8oc5GXB1YNCNGZ4xO3P9sGacTCzeycgM/XV7y2G35cfUSpyKNHolZi4NQnkZ5LnTPUicWStEo814Xuc+RPmd+5k/4hb/gb25+nd+7e8rnd2uG3qFN4vHJju+tbvkL68/5af2CM7NnoQaMSg8eM2bNPtfsUs3v9x/xYlzze7dP2Y0Vu6GanYE6csQ5lwcxH/49mXMRZyJni47zes8PFjc8cjsWZuDaL/nZ7gn/2S9+iPudBad/mDj/O1eoq1viy0tQGmU0+vEj8rJl+P4psdWMK3PYHI68usr5cP/LZpLMYSOZFveDNp27VqAhLCA00H8UcBc9f/7j5/yViz/kzzWf8ZvVF6xV5lRXxRkcDupzZJs9Nwn+wD/iE3/Bf3zzZ/idy4+4/Pk5J79nWH0WWf6qQ/kIWjNc1Axnlsu/pBifBf7qX/5d/un1L/gvLn6fC+PRwCZp9tlyGZdcxRW7VHMbF0QUPlkSipg1PhtSVuxTRRcdG9/w2f6EPlj+5r/47/zJQx/923IC77Sc828Dvw1wap9krm/RXYNqa3Rbka1CJcu4VKRKkUZF2muyz6Sx7KJZya6dQcXpZwfPr8p/clCHB8Bk2eC9wvYKt1HU15lqk2iuIqZP2O2IigkVM9kZYm3on9R0F5quM3S5YlgZrJVd9l5UcO8zKlJWdLFioxvuUstSDzSMnOgepwJneqBRkYWCRmmcer84IJJJgC9v22jPo2rLx4saoxKbtkYBZ03H0o74ZOiTo1eOxnjIPOgIjEo0eLROfOxuWOiRLjquhwVXZsEYDTEdzjEdLfzp58dXwuqENQmtMiEbbnwLwF1ouPJLXnQrwtbR9GCGDCGSYwKlQSswBqx8ZatkkZY3UUkiqdeuuzqchJr8girBojp+OKY/kH+rnFEpQwK7V6gA6cow0vDH9QVP6i1aJS7Mll73wEijNLU6LKMhB/oSBQAYEq3xVDaQbSZbSEaRlUQ5ykfsPpKtwm0dYWm4HJa8bFfcJLlWWmWu4oJdrrgMKzapZUgOnw2xeDVNRquIU7JB1jpwahVnruOs6hii5W8+/Ch9a07gE+CHR//+QfnZw6aQmw6omGAI6DGhx4TxhhAoOzMkpdBTmA5zKjCvwXyULSjuPREqAUEiB9uB3Smal5nl80h157GXHWr0qG6AlOTLWoyzmH6F7Vp0sGRrGL2ibxy5BmtSSTXkfbSS6ECpTMqaIVnuQsNzfcLK9MSsiGbPUg9ERiJHT+17WCTjc+ImwT45Pg+nvAgnXI4rtr6mD47e23I5WlJW+GTQKrO3NXqKQPAPOgJNolJwZvY4Ffm4bql0QKvEPlT4dHAEqXjclA/LcfrZFDFolUlZMUbD1tekrNEqcTe29MGCzsQKxpUmni0wOaNDgFzOTal7O/6XsWlDyCUymNK7h19cMscExIwKiOMxhm3b8kerR2iVOTUdj8yWvb1mrUcaNVKXY/oM+2zYJ0efKgBWZmDlRlQbCI0l1pLuohTkhEoZFTI6yGY2RnHYu1xBuQSXcUWfHftUv+YAHjKnIgmJarDQ6je/9ttyAv8J8GeVUj9FFv9/C/hX3/xyJV4f5sWnx4j2BhWz7PJZHIH4ijd48+Pvx0dPRy9McpNNp3AbaK8SzYsec72Hq1vwI6kf5FA5o4xBWYvxgTpmslownjiy1gyPDF4nUuWZAnipA8iDL1+JIRm0styGVtIAMpWKGJXwWeNI9+scb7HJAfQ5s0+Om9RyGVe8DCuuxgU7X9MHi49GIpGjxbq0AykrTs2epDWViQ9GBEYlyLDQA1olHrttOYbG6kQfHSFpWfiTE+DgDOZzTfcfvDjVF8rDuwsVIRowmVRn/FLhTypUyphuIHsPMfJaIefLmjp8PbRuFECm7M4SCeggj2TWSL6+slzuFizdCZ805wzWAXBmdizVyFqP8/F22bLLVXHusiu31mOrSHIQHTA7gQwpy3Nens2cFSFpdqkmls1xkxr6JE7gXQ5gMk3GqYjWmaTffA2/FSeQcw5Kqf8+8P8EDPC/zzn//Tf+gQJl7VRFkguTMzqmcmEyanbpvHXB3z+R1183eXrtwfYZt4uYux51uyHe3UlRKsbpg8gOpDQ6RkxKtDEynpyhsqZ/ZohGFtBUBzA6YXXC6UilI1YnUtaMJRroYsU+VWxiw8IM9K7izOxIbFnrSEOmVhrzwHYVyQw5sUmZm1Txh/4JX/gz/s7mh1wOSz7dntCNjnG0xGCmuhP7vuLWtQzRcl7viWgu7I7e3bLQw1yXeNUZVCqS0KxMT58cW1MzJHF3Y3l0Hlr807+Tljw1lbQoZcUQ7T3n0DrP2cWOW5MZLxxhVVPdVJz8osXdjdjbnqQUWI0KGWUyOkitZsrzX7MS9h/XA+Q+Hl4/p4xT+jhtMkHqQtU2oUOmvlN0oyYbw6Zd8vOkqXTgtOr5VX3Oqek4tXue2Dsa5alUZMwGny19ckQ0CzNyUnWsVx13ywV+pYi1QfmEGWUDzFYTK0h1Foeh03yMhGYTW3w2+Px+9aLJpjThbfat1QRyzv8h8B++36vL4j/2VqXK9J4b5Jez45QhicPJMUHK4gCOy9w5Q5ZqdR49qvcYL+Hb5JiMlt1/cgBGyXer471aQUgGNHTRYUgkFDd6AUCjPDBAqZwn1L36wBQB7FJmkxyXcckX/ozPxlO+6NfcDQ37oWIcjTiAcMjPc5Yi6p2riUmzcgOdk5Dy3O5Y6oF1qVFUKs47mM+WsRSagPmzTBFOyrp0GnhjiJ2zdGYmJwAQ0HPnpLUesxDns68r9uOCsNDoYGkWmroymD7Mx5ucuEqlsn703vM+odWhul++0EddguPzU4dnQWoCoEPG9AkzJrJRxEox7hRqb+jbiuthMX/2wVmGbDEkGu1Z6IFUqvepFAxd6YpUNpKqLDUuc6hvzOdrINtMZQKGRMwKny0RKfjFUvz7pu0fWWHwjXZUnOH1dPXr2ystv+Q0aH3fAT10Wj5ACKgQ5fyQlqUxmcZKuOdMxKqEnpyASvOCiVkRUaSkGJMtdQLx7ldmSZ8dT8wdj8yOCz3iVKYhY1DFAWT6DJ/GBZ+HM/5ofMI/2H6PL7o1v7o9ZRgcfrDkqOaW51zxDoqkDDdBc2cyt11DW3lO654n7ZaLascPmysaFVibDpC6gM8Wnw23ccE21ozJksrO/qrpV8OysvCVylLDecAaE1jYkcf1Fs4l3fijp494sVvx8qMzqpeGxecNi+dJIrZe8mfTJ1StJHO0xwteyT21h/s7/QwedgLz6RbnYnzGdYnqZsR0HkJCjwuyrvBrw6ArXrQrdnXFLlQs7cjCjmybmpUdeGy3OBXuOX+nIq0ZOakGPm8SsdakSpGdJltNcppYa2KTyVViaUe0yvP1n53At+AA4EN0AsVUSQnU+6fLX+LgR2GikSKNfkvhBKVQzqLqirRoGJeasFSoJlLVntZ6ahuwKs61APvGFpzCqExIhqQ021ATs56rugmN4Y5aRdDS+kjAJhl22fJL/4hf+gt+tn/Kp7tTrroFfVcRg5bdP3FwANN1K04heU3KcHdbcaczX9jMz1cji2bk45MnrNzAqetLHz9jdTmnrAjZ0EVHmMP793sgc1ZzyjCZVoKNqExgace5ZdiowInteLFY8/dM5MX6hLtVTWg09a1i8Tyix4wZEyprtMmEVnrlySpZ/BZipQquhBlPcA/78dDzVNIB7ZH36DxqP6B8wNaG+sZQ3cli7c4qcgZnpnumZodeq4DTgUZJIRUgotAqy+ttlmjAKdmASr1DpVL7SoouOoZkS/hv58js27IP1gmQc/HO+bX+/9c+dNkok1FkS4kE3uYEtNQsmpq0qvArhV+BrQNNJQ6g0gGrU8nB3n6yU1RgyGxjfS/Pi1nRKAkrTR7nn92kmpu04I+Gp3w6nPEn2wueb1fs+4pwLwJ46MNK1Vl3Gh2kKzK9Lqwdt+2CzeOWqvacLHqMThgluanRApSZnNtUEPy6plSmMZ6lHXjsNjyxd5zonh9Wl+xSzbP6jt9fP+Vn68fcqHPCQuN2UkR1e4kIslYkp+bK/+QAYlOivHJZ1ZTvJ/n+WjOmOAYVSyowJNQgXaI8jmhrqGpLdWsJjcLvLYPO9JWfl+etbwhZ0xpPkz1Jj1KUK2mTIVHpgKoisbLEShwXSs1piNQkFH0QJ9BnKT6+r8P9qvZhOoGcpUXjIzoIcEfQfF+ulfYuUzmX477lmEpJh2C1JJ6v6T5q6J4ohseRR+uOk6aXRfLK4tdviASOLWZ1FGLL4tqbGqMya93hjVTlI5qfj495GU74B9uP+aJb88VmxX5fk0YjoKeHdrjJAYwa0ynqa4XbwuJ5xIwZPWbCQhMaRX/REpuW28UJyWWShXASoUmcXOxYNQOndU+l315kggmtVr5QrzmN6Vo5lah1YK07TnTPmd6jVcJnw1p3/KC64s8sH/Ef2d/gxaM1m6GlfaFwm4gZIqRMcro4AwitOIBYT5Fe2UBKIVilAjhN96/VtNnoIE5AjwlClE7V6FHbPVZrll9UqGjxJxYfFFuTSM1IzApNpo+OmBUrO9LqkVqHuT06JCd1EJeIi4RvNbbXZAV6TNhdpLq1xFrzy7tTtEqcu/2M8vy2UgH4UJ0AZYFOHvobTgnUUT9YlcIg6c2LVhmNqipSa0sqkMmLSOskFfgqDmCymBWg0QVPoFVml2pAioUJTURxFVe89Cte9ks2Q03fO5LXAoR6y9uppNBeKt5uC9Vdpn3hMX1Ad57UOlJtcF0lBbCVFMJSBcNgCcvEvqnRKrNyI+Etqc5XNaMymoRTEacSjYoYNgJc0p6fnz4C4OqswfSKWGtpIcd0VJ+RWkB0kFye6wQSSUpaQjhKD954vThsCqWPn0NE9QNuG6hrjdsYsjH4laUvEPJOSzG30pUUBq0s/Cml6qIjZYU2iWAzyR0iFVJGjxHbZWyn2Pc1u0XNPlZo821Uxu/bB+sEZryAP/RPvxFHkJGdYZQv0yfUEMghHsApR6aMEQdwumK4qOmeavx5YHHWcVL3R6HyV18YMUseaFQmormNAvBplKfPjj45ft4/5nm/5ovNmm5whN6JA4hvKssrSQF6hd1p6htYfRaprzz1z19CP5B2e4y1GGupVgty5Ujrhtg6wsKw/Z5jPNPcrSp2OjG0ViDS78gGpijgTbMGCYU5em2fHMnITmfKFr0u4fSZ3tOfOz5unvEf/NqCXbWgurOsMrjbfEjtnCLWSHHNHXcKJsxYRusCKovMG0y5VLLetRwn1Ya8qCUKjJGcEnnf4z7foLsWv1jS7TQ7XdGfGMLKEJKmthEfDc5EnF4cpYdlTiJYAZTZTKwllQHQfUDlTHNTEyvDzW3Ni3rJ1XKJqyPGpNcLr9+gfbhO4BX7BtJQoISEcdodJfRTPkgLcIoK7v2BBmPItSG0MuShmkjtQkHRfXM3J0699ORwKrJJDbtUs08VV+OCm7Fl8JYQjNQA0tsdAEFheoXbgbvLVLcBezuQux66njyOAsYZR9AK5QNaTzgNhx0swU9FxgP671WHd5yzvoobeFP9IGWFL1j3PrvSWzc4lWZH4ATiyRO7YV/XPDnd8smuYrioqHYG7e1h0Gw6hanc8Uq3eVrk84I/xgyUzoJKGR8VZIuKLaZ1GGskJRg8OWd076lvElkpwkIzRkVIig4IVUCpTJU0wejZCUzXxUdDivpei1qiUolEp3kVokCwE4r4D4Hy40+NE4BvxhHMgJAhYwepAjN6mVp7MBLQYC2xdfiFJqwytg60lacqLcFvymTqSyCjk21jw7Vf8Lxbc9O3jKMljebhIiAcLlJQmEFht4rqJrN4Gale7tE3W9JmS/aBHHxZMGUEpqpQgC4hthnrEokdcAK2tD8nC0nPxa/XHMAbgEQg4fmYLF107FPFPtX0umfBYdLSkDEq8pG9wanAX370KUOwXF8+wm0VOlhUKsCh40GhI0zA9M7iJEoxEe5Bz5ORxSgVe0VoMrGqsIOlai2mC5jNAD6gupH28z12X6Oyo+s1Q28ZgNhqjEl4bXA2YnSaN4lUZiti1OK8p0ikRLyCGMxz7WuCev/DsA/ECeQDPHSCU37DNoV/elCYAapNxm0ietOT+x68f7hAqGWSLVtNspJ3Wptw+pvNiwFC1oSo+SKdAPLgbELN3djwxWZNPzopBL7FAaioIILbKsxesfgi015F2s869M2WvN2JA5hQkUqjtDpgJYyWYR1n8K3GLxV5EVguBh41O9Z2oDaBIcrk2hAtYzJAIhVg0dtSgelzoaAPjkoHbsOCU9OxSENBMMY5GgBoVODM7Plp+4KXZ0sun63orxv0qKm2kp6ocOgAFHzS/RHiqXbqxNcfO41U5dkhjB50UPS9woyS/9uuor5rqG6klqLGgLsbWMeE7SqqG42KlrAy7JCuUdt4quIIlMrEJPc2R4UKCu0F+QiQK0vSinGtGU8Uqg0s65HWeMy3Apa5bx+GE5hhbW8Prb9q5K3Ke+gAxoMZwHUJu5eCTw5BEIMP/rE4pWn4JGvQBR78TZtPsoh2WQt23NfsfMV+dOy6iuhNKQS+AfoGEEF7JbMRO6hvE9VNwNx25N2e3A+HiEdplDGSChgN2oCeACxGKu0NuEYeylPXc2K7AoRyhGwkhUGGXoDZARx3CN5kocCpj6MBn80cAUzmymTjM3vL03rLct0zLmvCQuH2qtzbjI7FCdrDop4QgVk+LsnmORyfQEWpziWCmHr1Gd0qdIDQKOxeiqVZg9toqquIGgN2DLSAGRx+ZVFREdaWoCC4WJ4RjdFpHreeOzbTs6wFLJQqg18oQltazzbM3YVvsx4AH4wTyDIskgvVhpE7pKJ4S+0z2quDp3/PNElBaQ8pKQJ6qC8z1TbTft7LwrjbkrqeHPybD5TzPKuuEqSkXxuO+ao25dMpK7ahpguOz+/W9F1FuKuEJ2EahVaASwcM/PQkTQ9WUJi9xu4Vi88z9W1i9Sc79O0erm/Jwwg5oxeLMqJrZfEbA06mI9Oixl+0DOeO3ceK4WnkN5695NfWl/zF5SfzjMEmtuxTxUu/4sYv2FFx520BFh2Gi9QDnntyDEOUx+8LJZHPUEktZK07HpndXB8wZBoVeWo3/Lh9yQ/PHvG7Z2vGraG9FAfgtrJI53kBywEpqPMcCXCMJnRJnrUqSfCp87wPhSTOdhw1upcWa39lqTaG9S817s7jXmxxz7e4lxoVTxjODGAZHml6ldGrjNGBykSSTlij2FaRUGv8yqCSxny/JTQa3yq2P4TxUeCH5xueLjac2I5Gv+W5/DLP2VuquR+ME8AXfLhSgJX0ICS0T7MjUEGhjZKbCu+sUgu+QGF62f1tl6lLGmA2A2rfk6bi2JuikJkp49BaTOl+H/zrFgdTVozJsPMVm6Fme9Oi9pb6pSnjpdL7Ti4TF7rMpR92uxnsMnEk7KC+y1R3ku6ofpSJyLqS3X/Zkp2FypGsJmsNRoZYUm0Yzh3duWY8EwKVHy5v+EF9zUf2FqfkPjkVcbHBF3agIUmF3CdDH+x8alP94FUSEgFM3Z8p8Nmw0CMX1lGpWAabDsVXpwSJt7Aj2WSyKYXeAHbIBC8OvwxBlocgzxGcfC8OwWaw4gS0K07gFcaqnCDpTNKZbDQ6aLJVVBsDCuy2Qg1SU6ruPGhwW0GTem8Ko1Jk4Q4ThvtFxQ4Yzm0BOhliIxGAP4/oE89J3bO2A660F48Hu74KXuBdQ0cfhBPIOZOGoVRJs2D0AZTCdBVubwitJrYQoMR4rxQKyw08HgjRg8J4cJuM20K9iTTPB8x2gOdX5GGQr/gGAMw01QglKgHlIUWN/8YiAdk596Hiardgs22pflVR3SpWnxymKPsLTVgoumcFDOOSVLc04EFFje2EHam5zCw+7bE3PdzclR3QwsmK3FYMT5bERuOXRnrVEwZLQXSK/pFiOM80P97w44tr/tmTP+Yje8P37U2p2MMmVWxMi1NRZt9DzRAtnXfsRyeDVDqTrdzL42LitPCFoETGna/cgsaesTlpeFxt8QvDmdlzYbYsGTFlLHahB9ZugCqRrdwXMyTMALEypEoRlqXDoaWFiKY4DWnPYTPaRZQW56TNw44KILtIrjVpoRgqh++lFeGXCj20VJcKvdljr/coX9OeG8JC4y+kO3BSDXy8uKPSAacSJ1XP7dDyRzxm11n2W0uuIlSJs8dbzhYd31/ccGL7gpt4Nzjrjc/WEdvQ2xzBB+EEJB0okUCMEp5S6jnbCttYqloTGo2KSjbuV6KAfDQANAGB7E66AM11otpE3N2IudyiuoG035dawANRQEEJYgxqsYBFS2wNsZIwUx1Vfb+OTRX1PjruhobNtiVdVSxfKuqbzOK5P4BhdIWK0F8ANeAOVGkpGchg9gp3l2muIvauR+97cI7c1uRFTf/xCr/S7J4ZQgthOe2QJRdWkjP7s4g6HfmLT57z66uXfN9d8UjvWKiAmardSna+M1PTaI9Wme1Qs+srhs5hbMKVlpkzcd7tY1b03hKiYb+vZeahn/C98MXZmvWi54uLNT9obvhRfclH7nbGTLwMa66HBYwaPTJjSKZx4azKgtcSOaHL7l/4KZVLKJNkJHkK7eC1tOXwbwU6odHkJpJ0ZjyTGpHpK1KlqRYOUia2dgZaUSXOFx3PFnf8pL2k0Z5ae55UG7aLmtZ6Nr7mrm+wJuJ04uliw9oNPHI7ah0edABTVPCuiCChpP06Oej05qX+YTgBgBTJYwLrUCkdqOL6CrtzxFrjFqos8NeJJo8nxaYQurqTibD2+YjdDOi7PfluQxrGQwTwIDumzAqoqkItW9KikUikUmSTmQhqvgkLSTNGw26oiFtLdatprjLNdaS67ObzqxaGZA06akAcgHURpTPjKJMyZoBqm6luRvS2l1K3s6RVy/ioYfuxZTxV7D/OxEUirwJKl3C5jB4rm1iue56ut/wTp5/yo+qSj8wdCxVojsLShQ6QYK27+WHtvWUcHGnnSHUkZ0XtwjyLEEubbPCOcTSEjUMNGreVmQaVFOPO8GLZEJPi5mTBsJJR2rXu8NlyFZZsfI0aVfmbo9RQHVX9p5SpFPywQhAroX9G6TSToL7NlMoFV5DIlaQHYa1BafpeUtfkFGYQANAEWDJN4Lze86ze8OP65TwP0juHz5bHbss+VdwGoRCbaMiciizMcO8cvsrsQMySmg3JcuNb+vinwQlAiQhGclCoIKyzyhi01lRKxn7NoAitfiASOLQBzZgxY6a59Ji9x764g64nlep4jhHSK152mhGoKtSihbMT8qpl//GS8cSw/YGmf5wJjz0Xi57WeQlr1dSuO9yod6EHj4uB+1Bx07fc3i6oXlraLxTtpcfdCUBlqtZnPTHSZtIisj7pqK3wG14FQ9ob6XrshSMRIC8axu+dsv+oYvt9ze77iXTqOXu8pSmjxFNEE5NGqczCjnzUbnhW3fGX2l/xxN6x1h5Hxhxf85ypVeRE95yaPSdWRpBTVKhRkY0m1xGj89xOHYNl29X0Vw16Z1h+oaV+cZ0Kbj8xrjShtew/eszfuzjnHzz6mGePbudz/WK75uWLNe0LQ3Urff/QKvxS0Z8rxlPw60SuE6outG86z6H/+y7+V02pLMg9lQlr8M6QnGE4V5jOoqNAlvtnEfes488/e8E/ffpLflBd8iN3RaNkgcfCAvx9e81YgFJpZhCW300o0WmUmILDOLY31QmmCGAbam78guux5Yv9eqabe8g+LCcwWS7kHj4I59/o0UPA9BGUkfDuVScwgUMS2D7NhKF6N5C3exgGcQDhATxAYQ9SdY1qGzg/JT5aMZ7V7J9ZxrWif5TxpxG3GKmsjNqG0tKb4aFMfHr6nY5gqqB3wdGNjry3mL3C7iXH1SFJa9IoITstYWaqhW15UXkaK+H2rY3EVzDmualItaN/5OgeiQPLj0aW64Gnqy1LN7Byhx0nZY3TkaUZeVJteOZuZ+os80CLyiju5ekrO9C4wFAFfGVRVcS5SG0DtQ3S+hwdwRt0Z3AbTX2dcTtJX8wogBnbSf0HpdGDYRhrPgtnvGwCWmeGXYW5ctidwL5lZkBmHsICYpvJVb6XLinKoi+7+rt3//kxfOXnEllom0lNlPaoU+hFSaWqjH3c8+xsw0+WVzxztzyy20LYckBCRqVochCm4IksBC0LHwMR2VOSjCGXIYg3RgTHA0Yxi0PpUsUuCgHKpq8Zxz9tTgDEEQRP7jqwFq0UVSXc7SrY+9pJucA+y1Sg3QVM5zEv78hdT7q7k5rDq7t/MWUdqnLop4+JZyv2P1qyf2roHiu6jyN5GVld7DmrPOt6mFGCt2MDSNGrNgIjrkwsfd2Hb9jUUeiDY0yG265hu22wN4b6Fuq7hB4iKmZS7UgLR1ha+gtN/0iRHw2sTzp+sL6ZJ/ru+prrzhEbW1CNFalq8WvDzZ81dM8S7vs7fnpxy6Nmx+N6N/MFHINRrE6cmo7HbsMjIw9vowS4ah7IfxyZhQp8ZG8B+MuPz3mxXvHZ6oTWSZX7SbPFqsT12LIZasJgaK407fPMyS8Cdutx1908tVcVoFL7RYM/dQynhv68ITbS11+P4LYZ20n0MJxoQqvonkoEkFYRvRCHYaxcn4ewZ2/TSDj+91wgnLIOlbEugIPcBIxJGCP5/6oa+I2T5/yovuLX6ud8ZG5ZaE/zSm7v4MARULotCYVXAyMaR2Sfa+5oiEnjgZjNjMx8yCZH4LNhG2ue9yue79e83CzpNjWM//CJRr8ZK7RfahjAGvSusABPYPCjzxXd4R/zZKBSKF24AIDsOQBlpt8bg162qOUS//EZw0XN3Y8t3ZPM+CjQPOlYNAOPF/sZBnrTtwzBstnLtJ/WmabytJVnXQ1UJgqtN9yDjc5V8WTpgqMPll1Xk3qD9czOLDaGbOXzhIXBLzXjWuGXGVsVyLKOtOU9lpVn23rGk4r+QqFDLXPvS4lg0pnnbNVxWgn9+ER+Yrhf4Pyy6DSjoCJxZvYYlfiLq0+5apY8aza0RhBvF3YHwCfmTFKftiVV1Tw8o1IWBxAjKkSIMsZrlWD59WCxvSU6GReeWoITJsCvFH4JYZVIi4QqAzciEFOehweq/hOV5fT/SmW0zq9FCQfRlfueRKmMtYFF5VlWIz9aXXPhdvy5xec8tXc8MXcstMdxFAEcHcO8Go3kMiuRYakHiQK0xhfq8lTShbc5ApB2YEiGfajovGMcHIwa5f+0OgGQaGAUSKuqHNoHEW043GFhiV24MleuDz83hwIfWqOUEmRgTuSUUVrJ71cr8smS/ccN+yeazU8SPBl4crHlRyfXrN3A0ozsYsXG1+zHE7a7Bn9TS5FSZ4aVp2sCaaVobIBqmps/4OqnPrqPht1YMQSD7xyqN0I4mSW8DSuDivL/fiHtKL+GsMwsGy8jzMZTm4Amc9Z07BaO67OWfjBkbUQ8YwHpac/JacdHyw2P6j2tmcgu3p0Tp6yJ74BwOwWP9MBaeUz7c/rsuFs0VCriVMCQGUv/v4sVXyxX3DYLod2eDp3SwQHEhPIIjVs3YipHdW3IRlB12WpSpRlPTAHdgF9n4klE1RHrIsam98r5J0egS51gcvLT3+Ysij/Hk+YTAMqYROMCF+2eZ+2Gf2L1CY/tHb9RfcFCe9Yq3Huvd7lXowQUNaEDo9Z4DE12kMArwTi/yxGEpGdVp/3giL1BjRrlP3Sw0NssZ9LoUTnLxm8tel8L1t1ocl0J2k1XxNYUJJaBXOFOapnT3p6gxiB6Al0vw0Klcq7aFv+jx/RPaq7/nKF/nFj89I6n6y0fL+54XG/nRbOLFVtfc3O9RF1VrH8pVW0yjKeWsMp8ft7IlOHCY62wDceCnhtHS4xaeAC8hqhQg0YPUugMC+iVZlxJFyRrpOLclBFZmxkGx52p+cKtWTlJTayKnDQDmycdXVPTPzHkOkEdefJ4w0W757zev7HtdGwC/nH02tFnh8sRrxQpZ8rmLelBeb1RikZBzImlviNmmPh2ph1wzJozs2dtOloz8h95x3a5xO4d7aVhHRJmN6L2vUQF01Td6FEhkr0Fa1DJEVtHrDTDiWY8FTxDWiR0W0Lz93QAk2ktEmlGJ9rKz4Sxk3DMJLYyBlPQ7QIOq2zktOl51m74fnPDD6pLHpktF6anUbmE/NM1fe/TwaiMI7FkBE2hqBeiFY8l5oeX7CRJBmVYKRe6+TJt+rYS1YfvBEB2bh/IahDYbM4CeXVOeuZGiCZiLfDLKTpITqG9IdUG00d069Abixo9DCPUlchbXdR0F+IA8qOR753c8aTd8qy+kyEOleaJLp8MubNUG0X7skCaI5hR4TsF2RIbQz8asAlt5YEiKfIgN0WPeuZHULFoIGqEHQfQZbXJcMsB4w4Qg6EfHTe9cA5UOs6oxbaR9CDUFlcFqipy3nScVD2t8TPBxbEdY/s9Bl3Uc/p0GO+NOUi7PB86BKZECK48eE4pGhDA0aHBi+h4JGCPd5ds2pbfP3vKn0TN8GiNSormUrQGtA+olCAW0JhGtuCZAl5Gh2MtHYHQQmoSuY6icmS+nANQZdc3OlHZSGWEJt6ZSEh6XvATMCyW1MCaRG2FH3Fte07tnhPdlxqKOABTwoyvAvXRM1RaKMyT9ugkxCvvGiMQluMyipyO5hTeMsPxp8QJZCCVcd8sIby15GVLeLzCrx27Z1aglycHvnkdDCqA7S2mz9iuob5dYsaEGSKxNownhtufGPonmeWv3fJktePPnrxgaQcWepwLLhHNLgis190YmpeKkz/pMDuP3o8sT1tia+meOkKjGE6dSE7Zo9blMEGZC+20k91f+PAyuYXYqPvoVZ1nB6C9It459p1h6Cuu6gXOCpU1gDWR05W0Ddf1QGs9T5qtFCx1uJfzz+Ad9CyVNv2scw7vLEs9kBAS1IYDes0wRQMKrdRrGgmOV0xBoxJLfcsjvaf+2PMHZ8/4D/hL3HyxAFWx+tTQaoW5zigfJDWYzBhybfGnDcO5ZffMSCHwJME6YJxEAF/WtM5onVgUnsjTqrs3Hp6yoo9W2KGjJRRn0FrPyg38sL3mh80VH9lbzvSepQr3ysGRd6cBb7KpPtAoX/goEx7zWhowRQBxTjnNlx5B/rCdwJz369LDd8L6u1qSVy3hbEH/uJLC2amEzX7JAVdeACixBtPK7hErGRE1Xkgjx7VieJwJF4FHyz2Pmh1LO+CKQtBkKZecPhgZRhqyOIBNj9p12BAxbYXKLaEx2N6U0eOp3ySDLtMC90vZWWJdnLSdnPVhiGUGwEyw3qRQo+SoEeijYrSJ3qT5gbYmUbtQxE9EDVnaUw8/jtMDfjksGcsDdFs1bOuatekBONN7nEokmHe5NzmAN5lWigbFqfb8xL0A4MePvscfJ0X3ZIXdG+yuwuzKnMNc/NXk2pIahz8xjGuNP4GwyKRWIi39wHzCu0wKgUUoxkTp7pg4w3vhMHRjtZCETq3flRs4sYLsm9Icp6QrNF3lmHOJgh52BPGVnfnVQqG0DdUsZOLnqOw+JmBa/D4JNXkXpevkJ2XqV6cWH7APywkcF6GmOfeiTItzqEWLqiri41PCaU3/2LF/rPFrxXgq5JipyfOQyES/LSyyIjBpO5nKU0kw+H6dyR/3XJzu+Mn6ilPXzRHAsU1V/cFbTC/9fL3pUXdb0u0d6s6inKW+WVJVjuakJVVWqvwF0goUMYtJ9ADUUqEKfv+BIvThclCiiEHUlPMog0RRZ8EIaFBVpGr9LIJSmUitw4NpAFBAJRX7UPHJ5pRudPS9Y7EYOGvXrMyArw1P7B0uR1BQKYV7RRjlfcygWCiD04k/6655arZsnzb8rfrH/H+3v472FXZwVLcVOmeJBJQqk40VfuXYPxJwTv8oE08CehGwLj5Y1X/VjvkNpk6AMxJFtdbTGCm2TroR06Ks9KHA58rvHrkda9MLDsBsOdN74UBQ+Y35/9QZmEav4xEBq1NJ+AePWIhSVpKWFeDQNGY9HNUEJnWnyUGMyc51qzHYQmDCa8Sqr9pXdgJKqR8C/wfgWXmL3845/7tKqQvg/wT8BPg58K/knK/ffjBQrpJdvnKHMVelZPDFWbKzpHVDahz9k4pxpenPNeOJAETCQnZ/bCaXyTEMkkoYRbSCj5/aTCiITSKvIucne54sd6xtT63D6w4ARZcqtmPN0DlWe0HmqWEkj+XLB9SoJZetHCYmdAHspNaSK0NoDalWhEZafqFVr+X8b7o+uXw/OilxZqjDZFwCbzKDSfhoCEbPD9SrfX6fDV2seNmvuO5bXn52itob3J1it27YrJcs3Ig/MfyZ+nOWauSbMKc0S50xyvObzSf4bPjj713wfPMEu9e0LypclvrAFAWMpxXjqWV4JM4+riOqlS7Al3UAwDzP0LogeIZSM1ka+Yy6iMZM0ZMwI8e5sHphtyz0wCMj36coAA6L3ZDvtQSnnX9fdvVdruZdfeqkNKWjMFGu7XJ1JEPeSmfpSBFqSuWGwsnQR8fz/ZrNULG7a8h7i90cJlHfZF8nEgjA/zjn/J8qpdbA31ZK/b+A/x7w/845/9tKqb8G/DXg33zbgRQK1dSoppFx17Los9FkJ+CRVFn8iSM2mu7C4FcCEQ3LTKwEJTZHAPcPLj/PgIHsSvvHZKgT9WrgtO05q7s3Vs9j1gzR0gdL6q2kAkOW8efCT4hS5FhoukJAFelprRRp4chGzfTefqmKHp0MuUwiGSUbeO385+/TZ5nsqLiYDaCk8xCcYSxoxpgV9oHoQtSSDdux5m7fYK8tdqtormDsFOPgePF0yVm9Fg5AzHxp9ddkfqqVRpP4nr3lrn7OT0+u+OLsjPG0JiwMprciBlOAQ2EhhBt+Jc5etUEcwHsUAl8H+8jX/eLeQK0D9dHs/qSzME3yORXnIaAzs6fR/qDhODmA8l5CGKvu/QzAo+fd/TKuZrmyZdGDjHoonQARNN3EtnyJGGlIujiBAjtHzSS1W1+zDxV3fc2uq2HrhFuiK4XnbyMSyDl/BnxW/n+jlPod4PvAvwz81fKyfx/4G7zDCVA51I++h38sir9+oaWPrLknKRVa2cn9Kgs3fmmbZfM6jPg1U2XhT+YSukBbKx1nybBXo4BpXv5yWHKzbbFXluY6U1+P5H0nZJ1yERDNQgEoAeS2xp+3bH7cMJxq9t/LxDYTl1GGWhTSJowKNSh0oQ+/B9EH6RyUYZj5s5Y6QS5FR0knMmRFjDKUtA8VY7KzSvK0sw3JcRdqnndrPr86wV/XnP1CFTrywHBu6M81V49P+COduTpfcab3YN6Eg/xyZlAYZXiiA717yT9z+if87MljXu4u6M8MerSYW0tuHWHp6M81w7liPIvkNuGqiC51kIfseOGnpMhHY9/WSfX/rO34eHHLk2rLI7ebC8BTnm2UQMFlQlLo0AX/H8rOn4iogv2XjUOX9t6kMvWqAxAZ+TMu44qf948LD4PjxHac2o7HdoNTgVQgxJvUcO2XstOnqhDRHj6LcDjIfX7ZLQV9+sUKszGsPhceDdtJmvy29fGN1ASUUj8B/ing/wc8Kw4C4HMkXXjob34L+C2ApjpleLZi/5FjOFWERSmYTSyyZRw0Vbn0zmUqTBbEcSn9PXeoeXz03cWkWPQANr5m7B31XmG7iO79w7RkhZ2YuiKua8bziu6pZjjPjB97bBs4W/YzdqDrKuENxJKUhPgc53Dls2cDmDIZd2z5/munzxYnYFIyM5ag6LoLmMTX3PQtflvhbg3VXaa+i7iNJ1VSQFW9YT8U2i8OFef3LQa+y5xSLFXg1OxZ1wMv6ihUW05ITlIBBsUKGeOusowCH6EB32Q5C+9DykoKZBxKTrrAvE/swLnbc2r3APMu6wtE15CptZ+RlabUCqRaf0DzoUS7scpJBsrgtZ03ZcWIYZcqtrHh0i9LEc8WZ2DRKs0pQZ8t+1jPcmSTAzhu6QoxraaPlv3o6LoKeyspXX0t0artC735Q7jvYl/bCSilVsD/Ffgf5pzv1NHdyTln9YZVlnP+beC3AZaPf5ivfrNm+6NMuPC41UhVHVBtws8GVk3eXXqgKWqZR48KQsnH39cRcHQsDpRYx7qAIKIRN77l+WYF1xXNS6gvR8z1jjiOrxGSKKNRTUM6X7P/Xsvdjwx3v+lZP9nyX/rez3nkdjyt7rgOS7ah5nfuPuKqW/Dyek3c2QIeKlVCVVIFU4ZiTAZ7xIIz7XiT9oACZWRgZgwGoy37IA07XboECcXlsOCz3QnPX5xQf2ZpP1csPx+xGy+sukWf0d5p9uual37F91xLeq339/XMKc1CRZ7YOx41O3659MTaFbFOwXeEVph6wjJDldDu3XUA2f2VPBtJ0rRpmpAKnElc1Du+VwvIZ6klmtul6jC5h+zslQrSnz8+ftaMUJ4VQ6UiaPAkGg7dhWNuwIgqu3vLS7/is+6Una/og2XlRpZuQZeqWbkoIpvPmCw+GcJbEIJ7X7HdNaTrmvWniuYqs/7lgPYJ5SP+RHgP3mRfywkopRziAP6POef/W/nxF0qpj3POnymlPgaev+s4ycH2Bxn1wz3PTndctHsa4+/h7kMpiIQk+fkYDb239KPDe1P0+LT0zqbd8W0OobxGyIyOMd2ptHrUPI219TX7XY3bSsg80ZQzz7JPXQyDWq/h/ITu+0s2PzBsf5T43g8v+Y2zF/zzJ3/IWvesdcfGtWxSg1aZz+pTAK7tgnHviMbM+ou5VP4pTEJ66odPBYTMPUVlbWSB+GgwIbPxzRxCapUIyfCyW3G9WaAuK+pLRXuVsFuPHsO9ToZKkINmnyr67L41YcxKlXTMpFmm+57lw/c3s8CpwlWriMGQonRRKDJkaCmgTn/vVMLpMBflEloW85HJs3DfAcQs91pSsATZigR5YoZJQ/kdzM/S9Dkb5VmYkUoHRm0YOMwG+GTQuFm6bKrpvMtiFoene4XbZapdwu4O07LZHIROHrKv0x1QwL8H/E7O+X919Kv/B/CvA/92+f5/f9exkoP48cCf/+gF31/c8qTazMq4wFwRnUKjTWjYhYq7seHGtHTGsU9a+OCyYeZ1f4dNDLCTE3gITz9Ey87XpJ0UzuqNyHflQXYPpSXnVM4WzcIF8XTB7qll/3HG/WDHP/vkF/zm4lP+yfqXLHSgUZlNumOXLZrEud0X2m64US0D1UFhWAPmaCy2jMbOn4H7GdG0Q6akGIJh7w+RAEDIMrk4bGraa019k6lvIroLqEL7Pi/EBERpjY7ZlvbWl8DAvoeZEkpbLa2+SSAEOAh15qM21+wIDiSmkwOYwv/kS3Q4ymJVSchgMAd+yFfv9bTYpTV4H/f/qk2OQGoIUhuYagTHY9dGxdkByNh1wKnAQo80JjAmO9OuTdGozhqbJ+bmd1dgphQheYMblUiZ7UVQVVqs+ttzAsBfAf414O8qpf5O+dn/DFn8/2el1L8B/Anwr7zzSDZxdr7j11aXPKk2XNjdvXn86WJMs9dDcuxjxbapeVmv2Iaa59WKbnQHyip/HBU8fAGmfnFVgDWLIn0lMGFRhr31DVtfoXojopYRsjPo1QJjtABblIKmJtcV3Y/P2D+x3Px5sL+25V/44R/zL6z/gJ+4lzwzHqMUGoXTkTUR6s84M5KTru3AZ+0JnzdrfDSMo5Xd7aiw9RoNFq/7u5wVIRhS0mwQMo/OOzLgo2Fz16I3FrsTgk5yJi0c4AitYTi3DGea2GZUFQtDjWPIhpg9UeVvrC4gYBpNSIYYdVGIQsRoO4VTivpagBR+bUlBEzRoE9H6UPiLQZMHGZYxOy0CM+OkWJxJtdSSQmPpRsetb7gNC9ZaAFFfVkZuLv4VGuMeh8kStUUUDb4s/EObsSHw1G6oVGSzbLgLDVf1cuahmEBqQj4iaalOllAwJdMY+rFZLfgE7SKxtviVQgeDHpekShNazfZ7Br9682f5Ot2B/5g31xz/K1/mWEpB7QKtGWm0VGCPq9mxFLR0GQOe2zYFBFObwBBEJy9EzYCb88J3bVwTdtypVAo/8h5TV2BMFl849bMpAz2rWuDLTSU/15rUyjx/99jSPdaERyPfP93w08XLAisdcccIOyUwhrXy9GbPY7fhtm7xR+3Izjh8lMURoy6dgLcvvjlyToqYQSlToh1xDj6KirFQc8k4bnKasLTC0rPQjCuNXyiSyyiTCaVYNqILQ3CeZwe+jqWCquuzoNxi1EWFOhdVauFWcPtMrMDuFUFpYmXIrkCtkyJHJQ6gM6K3sBHqMTNSOkyKUEAzfmEYayujttHNEmjVVzj/Vx0BRxDzqMrEX4kODJlU5gHWpuPC7u7VnrTKM0blOB2dMQHCiQ75gGSc8AxWJ4yLhDrjV7rImQnfoV8q+sfSSn+TfTCIwVh6oJMdw1zNHKodnECjPCvTszI9Q3IszciNb/nEnHK1W9ApKfJQNPles5LaaSWRQG3CrCe/jzXbWLPxDV1wMjhSJ/yJYvuxwS9azNDMk37JikhFrBW772f8ReAnP37BP/PoF0LRpfesVT7qtE8V9kylEks18tTeMVSORgtybRdqtr5mFyq2Y8V+qAhR4/0RmWKx44U//SAiwIMUhfp6NJZcmH0ps+WhRViMWyUeTlOouiCsMrmVwZxdqNjEhk1qGPQwD8l8XfNkdtnyuT/l890J8a7C7sF26cAo1WsWVuN2lmQd/kQxDI7YGlIlADDlFdWdkqr4rfAzmkGGu5JVxEbRnwlb8w7LmOHyZMFVs+TE9iz0CHw1MNScs6tDRABAYiZ9mYRUnEos9IDOiR9XL+iz45lrX2MCFhCQrIe9ruiTYxtrQuEMPHYEViXO6o7+xHKjM9u0wOw1u70REN0qUT3dc77s3/gZPggnkKNis2+4GhesjAzuHEcCr5ohIfp5kUYFjM4zx13fyM6dSrEkyTxmeaOHd69QijKbKExBkwPYhHqOAqgjfq1QwRAWCl3ms7OGbKWFleqMv4iYtUiWAyIomi1VHmcoqgZ8znjgJlXcpJbLsGKfKnwSUc6lvU82KWIntkQE6iCHkKWTIGnD65FPVtPcvBH5rSKImbWoC4FEN7NDcxBakc5GyfG3vuYuNFzGFU/MjjoH3NdMCSKZPmeu4oJfjRdc71rsncF1CdMVghGAEDHbEZUyi6Vh7AX2LezPojqkR6huRW2pvk3UVyN6FN2K7DSptmjv8EuNXwoAbduLg50WoPAJf9Vxn+kzyTGknKgLwOr+MQ2ZCtnEAJLW90RHJ6fis51/7lQ8Os+iUViwIehIZQLreiBlxdWFJaw0YdTzSPtHZ3ec13v+9hvO+4NwAkRFf1fzsl9x5jpO7R6Xw4OUVpNNDqLWHpcDjx3UOogkVpIi1tA7WTDHifMEKT5q54Ykgxd3QSrpd2UX3odqHiOtFl4guXViDKXeMEVok4qNTbSLkbYeaYzHZ8NNXHCT2pIf+vl277Oiz4bP4wmXYcUX/pRtFAViqyOWWCrYEavj7IzClBogRc0MsrizOrQKj52dysylxKKqM5GXxDaTqrmQXfJnyG4CmGRSUmzHmiu/5EU44fv2mqUKrL/qvS6WcmaTDJdxxR/tHrO/aVlcK9w2SfclxtLyzZjbHXpvWaZMvXa4vS3pCkV3AJrrSH09Ym861O1WFK1SRlnBbJjtmnBa45c12Wq6Tro+Q7KzNuLXsSk1EBRgZsTQZCEGiUfaioZMVIlGe0w+pJ/3B4P0TBVmSPP8gEG6O0IqYqRwriTiOK06ahNYOD9POy7dyGnd8VFzx8oMb6zQfxBOQHuoP3H8bPGk9EQNP24uqbWXYZ63eOgpKpggn6kSkcyUFbuholcOP7WKphRMUai2s+x0o9CE7YOTHnAyDFEiipg0WkFVCWlFqDQ5FRbBQl55jFwLXroXvzs849PFKT9bPOEXp4945u74M/XnVAVt9nk45Squ+Pvb73M5LPhiv57zv9O6pzGBk6ojlK6BKdNuWmcRa0ly7nmufRwW+H07errn+XIFWhzAPGBSXpqLY0NnCJo0wNVugTPnnLqOM7MnckmttjRK0agvr5zrc2KXE78Mj/id/nv87uVT7AtHc5mxu8IcZQwQJYobRtTocaPHXjmqy5pUW7KTKE+PEbMdUDvRW0xdDzFK0bboR5iY0H1Le+HwC0PX2VkG7Zu2CUwUUTiYncyDhK2Tc7jXnoxCRork/U2WDWUf63lS0Kg8RwRTbazSkYWVtKYxHqcEFBWy4SYs3ni+H4YTCFDdKDY3Nc/bFWvXC1lDGdFMqLcy4pgJa6thkQ1LO7ByFc5EvLF4nZlFPKc1cUwhVbAHk5CmjyKpFY/ah7ZQT1kbj3jqcpkPyPL6qBl7UQ32SZiEdkOFVYmrZgkw1x0+Gc659Et+5/qZ4L23EpsrnRlWlrby9yrBMUmRaCLCeLUuN8OH34mNOFwHYW1W97DAeQIiwaFFOFg2Q83zfs2LZs1ad+zNDsg0X2EH9WSGDJdxxfNxzd22xe2VDGUVBGZ2VijlVIRhJKckzsAaTD+gnRV2qZwldehEXTr3A2n0TFySuUyj5sqhtMLuE7aXush0Tb9Jm1KCifU3Hk1bTuPB72OTc5hmExYF0DTkErkk7qE4pUgIFukWtMbPLfa7UDN+6OIjpk+c/35Ae8t2e8bfedZy+72WJ+2WX1u8ZGUG1qafIZwPHqM4glp7VmagsxWVjQwmMc1Uz6ak6q2LbPQYpIbQl57thB2YquogUNOHurY5F0GN3hEGi35Rie7hTpFcxVi1/OePVqg28rfOfzhTWV3dLfCdw31RYTrFcsM8I7F51HKzSHxxMWBMuud4dIHMOidOMSHtMbI6LPAv08pX+WG/kWUwiazwO8d1XvIzpBNz2y5Y6oFHeo/B0yj9XqPFkYzPicuoeJGW/J3dj/j7Nx+TXjS4rWwG/qQiLCwqgx6TUM3fdqIefbcV0Zjboz5+SqQiX/egnkSOki51PSiF7QJmsKig8KlU8r+RiYijU8p63tlT1oxKY3KeJwult/9+71mVlvUZO5pC/36rWvZR+hnTBOGYhPRkihxvVaKPjr2vuNwtPnzKcRUT9dXAYikUYX2q+aU752Ylu+Pjasejasup6aQGMDPcPOwQphHLWHZnvITKKipyGaZQ89chIpiUhl8dPX2bTXl5jDLjL0MbCrdl5sQnCwT2ZtAHOaytwXaa+lJhO9FLnGclUMS9Ycg10WVGm1BWAENNO86z8DkLrEV2my9xziqjSvfgnQ4jA0ETvWbb1XzRrbEq8cv6Ed5aanXNUicW5HuOYCoaTlRjKWf6nOgzvEhSX3g5rtiNlbQqHYxLRXQWlTNZKcyYsb2jMRqzsah+gCFBSvcIY2d7A6U8QI5RiEqi0NIT5X5Pi+abtoiSnTqDzpLPH7MOfxn0pSEfoMl5mIuEQ7IEJTwC2yJj3wc7b2KDt4ze0v+poBwPAfeLl5zuT6m2S/YvLHf9kuuzlr87Op6ebPnJ+oqP69s5KtAFB3+M/EplyGJiXBVZLIvuhL2XpEhtkkUwUUzPf8s7e/Cv2nSxY9Sk0aAGkQV3O6hvDudV34rQZ1i4MhAEphOlpOY6YYaM28kDnBVUWyl82a0hVUJ+ElaJ1GRiFaisZ1WPEsXoTBhNWQzvcf4qC4Q2Z6kNwJsdQUkxVFDkztIlxa/sKdux5sR2XNUraOAjc0fUnuVRtJSOnIBgATK7pLlKDT8fH/Or8RG/2p2x6WpQMhmatfAryPeM3WvsTrOuNfWVpd33cqpvUpF6m6UstOYxo4u4rI8a/xUkvt5kx+3CmDV9yvjCT+m4z1Qlr3//954mGA0yyFSnmj45hiRjxC/3S253LcOuIgcl06lBowZFtVPCZv0G+yCcQI6JfLdB50ybEu62wfiG/kyz7U74+eOWq8cLfv2i4bzqeFbfUeswj3lOHtZnw21o+aw/5bP9CZurJWpjaV7oed7GZ01cHJ77LyMsOsGM7/3slddMApkq5aP/B6OFY3DqTGgvdGNmzOgg4awq69gOghcWB6DQnllwdYLGOhNxxpBSKhJb79jUjzoigrEVLL1K73AE0+8SMGr2u4YQDL9XP+PKL/HZ8Nxd89RuuDBbKhL1Uf3GY/BZs0kNN2nB5/6UP+yf8sWwZjPU4kiXkbFRjBnUIqBdoqoCQ+foto5sLc2qwu5OsVcWVRSss+dLOYIcE3oMIlPXa/rRsfENQ3IFe/LNEKeIhJjh03SGT5aEotZe5kZMP/MHfFkzZBo9zlFEU4rhd0PDy5sV6bKmutbSvi5wa7KApj58tuGcSfs9KgRUCFSblhXnVJuKbC375LhTSz6tPH0rAxatGVmZ4d6MwZAsd6Hlalhws29RG1sAJPI2yUBsERz1Ua/9/U7x/pzBqz+//0MO8NckbMRZZfHG02YxOYqYD6KalA5mAKMzplfFmQjgJTmJduZupzqapnubM3ulGCpeSDxOJs/DSm/+8AV/D8TeMCT4YivQ5koHqVynmt65WXhzskla6zKsuIorPhnO+WV3znW/EPm1rGa6cOsij9c7Wus5rTsu+yUvt0t2+1Oy1qw+rdA+om/qQkYaH07dHrqxWdII5QuQaFD40dJHe2Dr+QZqhAnREtzGhs/GU3ahpotOhFiqHR9XN3hjuDBbQah+yVmMQ2owzEXz3eiIW0d9rWleiDgqUIq/vDPl+zCcAMwy4coH1GZL5QPucoEeT3Ebx25X8YW/4OXJiquzBZWJLJwMYkwcepPE9+dXJ/jbmtWvNNVtZvlFEORYpYiNFqBMer/cf1rkYRpfTnp+xqRdp+4Re7y6404eWMG9/PV4PSaryMoUdiCIlZ4Zk5NhnqpTGUJv6YCro45EioeZ+ddsqvYrysJHTjarEiKrwmp05AgeemiStBbzKO93rZfs+oq7seZP6gtOqp6n9Za6IB4n66Lsilfjkq2vuR0btkMtwitFJHOxLOxOTcefWb/g1Hac2x3XYcmL0zX/MT9ls15R3VhWzYLV7hStNbmb5vm1hFpTnaAfJLo80p3MMUIImE1HddfSXFlubms+PT3lar2cNRW/jvlsuY0LfjFc8Kv9Gb/z/Bn9viJtHVSJ5mTg15+85CfLK/6p1Z+wNh0n+s1IvjeZK5wDE63ZGCx6b6huFe2lpJfZKEGyFnKeb51U5BuzSYgUyH2Psga7C7i9xe0Vw14TjePWtVgb2VlH48KMxJskvv3eYXa6QFAFhpqcQmV9GAJ606J57ZQmIYeS+6cpilBkkw54/skRvMXeuFkrQR0mc2BTSlYow6aJvtlfBUFC9uPRfMSEEzg63r03LY5p5iFAzjUjYePULlT5HY5g+oxJJvVGZdkaCellvNtSmTDDZQHGJO3Wna/pgmM3Ogbv5muptWDfaxtYuYFzKyQfF2Y7z4g8Wu7pTxz+xDHsDItVgw6iWI01wkXo7EHKbLtDjZ60T/fl51MGH9BdwO4yZqe52zdsQsPa9F8LNegLbdhVWPJ5f8KvNmfsXyyxd4b6ThHbzNAZfuHOAPhhcwWI5NiXjQZAph6dEiCZ1YlsBeUZnZDoTjMTedpE/tQ4ASjwNyU3LiZREBpSGQzRhKjoY3vQnC8jtgA5aPKgsbcy9ut2GbefYKgCl9XeoMMRyOZNp/FKBOC9TOWlULblDKkg9HJWEPTMYvxl7mnWZdKteO5kJOyfhpWSK6w6Vlp5ahCijH7Uhxt7/Dnm9IDD9wlXMDmE8gFUgTzPEUHJ/Wdn8Op6UCUayJnsNTEp9kkxDBZjMpdugUKATZPFAqySRS/tzFTamZLOiE6CK9qKayMiHhOP31p3/IWzM6xO/PGzBWRNc9Pilg49RMLSkmqNXxrRk+gzzacb9KaT0ejRk/04bzB5t8fcNCy/aNh/5NgvF/zqyRmazLndzQW4L2MJzT7VXIUlv7t5xu++eMb+sxWnv2doLhPtZSAsNN2F5nZ3xj/4uOWi3vHTRcXadLPIyPuaKfnmQovTfLzasTlv6YearDRmUDOZbtZ8exyD34ophaoq+VqvSKsF43nNuBTOQSi59Kikwh0yWWvh1gN5sPxBcimWwlpYGFlk1btDo8mmCGCa4EtRiCpIR7WEORWQ99VeaM1VzG9N0V//3Ic2/4QVSKZEBK58WQTUU/r3maMcoRxj/lxT0fkYVPSmE1JHgKF8qBOQQb2CSMoqHyjMJm4DhYzyksiFbusYQZmKg5oiqMkBQPEzSc1UaEPJz6e+/dQWW5qBteuJy0RYaoZTTWhFtah7JANQfg1mkBbtqTuhum6o+oG82xOnTkJO5HFE73uqm5Hm0hIWhl9+fEbKio/qW1bFCb2vI5gigF+N5zwf1vzB5RN2XyxZfGJYfhZprgLVZUdsHaYT6vQu1/zOxUfsQ8W53XFhtpyZ/Tsdwf0oRQhKH7sNP1ld4Z8YPrMn7Fc1ypcNKcqzogfQ8c0P/QfmBIqAaF2RVgviacNwZvFLERaRkFWUeGQwRh2grpTneZJ9UQIhDg3EVkvbqUjnvKsUcEgBNCEITDiF49C7vG7qs5dJNlWonXXxvO/jCOZzOaoniCMokUFxBLlwDZKV9LqTOnwWTXnD6RhvWfwPndO0aO+lAoWI46h2Mb9WC9hq+ruMOIIUj27Eax+w/PPoV6rcvJgEvTe+QqmtSVQKVmbgtOpRTSQsDMOJlu6A1uy+nwnLhLoYSJ3kxtobFnWNe76UYmDXk2XAgjyK3L293tNeNsTGcHuzwJrEy/VKyEWVf5BV6FWbioD7VPF8WPOL7Tl3L1Y0n1uWn2SWn/TY6z3q+g7d1JjNAr86RUXD5UdrUoafLs8xtRCafplCoSaxNjI09xvLz6l14HG75fPTE4ZQ2uOjJYyGsHVCh/8G+3CcgDbopkafnZJPlvQ/WDOeGPbPtKj1TJNt02541Ba/95ilkldXmXEt473JmkOLcCHTftMiSWXwY7IJNBSjJgRDDEZSh5JOyIuO2mqFLViPCjMIwEWFQ3vmXXZEFUhW4tySVYcIoJLPkq2AidSUy2ek8KjKzq2KR1K80vk4OtdXz/94mOp4kU/pGEc/M0JtNlF9OxfnzsRxqP9qt+RtHZiJ2i0ETR8E8LKJDbX2+Gznvvo0Y69sIlWZsFRFqDXDT/c8Wnf82fMXXPZLrvuW6+ExsTEsPllhU0JttqU2EEXluuvR13csPl2gY033pOZlZ/md9iM2ywbfmjLvf6Cgn9iIQfgPfDZsY8PnwylX44L//PPvsb9asPp9x/KzxOpXI/bFBrXdk/cdeI/uB9Y/d7htQ1jW3Dx1/H/Mr/MXLlb85sryQ3fFQg9vjQgmR6FVosqRxko35nvuhs2y4fZsMY8e34WG63HBJ9tTtn39xmN+ME5AGSP6gm1NWtaMa1HnDYuyEIoDuLexvK3QZuTvUBT4a/kTNxVK8oOHmNqAKelD3eC4hXZ8AtPPYklTSiQgdYH3zAfy4UuVcHyOBmwhzphSmKO1O5/G1L6bFm+afnh8/MnjvXL+83EzD6ZI0+8mB+AidR1EuKM6dAB6L8NWOdsDxfeXRF3GJACbkN4Oqc1aJhxjnYlN5mTZ82S55YeLa+GEMJGXywtCY8iVLoNI995MxG2HEbMbcBtLtbHEVnPVLWitZ+16AeRoT6OCsA5lyoSgYhsb9rHiJiz4dH/Ky27J/nKBu7I0LzPNVcTd9KhuIHt/KE6mjL7dUytoXliS0by8WfFp3fO42nJhdkX96PWIYKY/Kw7AkIpz1JzonkZ5zsyOi7TFZzvzP7x0Mpi2cQ1/7w3X9MNwAgoRHzlZEy5WDI8b9k8MYVlkwkqO/F7x9cTQC9AK2m6alweZlY/1/Z3/wFMvaYBEAEdMxq9W3ycrJJYqCCLLeMG/63DgxnujTal8Ae4YL4g5daSzkKw4gnzsCO4dYw5npJhXqsKTI5ne57VCX1ZF92BSL3rDglUZDCiTsVVk2Q48XW05qzseVztAwFcv+hUb33C1b2datNkZvBaZPHApsoxID0F4+CcauVjgsX2hestRzeCmbOTaVDbSGE+j/awd8S7LMUptYNvjKkvzsiIrzcuzE3Z9xe3Y8GyxYWlG1q4vl1hEW4dkuBqW3A0Nl9sF++sWc2s5/bmmuU6c/FGHve1lnLnrRKAGBOo8jqirG+y+57y21Juay2rBH4SnhKypHweeuTueuZuZANUgasT3IoBZ5yDhEHm4aUZwrXtS1vTGcZcaTkvKcOPbN16PD8IJKBSqcuRFQ1hV+JUhNqUqbr7UpnLPpnw5uUOhbtYqmHd8RVRKqt5woDMvkcAbSUvnRSyFSCnC8O7FX/5WZcm3dciyChQkowUrUIAr2eS36hPe/6D5sOCnWgXH39X9FCWXukqpnzz0HgI0kdrA1ElpTGBtB55UG+FizAajMpWJkosGwU7INRQ+gmOexPnSZCW1C8CYPCsC2SPwFzAz7IxJ0rIZgTlITeTqbkFMGqsTz/drrvYt7lpT3YHZB5GKS4IfkBumJOp0lmyNaEVmcd5qZ+ltzXOV6YOlNhJux6zxUUhbB2/Z7Wtib9G3luZaU21g+XmkuoviAPb9zGcAyLQjIDLriew9Zj/itha30Ywbx8vtkhcnK5yKrE1HUr5EagHQVMQ5AgDuIWUnBOE0mDRmwy7VbGLLbWwLMe+Hng5ohWoa4roRzbkTSQNifcQ++6XK7WJzVHmczk/in1EJaWUJF7XO8+JPUUuncnIADyACZ0sSjqtY0oF3tQgn5zGhCYd0IPXQaoYHT52Cadbg1UX62uV49Rxf1UR5oEahQKKBNwUCujgArYlOineVCSztwA+qq5mqu1aBVo/0wbELMt0WkyIUhqecFTHmWRGI4nCVkg6DMYnKBmoTZjj4NHE3ZjPr7BE0OgiMeioQd5ctLzvHfnB0W5HfOv1MsXwe0bd7cteLTNzhQ6GchbomN45Ul/vvwW0UITn2g6Zra1E8VllqHoXDUA/Sqm46qK8zzU3CbSLN53v0fkDdbcnh0I1Ai14kKUtqkhOEgN4OuNbRXFnGc81m1fLF+QlOJS7sjlgch9bpHtfAFA28alOqMmbDPtdcxhXXYcln4ynP+xVb/6E7AaXJbU1Y16LTtyy9cvvVHYAcd8r77+fxqnDS56TwSGFLF4KRjDiIHPWb04C3WQk01Cs7sSrx8Lz4fUaFjN37eW4g64pkD6nLpD703nDWo/e8d8leqWe8dm5vOpwWxyZ1cstO13y+OwHg+/U1a92z0AMLI5Nt5/Weyogs+iTkMvHnTxwNPmpCPGS8VicWledxu+NJs+Wx27Ay/cz43OdKFHqCnQlQdADVT9fRkKwhuZrVDuwuc/rHI/XVANd3wjGQMjJgAbpyqLaB9ZJw0jCeONGEtKCDwnSggiXv5RyzV7heRsOru4zpod5ETJ+pNl7k6TuP2nVFl7IsWG1QzjHJq8uFVkVf0RLPF4SVo0wEk4MWMFWs2KdqblFOXYpG+ZIKvO4AJjm0Pjt+4R9xG1t+tn/K9bjgRbfitmsYwwc+SowCnCU2hliVSTKbv54DuHf8fO9/cwLlS5iqJPzP5sAlQD6KAt7n3B+y48U//f80LxByUYdJ6D5ImAgYb4m+vP4r+J/pfR/sTGR1f/G/wwGAOKysFMoDWhN7y3aouLUt134JjnnXdiqyNONMnw3Mrb6UFX0U5uQhWpnjT7qAhBKrauCk6jhze9a6nwd5IlMqYEWB5wiZaXsZvpqHskCEN7aR5vMd+m5P2u/nKGCSucc5lHPkuiI2onCUXBF0TRJdqFjqJEnGwu0eqlvZ9e0+4u5EqEVvB9ToZfHfC/+LHmXlBM1otDiCSWDXGPxKOA9TpWZ9yZCFyGZiLIbDrIArFGXuKAqY5M2nFOAuNbz0ay79kk/2Z9wNDTddQ99VMlL/BvtAnIAmNZVITjVqrgV8w1wPYklJezsURRqthVPPZLCZWSj0oTRgPt/7/y/HkVxcZRkK0j4fHECewtdUfpfQvUcNEb2Twk22BrWq5W8mB/A+9YAHzuvVDsr9WgCvOYO3+lkFOiuZvwmWa7ViuxOeh/Nmz68tXs5qzo+q7bz7u1dy+0lvrytkqiFrNBmrIye259zuubBbLuxWUoGsRZI7LNj4mv5oJ1MJ3D7jtgm3DehRxFP0fpjJR9I4kgeZBVBagTFSC2gaWLSEkwZ/YhkndWhbrkMs6V3p9ridcD20V5HmxYDZDui7vSz80R9gFNZAZclNBZXQz8eFyH/FRhcIryIWhzOcSsQ7PM74x57Facd5vefMdVxYAQ89sXec6Y5GRRolbsGVBlDM0GfNgMicfxrO+XQ85z+9/SGX/ZLPb07w3hB7K3wab9lRPhAnQNG/o/DbfUvvkw+wXjW11kpLXIZ4kqQA5m2rYjrWwz+epgelQ1A49EshUE1OYAwSAfgAIcquUwoYWR1kuPKXiAbuRU2vtgfTFJE80N58h80OJAp3QxwMHnixWxKyZmlGLqodp7ajVgGUhLATjdoU+PfZ4pNlyAM+CSrQIESqCz1yavb3euTzrHys6aOTcDYfoikdMqaP2J1HDR697YWGrO+FfWjSiBRe+ZlrUBnZmbNR85DW612XV66Pojj58sIoE4kSUmo55rKVxX/SEBtLaA1hqYmVIjTq0O0pIDC/hLjI+JOIW3rW7cB51XFiZdz4zOxZqpGFCjQqzTRuRilizmglNG1kwS3cxAXP/ZrLXtSJx8GSfCHUierAHfGAfRhOALnIyap5au4bSAJeeQN1yNVL2AeAyqQoaL+IMP/k4775Q9fu1ZM7WnTaZ8wg2n4qZlRMMiqcM8pHVEjgg4SRIQqrrjHgrBTpps//JQhvJgeQi3z5PQRglIeAAjB6Y1TwFjs4TgU7TRo016zY95WEo2vhgHzsZIpwqYfS1joSkCmefWLFOch/J9Ho037uhSdKFBBbLr2047rRicZAUMUBZOw+oG/30gG425LHUcL/Y5FYrQSFWpwA1kpebvVhOKsUXh/sAltpM/uFwrUG5R3aaEhaHEBddv7Ha8LC0T11jEtFWIp+Q6qkLS0tzVxasplcJ1QdWa4HHq92fLS848ftJU/dHT90l5yogVPtWWuFU+YevXtUmZSFqi2iuIlLPhnO+fn2Ec/vVvRdmVwsRevXnNor9k2oEhvgbwGf5Jz/JaXUT4G/DjwC/jbwr+Wc38nWMJNwFG//rdgU/qbjxVA07Uq7bGqLcQy4efUYb7EJuqzHWHZ+YbMhJQhlyi3IWGsutYC5Sl9SivdKAY5D+WlSzGZUUe5FSVsvDWYW5lSv/P2XKbfMtZSg0DkTe8OoKq6qBetqYGlGHrvtLAxzHAUAOKKw7x5h8ieWnKkPDuIcxmzYpIZrv+ByWLLpa/reYfYyG2B6MGNC+8QEwkFLAU4pRS4dH6VKGuAsWIuyFipHrizZ3J+we80BlOAsOYhREVoIK4OODlO5+TblRUNaNuw/bhnXmv1H5bWLTGwT2eUiJgvKCA+mUhnrIlUVOVt0PG63PK52nNsdZ2bHiRpYFwfQKING31N80jmTVCIhLdqrsOJqXPKyWzIMjjSa93YA8M1EAv8D4HeAk/Lvfwf4X+ec/7pS6n8H/BvA//atR8jITlkcwYx+/QZO7lWbW3hT27ikBcJx/xXedE4KOaollAggRtRQCn85FyIMaRFNHHkzLZDWpRvwHh7gyJkdgD+AzZgqUtUepaTrMebSlptrHe9Z8HzApiiKrNCDJinoesdmrNnVUuY2ZcT1VSot+Z3YQfzzdWScR+OzPahAjQ3d4Ii9pRqEnckOGT0KQcjETiz9f01ORqYHQYqAtjiAykkE0FTkypKcLqPbb7neihl5GoMi1BrdGCpnCz1bJi9q4klFd6EZTxX940xcJHIb0bUoOBmb7rFEK5WpbKBxgbOm43wuinYs9cBCB0EVKoMrTuD+ec2oMEZENOfWN+yGShyAFyg78F73+utKk/8A+K8D/0vgf1SUiv/LwL9aXvLvA/9z3ukEMroP2C5hBl0qtF+xOv7g8Q8gk7mv/9DFedsufK+YVlpthcuNJLtKbGA41UUBtsUMMgqtx5IG7Es1ObxhQq3ku/eq9/mV+zhRloXDZ0pJyTSxlh1mUXusicSkucsNY1Jkf5TT8tWbLhO4SHmF0grfObZDzW3d0CfHQgvajxLuf1nrk2OXai79ks+6Uz7frBmuG+ytpX2hiopykFrAWK6jVlLxB5TSh1y9rmDRktYtfl2TaoNfG0JdIOlL2bXTDCVnvtiZI585czzIUFp2ZiZDDac1/UVF91Qxnmbi0xFTRVzRqVAqz7Mpx0xQrQu0zrNyw6zBOb0uFiDV2yyR6EvE9Pl4wmW/ZN9XRYj3yzn6rxsJ/G+A/ynMgjSPgJuc8/SU/wr4/kN/qJT6LeC3ABqzghDRQyrcezBxpH3taOCoFqCOgD1zu/WBMPDNx+Jem2rmcUMw/soqwkLyTJWshKyjwfQSulr54LJ75VHySpCQtnzNqMPylY8ezDmViUqoyyb+ApVlziBzEFgtOgnGCN/CMRz/G+m6lmuZo2IMRlqAR2PAX8WEhVczJMddaKUW0FWYrRF+iO0RP4Q/IgspYf/UaoUyM7BoiadL/EXDcG4JjWJciUz3jEidYNmqPBtw76Gbn5vpGZoFbBQYTTJS/Es1pEYiMesizkb00aKfvuspHdCSAk1QZ02eiUoTSjoAvFnzUejbNX2q6GLFWIRrZz6IL2Ff2Qkopf4l4HnO+W8rpf7ql/37nPNvA78NcGqfZLXvcdsFbu/wC4VeKpLOzFpkX+XJnRxAaffoILp1ZKngHyi+Dx7/tYGaVyOALItwmhmYHg4BNxVZr6joHyu0FyYj20k/u31pcZsad6lROyXV7IOooBQSQ8YMmViJsm6aIcEHeLL28jkmZeFYBFfjUPreWWF0QmeFNQltMknlCY38tW2qp6gIBC24/8IF8Kq45kN2zOAzQV5j1vcKgp91J3xxuya8bFh+pqmvM6tPA3YXsLedOIGYyEbLEWz5bLo4gMrhPzql+6hh831D91EWRuNFONxfL4tG9xoVZQL0uEVIASZpjxQie+lIKF/qOkaivlgpYp1JVaJyEWsjpmhaPGSTc9DqMAyUUPhsBe6bKjQDjUoYIk5xLyWIWQqDu+zYpYpb39B7kW1/I8z9LfZ1IoG/Avw3lFL/NaBBagL/LnCmlLIlGvgB8Mk7jzStcx+FHWZQwv5j76U/729TuF48uB5BjwI3NSPzbiuy3JL3ZVMijzncf6UwMbXaUmm5TOnF9HIDWeWC+MvEpjieqAi9ElALVmjOQostXQMVAkUxFO0jZkiiXWAgtmWgaJ5LkMWvouTGKh4WJFqhRkUMBh8MVIeoQOl0SHW+qRTra1oqWO7jlCFmzTZKQfD5bkV/V1NfGZqrTH2bcHcjeigyZRMwx1npiqQMVmoE2VnyoqZ/UrN7ptl/PxM+HmhXA8tGatQharqhkl76xgkRR5KldjwLIjoSWViqthG7C6KElDI5G7SXdqWQeKgyK/Hmi3yICMQJHI+x99nhUuAuNYIOTAPoSE2mUYWw5ZVIy2db2K/0/Q3rS9hXdgI5538L+LcASiTwP8k5/3eUUv8X4L+JdAj+dXijDuKrB0T5iB7E4+qgSEFBle+Pzr6nHUcAppfFY3vx6FNIl+2BwkvZo+LgfE4c3vjIAehACc3VoT03V+jTzHSkypShHjRmpGAADKYXlhkmQQxVWIV9xPTCqYgSKW1dKMcm/IHpy2ca8sEBZCURcK8Jo5k1FI1OEgmUttSH4gAmmxzB9P8RxT5V3PgFN5sWc21pXkJ7GalvPPa2P7RVtRRUszOHWkeQ/n1qHXFVsX9i2H+kiD/o+PWPX/KD5Q2nrmNMljvfCKR2aHjJithZed6ymmdAtAe7z7h9pr6N2I3HbAZUL45EZQF+mTGjvRYU6jHz1BvsEAXke4CqibV5k1pMgQg75QGJCDT6sPPArJ6U8oGy7avkz98GTuDfBP66Uup/AfxnwL/3rj/IlSN874LUWrKVB970zNx72Qp76oSiexBOfISLnzy52ct4b3WbReVnl7B9mp1Acgq/kJ5viMUhwD3egnkM92j3P+YnOMz+Z4F/FoUhyiwCGaLLJK9Kb1mhg0WHFmc1+oaZclyNAQM0VxbbGXQQpFmy8kDqODlIQSTmgisYRxmsCY1Idvd1xdgMVAjfn7WJ0SXJfQ0ykvs16wKZ4vTKBGBr/UwMOrX+3sem1KBPgn3/ZX/OL3bnhBctixea5ReJ5uWI2ch8PkgHJTeOXDvG04rkdHluMiRhkhqXmt33FcNHnu89vuXX1i/5qL5joUd8NpzZitZ4rquWmDRbV9NHaQfoIKme6aG+kwiguhklDSkcAfJs5DINaHA7Q6oVIWpMEnKVN6UDk02ISa3yrEIMotHos8Eby8iWpQqc6kClFDVaMALkmXdBH49QfwVH/404gZzz3wD+Rvn/PwL+uS/191YzPGoEVlkoklWU0D1ZpN9tM5SBIjWNuB594DlPzSUM9xKC255SUBKVH7MP0rcHcqVR0RJaRbZlh9eK6I5kukpLbA69j3uvkzOaUI5lURzC7jy/LitINYQWxpWiXhj06NBdRfaljZgyKiRMFyTcNHaGm05hpxmSzB7EVCjT1Pwa24PvtaD6oi7Fp6K5aPJ9YAyv+9EvZXN6ISmHLSjBtylIv8kSh/HXW99y07XYrcZtwW0ieu8PrVYtOPxcO+LC4teSYoV60miA0EqB1q8Tehk4rXtWZmChxyJlL7vA0g6krFjVAzEphtqRR0ETAjMyUQdRPibE+T4BEsmVFFaPhfaupATv97nvvy5lTSLjy7XQKsmkphpxqsfnDIXEdXwo3Dh2BP8QuwPfiPml4sU/6dBBcl09luGQUeS6klMzX2C2wg+QNccdr0Mhpyx+00NzlXFdonnpsTuP3pVQbprrrh122QArtDf4pSZGZiQZHDmXV4EXisPO6koUYBOYBxSDy2IJK8hKwleVDdVKszQK0wfMXS8YAh+wt70s6o2VczEa7WPR0Svgo5wF9WYMxlfY3pKMJRlFbyzbZUNuB06aQdSKqsDgLCRNnshYv6ITmJyv1EIkEqhMEODPV/AsYzZcxRXPxxP+8PoxL1+sOf1MsXgeqS97mQkYvYT/tSMta/onjdDPPdVS6a+nmwKxEeKY9GRkveppjEh7b2IzU4TFrEX23iWeLSpqE/DRsMuKEB3uTroIyVAYoI0oIYcoeIQsEvFqCJguYLuMH4SG7r1ZrLNiTIeCqpk4DxJ4ZWQyMDka7dnlLY3yrPM4Yyt2ucJnQ2UEj6Am/okvaR+EE8gW+icJPSrcTmF6RX2dZcbb57lHq8uI8RQtTIXoqXWjkhQB7T5je6jvInYXsZtBxj27AUYvyjUhoFJClGpb7KLk7fYo3C/FCPXK4p+GhpIDbCa7BE5ugi5zBwdGYsRbaUVOmdQmfNLoUXZwOzjsTlPHjBp8KXqlA6ZBy9+rUNBx06hqzqioUTaTO43Vimpn8DtFbBVj5+h0pq28SFbbxOCSjElr5gjlq0QDU+tWCqlKuAMKPVjK6r1xAlMIvEs1L/2aX/VnXN8u0TeO6i7jdqJKPLX+srPkyhKXDr8yjGvFuIZU5+IE5DOlSir11okC9JjsTKqRzEG/MqKLmpVntIZlPTLUDt9owlJu9HAiKRm6Aq3QlcWAAL9CELBRqSW977VMR45giJbRWLwOuHwYunKAB/apJqGPJMj0HG3tUs2Q3Ovy6l/SEXwQTkBXkfpHW8bB0V9X2I2mvhFkWPvck62EveOJjBr7JfKz6ezz1MqRxe92CbtPsosMAX27m6e+8KOg9QonvYoRu11iG4MZjBQJ4/SgP3BxFSRbIoBaFr9pItpEtM4z3fZUJc7pQLOdtDiMUGmyNfi15KDVRqNixt2BiVkGi1KSB2166zJ/cJ+uV4GXOoKKidooYuXISjOeWQZgbCWPrmygc0nUiozkBMeL+Uvb5BiTsDGNyTJki8/2njT322yag7+KS37RnfNHt4/JX9Q0LzTtZaC69ajeC65CKXLliIuK8cTRnyuGM8V4lkl1kntRajHKZrRJBTmZ2fmKa93SRcfgLK7wB07OYGlGqGDfVIzBsMkKP2qSk+vke4VfKZpW47aOJheHve/vh6Nf4jqGQrPeR5EWr/SB1PRwPENSeo4InIpsivwYwFVYsYnN7ASUnjzRPyScwDdpRidOlx29C9xGRciOWBkg4zYjBIHgVjc1udL4pWC/BZknNpN09BHTyZTePPLZdQeorvfSm49RbmCZCDtG6s2twuNrqSX8TxZyE6FK1MuRqgos6xGnE85MmvQTl74QaIzhwLAj7EWKsXHEUZONw98qoGJhtRTzNuWc4tFOeoQnuPczJXgDnRKVUiSnURH82jCOitu6paoDtfO4KuCB5DVq1DM1Wk7vcARHUY0q3QidpG6SjWGzbflUZc6qJ9zWLTSw0IMMEhVo8DFbbyw98Zu44KVf8we7p/zd5x9z93LJ+hNN8zJTX42Y3ShRW5kLSAtHWDv6M8N4ovDrTFokspNhHKVziZwOnAYhGnZjRc6K2gT6aKmMcB9YHXGqqFuTaUxgUXl8a9idaEItkvJ6lCLhuDa4rSbZFW4bcC8NubGk2h5pQ7w9JEgZNBIF5GDRqhEKtWQYnaXSgZUZ5klMefSEM8CpiM9m/mxXYTVPWcajFGSWnn9P+yCcgFaZVSULad9XjI2wxWSFeNxuRPUjqhvBGsyyJltNsvJQSTogY7qq86hRvvKuLP7xoEBDjLMmoIqveF71yv8fO/nCcpurhGojrg6crTpa5zmteioTsGqaglOMUVB0QxRWnFgcA0gIPVQB7y190GRlcFuF22vszmK2igk7cM8eFNosMwkpo5WiuhPUTHWnyVrTXwjmrLICY80uMjojQUWQNsjEVv6gHTnG4wJpVqJEZJQmbC0b1fLFck3ImlPbsTKWWAQyEukeD96YDX2ueOnXfDGe8KvtGZubBfalo3kps/tmN8q9T1kiF6NJlSHWmtAKRDs2We6HS2grBVCO8PnyfgofDHvAJy0KwSmQsqIxnqTjTNdldcSZSGUDYxOINhOtdHZEu0JEcGwnupFm74QkpBRm0cyCLG+zlJnVpfflHENJpSYdR6ciCyOtSJ01qfA5OnUAY+0LWjCUbsRXxdl/EE5AKXA6goWqCoTGEBtHrEqY04+km9vyWiUFGm2w5gCcyNM0WRklTSXvJx30DSkCFABoIwMmdUVsHWFhDg9XnY9akaUFWCWoE24x8uRsy2nd85PVFUszcGq7eX4e5EGfGHKHZNnGmpCEKy+VXaCPji44ntvIrmnZ+wrTa2xnsbda2Hze1XCeqtSl0KlSwuaM7ivW9Qq30aTa4c8MW6CqA1UVSIvCGagsepD+9oQ+fHDXn0RVAkXNBlTIJTVShGtHaB0/u6v4o5XnV4/PuGj3PGs2XFQ7FnpkbXqJAJLlOiy48Qt+dveYF5sV+09XtJ8YFp9n1r8ccZsRtevFcWsFlSM1Fn/qGE4145nCr2RQRzcBZQQerXV+rS2XkhI4c1YM3jIGizWRna1orafSoueXsmIfqtlRV1UgmkS0UVK6pBlrix80YPA3BhXbA1FsSY2mwmDMr8/wT3T2mQe0GgDnIs4FztqeVTVwUUhGau2pdSjgojTXUq7GJZtQsx1riTYnnMOrDuEd+d4H4QQmk75pgblOyjvOgCkjoiHIMzqJlmp9/wApFbnq4hBykl1/qroeLapZ56CpCEtROQqtzH6nOt+LBLLOUCdsE1i2I+dNx5Nmy5Nqw0KPwonHoTKesmKh7Uyk0ZbqdBcdIRt8MuxCpNKBfeMYBkuqK8GgTynO9Bnmi3N0U6ef5yMnoLUUDX1Ad4rqLpCNpboxZK3xVZnys4IgVBP+IkzgKcXxZOLxwJXxBacwCthK/p3naxN6VWTiLGGr+SyfcrNsuVosuGj3LOzIqetIWd+j7P7s8pRwV9E8NzSXE32XkIQczwVkZ0iVJTSaUCuik+IfVpSQtE73pM9eNSE4FTqzEPW9y+q1mTkRJ+qzCfEnx5RWrEqZVMtiDQuNGUUuXnVSwDY+Y7wCrwneEKvXsQKHdFCLtqUXZeVpEG2wmd4munVN047ctQ2P2x0LK9TnTiWsjoRkSCg2oeZubBiCJRSRnAcnYd8RIXwQTkBNi19lahcYqkBsM2GhiIsK3TWwq+Vuej+P4R5LfQOHxT7/+w0PhlIoZ1GLlni+onts2T/VDI8juUnoNswPglIZraFuRtbtwNPlll9bveSp2/DM3dJoP/PDwwELD4fQV/rgln2Sls40JruLtRTUvOVm2ZIq0UwkSXpD6V6g1GFm/vizHaczEzdBP4IPVC8NpqtIpsXuNd3oGJ4owiJiG48xCZpAiqJurIepu3II+bUXzIXtSsF1n3DbhBkiuo8zeWqqDdFpFs9l7qN73uJXDZ+t13xyEtB1pG680LsHQ9g5VK9pvjAsNrD8PNFcB6rrQRiCZrJOLdyTC0dYWMaVxq8UYSVdFtXEN0YAcokOizCXTobPBnVUq9HqIKDqp935aNFMk39aQ66lKOxPNSoZxpUWEpkuUW2kbW1vDSFDrzPGxkNWN9WEJhLbrUV3mvpKFfVsKUIlZxjOHMO65ZOzFc/PVizbkaerLc6IxoKkD4a7oWHvHduuJnhTAG1qnjQti+ud9oE4AahKuLOu5ZSeny3R3rD/qKJxmspq9KYTldlhkFzfh8nNz8e6FwUcy1Lfe0ONahvyasHwqKF7rOkfZ9TFSF0HKhcKLcDBESwqz2ndc1btObUdCzPccwCzZBbMBTBTUoSUdVGeDYzZ0hR5q1oHblzLddVybdNcB5lt2uHVK1HAsQN4qG4QE6obMcjQko4WHTTaW8LS4M8Nk7SYKhTeplczLmMSUDGDfHf7jOmFZNNuR5nj7w+7tXEWazVmaIitwXYWv1CEhcavq9LWbSCDSYq6EzxIfZVxXaa59DNNWBmFLIAgS67FAYSVISyYqeixEjG+C5X3+m4sT1wCfBS+HqPVvVA9Jf3gsV5Tds4F09IF6muFDga/MvjOMnaa4DKTbN70pb3Mxbg7hdtD8zJTbRNuJ/MQ2WqGU2l/DmeW/smSm3VD/8RRucCqGcpjoOhHIW8N3ohIzkM7/nt0LD4MJ6CkMjuFZZrM87Vn9IrusQblUHmBdQbdB9TeSRttGCHFedGDtNKmAiAg0UKKx28mqUBVEVc1w5lhPAN/ETk/2bOoPK3zc7ElpNJLtp7TuuPEDqxMz0KPrzmAyY7/bYAJp+yyVHdHNQoaLAXWrqexAcoDMw9LvUuyp3zPxymOUrPjUKMsUnejUbFGe4uK0pbMypDqspt6oeyaAFZuW0BaXqYZzVjIPLsgyL1+kEJkOX7OWei7tMZ1I7Zy2G0tRJutSIYnC2FSlc55Zgp2d0EGprYjeix8i1PHwxrBBNRGHFcr6sOphlyXNqBO94qAb75c93f2KSqYVJenFttBierh4yh1v2ks8wUJsx9RMWFGh1+INLgetbBmF3k4VWoGAv8WzQK3yyw+97i7Ab3pZ6q5et3gT2v6C4sOhnFn6U3DuAhHxeVSV4iTUtYrnHxfol35QTgBpyIfN7ec2m4uqCmVeX6+4ro+ZXdlqK8a6psaM4jnNEPG7sKBww+Y+PvVIDBTXXjn0nZXoLmRY2LIcFLTXWiGR5H6ouOj9YaVG6iKM5qq/MDcVmrN+Bobzvva5BymAqJTkSfVlutmwS9bT2iFcTm3juyrQ4vwmOSSfD8yeNWmyChIuK5vM9V+xF1bqrsWv7SY3uBXwoRjBokE2hcZ22XquygQ5UKJrmLGdAXE5ENJU/LBAYUwF15VCChj0Psea40sZK2FRLYwJ0lcLX8/0a7dmwos/H+pcZIGtBJ2+6XCr8AvM7mJGCfCqO9yAK9fnvspwvSz97GUJJw3ncbuRX+guh3RN1t0ylhnMbs1YVURFkYISF6pzwn5LLhNEKTo5VYUi/adANiUQt801DdL3M0ClVf055q4sIRRs8/qIAl/FGHIG5T2oFLz+/2piQSMSpzajlMjXz4b7lYNlYns+4rBNCRrpT3TK0Jb5gLWulS1S4hWCltmqDFDxFmD2vfoGMnGk/rhQDhppHiTashVpq4CCzvSGE995ASsSqSssDpSGwHvHPO9oxLm7R/vtc9qgKQ0MLLQI0s7Yl0kVplYQXIabaUgChzqApMdcxCkNHMV3vv9lBYkPxcMrdWolHE7Q9YQG5muNCMC0/b54AAmtuQsw1uC03bzLq20QoV4L9IS7v0kzMqjnLOCgxNT6sDBfy/FSfO/8zQb4IwMT5ViqUDHM9lJMXCi6voq9j7DPQ9ZSpoUNNZTIidpSed+KHMEFnNj0EPA7p2wGt8LHWTlqgx679GDoFiPGZIzEi1pa9HOYvos4+MeVBDVLEwSstKZ4OYNFyIzaze+zT4IJ1CpwE/rF5yZ3bzLPrZ3XC1XPGs2/OLROZ9v1mxuW3Jv0LsC4Bj1DPGdxEpUBtNZbA/LT2uqu8jiT2r03Q5u7+QhdZZcW1KlCQ2oJrJqBhZ2pDX+3ox3pQ+oPVvCep8se2qpCRRxyLfJST9kjfbonDi1ey7cjtNlx/PlCr/ShJWTQaIjxCD2yNUU+LAqAy1T6H/PjouGXnykViKsUW0rspbwemJymlqEwrtnoH64yzRpJ9jtiOoDerMTchTvBXgV09yanc5jBmcVym/qWsQ5bHn8ii6A4AEUqTKE1hBbIUmRrk2Rp2/SV44Cju3LOIK5qj8a6A3VraK6zVQ3A3q7JxXp8QyofYcyBmPtXMhVkxO3Vhz7lLbFRB7Gw72a0lrvBd16DBab7kUSjMaDNonLTuCviQPjHZ/vg3ACViXOzI5lyZUBHlmFUZkft5fzrhyTZnCOYDIpaGHUmTZFl8tFyDBqdC+wz+pWodIJ9VX9/+fuz2Js29b8Tuj3jWbOuZpodnfOPeeem/feTNLpphpkIYxAKlmYh6KE5JeSBUjIVRjlCxSIJyxezAMPfkBClpAKWXQuCeEyJSTTCSFKIJ7KUjWUSnaRtjPzZt577ml2FxGrm81oePjGmGtF7IjdnHOztC9D2ifiRLNirbnm+MbX/Busd1rLik4dQqduR+LSrY1/yvG+ewVTFoaslli71NJV40j4IApt/fnaIOxcIDWJ2BZHHK8y5FkEnAJlqky2FLZhFdiQOh4M4YFGaNlkFRPRFcONavduYTwrct5rc0tl6UipLtemNA3ba6tuPAbMRtV9Zheekx7N0ZZbpx05aTMuWzt7LogItI2SNotjT3bmSOBxKjBTEXmnWn2/ylX31unjp2TmHkKukudV7LQPx41q7TzNOPoTlAwpZzCqS0g60T/IeQ6A4iqeRR9HnCU1ntipFFoqMmgP7uh7RDeq6tW7SoKPIghYSVya/cxFB2iMSlfXZSQTkmHbtOyaptRnZkaIde1E4yKtC+qOOzm2izX+yiLRExeGhaBQ1JwJK69SXI3i/euY6O4yd65gRCGeGJS8Iab+IOlERfd9X3c14Fi4CbzKk8XGkJpiWeUtyWt2oFJWZvYy9LuAGaKWI+OkN9l9E5Hia5CWXjERSy2pUlP2qmemzyLMphxvKIXlkjFEYVoKzcayShmXwCQ9weYxbSo9gxIA5gZmIXDVkkCsevNpIJASlE48KKzMkNzsk2IDSkPwV7lqg7E2/+axYUGM5YyO4CbBFakxGQvHQwSaRvEs/qgKmCc97dW+KZOz0RLNHsshMUYzL6sNVnKe5dFyp7qI+l4lVVC6UxnefhEnn5dG5PuwRT+KIGDKiXhqthhFvde9BJZGG3KtDYzOMrqCxXepuLZmVu1YILyHuV7/uQtsHnVcuRWHZ47l4zXNJmEi9BeG/okwnUeW3TRz799nDYX+CdBKKFh4q5r7ZsLmh2W359dXXGQjZh4pInlWto2NQdaNNoSWhv7SEltlUKpiUqbZWtwhs2gt5hCw14ejqUntExij2vjrlv7TBcO5vu7YqTZ+vXEmtJNd625OT50T+WoJehrGhWG8EaBllcCnpCYgIrOa8uwBUL9WJzezTbgBW5AVi4VmKkttqoVlAQaVjCU1+b1huR+6dCKZaJ1ChhsbcUZ7QX1wHEbPITfUI0nfIyEtGwxrpG1Iy04D9sLNzlNm2yPDRN7tb2dH+di80/dINIBYO09bcuM1a1soiO34ntxzj5qsKld1tws6kSjvGacf71kfRRB426qZQZVjqlHaZIVmOpOwJtMWa+ulG2fTxnBmWPjAL3vPYDT3DZ3FhKwOMWeQW6Wb3j3xH1qpqppkVcPBgE3pWBIUVxhTxBHvCwQ1AEzZMSRV6Z2SndO5bFTkJHa6GcaVYTzTDRE7ZnUjdW3KuN7hjKjwBWXvFoEUrCV3hX1XwDYVd5+afEz5bdFE6CLGJ6yrOARFuCniuvDlozBFIJlZIMVtirnH6URDX6w+JznepEeLME2HxYiWPp1mKmF59KSMXmYquao2/UmUAar117jA0k/aqC3NYSet8v6DmyXaklPhmdhp6i+NI5y1mrEtrQaAmGlApyVFZj6HY5Z0/2YuOg+mjEhPHLko5dmMVYA5SEs5PFTu7h5NgV+HTCBmUd50ca4BZvrkVVyyiR2bqWNKx+GcSL7VlZ+iwj9Dsiz8xMJOtDbwrNuy8BPfXK7ZPl1wuPGqBWd11OTPBxaNUk5jFkKyWJvfCAqnnvDao1Adv2rB3SfPvmQCyva6zQRTWHFiyq4IXCzos+Pr4YIv+0u+3ayR3iABQicglnxuy2hMGC8UJJN8HTWhQitrITmH31u6xuA3Xsk3/QRG1Oj1sqV/4jk8NUxnqOquKye+lFm2VyJOu5xwTok0uQTaEOxMjU7OFA0BvRbjzjDeWNyuobl2SAiFmFRwG1LGg/PE4nhNxQjSNMjFOdPnj+ifNWy+cIQqElJ6E2GViV1GmohxDyMEv8sSYTYC+XS5KXDwLUszkhC+Hi74dljzS7lg6D0hCuOFkokkdbOY7biWYlkmRf4NlkuD3zS0MaqWxeHYL8k1Dp7wX26tE42CCt+mXEIpQhDKmlTeBOj3puxhMnp/pyM57G2X66MIAgHLVVqyFDWkNJLYp5ab1HEdl9yEjl1hS90FdkDtRUmRrZZZe81JpDHCme/Zd15Ze0lPM0Qtu1xxh8knAeBDVipQVFD2IAli6dB6CYzibklu9dmzjR0vpjW72PL14YwXhzX7favAnYz614meArE28arPoGTVOZByOiaV0wKwoz6PbATri+df5whLBdvU1DoXqbE6V34fJWIpuWW9VrWhWEVeYldq++C1N5COgeDW4xghl++Jc5pKX6zonzbsPrXsf5CPWQpAFtUKsFmnAm+R8v7QVfsA3iY6F7hoep74HZ/6G5ZmuPWzffRcLzp2wRBWqnM3FIg1UCTq9HqYSacu00KDeuMtTPZWNlQNUmaEK9zmilQJ+liDgOi/OgIT5TYYm2jbqfyKkKIt5iW8eyxQ1kcRBMbk+Pn4hMdOJZSsJG5ix3Vc8dV4wTfDOVfDQg0WTggeii7UFZJhipYxWpWDymqSiYGLpmcsCiQ3Ls4nm7UJV9LeMVnGpFztxqi77vuWCCEZUqnxJrGY1DCUjMCcZAND8lyHBc/HM77cX3A9dLzarNRB9sbjh/IYK00LqxlmKlbtmdtz3+wySWBa6zw9GzXC8AuD26sbSWoNw7llWkNcFPqtRefMpxDYk4Z1LrTnTO2Oy7FDfjIuyFZxDWEhTGeOdtlqL6K3QLx16iM6Bchi9HtiNAs4WzM8W7H5wrH9jYz96ZaLxUDnA2NwDMHOHXq58xy/z5o5AZLnXtIPuyu+aF7xo+YlZ6YH4NLuubAHnIlsRlUn2l66Im5q5lM6e82ostUgIAWKLVn1EGWKb3b0ctJrdJcsVl6khDTL5JupjG+jFL0AsC7SNIHzbpjL5edJGGhI1pZs49cEJzBkxz85fMIjv2ZpdEy4jR3b0PJVf8HVqFTbMbhZPOGupFIq2O+QLVNWGWZ1b9Vu/qy+IhVuepwTx2SIRR3HSKZNliQZb+K9geBUR6/KPdXHn9DdmrLM1E+AKVleTiu+OZzzy+05r67WxL0rJYDgJpW6rnZmQFFbRmvBeqiejnzqoeLKflur+nBoLW6hN2i2MC3UIONec5X5AqIimdEQKEGnkl7SUSEplZ4AwWgNainOPoZw1uIA04/HcaFRqbQMM1pQsFr/Fouw/omnfwbxBwP/7A++5geLDSs7cIgNu9jweljSR8dVv2AMVutz8ncKBEdHIHAm4V3krFFb8E/9Dc/chkuzZ1UnU+6mvM/Ci9UagN2qI4pTl+SEWubN1xZSzpgspXdQwF8FBHU7MqL9JZKa7BRQlbQtufGkVkFdWUpDdqJ4JxrySXZZxWxyLd0mMzdwb2EGHlgfRRCYkuXb4YwhOdois7SNLYfojwEgWmKtS2tJwO1sYA4ESQMBSVP0KtgA5X2aU1v9vVr7jtFiyEx2wpOIWXgf4cxT662Ytf4fspujc0hqrfVyWPH8sOLV9Yr0ssHt1aGojnglyeyEC9yWWM8ngaCuGgQMUAOBaG2aS6kARZOxNpjuWxlNNU0JrkkQOZmPF/priqWUinIEaZlMauq0wSLBY7xDqiJQveeN0XGatfpCjIG2IbfKPJzWifXFgd8+e84XzWseuy271LKNHX/sH3M9LRii3q5TtN8rE6hlgClqUGs3cOZ7HrutmoLKRFuyt3MGJrujz56n7ZY+On7ZXjBGIbWa7stdU9OiQqViI7XRq3R4Tq+JmNJAPXa3xBjVT2gUzDYfAnUyY2swv61qHAv7MUZDngx29qr8NckEQjJ8vTvnpV3NirVTsiq+ERzhTgC4b1vWjTxFS28cRlq8iUzJcjN27KeGfnJvUq2zCmVOsXToARcaOqusxiTHJuH7KunWoBDLqb0LLbvQ8IvNJS+v1uRvOrpXarMN5fRvmBthtxSNavb9NgXbWt+XOl2ak1FiOvl6eT61iyy51KglyBBFOe5WSEkjU1Ug0tNf5gAghQ2XrZYs45kwPFLZN7tbYCoqrlwRQcdgc9pboNvJmePzLyXc0gw8sbohe6sN4vnaJjOXc8CsI/C2PsHp9+o40JnEWTuybgZ+uLjis+aaZ/aGc9PTSqQ5ibiX5oB1mefdGZbEH60ec51hOhTH4NPGW2YO3Drqhbh0mJCQrkGc1QlKOfWzszNmIntHdoa4ahgvPMOFNoZjo9mGEWAw5KgBZsot08Gz33XE0cBocK8d7SDzvQV6ILzNxeujCAIZYYh2ZuyBTgxSFkK0ha8ib5QAp6v+TMia1ruUCNnMiq5TsWq6e4KcosNi+fshWUKZE79vNvDw89Ln1EedN8dJBSkknKSRcAvw9V0OudoLqidHgjlVnYMLHMuJN6KhQM6FjZZ1YgCQZHa6lRII7hqwJAfilSlo2wJyclaBMFJGqnXVQ88W9GPWTroZhP7Q8Hxcc2Z7zuyBKTv65LmOCzZTx25sOIyeaTy5bV18K4/goQDQ+sDSj5z7vsih9XRmopNwCy9iJeNJKvdtes5dz8JP7L1nMpoJaQf+5Aloxl4ahUJYKI3brDoYC8rQWapgSkWCzsCwZaEkL2W2yavvnSTm9yAlC6IW8bZXtWx/U6DgU2ncupIQvoXg8nEEgazSTw99D3gwA6jfk9LM6oPTjn1h/43Jcpg8Y7AljdTfqTeElYwRPeW1lNAN60xkEP34XVcFLR2iZx8a+sEr9yGgG7NE6FNTkO+z5vKhANIklyBwcuFmxSARvYE5yQaSwETZuMxlQlW+qWjBWyeflAZlFsISTBTiQhthcnCFHHTnGKpQ4ZSRIdLeRLoXjl3X8R9cfME363Nena8Uop0c/3TzjNf9ghcvz8iDRQYdgWEyeYnqCvjI2+DEdaxYAUEXbc9nixuetlt+o3nBE7flXIYZY1KXJdOVBvMzd0PE8HSxpQ+OvV9AsPPJLzUL0F8kFv7F4bEtcHCDGZV7UV2TYlNg4kYDRnLFPKUTbeS2ZQpU+0JZNzhZaHp9T/yuyOwfMm7QBkwdV4aFjphPkqk31vcKAiJyCfwvgH9Gnx7/TeD3gH8T+AnwM+Cv5Jxfv+uxKqHj7kn9ttMf5uxrnl3XzT9FxRTEZBiDZhn6nPX3agBwNuFsfOO0r/2FlE0pCT5s1d+dsqEPKjgao3Z5aqoIZePewen/SlYZzesfOf5TtGE59UUUbVY8ESrSrK5ZpSbIbIh6XxZRuQbVMi01duY+yKnkW33cSqjJGTMGmpvA4oUSiJ4vH/HqfMXXu7Oi2ixcb5bEg8O+8qpvOJVRp89EA7mNWBcV33EH/l1fjbMRazKrZmTlRz7tNnzWXfPUb7i0+8JbeRMfUpclszID5+bARdPzuhl54ZM6FhW25Fy6lc/DImM89E/VESl0HjsokCjZUkp5mSHaM0y6KaPXCpc+AUhJAlPeD78F12e61xl3KJbtFY/UGOIkSFQuyKky9931fTOBvwX833LO/7KINMAS+B8C/3bO+W+KyF8H/jrqT/jOdXrqz197n9/j2CAM0RCTaP8lyQzoqWOuGSOOfrRzNnCUqc5183+HoznNr0EDQCja8n1wpGKFPc/YYe4o/0oDAMesQP/nJANAwSwpKy89V+DJfIRxDBh3Uv8Hq6KTfoQGgcp7cLOis9ylO5dGIeOEuxlYvLCQHbFzTFeWb7aeSpO1W0MzCH4js1uwCsIKqS2jzo4y87+duZnyfjdWAVCP2j0XTc9n3TWf+hseuy3nptfR9FsCQCTTycTSDFz4Ays/YlwimoLfSMd6Ls/IwsKbKqd77FS/oYq0KuKTuWk7l3In3I1bpKH6vkTNBvw20+wyi+cjpo9KaIKZiWlGpxMnY7Tn9MD6zkFARC6AfwH4VwByziMwishfBv5i+bG/g3oUviMIaARPdcN+h+eT0T7C6W46nWvPIsNGt3bjIs5GWhvneTEcm39VFTiUKUMVnby7KjCpBow6jpyyKgqPZbS1GxpNZcPJjfJ98//3WTWtrzdPAjMqsCUG5lMnFyDSfPrnk7r/nonirTdJKI5MKjg6rQxm8kjoMEZUjKSqBlV5NNByYAqYTU8ngt95XK/CKsOFn0skLUMyZjz+0TCpFkJYGqLLeB85X/ScNQPuJBswknES6WxgYSeetlsu7IFP/TWXds+ZOdDJ9Kbxx51lRfktK6Pq0ms/4H0kWjf3Y/RaZC3znCJSxekwbxoM497idjUQnFy7GrDr56XXcN/BUEs8CSoAa8eMGSJmLJbtBkiCKQrIJiYkO2LzcC77fTKBnwLPgf+1iPzzwL8H/PeAT3POX5Wf+Rr49L5fFpHfBX4XoPnk/Hs8jdvrFpLwpKM+00QpTr02YksmUNPAakIBOrEwWB1L2YDJBnvy/YTMgpTVGjohcxAY04nvwOSYJntEfNUncg/9809ymYhu8LIf1dW4kFncsaCdR0uV5/M+Dy7HTCA2QuwMZnBISKVBWJ2E0m2efM5IiJj9CDHTOaO/O9l5vDbzYkrJkqUoJZsjYs/byNJPPG73OKPvbcxKR3eSlIBmAk+9KkSvzEBnxhmherrq791dM48FvV9kRlueBPQSELPLmO5YpkyNJRoIWWf/djjJvOT4MZfHOw0Mb6yalZ1mbgkVgEka9UUyZookwJ44cd+3vk8QcMCfB/61nPM/EJG/hab+x+eac5YHOjU5578N/G2A1Z/6LBek7dyg+9BsYC4l7hmlzXNhybcygOqmW59iLb1CNoSoU4VD8FiT6KwyDU+lx2ojMWQ7Q5qnpKjGoajZhmjZ71vipAIo876vU4FZPvw/gWCQjgIi2ehpIkmUk9CcHGdvS/3vfr02IpOOw7KgcliN0CwMfuOwQ7otUFrdlU6oxnIYMONEOwaytzSvvGoKNEZPsdI4m1NoU272sn8vFj2/sXrNT5cvigR8JiLzxvWiI9+VGfAS9PTnKASLJKZs5sbgmM2tQBBz9S8o0PS7u7NuXgu5yUgXOT87sGxHln5iP3mu2gV7WehYUEwpbU4ep5aj9wWAua8j8zVPrhCsOlsCQDpmWRVxGIs61F0sw8n6PkHgF8Avcs7/oPz/v4UGgW9E5LOc81ci8hnw7Tsf6c6NVQPBA9++/yHu2UR3m4DmngzgNEbVEmCMikoL8Sg/7UzC2TetxmIB0tzVlw/JEKMpYpBFZlq0KZFt5lYO+bZN9z1Xrj04Lbl1dFgmBiZADuVCmTyj0966anlRPz/5O4oyVEXgKiWWrGDHTHKioqKNQ6ZILtZySvw4uXmLNqIFsrdqupoUCk4286jTBEilZwGqP7GwE4/cjjNzeGMDQ03pj/LwCTXxsAVMESVDPo6p5xEoWt7tcsNVWvJyXLOdWkIBTt0qQYuwjfE6hlz7kbOmx0pSDEvXKOryQMm6Tu6bk4zgbate6+Qrd8OqHFwoeo3zDxZh1JBuU5fvrO8cBHLOX4vIz0Xkd3LOvwf8JeAflX9/Ffib5ePff5/HuwudPjXyOL3v3nwet2v+01VHRqdTgLsZwOnfqaPF/dBwGDzT4Eh92Tk2g80Yl+a/q2w5fXLiyptfON/VCCJXwlLNUGydgOT5azr6+RMqDcrpBJoliWhTSZVymUdPaoGej6Oktz2V+74nWgfrAERHUnXUZadM6DxuyLidxfURmZJSn2PWDCFGFdyYgoobhIg4i3iLBIfxVvU5nI6Dk1MV4xoEvI2snIKMLu0eL2HWaZiwTNnNFG6AKR9vfUsmFmxzlITNWhMdf9bQZ8fLuOZ5OOfFuOKqXxBLiVefQxZ0Hu8TzgdWjQaAR82Bxigrc9u1HCblHdRJ0RvX9W0lAMyTn9hKaZIKZjJIsHrypzvl1n28hZP1facD/xrwvy2TgT8A/lV9ivw9EflrwB8Bf+VdD1I3X92sRzVYKY4x8sZJX7v9960jO0ybfo3T098VmGit504DQMyKMdgPDZvtgnjjcTeW7lrBFxXYc6q2U9F4ySs9Ny4ycRFvC3JkSi9Ar0wl7WRRmTCD3ghGn8g7cd7vvUpdmd3x5JAmn0wIVGJcR4D6r47tCgju+FDv/FvlpZZGXpJ8a9Rlos6rzZixg8ENrjhIK1PO9gnbB3WQ3o8aAKZATkn7BTErKCg6zS68mf+g21vCzmjzddXSZ49BSVteoqb6WVGcM5KTI9MUwJCwyC2dyKkGkGy5SR2v4pr/uP+cr4dz/uDqCTe7jrR3yHRsns5tAUHJaaLApJUb8MXubDtpm/5wsBgxmBPQ1ftdbOZsKDWlTFnW0iJjQiIH3ggED24WvmcQyDn/f4D/zD3f+ksf9DgoW825oCO70t2t6fY8ejnZ+PlOYJiloU7qf1tkw3x5TFUPSm9gAioacQyOMVhib7Fbi78RupcZ22sXVsc3t9+l5CEsZY60wZc31Z6c7HP6rHdKNhkpGiK5AnfuNnu+5zp2mSk4dg0IkiBOAj0Ye3xaUu+Tek+eNJNzev/m4Pz7ksvjiY7JLJimBIYGVSdqMiaAa1RExYkgQ5z7BbO8ealnREQzrpwxTrBO4bFmFPajZxcb+uRJ5cnbqrJ5Ernr5o8YDAmyo5GIraaptcFRfraWAM/DOV8P53x1uGCz7xh7j4Rjn+d43W+XmFU1y5qE80pWGjvHwS/IoTQ3CwjovYdFZUskp4dTaHXiYBtD7uWoP3C63hxszevjQAwmJT40XeSsHVi4afZv39AyTOosG6OZ2Wz6iwJl5u8KYqx2/itHXINAvIUDqKvW9WNSCvKub+j3DfbK0b7SAHD284DfBtxm4JTPmq2QOs947jk8c2UOzEwCuveiz51kOK0ItFtfegRVQv27ZgOnGYAp1GGf1NwE/ZuK99cmmyl8dVXvycS2lA8262vImjUQtXP7Xs/qJJOIpduaGv07oYh1ajmif9/2QrMx+J1lkTJ2LyqTljQISEowGUyIRcfPFjVmR/fSko3w+tsz/knzjKeNlgOGRGfUAzJhSBxLgVPCl/6/YLIhisGi2cOIpU+er8MFvxif8Pv7Z/yj159yvVvQX3UwCWaoG1hm1SEpZeI0OnZTg7eRTzrL0ow8sTsOq4bGRq5ulgQ8uS8gtrds0rsrC0glmgmEIJhoMDFjD1Zt48OxzyLx7SfLRxEEQE9vZxILN3Hue+UARMsQHFPUxluKRmmtxfmVjGrQ26SnnCgyrHERbxKtDXMGANwa78ER2DNFhRRPoyOPBjMVS64BbWb1YXbJ1QdSA5PUOBUBOQF8vH80L6HfQM5l4xUm34fcEHcf81YG4LKO/mwGl+ZsP7V64sUAedK6MjXaD8htkRlziSpkkaT0EoK8f1YwPyf9cCpiYkrmkw0z5z0U6fPUWsxkj6VJ8U8AZhFVAcQYjIk0u0RYGOy149Xlkp+fPeIn3QvVrCzpffyAZ1x/tk+ePns2acHrsOTlsGLbtwy9R0Yz4/cfsv5KyXCYHN42bKYOPLRFvv44Xsxz4/ZD19zwFc2y9F+1SD95wNpvesv6aIKAMZmVH3nS7njabElZOKSGMSn5Z5g8Kar2u/S2ACaE3GaST+QmYkxi4QMLP+GNmjfezQDqxq8iIzXYHEZPODjkYLEHTTPdkDUAHCZk38/wV/G+SEsvlSCyLOo/TSHevE+rv2QFuXyeK0iHsmFyfi8aqD7WMbtQwRBmwdDcRQ2ULiHFEisJJJuZjFVji4TakjUJWQe8j2qGUqYbcVskqzLF9vp931VuNbtqmRGLepP4iqgrd7NAc1Magc4iUz4GgjpWtIacEiYmcskEyA3TmWXbrvg9/wk/7K5InWF1Rx3obSthCkpUM4ZNWrBJHV+Nl3zVX/DNfs1+15K2HnuQI4S6PPVbCM0oxNGwO7SEaFn5kUP00FGcqT8UhH7/mgN+KfWSk7nfVJWJbgXRB9ZHEQSMyay7gYv2wGO/46nfArCJHdu2YYjasEuh0CVvjjPWiaT01TK+WzcDnQ00NuDuqf/rjF+VOB1TygovjmXcUzn9jYpxTGfaLpdxgUxB01SvlubTmWc8M0yrItxZiB4fvETrdhKzdJeSfN7cbbdutvq1U0GLAjVNRZJLXNIgYMq4SPRruUwukhWIQm4SuIxvA00TWDQTUyFd7Ue1vc5G5gP6O7cuT36x7oVTmbKwNEh0uIU20GZ35troqr0Cr4HB3lg6YFq2xMZyLRf8e4vf4NX5CiOJlRlYmuFWL+ChVcVfI8IuNexSW8hfnn70pIPD7A1uZ7SHYso0pcJ+q1pT1ms6jY4UDV/ZM678glfDkv3UcJg8U+9UmOV9A+occ26XipKZewCS1DXKhDRbvBFPrt0D6+MIAqLEjguvVM0Luy9fT5y7Ja9swNmo45jR4PYyz7rjQkhZu7GNU3ho5yacaC/gdCktuDSHDIR0nEKkkvoe39wilLGy6mq0b2EwCrzwjtyodl84MYfAlSzgQ3bISVlQ4aJiClgqH3+kbv75ELm7maTchNWkopQBNQCY2v0DBINxSpXOIqpqYzPiVW6t82Hup8iU6X0iTuaocvMhb+69r/n25xX4kgod2UxFkitmNZ6tN/Gdm1mSwez1tF+8dISlYuZ/+eSclIVP2g2P3Y6nbqMN4ZNGjZU8g4SglAHZMWYdJ/a5YUieIXnGpA1jGdRs1B3qdS/qvqfvSX1txbEoW2G77+hdZD96xlDQo6O5PVl432uWmW+IuZFcgJgmoiIWNWjGdIRr/0lNB35Vq7GBn6xf8ePFSx653S2Rx5UbaEzQE3Iwmqof0LTSQ1wkZBlm3PiyyEXfpYTCCfZA9NRtbCBkQ+Mik4+MXSSZzOQMcSkMj4Tx3OJ3luWFxe8SzfWkqLXG0D9SU8+wKHr9HyhSCpQ3t/QEcknlyMcG4/zkmTXsVII6H0eOBfevktz6uXFaW8zKvG87csoYM0+GYfCkZGZtvxiNEp9iIe/8iqYXt6+BjhSrOEm2Fjt0+MZiRTC7gwaDcGLLFgIkg4SIHSe6GJGwotl4rsKaXz5a8n/6yYrH6z2/cfaap+2WtR1Y2pHWTCzNWDgD4dZT6ZNnyo7ruGAbO25Cy1W/4LBraV5a2leweKHXNiyE4VKNUpPPihE4CQIEyNEyJmESz8HkWZvBHKyOZh8IAnPQPwn+crLhNdXXxqo7KJHI7SL2MCmRaDaPTW+CcO6sjyIIOJO49Pti960z/SlbImbWDYzJzJpuUBohbSa3Cd/qydXZMOPG37YMGSMJI2ae5XobsU0kmiKg6Yv5oxhip5RMlekqTUUvBQxTGnA1AHyPYzKXlF3HnzrTnwchVRy0gJLqhhejjVFqg2k+7cvTecBZSR8UPVFKPyJniIMt0xq9A1NSuap5FPYnEQTKE85CISFlLQuSQyavZRhwy12pui1VJ5/DiL/xYITFucdOht1ixVcHz2FyvFyuWHnVElzYiUu/58Lt6STM8vBwDAJDUkWjfWjYj568d/gttNeZ7kql00xQeHQdE+sk8nReWP7FCoMXCGbmb8x9hbdcj+zzscmb9XfMKMfHL6AvM2XsUBCD4ThmfVcAgI8kCLQm8KPuVaFzJqas45l9atjFlt3UMIxuHi3FVrXop8vI4vGBx+s9T7odTckA3kcJyFJUhR1cdgdaF3A2zTDgyl/oe88wOsbLBr8RmmunSDtgvFDOeGpPNud3WULZ2JDJ5ObN79UpiPNKSPE+zFDmCoEGZjHWYXL6Okpz8VY2mFU3kKAyYVUvABHSqIIdU23eFVEREyg6BL+CcmB+HiefGsAp5Fi57+pAlJzQGsHsR2XGFc/FfBoMihOyCxG7abD7FdOZp33pGC8WbB91vD5/XHwmRrrFyKdnW551Wx43e541GwUXmcBU3KVuQsdNWPDt/ozr6yXtt5b1LxKrr0aab7dkZwjnHeROn+sKzU6LTFy9zoDKstW380SibbYJu3tZTgJAXCVoEn4xaT9rMoSdwx4M7BTI5vcZvwnFJHZS1GV4fzGcjyII1EvUZz/DOfep4ToseDUu2U+eGOyMzksuE9cJcz6xXgysm0EDgHlTHOShZcrd7CTNUwQoGnZFgCSjcNSxdeyAoXPExuJ6fQN18/P9d0UNAuZoKFFPdhHVlrflX+PCjIHwVq3O6gg0ZWFwjiG4wl/ITFPR47sLuJrvtJpe6vd1o8mRcFK/n+RPphQ4uQa59gcK5FihshYTGpw1OvseJ6RHTU5OI1uMMCnE3N1YhSSbjnFraW6EaW2IrWE6d+wWC/7p5YqvLs95vNJyYeUGzl0/X8fn45qrccm312vyq5b2pdC9DvjXB+RmhziLE6HZeGIrDAfNBFKSWU/g3nVSmd33I3PH3+vhYtYTTRu4XO8J0dJPjh0dEdUjnLUjYXZ7+tD1UQQBKNLgBdkVs1FdudBxM3bamY3avEotpDYhq8D52YFH3YHzpldY5gcM2GcVmVJ3u2JFVklEda29gokaF9h1LUPbErcWM5VNZbml/PLBq6bwRut4YzPO67jT2nSLuGQls/QjjY00JhQx1KOH4pQs++DZGzVqYXIqgU7Z/MfLfZRZO2mI1nRzfl53V37g67+qJWXclVQ0JIv2RUxQApEZGhUwLc0uOVEsyjkjIUCMmJyRg2MxRpqlJywdcWGJjTCcG8LSMDxu2H9qOVxq2nXe9ozddr4HXvRrXvcLhlcLuleG5fNE+7LHXG1J1zeIcxjAn7XE1mAHqxnMktsNwrvXr358W4vGaI8ht4nVauBi0fPF2RVjtFyPC76KhkOS4lR9h3/wHdZHEQTqpp+SVYRXwQjcTKoSPMUyomoSuQG3nlguBz5Zb7loD0rz/Y4IG0PxF8hJ09GiI1BXMgmbErnVMiEES9orcaRScrMYReU99Oa/bZWGnvEJ3+h4btWO+NKnUMx5ojFhpjLXgHfXF6FCmo1ktmPLZBIxWGLQuv74N5kRjblkG/MTP71R3/q8P/B1PrRqVlL/V1A03Mx5UAMPuzDAAr/1Ci/uB2SYkGm6nRFUfX9niQvV7k+NQaJq8PltVKXebw37l5bxYsEvvmjIy8Dqop8l7vY3Hewc659Zlt9k1j8fsC9uyDdb8jhqw+3Q4zYDrTc0NwZECCtRfE7zlgtYm313g2qZvsQ2k7uEXU2cL3ouuwMrO9IYpa2vupEYNavJYnBbo6ALIzTO6HXZD9oXeI+y4KMIAhmhT17pt6UZOETHmI7agJrnaarcdhOLZmLhprkP8H1WLQ0MeXajPT45M1OOZ0x4Ed1UhZ5MDgWtVbv0H7JK2m+MquN0Xk0xFeykjc4aBKzk+WNtbsJx6hGSpTU68TDlZk5RyEWOer6OcxMzFwqx/Ikf8ve+9AcuVU2JyaU06HQUF1YGyQ4zeGxhHRJCaabmWj+RW09u1dw0LizT0qg/YMz4TYKQMUPBl0xCatX4dXcSKO21w++E7kWmexVx1wc49OTirqyc6YCMxR5+QIFXMyHo4fvgjZNbjl/PVmHe0iSaJsxZnxFtDBrUOLVpAmOXiEGYzvR1SLRIaLDO4GImV7v6dzQIP4ogELNhF9TiqSr2VBaXqXwAH0lOG2DLdmTVjHTuaGD6q1hVx+AWvbiUB31w7A4t8bph8dzgt8w0zuFRGRFVjMA82Of+e6F+rZwIYhNNO7FqRy7ankfdnsYEvKR52nG66etzffP5H1WSxmjpR0++avA7g9vIrG04nidyo2jC7LWpKblAYd91cJw+9++6TrKNauj5RooszOPQsNLrnI3BLwzJLWidwW6t9jBi0lKgbciNZ/z0jPHScf0Tx7SG6TxjBiXZLL+yNNvM4sXE4tuJxXPoXjtiI4xnx+3QbFS8c/F1j7vpkVfX5P0BYlQT0Yr1nQLmEPD7TGwL9DmenPKzNkEBWBQ0qOoJnpReNQPwitrsliPny57WBlI2vByWhKwcFyuZVTsSHxnGpWffNIyXBn9jWZwbmp1n+bXF7SbMzUEbhXedj07WRxEEMkrdvKv3bgoduM7xUxKMKaKRJRV+1zjwQ9ZRJPRINR2jSozthoZxr/Riv1GRR0q6Cuib6zVTyWV0M2sIzDf9yf+fbKbsZLbFap1q4d1N982dqYey3t5Ev4WsuojD5BgHj9sZ/I3QXOvfSk6zlpgVoYfREyYXsNI7g0Bd3zd1OL0GD9XIZXOkUiKEVR3DWSQ1eGvwBVAkkyV3DalrGB55+keGww8y4SxhLkfGQQE6iGO6ESR7/Cbi+kj7OpCc4HdHMo/bR8ygGYDse7VVAwpPuDgplfFkNQ89tf2qzd55UnC8zoJyMOr367XPbQKfaLqJtgC2jOSiYqVZcc6CNYkGWLQj1ib2GSbrSY0+UNgKJniaxtDkjDlMirN4YH0cQSCrdRgnCD+DasO1VrkAKkBjZp0AZ95vFPihq+ISqsjIEB2HybPbdphrT/tKWLxI+H1W5mBTTliXMU3UIJB0lAMoFBnKELw03k7xDpLITjAlACzdSGMCbU0B76xTh+PTQFCf9xCdzrb7hrh1dK+F9nVm8bJMELwq/owZ4ppZ+yAXzIHigt//ej20d9/6CzVpmqcU9/xYfZC6SSQTOzBFtTdbR7MwSEyqQ9AH0rIhrDz7TwyHpwI/3vHJxY6fXLxiHxp2U8MfLp8yvGxADOsvM66PNC/2R0WekjrLpE3G3A/kGGegklj1UcRaxPsZp2+C0qJNFGLlhVddiTnT0f/RwUwBhFWEpwFZBqUbLwcWfiqZ7tEPo67WBrCwcJNqWXYDu3XDMHj2yw63sSCWdqFy4821QYaHt/rHEQQ4MvvgyPkG6OxEdAX3nXUE1NpQ6qRfTSlQDUz76HQjBUfIalK6GRoOQ0O+amhfGboXmfY6YftIbLzOhruMPZs4WyuedIqW/bZV665i2SVRQR4mgBkKPRcFmYSVYd+27NuRsbFwYhRxuukfWjUAHKLnZurUuHPvMTuL32pq214FJKnefWwazCQk75Rq3KgFNm+bDJyuOnEsNey86o0e73mYkvofPx559Lf5+Ce/Y5TopOIkmbRQ9R8Jwnhh8FtLWCwUMbeJxNaoUvGlMJ1lum5SnwE7srATazewe9rw0q3ZTwvMZPXUvxmQMRSSWNLNX5F2KR7r6RoAjNWSwBXqalYlZDtmzCjHCUsd9568uHk8Wy4XohgQYzNtpye7iIqk9sHPY+CmiOHMJd9JyTomx3kzMCwdL3ykP2/YmpbxzBAWnvbaqSnJv3//2/lRBAFKH+B089egUHsC3kZMyQR+1RlAKin0mNxsWxaKweMYHOPoMAeDO0Czy9g+Yqc0l/7ZKXhn1Y6qZzfWBy6efYNR3vxBZbJdzww4kqS14tirKvGYXFEvNph35ObHLEC1Fw7Rcwie/ehhsJhR9f1mWeqQMCI0G0eyBr/WUziV0mWOqXeCwL1tDZvnNPZ4ISmNKOGN2FUzgOrkW4U06ron8FSYdGqVEGXOphkVOfiGsLCYyeB2qmFY0Xux0985lY93kjB24rI7MKwcV+cN09oxLQ3ZlhcxjOSYbm/8+fkZZtdga4qX4ok6UdRsQOIxyNXXdVTKqi+z4EFE9S+NTVrmOh0Nq++mY5JMl0VLYquiqNUj8/QA7HIguCO+ZeMnXg2WwTgkGZITxbY8sD6KIBCzsA8NyRa+taR5Q86y3cHNLzImw+R0jNfZoOlRvr9Z9tCq3gApG/ZBSSLbsWVKhr6g7WIWht4TB0tTur8m5CNrq77xodh3A6GoDOedw20sbiv4nWoT+E1Wnb1DwowqJTZcWIZLYWM8V3aFs4lz32N8wr2lmXMKqb6ZOvbB8+Kw5vV+wWHfYg7qeDw7AzVWJbxCYvGt4A4eO1n6R4awEqZVJR+VzXfCTKzrmKIrTXmeNNTrXpV2RubMol6rijg0RTegBoFcqbCGW3+zMjljl8mPR7rFxOePrmltoLMTr56u2AwNrx6dI3tL+8LOGzC2+h5trpYcDg3Ptys6H+bUegwWfCKsMsOlMD5qaYzgDgMyjOTxgete+gE4pwGg9gRCwu4D3gnNTTF09YYIhZ1Z9CydYj90EhRmOb26+skx9g3jptEyMgrmbKLpAl88vmLlMouZHn87ylbZe2cSl62ncZGrsyX785Z+645Q43vWRxEEchb66G+N5moQqNTLIVimyZEz8ygtZYFGU/APCQRHyXBz2x8gqkS4mqDqxk4FWVfprqEVXHtsIJkR3E4YNy0vBFI0hN7hrq3CjEsT0Y6ZZpuwfcYdAjLVN7EhG4u/MYRzz3bVsl23OBNVhELMGyXBPEZNjpAs20ldjzdF9CIOFlvIPmoNlme5bskZ0084IzSdEL3uutgIyWTddDYXemx+84Su17dMQ+SkHNBLVRsMAiHPSEMpY1U71lFa+R1bsm5/pw9gsz7vNtF2E2fLnmeLLQs7sbATnQ3cNB3D5Dm0DWNosUPB1QtKiOotU1SaeO9iIVOhLL5Y7OA8xIUhDg7bNqWhn3UU+C7s/YnwiQlJpdX3Cmuv04wchdwUf4es+a1ILB/LaDfqKb7ftaSDw710mKjKS0MQ+pVlu26wkmZvjDcb4gkjsEBTzHUzEhaKEZmEY4/qnvVRBIGYhat+Mc/oUxbGaAnRsO8bVf09OGQwkIShSWy7yO5MxSQuu4NOEkx8ZyCoAaDiEPahYTu2s7BIysXjvdZvaMMsdpmwFvonBkng94KZMs02s/ylwR50Lm0SLHqhfQntTaLZJvxNxA4RuxtV/rlKZ4lg9x126IhNS2osB7vk2+WakAxtDQTmzdcQsmUXWvroeH5Q1ZvtppuvU3U6Cgs1ughLi99aGNXoQ4KKdEhsGQdDWBmFZC+0Q22aqEafJ3/3lI1obRFrLdOQnCt/Xki9U4INRmUbCsnFDuB2Gb9XwguiuvnTuiTJUrOCXJ6LwmY/udjyyXLDby5f0JrA0g4MrRJ8zn3PN4cz/tg+Ztp4HRsW2TK5tmRjydYRXC4jUg1MFfEZWxjOLFkE2y8xW4cxBhlG7Q3UYHC8+MfgYKGKdpj9iEuZ5XODnSxmVMar0tGLu3CbiJ2qDrtC5Z4i9IeGMFjctw3tVlj9ImMnPTi2P7QMjwyv1ivymai5itN68xQoVqcIjQmKemxLsDCJXdeoXsYD66MIAikbdkNz1PKPqtGWJgNbh90bmr1SiNXkwhDOLIfR8Kpw5ZduxOR3TwxqCVAzgEPw9MGphmEy8w1dPwogLpEWCXXB0rw17IT2OmFC1mnBTt9o9YnLtDcRt4u4fcDup3nzS4jc0tk3Bucti5eOaeXIxvF8dcb+zONMYu0Gznx/CxA1JMch+lmk4vV2yTh4Vb8tPHXhuKmSV1eg7JQFKTEhY8CK4DpHckVROQM+YReBtp20ScXtoFpPr9mwRY6ej4C+d1nIWHJSr0OhXpcSCA5pFm6V1qivICXbcplcKOJ5EVksRy67A4+aA2s74E1Q+3CjAp6XzYE+OtpuIvROR4gTVJUmSeipHwq8toxEZyMYA9MasjGYqcMvHHbnMXsl4kg/QEw6IqxZQdU1iBHJuhUl6JTC30xFPzHjekNsYTpTL8DYqTNTajK7tmy9JNitpd0Li2+0ibv+clT7sJBJrkOC4fqzlo2L9GtHawP+gVKxemjWrLrK9r1tfSRBAA6DJ0VDDEYpvL3FDAZ/bdR6eQt+p7VeWMA4Ctk4+nXDrpkIC4OTo0jE/X9H5iZgLQFqAAiFcXdXylxMwmDIi0ASq6PmqHbS7iC4Xjvv7RXaNAoaGNxuUvjmEHTcVFVe7gg8yDhhdobmyrJYK+R1vGzZRsOrbiC0iv7TWlDdcuoU4Gbo2A0qjlotu02ZRMzSV7Un4DPJGqwx+vdDRFLG9h7XqGY9WRCnSLWzxfBeY1iVaz8GAVNGpFPZeEenJYp/nkqN2yHNohyS5GheUghiucm4LrDqRs59z7k70JmpSInrZMjkxMoOLF1H6yf2ri3TCpm59/N1rr0GOSFCZX1+YVFk34MheY/3BufUT9FYo6QlrSPIMakUOmjdXqTjczFNcVtRGfXBYkcFIZnRFNUkHW+mJutMP2tg9BvBbzLrryLNTaD58lqZkSmzWD0lG8dmZxmWVW7PHlGED6yQjtD3dzXTP4ogkJNoyj9Y1fjrDc2V4HpoXxff9T5jJu0WSzKkRjvtU1D76hr5HnqxtQzoo5ubgENB1U3R3hsAoNw8JuGaTDSZ5DO9ONxKsKOhucn4XcRtJuxhQsZwNNG4K+90n8xTgXb6nFkBzcaTxTNetHzZP+Xr85GzVc/FQtFjAEN0bIaGza4jDI68c8hkynjqZORWN1YVoTzNCAsf3wwB21vscGweVUBW68KDVt3zdS2IygpiGYMt7F5DjEJy2iycm4ynmbVXc5LxTJjOYDpLpGWhzi71dT9d7rj0B9ZuKOOxd4xM8/Hjrc0ueQbt3RpJWj1UVArdMIxgJ4PtPSZkmu0Ct4/46wFzvUcOA3kYyUk3/Yz6q0Yyw4QNCdNbzBDJ3uAOyjQMCyG0MjdCTWAeb/pNpP16o8CkV1dg1ardbS/wK4vpDWE0s93dg+9HViu8MSlmJJb35m3v4kcRBAA9/SeD6XUU53d6gdyhjLiCdq91zi3ERtFvxhV3IZNu3SR3bahStsQCSlIUYPEKfCADuO1pAJniLkQktYaAYVppIEqu3ABTRPrxzVP/bRpvSUkeMky47QgidK/UfissLWFquZ4MY7CqouwiU9DgFQanGcBkbnXc31gnXf55clVqWwkJMyXt2kdUyblWKuS3niIpF68/Uz5PBm8Tk0066rJKda0UYfVAEGXbZUPoysZYQFxk0jIhi4D1ibYNLHwoHpDvz42/9Zrvfuk0KJx+WrxiKTJxKarKkUS1+PKdZgpNzCrcdFfPYH6wAjSSBBHMpIJmdkx6NUuiOhO9QsbvE81NwG1GZHcg73vSMCDOIV17fD+FWSbOyMNI2YTMGJf7THvuWx9HEMgCo8HuDM214LfQvSqbf9IZcOi0gRQ6YXgM03kiPxl5ernlk9WWC98XlJ2msJY0j4SUmWgI0XIIpQ8wKmEphEq1PXk691w4ERCbEAOsIHnL8NiTjeB3Fts77MEiu8Lcih9w48YII5jrHX4/cB7WxKWju/IM54bxomN41HJYZPIqzHN4GQwmlCbXPY38u5f4viVTxPQTzbYjLgTZW4aFZ1xYWhfeKtJy5FqoSavxWbOqfBRmiU2RKWs1S5kmNDtIMJ6rNFf/WYTzieVqoGsmvE0s/MTKj3Ql+4mllzMBSB2J3T4Vpcqv1aAnZeOfcjnuuzYlW6rOZDo1KL2PBG5naa8M67Wlfd3gf8lRuOOUR6AXhQIAUEi5iAaGmHUyEvT5SNR72+8CdjtiNj15tyeXhiTOgXPEpWNaCeEs0i4m1n4osPL4Bq4mVuJdgbpP1SvzHYHgewUBEfnvA/8tNF79R6gN2WfA3wWeoHbl/42c8/jgg0BBgWk9W+fIyULuhHR2PC2m80zsEvFRwC9HHp/v+Xx9zZN2z6XfF3WYeHR9KVnBPjUYybPUc51AvM3H8OHXzCzhHbtMXMC0EuLCkBqLMW/vSzx8DfJsxulueszkWQi4g8Pv9dqEhTA+cjOxxgSoFmf3vs35+K8+pVv7eSbfJFyfcDvBbQ3TwrFfesWukzH24YB2SrrK+Qjs8jYyWUv0SeXKutqkEEJ5LuM5hFWC84luObLqxlngtC1q0aD17WSUZu6BRJ79AZxJtEazhq2PjE3SUnGqaXp5/bw9EEDNWEpm0xz1HNX4U5BkCZ1hHc4x/YTZDrd7PHXze0v2luQM2RffwVul2PH9yEZm6rNyEgzZWqRrkeWC8cIxXBrM+cDl+sCjZs/CFgWu8nDpVp13e1VDnrdlBN85CIjID4H/LvBnc84HEfl7wH8V+JeA/2nO+e+KyP8c+GvAv/7WB8sgk2B7Nf0wU54v/Hihm386j7jLkeVy4NOzLRfNgSftjid+x9KqNqER9aBTAUltICUMm7gAVPO9/LkSBN6dAdy3jGRwmdhFwuiY1uoH51tHdlbNNT90VYXYZJDtHrs3dPuRZtXSrjx2aBjXyq2PrQJi6nowANRrWxB69x7oMSKj4LeR2Aj+xhJWlr73TN2At+++JnOmYBIkVAvBRZomEFpFOSjeX9PtymacLiKyiJydH1h3w0yhrimvM3G2AVcEp9FM7+QVe4m0NrBuBm7altBZ8qhpuDkxfNUQ9I6MoIq32hIEisV4WBri2oBYwlIwU4vfeRprMENQ3kFMYETVfpweCKm1JK+IvVxESCXnIiar/59s6Zt4i3GOHEIpBTrSakF/aRgeweXFjh+sNjxrVI6/TtJOT/m7wUCvI7xLHPL7lgMOWIjIhGqqfAX8F4H/evn+3wH+R7wzCKilk5lUo003fyYsE/LJwHLV88PVns+WN5z5ft74S6PKsbrxRxqJLM3AygxzIEjZcJM6LuwFrQQ2U4dIZj80BMwHZe11ickYErFJpC5p7b7QGt47C8FCRYKJMDvC1Bnz2/5oSjpbF9XNMyHiB0/XGEywTGurSLuSwr6tBpg3f+nKm7EIUZ5EP4mJTMDfjGQrdK8McWEYFi37xag27nfw6g8tIxlMwhPpSnYQoyE4PbVSQcFRnJHa9UDbBB4tDzr2snGmQp/SoudLUwBeqXwtIniJrOzAJ4sNOQuvXOTKZsJgS8NUAURVon42fT1dbwkMxidoIqygt0rZDZ2l2VgWzy3tdcRvJkw/QYLsSwBoLLE1OgGZy4yTXoQFUGRhai2SPaaAlUiZvGiJZy3DpTA8Svyp82u+WF7x1G/Yx5ah+sxzLAVMoZ43CJ2bsMniTfqTmw7knL8Ukf8J8MfAAfi/o+n/Vc658hZ/Afzwvt8Xkd8FfhfAPno0K+vm0vAbH0dkFfjsyTVPFzs+XdzwrNmyNCMXbl9spgKGRCORzkx0MrE0A+cy0EqkKfbSnQQmLH1u5rmys0kJSfJh5cDxBRTasNOsJXp0Du+M+uW5QjUVmbHpM0YA3h0I9CLPe9wOEdtoA7A2Ad9me1YZeiae4NprMzDdSn8UNzBM2L3FFxFLGbQZOZXu8vuKpZgSNHwpIVoflBBTfCRzFGyTsC6yWgzF42BSGbVZJOUYdO4GArhtK2ZFy4FzN9C3BwWCBctgPWNU1B7ZaPaQTtoDp9OD+15bPawlY13E2sRwpj2OcfJaHgRBkqoiu5T1lLdaAlSEpvYo5NZjAmREvR5SaZY6U7LIwkkwNaOA3CoOZmFVJn0QPx/sKSu8/Xg99Ho1JaBGMbeEaO9b36cceAT8ZeCnwBXwvwf+xff9/Zzz3wb+NkD3xY9yFk37w0XErCZ+/Olrni62/OmzbzizakjSmUltpCtqrZhOeglcmj0rmbgwEysjtGKw5eX1JrA0L7k0e/apYWFGrscFu7FhE43Kan+HQCA2F7efTOy0JGiXDcaoVVZunOrAeQMJtTSr0k/9+GHNw/mPHjvtyO0k71Y8SAWgM6gFudsnzBjfyATq57IfNK174eceR3/RYG1i6fWU4z3p2xW96SQRW8FZhzF5nsK0PuCdms9WBaVTlSQjRxp5YwILO85+AW/8LTKdmXjSbGnNxKNmz5nv2U0t33Zr9r3qQKSDvT1GLZwGvQbHx6sGK1UPwNiEc4nWTzROS4ND19DvPHGpBjTT0rD0iiDVx5Dbmdp9gVo0ECQH4hUzkDuv78XgqNb1EoCg3JpDbNikTmXRkyUke4t9C8xEKWfi3Dh913v2fcqB/xLwhznn5wAi8n8A/gvApYi4kg18AXz5rgfKUphii4S/GLg4O/DF+orHzY7HxYykAkUqh/42rz7TSaCVOAcAjxpX1HUmgckMPLZbtr5j4SaG4ArsNb93P6CucsiDycXNV4PAdN5gFsXncKFlQnIKUPEbh986HBx14e8GgrvulCJgDdFrh13LJWYTVExmdk4qKDntA1SsfqW4Jgjpti9dhb/mjBCgN7hdwO0d9iAMg2VsHVO0YLWePV3vJe1eGlPmhGfgnbLidJNHmpk4pt835JMAMNGaUN77NzMCW1IiQ54NP0NjaWxkTBZnE1uTGazXXoFV3oCZmP0W6jU7gqvybOZqi6S7t4lkMiYlUisMWQiDYTpYTBBcb8u1Tnryl82f776f96xj0NFJQ64YjjHi9spNeXFY0Tn1S6g6nPctzaLQAHfCNXjb+j5B4I+B/5yILNFy4C8B/y7w/wT+ZXRC8FeBv//ORxJIq8jyyZ4fPbriR6srfrp4UTKAnd5ID/DqTTGe7iTSSaIVSycOdyIW6sSW7mzgB+6aKTvOfa+gm75VR6LvIpNThCNSkwkrGIOwD65Am0VHOyvdtGToXgrdlWUJ+CkiIZJHPd3E3tPhLaVE9nbWygtLCMtMXOSjaEXMCoMNAoU4pIYUKObikHCHiBnDEbJcA0Aq/PlJywJ37WlvPOO147C3jI1nXJ9ey1SeWtmwJ7z2e9+fAlttXJxRt50Ls4DKqX7i8T3NhSg0zr2f2vitqzbBZmxIUiu6tgiyXibDyo5cdx3XiwWv9gv60XPYteTJEEdT/BZO+gXCbCcnXcQ0EecizkaVuAOw4K0qQO+AITVkqxmGOwjN9k4f7l231RwsNGBIHSkOE2bb012tCEvDt1drAC78YYaQ117A6ZqzZPuwktDd9X16Av9ARP4tVKogAP8Bmt7/X4C/KyL/4/K1/+U7H8xk7Hri88sbfrx+xY+61zx221v20qDsuZiFPjfzTRcxJHP0nrcU4srdP4HBi9CUqYFq930/dSIjqgmXFonhcRnhnRsQiotOQcCV0zqsbDHcbLCHFtuPkGK5nonKWZeaZohA44mrlv6RpX8s9J9E8iLh1+Ncy4bCs5C9IstMPAaAZpu0FOin28405WMuMt2EQI4R2Tf4bVTsw0GbYMPk5rFqLI0mZxIxM49h766cFbRSdRlSLoI8xX5+4SYufD+zJc1JiWckz6d/zQDr5GfugEvCwCxTP2cEElmakUksRnYs7KT1dMn8XncLhsnRD+plkUJRgaqbqYx/fRtmtqo1CW+Owc9IRlp1lN4GYTQee7AKMMpG3YCm/H6T4tPpTYzH92IUZAvtVWRaCNvrltc+8nq9ZFXUp06XPcmiPnR9r+lAzvlvAH/jzpf/APjPftADCXTdxKcLHYFUP8K7qKgpW8biE6cNEXPyPTML4yTSLdnwP7FVWXStnnKpFcJadMzURewisuxGjMmEYOlZYIJjfF0wBdaUdLwEAAskowahRhuMuXGkzjKtYDoDLia65cTF6jDLY28OLePgCUHIkx4tZkLVbvqMPURkOCEunU4pShDIkyoPST+qtt6QddYehCnaed5MOioZi+R5/n335gulHj3lFdTg4W2kcxMrN+iIzxzLgRpUWgn4kzLAnDQNUzbYghWoH0EDQcxmxojomDjSmonWRA7R07rAIXg2Tcth9MobmewMrTWifYC2URPc6vdgzbFn4UzClobpuHCMWZjOtEkT9wDadzD5nknE3VsoMTdrJWo2lgt4LKeM20eancUcLEPfsJ1atc57Hwj1e66PAjEoJnO26Pmsu+ap33BmDrcygD57xuz4djrnOiz4w90T9kEtyz9dbHjabvHnmuovZcvSZDphzggSiX2euE6Zl3HNi3DOzbhgPzUz7v29n+tp2moAG/FNwKzzfJM7m1g1I63VtBdgCI6fS6YPK5prS1g6nHdHumpK4L2CRbJDRMjeEc5aDs8adl/A+MnEn/mNr3nabfmiu5p1Bb48XPKyX/HH/hF9XpBHqyIm+0xzPWG3IzKMRTcv6UmTEoyT/u1CjMFGGEadRAwZMxpkMsq/R093Y9J8os+nIrf7A9XCLWUp2gxgCx9h3QxcNgcu/YEnfjdv2BnufbLh6+dvlIL17coFlkt+o1g05YhtTcCXbCNieNx4ZWEu1WdwTI7d1KhQzYmOnz/JeGrGWA1qtOkmrLyjsZFt1/A6nRHWFrD4HSpGe0CRgff1fzPzxrdj0qbxYYB+UF8DY5CccNuRZmPxN46h89wMHY0p/ZQ7QeCuIvXpehug6KMIAnUd5aBMoe3q57vUso0dP+8f8XJY8QevnzAGRwiG4dzRR8c3i0saieztFpsTEPAlG5iI7HNmk7wGgWnNLjT0wanrbnq/IHAaAGaLMKOnRFNqx85pTboq7shNAbwYMq0PHHxSKK0vyDCAlMgxISZqaWrLSe101jwthWmd8WcDP1xe82l7w2fNlWZDJzJsrxZL+qYlG1tUjyi6Adp/qM49ufQB1LqrMONi1FIka7Yg5RST2j5IQjQCSTdeTe/nQHBybfIJIrP2AapydGdrs2+6dWKfbnz9+fr/b97UpwKrNTOoH9/4WUlYlBRVO+maISRaExmSxUmc5bxzIZpV3kRlUtbNXycXCVXxCSUz2q1aRhqmc80IJOh41pYT/S5eR4rhrCSQqeA3YiQHdVWSonEoU8RMKpkuo8z6l1OyczbwLsXtSi9+aH0UQUARs6qRt48tnWizLKLWZN+O5zwfz/gPX3zO1WZB+sVyNif98pOW148WfL64YcqWS7Pnmd1xYbRRCNDnzPPY8GW45Pf2P+Dnh0d8u1kzBcs0ulks422N3PsCgHOqerxsFOc+N7vKCXTaNDNkumZC2kRsITUCRUA156ybMCeFjoqQnSNbS1hrL8A+O/DTZ6/4T5/9Mc/cDU+sIsem7Li0ex65Pfvgud52xMYhqegLHiak9APyNJ3U/0lPnBj1b+sTnd+QuVmWISdDSllHqeU1SdFagLczN41kjEnzNGDtB1ZO8e+nAeB2BvDuNNfWtn5h5cR8u/y7e9OfBgMMJAampM7XB+9nodYp2bmUqa9NT9giWsOxOZqy0BidQohkbpYtrzgntq5gQ0T7Mglt3ubjhEUChXIcVfZtO5D3PQyDvg+2OFOPQaXLtg3hzLA9tKzbgbUfyvUt78GdcqwGxISyCt/GH/goggBReHW14p+unrFZdDz2K0B7ALvQ8k1/xov9iuffXCAbx+Jl2TwWpqDp6k1oeRVWfBvPiAh9PrCUQELYZccvwyP+YPiEf7L9hK+3Z2yulqoGnATxCSn6b6fBQO7c3PXrxhSduOKJsPATrVPG2+wQdOd39SbSv5N8VjmvzmHrps+JnAyClgcSo96sTkgNtN3Eo27PmTnMiEgdlWYu7Z7eeX6w2PCz5WNeL1pS44leCn8+K3MnqG99Lk3APAUNPKejP2vBFKjrPWCkuRfA8WC726Ger1e5Vt4e5dRrH6DW/hG5p4374cuW9D++Je2Fk0ZmNsV+LiuLMIc5I6ibZha7nbOT21lK3YDeREK7x5nEcOnZSccgHhO00WvHrA3rmIoOYkaCEohMHzGHSbUNU8kCipCplJ6RCUnZtHth7FUDY4yWVYl75k72VNeUDSFZdrFhjA/3yD6KICBRyK8bvlxfcAie180S0ObSdmx5uVuy37a45x6/Fdor1cwLC5Ag5GTYh4ab0PE8nCvbzDqWZgBgkzp+Pj7hZ/0Tvtqc8/pmiVz5WVkmt6oDJ63etNxJb28911ICGFF12LaccJ2d6Kw2oB5atgSPXHgRyVtsQYcBkBM5UhRrMogottzDqpm48AfObV8s3DOWCBI5MwdGZ3nWbDjvBq6Xgdh4HU3Wh47pGACmcMw8bvn4CWKMotes/m4uWc9plnTaQ3mon1KzK1vKpYVTXcC2eCr4k55PqhzbktZ/6LqvJ/A+S0/4Y0BSMlRgMkcgTszy1nTb2kDMwkXT09g4i+HugGnbIEnwG8HErH2AqGWAHRMyJexhgmHUf7E0b6u5iXNUurfrVZUpDfaoSn0Sod8QHi0Q6yE5bsaOIX7kvgO2h4vfs+yvLvn58oI/asv8O6NSWUGwAWyvELn+qRCWmXCeaD/Z8+Rsx5nXDf9iOmMotX89Lfep4Wf9U/5495hXr1fkVy2rX5h5hDOdG8IiEy6F2CR8p7p+Kkx52gjM5WSL+IKkqylutQ17aBkpG2Exsjtr6S8tfu9pznT+Sz8cN2WM6qoTUoH6nszluZ06A3RMnJue32he8tsXl4Rk+PrzluwM3cslnQj2MJCnsfQfTgJVxU2bcuM1auI5LQyxycVo9c2g+K5mqohy3n3pk1SZtFoGAMdTu6TziayWC/JhJcHbml4P/u7pe1V+vyuIxJgNk9GTs/pjwhELcR9Rh6Lt92yxxUjmtQu8Hi5IXhWnJal0mBkTdkqYQ1AE5/agQiXF4Ui8K2rGFnFOz/ioCtVur6I7w6QArio4el/gjFnYhYarccFXN+f0o3/jZ+r6KIKARNXpIxvCsogynqSiFSabfCZ3OoNPS0UXPjnb8clyw8Jqo2nKln1qmLJlEE9E2Md2FuVMk8WOgtsrqUaRVQoaiUt1bMmtkjGMSbdKAIEZPeaM2oQ1BR77PnZodTyW2ziDf/KyU+1BazULyBU3cGzOkd++6RQxOXFmDzxrNrxeLfjy8jHD6BkuLXZoMTcd9D3YgNikWceJ2asUVx2sLQrFzL4C5ntYr9fJQWs1SHq57aRcA4EpPA9TYXvvGQh+FauOFufndbqpjI76UhaQh1V9vCQwsHIjo3fEznC1DMRRDU/TQcFAJmopUJuBUrUncj7iRIqWgNKLj8rWiiWQuXn50KpaC2OyqtY9eMb+Iw8Cdkic/96G1corA8sLqVGYbP/I0D8RhseZ9HnPcjXww4trHrd7PuuuOXc9nZnok1f5sOSVrVZWRAErQ7IFuqrgHTuokUhzHRl3lnEl2ok/E+RMnWBaH3D2ZDwkR39Eb2KxDDtqGLwNeFQdhdftyO5spH/qkWRYfbLCi2A2W9IwkMek3eSiTlTf/LcpXxtJeAJP7JY/vfglj/yO658u+MXFJVfjGeNZw4W5pLMGc7Mjb7a3rLUARPTmy60nFfHP1GYVHrVpboZ+KLy6BksnaQZo1VO4duFv4T3gVkYA/8kEg7uZwanxS0KYsDOe46H3wkvC2gnaPc5Eri86NnbB+Gqhjk9XygdQ27QRGbQUyDUTa7QEyK0/4kRaT27t8VA8ORB0UhHfGM8CDEWK/vrQMdy0yP4j7wmQEmbXI1MkewtOMfjZWmIrjOeZ+MnIjz99xQ9X1/z26lsu3J7HdjufwK/Cmn1qigKN3ErhIgYvSRt3i4lhbZnOtSfgeiEVoocKXarO/bobWJdZ/10mW2W5NSbq2OgdAeB0WZNo24ntKjGeW4ZHHjN2+OUCqaAdI5qaW6PiKlbn7A9evgKe8RI5tz0Jw2+evcCQ+cefdYDHTA0mnuEbp1dmGLQOLde/lgLZW2JTWHCNSo83Ls7XUrEt7x8IKtKw/gM9/e8LBLebhfKdewRay99O2R9CNt63amZw/L2TLCEnktxfFgCzh2Zn1ZFqDJZx2anIaKtsQVM6irm8z+J0G+ZFC86SFr40Z5VmHFvDtDKEBeSm8BlOXs+tIFmCVAXThWTUFCZ87NOBlJDNDjlRWEnugrCwai39OPLjz1/yL/7gH/Gfar/ht5tv8SQaSWySZ5c11fGxY58a0omhaF21dr9cH3gVDcMjZWq5Xm/45ApuvEk8Wh543O151m1Z2PHWqK8aftSb9303fyzjstYGls3E9jwwJuHwyGDGRoPANKknARwFKqzy0a25XXLcOxMncWn2dDLxz61/wRO/Y/fjhl+2j8imwYSWZWtZ9JPWnmKYLbesQZqG5DUTiw3kNtK0gbYYvYRoyB8ArjpVcKrpa7zHXu1uRjDP/Euz8KENrGApUz4/2tjF++bitzb1u9fdQKAB8P1QqHWUeNkdiFn4+mzFdHBMC6FpDbbXCQzGgFchGpwlrTtSYwkrNzMRY2vUNv1cCEvITZp1F6A2VR+SH1cPDYknSkv3rI8jCGTIIUKekOShgdRaxjPD4ZPE5Wc3/Oef/QF/Yfn7/Mjd8NgYjAgWx1ImNjmwSTo3fxUCUcwtYIYl4SSysBOfrLY0NvKL33CMTxzDE6MBoM20X2x5erbjLzz7GZ80Nzy2OzozzeCUClp6FVZF+7+5ReJ4qEE1i2GUQNDYiF9MjEEYnjTYybJ4tMJOQfsDziGrJePjJf2lMF4mHnUHLt3+nTdxleP+oX/NygwMzxz/sBn4w8UTrsyK4ZFH0iX+asA9F7WsDkHT0K4hLjzTUolP7fnA03PtufTRs58admODCiq/vRlXR4kxy6x82xhXauv7r9FDuPe7rNE3v38MMLUnVAPQXMLVKcAHBgODgoyMZB54evf+Ti0Xh8Zhziam3jA8MrjBzoSh2vRNnSW2lvHcEhuYlpWKLDNjdLzIhFXGdKH4FdYSN+FOro9FMxVfytTWBw5NInYfexAgzydSttoRr6q0aRV4vNrzk+4Fn7sNT61lKc0sMRVNJiZVFNqlFm8CNr/5siop5dz3GDLXlx2HzjO2DdIknI/86NEVn6+u+a3uW565DU/sFi9aN0/ZscsNV0YxDNdxwVC6Z/EtTRqoasfHzMSahPeR0EZCVySvO4/pGqRt1PJ62SnhaCWkpWYxSzvOiLqKm5//xqnQBokzc8CQ+M3Fc6ZsidnwT193kC39N3p97K6DYUQmnUvXyYDaaGXW3dH4YxdjkbOu6WZJad8m14UGi5hMIRMpOCch956p9eunWUAlCj0UCOIcYM0tKbIqLutMxOZUsP8n+o8nAfu9A4Lkd4p2nq6qieB8ZOwSsVP0pwm2gLEy2ah2Q2j1tFdp8tIML+KnyWkASIuELXTsej2DJNIdu7pavjZW1anFJ3LzsQeBlMn9wKykI+qUEz2YVeDZYstv+JdcGlhKg5fjLeSzZSXaHe8K5dRLvJXCg24MK/C42amZRXOYT+7HzY5z1/Onuq944rb8xL2mk0h7ct2mDPts2Zid/l2J7GMzB4KHX5rMN2g44YE3LhBaw3SWGQahf9rQ5TOcs8Rlw3TZcf1Tz+6HicefXfPT1Us+81fzSQ+8Nfh4iZybnp80z1mZgR80NwD84ok2C7vnnnN7TnMzYfYTGIgLT//E0z8WpkeBn1y+4rfWL/hh+5rXYcXz5oxf7i/YuJbN0M4eevlEtBUqU1mJTAOOnIWbsQPQEaFTvMR9m2/K9naXvsiJ6fTg9nU+zbxiNvTJc4iebWxnkw5lH1bh0uO9UT9qqm8fzEK+z6pZSNtOTEvHeKnNp2kt2CcK5MpOXZSjh7BWkZrYFry2Qc1UBKWNO71ew+S5EQ0yC6fl4102Zmt1LPt4YTlcePqmefB5fhxBoMzGc87aHCsWTJIgF8nkGSP/BhxUsQMKnsnz6WhJ99ZwNUg0jQJWlnbkqdtyYXf8pHnBuQycmYgHvJyerplEZJJQ9AvD/CbXdX8Tq4hjnmwSU0E0NjH5TGpUkMSeechLwplnuLD0jyFdBp6tVFxlZYY3hDUeWnWG3jFxZnqe+g1frK4IyfBHz5ZksbjeERcGt9eeSlgYDk9U39Gdjzzrtjz1W23Alr87RIczUQk3Vh2cQ7Rzw/Bo41YyNYExQB/UVu3gPK2ZbjEG37XiPU3CGRZ7TyYWkp3LECOGUFN6XJlQ3NYxPA0MM3Donuv8IVnA3Z+XYlwTFjoqjN3RdSm2xXqtmMFmX4OgBoL6UUT3QwiGXhxb1xBKttPZoHbz5jiCrV9fNNOD4Df4SIJAJpNKQyyLdkzNGLFTJo9qGTZlR8yZRH5ja2t/IOMllH8Po/acUVbahTuwtj0/cNf8wF1xaQYuTKQRwaM9h9PlBUiRaAKdGWeee7gH9HrawLNFkx+DwlLlSKs1JhehUmG4FBCVqxouDMOl0H8x8ezTa/7cxVd85l9zaff3s+oeWDUQXNodnRn5588anjZbtr/Z8vJyTexa2tcWt9MrGjvoP8mkL3p+89OX/M7yG75oXvKJ3bArgbKVwOuwxEhmO7UIHYOos+4Y1GNAZcSALMQoGGO4MW3B2oeZiHO66e6uUyDRQ3nW6eav/nvKNzGMydHH42y8L4i506BddQ3b8pwq1sTfM3arjM272ch9q/aJqp5CLn2Q3GTCWq8zkoste1ExMuhJL4B5mMeSoxq/xqBkp73VgLz0I0s3cuaHmVjkJbH2A0+Xu48fMahElSJyES2EgO0Dfp8xW8fz/Yovp0dsmm84y9OtbCDmTMrFlYUa1dODjRyLzq2XZuTS7nlit1waPf07EcV4v4VJZMl44nyzTMZiy+A4ZilmHWm+kYxkVlYNULwkdjGynVqG4BSV5rVWHC8Ur2/HrOn4RWb1dM8XZ1f8oL3m3PZ4CR88M5+JNsDn/jWdTFw9W/Ll6oKfLR+z3bTQq4IxPrN4dOA3H7/mz118xRfNS57YLUsz4LPqAlrJPHYdXiJXYcnX9oxX/YrN0BCCut/eEukQFWTdFV2BirkAZp+ItwVt0BIhkW+976cZwFFlKBf9ABUfqU29kAxj1k1zCJ4pGYbJFY2EPCM/L9oDKzeqqpEZb2V67xsAgNnpah8a9lPDODoVWTUZPEfrd8ma5htUeObu5pc8X8caVMlCjpBENRsHkxmDpWsaVs1E33q6IqRS0YRrP9DZN/UZ6/o4ggAcMexZaa7SB2yfsAfHtm95MZ2xSZ7eTLSnUZpE5PTkVVjt21JN1aNTdeIz07MUDQBe3kU+UYitst9C+VtpJp7Ui+7leKrU5zGdsNxCMnjb4GxCfCK1mg0kK5ggjJcquPqj8w2fL2545jazhPp3WXNGYPZ0fmJaOz5tb7hsDnxzecZu1G56YyOfr7X/8Fvdt3xiN6rvKJEGnX1bSVxaT8SwtCPTfLN7DQCjJffltda61mQC0AMbG1n6lsZEzl2Pyfmtr+vIhgPy/SO601Geqe+B0cMgJDc3JmMybPpWsfej3voCHBaj9miyYfSOyQ8Yn+em4u3nI7wNplwnQVM2TMXuLkajZDXRkx/QTV9r/crNeNu4OYtehGI2A0IKhmQzKQohWEJUQFxwZs5yVMh1IpmHn/PHEwTKyinDFLC7A811R/fCs3mx4j96+jl/ZvEllm9JrsegUmJ9Lpr2WaG+nShXvU2eSSxRzEzqgKN6TSejpvUS8cJ7xXiDlgUrGZlMz4U96HwbQyoq60s70krgwu1Ls7Li0YVrt+LaL3gxred6cbtoGYAxCUTFmOcnI+uznt+5+JYfL15waffzlOK7rEo2akxkxcCqG/jN1vNnFr9kEzv6irOQyKVVpuKl3XMug2Y25Dmb8HbHlA2+ibyyawyZl8OSl3lFvG6wO4Pf6s0517lOT60pCZvcYU1mSladg+w0u0edAogAUplq7FMzd/zr8qI07hpArKSZ/Xes+Y9aBzEahmjZHRpC7+FG6dYZ2C48NIndqmXZjZx3A/3CKeDHDfPjvWtV2u4uNGxDy+t+wX5oiIOFUMubcvLXzf82SHYWchFDJRgosOHZdRoKyM0y+szoGw69p20Dw9qxcBPnTa+8lrcQ2z66IAAoR7Po4dsBZDC8OKz55fSIlRlo5RWNJLxkYoH/Q0n1S5remWk2aEgPpPeW45tr31No1ACdBJZmYGkGIsXrsPz+2vZ0Ejgzh9kLAVQcpa4pW5ZuzT4EnItEb5haC0mbat1CEYsrO9CVPodu5O8On53pr+gG6rI+t0vrb52knUzlXzgGgJPli+T7ygz02bO0A0YyMQnmYHA7g98wA45S9QaMQg6GaC395PDWswktAAvr5ih8msFN2dInz9W00K7/1M6n8NIpmnNhp7nJF4rp7C60eqond28zL2cw6SgwKkFf5eQdvdESobUdwU1zbX3aULxvTWVMOSRLH536XU5OVZmiHG/SeuK/Q7+C0mQl6bWTKCoiOwujylwtSKwqdYZgPTkZtj6Qi96BcZn75Y10fXRBYIZRSvF5P2T8teGXLy/4d9c/4fVqxW7RsjIDZ6anKfV5rQ9XZphPjClbevwMbDm9IfQNrQCL91taLiQuzITPib1/zS61LM0414x185+bfp4i1LWUgVVxTdqWDXA9qCNSfW4CXKwOPOoOrK0GmvedCLxrNaS5d4IEIm9aRJ4GxvuWLb9/Zkam3LM0IyGpS3L7ytBeweJFKmpICnYBUX0CFGOwNy0hqDvOvoyuVm5gbYdZb3DKlm1s2Uwdv3/zlJu+5WazpLqBN+1E6xWa6+1RwSkkTcPrcjPrMyKSadtQDFLtrDasWmiiNN0MMaqR56KZCNkUmniYR3B31Xx0EqEjye3UcjUsuOlb9vv2dhYApfbn3QGgZgCTIv5k1OdqIpoRFGIZAKKIwCwQJ0NoLK+zcFio6e7jhfya9ASMxTQeWSyQtiFdrJjOGrJVzYBx5/n96ydsp5ZtaDl3Ovaqjatz0wOKmKs6ApPVOXHFkk8n2z1mmU/nD6m0LUInGYg8tlvl9ktiLOCUpRnUEakEgJqupmzozEQqaklrN7CNA96kmZxUtflU784yJFf0Fe2cUXzIOpXqshybpw9t8vedlVfw05jVJDRknQjMt3q5QSXNh978NZKq+444bvqWkAyNCXe48ZkhOa6nBd8czvjlywumTYN/4Wb79bHr6JvM1SKqQrDNR2mELIjJ2CbSLUYuFj3rZsC7yPmix5nEFojBkKK6UCk+pTR4g1XTkqD9hM6p16GKxsRbWUHNAPbB00fVANwNDfu+IfYOprKZS9efk7/zUA9gzgDCURbdFCWtU8Wn4y+U52/AjELOhrhzHDIz9T35jx0sJMqjlq5FVktFy120hJUlW1Qrb+d4/vqM3dBwCJ6L9sAn7Rmft1c8dluSvz5JZSesTfTZq3trckzUerHWndpdfhsl875lRPAIVlSialXS9T5rWt2U6UTDUTYLdFRoSCTRQHFhD+xdo2q2pqSHJQiEqKfZPjUMqYxHUSn171ISnGIobhGhvkOGUSHSeu0MQ9LT5lYQOL2kuVLfmINAnvSk3VkNAq07krTqVGVIjuux4/luTXjZ0b4yrL4sslwJwkJIrTCtTIHYHjvpSCY5CBeJfRQ6H7jsDrPPwdJrFlF5+VVnsjbwUhCm5AihdOB9ICTDyttZQao+37E4AW3KxOf60DH0njA4GM2caeQ5ADA/x3tXFm3+lRJCQrE7C+VX0smvnrxeSoZgUCxBFr1TDqZl59/eT/oogoBYi338iPzonOlywXTmOTx1hIUwnanMle2F+Kpls/Hsth3Oq5fdJ+stj9s9v7P+hsdux4+al3MgeGy39EabXvvUAh1DCQjPwxmgTT7DFkxidQ8+4L5lC7x1KRmDen31OcxlyF3RzNPlZ7DRcR6dMqRoSZMhB2FDxxQtf9w9wpJYl/HgygwfzLM/Sna/X3PrfZZKtjVcxSXfTOfzDHpaKwxWnXhRj8ZWHZpuBYYi7BqKivHGFxntYkkO6iD94rDm9c2S7htL9zxz8YcTZkyYKRFbS3ZCWBTTz5OMO7bCtBD2n6v/ZL/W2fmZG3ja7piS5bDw9NHNasNDUCOaYXRMoyONFkY4DJbBeXa+oWm0rOiaaR4dxmSYouEwNIRgCL0nTwaKGA5ZTjKAd1zYuQ+ggUAmtaSvdHLNru55kFpKZlTLMAG9aFPcODa+I8SPfToghrxeEi4WjJcN09oyrYXQHcUtJIEZRDdLFkZfRi9Z2E8NZ75nypYLu8NYHeFV2fKlGZgKn2BIjqmksSmbk7q9p7EZn3lnIDAipFxPeBWdsGTiLXrn2zdcDRQpiwJsopCDQNAx2yiZ63HBVbPkOi64tDuaXPsf784ITv/+3Af4Fa1YdBv67NmFdq7BU5uJCaYk8+mcXX67Jx8Qk8xgnynpYx2ipw+6GW0PfgduM2H6gBkD1luyFVzjlHEpzIX2dOYwwdKP5RSVPIOBVk5LxSFpAFBiU8feNnOZE4PKpBMFguh7UzwrJ2tnH4Y6dUjJaOAIQh6MNvKCzBv2fa/8jAVIutmlnO7UAPAeDySpkpPATKI9gsky2o8cLJQbx/Abj9h/6hnOhbAqJAoHqYIpstY7ZoLcW7I1pK3jauO5blfsRs/T5Z7hkeM32ld87l9zaXd4CUzZsUkLpmz5o/1jrscF27Fh3Yx8utjwz559yU/bb/lzzdecmcjZOzKClDORTIQynbgDZX7H217T6H3S+fowOfJgkdEgkxRUmOHr5oyUBVcANZPbUF2Y3ycjqOXIu5p9H7Kmgsi7SkuehzNejCsOk9NAtkhkL8SFzEw5bNZUuAJiisOP2EzT6nSk82Eu00IuLlOhXJfRqKFqn7CbQf0T+lGVkABb598iSsm1Bsya2JZspEtcLnp+0G34vL3iwu3n1zIkT588r8OSXWzp7Bnfyhk5C3G05KinMaUdE60lmsxoTzKbenJH3fQSZE7b9Xl94AWuWIBw0gdID2QA960sCFoeyCSY3hBa99Z3/51BQET+V8B/Bfg25/zPlK89Bv5N4CfAz4C/knN+LSIC/C3gXwL2wL+Sc/733/U3khf2n3gOTw3TCuIiqyVzPT3mGuj4UaJST5NoNrDZLkjJ8Eft44IcCzOgJyLsU8Pzcc0fXT/metsx3bRIk/jF6pKrZwu+XF9izzPP3A0/sls6gU4qEu32GxDJTDkzZBgLMGQ84bZz0gu4u2LpRfTJMybHGCwh6OlRdEOLoaXQHxqu3IIv/SWX/jBnLp0Z6ZjeGgh+Ve40978GYcqOKTn64EnJzGnvLQKnlFq4NsVcxviIKx5/XaNOv0s/FWfie65ZvQfkBMyRs0Zf0NmYGiAAluwt09oxnhnGy4w9H/lkueFZs+Gx23Jpj0GgN2pqU+3Nh6hQ4zFaet8QC8CnbkzJaNZRpYXqvVnux9oM/c6rpvVJjun/3Sbg+z5O9Y2IQFRg0UPrfTKB/w3wPwP+jZOv/XXg3845/00R+evl//8HwH8Z+O3y7y8A/3r5+NaVvLD9oWF4khVc0pTTo9ZAZTYq8STKZkqkFPKYmWi5nix/aJ8Aujmqm3GfPK+nFV8dLnjx9TnulefsGy01wqrhH25afv74ksWPJ36r+5ZV9zMuzQgkJRGdvAk1AExAny1TNuyynwUujCRslpn5dut1Fr57nxr2qeEQPcOknnhzAygKJmfV+t96NqK2zks3cug8a9tzng/zXDNxm1hz1635T2LV1zFkNX4J1dq90Fyz43buKswBwLeB1WJg4QMrPxZDkmlGuNXHh4LzFy0nVP3pJBjfslYXFaPxjtR59W18IqRPej5/fMNvrV/w0/ZbnrgtKzmORcdsmbCszMAmKstxKsjCbdsyJEiDQSibspzGcl+WeN/mv1sCvaUkmpt8FRn4XQNA/VO1iVhKmvx9EIM55/+3iPzkzpf/MvAXy+d/B/h/oUHgLwP/Rs45A/+OiFyKyGc556/e9jdiC7ufRszliPcBVyiTKQlhsjrKmSyE0jAJJ53TGpQHte96ZVaIZMbSrFrakU3s+A9f/5Df//oZyz9oWDzPnP18Ihtlzm1eNvRPPf/n+Of40aMfsv+04SfNC37kXnFmJjwJf4L36LMwZcMmNYwooKUuW7rmsQSD0404Ypmyjv0O0bML2kxKpQFkphLs0FNPoiHFhqve8h8nwy8XF+xCy+Nmx2fNNY/dtsB6pxkuXbEPKzMo2ep7IA3ftWI+oRHXG3YGw5QfKv9fTT7PVz1PlzvOm57ViWpTXRX5aSThXcR0VXNBSK3DThEOw/EXihpVXnZMT5f0jxtufmzoP0n81hfP+Z3zb/lzi1/wQ/+64EqOu7UCvRQ+3s5f95K4GVtuJLPvXQHknNwA9yUsNSadbPRcGIBKECr/3gYSOtn03ycA3H1Mqf2mB9Z37Ql8erKxvwY+LZ//EPj5yc/9onztjSAgIr8L/C6AfXKJe3Lg8uxA5wK+MKNCMhwmxxgU6x0mrdPypLppGGVVzfPTSUgHx03TYU3iUXtBYwKbqeOrm3Pi65buRWb1daT75UavUeuBJa43vH685mfR8I/PfgBQZvMbhRaf3Dw1A+izqhmf4g90I1RBDDg9IlI282x9ypYxOlI0J/NgHYfOaWHQNzBNws4uGEdH6wKbThl5U7asbT97N849AEnYrOjJjlDGi7/arKCiDO8dsd46/TQgGKvU6aWfuGwOPG72LKyeylM+6vxPyWI5GpdaF0kNKnfmLdkZNeWI5bo6S248adkwnnv6R4bhacI8Hfjt8+f85uI5P3DXXJoDXVE6rkCfmIUkEZP1uvVuS589U7acN5fqiuWTktpOXs5bVwkCt/shuRCF3oES/FWvfHILvoUG/b0bgznnLG8jKz/8e38btTLn8k9/kv/8j37B42Y/Cz+oSrBaQx2imkfugo5ydqNnDG6ex6bRIIPRLGFvGXLHN73nZq8pXr9vMF91nP9CuPz9gfbrLfzyW8gqsHm+fcLycgks2X96xv8j/Sl+/9lTfn75mN9Zfs2l3fGJ3cw03jq3r9iAW0v0prZkvbmqAEg2BSjkVACygGzCpA1Bv1XLKjMcEZ5VdCI5IbxqSW3DH75Y8AfLwD88/wFP1nvO257H7Y62iEqoA2/gs+aaM3vgJ/5FOenGX2kgOIX3Sj35T/s3d64JknE20bmJp+2WHzQ3PHZbIoZt7NjEjkP0xNxiCtx16ScW3cRunbG9MF54Gii+fSVbXHXE84bdZy2bLwyHzxLPfucFv3nxkn/h/Pd45m54Znd0ZVR6+zXo/xsynY3z+PbM9GzOO1p3yWavHoM5CFKBP6fz+ZMsvoqAZFcaoE3CWG2Cgs7vyRwzp1vXKHNELc2EQQ0a3+dtqyX1WxLC7xoEvqlpvoh8Bnxbvv4l8KOTn/uifO2ty5vIJ+2GtR1mUYSK8ltYp3p+rmEZGsZk2biOQ/BsbWRnWoK1pOQ1lZ4EJkNOwj5qP8HcONpXQvcq4W9GZN+TxlIbxoQ5DFhv8dsFfiX0m5bnyxVfdpecu57Ja8rYiN4odUMfT8OTQFDfsKJSe0v9Bin/jl/LUTDFB8FvswaCUG4ao+pK2em4JzUCyRD3DfvBMgyel+2Sb7u1SpYVp5/WBW6WC540Wi5cGiXo1JPw+y4jaS473vl4pUlFPoqOQuEpFHJV7TFM2eKTAqoqE7D1gc0iEZaqwUcGmbr5RB4vGqZzx/4TQ/8skz8Z+I3z1/zm6gWP7ZYz0yvf4R1QaHKiK1iMye65dHu2TUPTBKbRzZinitmvr+3Y+CwBoEngs6o0t2H2rqilbUpGlbS4JxDMF5i39w9+xeu7BoH/I/BXgb9ZPv79k6//d0Tk76INwet39QNAveh/1L26l1Ja5/kxG4Zc8NmxZRM6rseOF+2a7dBwzZLUO2SymF43i3mlRiPdC8Wzr74asC835O1eDR/qGkak97TXidgKh5eOK7/mnxSAzetuiZfIygwK2HnLqgFB+e7pje/NTa8C4GE0uJ2weJ7pXkeam4AZk/aHitx07FSDLjmhfU3RoXOElWNqMy/bfPsmbBJfPj3n6XKPeZr5cfuCRiLWHGi/Ix35vvXeMt66x4jRcAhq/glablXqdV1DsdcaklXWYzPy8mJkzA27Ty3TUogLtanLAv1jw3Ah7H4SWf1ww5/75Gv+4qPf43P/mh+4zaxK/dbXMZcIAVvUmn/SvcBI5meLJ0yTo7cek0vPZg70lEAAuU1kn7BnE103cbbouWgVojxEx3Zs2Bw6xtFqSIq8mTmVzCKbrHoDBf9wi2b8gepGc2nyll97nxHh/w5tAj4VkV8AfwPd/H9PRP4a8EfAXyk//n9Fx4P/FB0R/qvv9Twl0z3QwLISiKIcbZsTUcwsHtGYQMoGbyPD5BgMxKCzUTOB2wu2h+Ym43fq/Za7RoPssptny/FsSVx5hkvDtBayT+QkquU2dbQ2cB0XAHMT7vus2sFPWefQ9iB0V4HuxYh7vVcFYGNIyxaz8qrAjCHFAgIZ1brNHTQwJC8nLk2W1GReDZfcnC9Z+YH9WaMbzifOGX4lgcBWAY9ixS71RDwRwgDKTa5gmxAMh8lzNS44cys+869p4JYilCXNdbsz6vO4XA/sBfafd4wHoX9cFHsNjI8y03ng0RfX/Kknz/nnzr7kR81LnpjdzHh839UUnklH4NLu2PuGy+7Apm/pjZaWc/+pLgdVJ0DaxMXZnqfLPZ8ubzgvwKSve8V77IcGETs3B+8VaZ3xFKJBvRrp1GnB3ev7tlUDgMlH2PI9632mA/+1B771l+752Qz8t9/vGR6X8Pa5tnL38qxH703AS4sziTGpbtxN2yk7zDkQvVZ2BNeD3yfMpCPH1HrEW6qYabZqdBJWlvGsaLuXPlAIht3UsHRNsUwPs0fd+wSC+4SxbuvkaZ1pB/CbiLs6IK+uySGAtZikPoW5GITq/FjIE+QBUp9JtqDzSrRPHmIjIJZpavnFxSVLN/FFo/JknZ1oP4gydec11XS+0LYbU1yaTCKZhx2KcoaUDEOwbINmcn1usDJguK31V5crlubrbkAks3lqCaPBnpmixpvhcmJ91vNnn37Dn11/xe90X/EDe6NCKN8hWCt/Qf0beuu5bPY8b1ba2Cs4AFM77YK+5rJxTRN5vDzw+eqanyxe0pmJmA272NBHPxvSpljqfW5fr2oLWYVYFI5aL3zpOX9ANTdnAN83E/hYlqlac8VvcGkHrCSCVxbadtliTSr6a4IZLclCcjAt1cBhuFioJXjBteuNJMQWUgPjeSa1ibSMaiGehDGqcs6U7TulxT9kTVmZghIEE8AMUS2p+l5diLzT2jdGJKRCzGe2ts6ik4RcG0plbq6ZASCC7S3PVxfEJKzcQGsmlSq3N+oL+J4chKG89ilbRuyMh1iakU+6LZuxI2W4SYYYDDkX8NBcO6OAlcnQ956XB03nH/sdF26vZKrUqIVcllmjD6AxgYu2p3OB1sWZXGVNwtnIZ2cbnnVb/vzZH/Oj5iU/sNcsJXynAACaDVhUL+HS7nnWbPmmOefbNoK4OROb19wsUMbek27HZ+01P25fMGZXXLGU4lwp7Qo5VrKYlOCSy+OcWmbkcuhJQEuRpGWV8P7ZQK7lxffJBD7Gpcq/VU9OiTudDTQ2YmwkGqeplCmAoAWA1ldhVcQuPHOETC4X6eei9GqPLrwpH3XsNRAY/Pc6Sc0MLFIjjrKJ7W0gTI5J7clP6LiS0LkzJ/gIYQ4AcmIzbkbRm7U37A4t3xzOedGd6c1t9kB4p2JOKo3MXVaewCYtiPmIeARY2YGzpucQPHuvQm8xmKJycedGLTyJMajoxk3QFNuS5+s7FWGQ04aqahIGaIuPQZFCa0zk027D03bLY7dVLQcJH1QCPLS8RJpyf9lqTFtm7pUiXQU9FOMvCvC685qrJ8KtDOlUVKQwR4XSLKyBoOAEKjsyBxQ78g78wNw6QGZDW8mifocPrF/LIADMzaRlmTWfNwfGZOm6junglcnWZjDCoRVil4mLTLycMG3E+jh3rFMoGnBJRzRzfZuFMVgG5xTdZxVr7m14byGS0xULTmBIXjn4uVhNNTAtHa5rEd+oLbmIGpKqhjeSskKKjcybvR52cuKQmcuYyYSs7ss3lsF3/GHzmPPmwJBdGYMdeGJ3RZz1TQUhUDzELjf8bHzKi3DOl8PlPNn4rLnGS+SH7Wt9H9zIlAzbvmVXr6c+obmrniOkyXAYPSKZb7szDrEpOAENjrPNW31vsijD0GppUFfl9n/WXfOpv+GZvdFJwFsg2++7LFk1FWVibQcao91AtfPSfozkAgUpr830huAdL/sVF77nk+ZGdQaSyoJXPIUxSTOlmoWViUnm+KWqnJKNiotoIDBahpQgcDcbmONeeQyTNYiLZAXZvQWg8GsZBGpvwJSbV3UDFWAyv9QyaM0CuYHUQlwl7CLiCioxFQ33itWfo29pySbR1HOMll1o2duB3nm6PL1TzPS+NWWltlbI8BQt2StPIqwMad3gNp2ahYLag40TZnBYbyAVfvp9Sypzj+KrKPNNKqNht2/5+fYRIVmWZuSR29Fnz7kpNGWZbrENI8ImdbyMa/6/h8/52f4Jf3jzmJgM1iT+9KNvedps+Un3gkduj5HMq8VKtQAGlcROGOW516dcgTTlGvfB4ySxKypLs13bA2XXbVPYo6irKv0cR5Z33Zm+z1LTGEV1Nr1iOdyhZImlIWsMmF6bzM+3K9qiTbi2gyIgUVuyRTMpV8QekZYpSZFoL2Yk8xi1jIldUmk2l0mArYpDmdv9gRoc4jFrVPBZaRp/T+7AR7mMJGK2RflXra9PDUJrygbauY1dQhaRpp1UOtukWQwj1pL6pI7NYkgkQlBNvE1oWbmWfdJOuxqEvn8QiLmedJ7rsKAPKsaRnTrOjGtDu/TYZQfbHUyBXAxKTa9uwcR8LBvqnzaaHSQrJGdJVrTvUTzsJIIMQtx6vm3X9MGxsArYSa3Quy0rMxBNT5PjPKWJyMwU/MfbT/j9V0+5+vpMr5HNeJOYziy/1X2rqbjt5yCw6VuGoVCKT66pmNMyS70AZmXg8kO1H/AusZfawKuKz3WlEgA+pH/zMNmrKFJFSy6UZrfL+L2WmoE8axnYQcAYtjcLvjaJpdN+SfVZ6OzEwitHIpSyISZR45YSDCo1PkWj3BDRV5SNmUlLOcjxUD/pudQywdQJ0sRsbpKNkN5Swf7aBgEoenjlRqhd95AMDBa3E5or3QRxobiBCcchCYNXKm5OKn5ZOQlaf+sVzimTRRiMSml/3ZwfVYs7PdUvua2S+9Dqk0qEXccVL8Kab4ZzroYF+9GDVdLU8MjQbB32sMRfbcgxqjUbIOOEHRZY7/TYAe0bGANWiOuWtLAqwrLSf2F5bH6aIOSDZf96Qb9vGCbHeTfwR6vH/KC74ZHb80XzipUZeGy38/N+GdZ8NV3yxzePuHq5pvtKAVnJwTfnZ7QuMF04LuyOzmz58WLBwo4cgufKLdialmmyM1JOCnTY2YS3VVLNzHZhVvIb/YCHVvUd1D6CY8queBh+4E30wKpMyddhyet+gdla2qtM9zrhd4nkBYmmjO2EfNDpTXrVcJXhDwAewYXvOXcDrYks3VSUiLRPEGZZcseUtEwKyTBNTjOECsmSpLYcGFLIigC8I90mZfPbgVKy5LlcDAv5NZAX+56rngpGkrrhBiXjuEMuclTlpGwMwWVSkFI03dn8Jc2C0kwxkCZLENgODd4uWPuBc7fEujxrGcL9I8O5iZaVsrqJHddhwfXU0QdHjJqvZ4eSZFohLhzeObCWfOhVrD9ltW0v+AFAU5di485SxTqrwm9sITuOYh5Zb4w0WFIUNnbBGJT9F5Jh17R4iVy4Y1AzJPappU+eKVgYzQxplgBDsAxB4dNVU/GR2zFlqwIv0TKG4mwUtWlmbMK5iLOxGITqShVzSzo2TOevP7xiVq7BlC1jmWBEBP/W37rnce6JHPWxlOnpMINiTvw+4faR1JiScQnG5Vno0/RCODj2i4Z9aNQVqHggeBOZksLFAaZkGYtvQxVmGYNqM4AhZzVPA+YxZC4+Mceyl1lzwISCIRkz/pCPXy+B+6H1ax0EjCTt1BvUTMQq+YgyxuleJdwhk7zQ75WpNwarklcnd8q9HdeERnggTcI1S/rRs59UUPJRs6fvVHL7rNTVVckoUoQxsja9XocV+9jwi/6Sq3HJ8/2KzaFlHLxmJD4T1pnxXPAHR7vskGlSeOkwkIcBORx0w59QQqVtkK5DHi3JzmgGsBLiokw6zHGCJUEwB8jGEifDzjXsu44XyxXLbuT1owVP2h37ZcPSjEVSXCW7l+3IzXpivLCYqMGlaScWbppdn85NzzN3gyVxvVqoI66NbMeGEC0xifovmkTntONuROHhY7I6AZj9BeXWaXn7PdeJhv6MirHuU8M+qQL1Kpcx6PeYEMRCztqllutpweHQ4LdCdx1pXo24m57UOCR2SHJIUtwCuQC4OsPYe0VHlh7MXTh8VacOSUvEITlejwv2oeG1XSj4bXJMk+JNxCZyMsWvQEPAse7XDMAOmeYm44aMvwmYmJGQia0hu/8/7AnAbYutc9dzaBrOuoGrNpEKIEhSwu0TvhXCQTeJ1kn57alj/XYsgWCwjAI3kvnWnin4QzIrO/DI7WdvQtBSYZ8a9rFhG1uupgV99LzsV+zGht3QME2WVOWnKmfeKQJwPuHRsoTqglOZc0VVR6zR5mGtFwuKbt78d16fVPeaSV9fxjIWDcbrcUFjIvvYzhvbonbuj7oD23XLzROn4icm82R14FG3pzVHBGVT/B7WdmBlR/aumIaUNL82cO2JdgAcN/3p/1fPgNNsYAYSiZZ9xuZZHagvY8wRo/iH94gBbwsUt/gdZUZvpowZA3IYMCHiWkvyGgBia8gnY14xeVaRroFL+xc62aq8k0lssWaLRUUqzRJrIRntE8xTg5PpQDqyThVBmhUYd0j6+SEiMSExI1F1GB9av7ZBoDaDqmCn97oBX6xW/PLsgrB2JKvR0m8mstOG2XSm2AFRccCHMRcnc2G1fhJiEPaj4etoeN0sGIJj5QeetEqLbU0gZWFInhfjipuxYzu2bAdN94ZBU70ci658fVNFsUCp0ZIgNQ7rTunJWVGEp8tYsgvggo4Icz523ysB5e4qMVPKuC4HIeEYonC96rCS2C7a4zUtJcJPVq/o7MRX3TBPB37r4gWfdTd0MmJQ9aYq937h9hy8nm4AY1QnodnoxdwunULNcTmWAFO0s+5gXU4SqWwqDBBVi9CZlk1csJy9KNKDY8/TFd9RbtTeRC7/kZCRIZJ3e8QYbEo0KSOhIbZehU/QkZ6zhQBlil3dHY5E/dznOCMLWxNYFRNVQ1ZMRFFC1lPiBKcQS+0/gttrs9LvE81VwAwRuxuQWMbLxsBb2iy/tkEAagAIM5e+k4l0Jnz79IyfR+HwC8X7u52BpLWSmVS7bfYvPL0P7o5cypJcurJZ2YlDbpm8448nh/eBXzbTHPX74Ognx27TkfYO6a12jlP5W8WeC3fEc8uoXAc7gBvykSqb0zELuLtSVBJUCOrbOKS5KWTCSU/gvlVOkvnvi27M+hrqMpLoCDxrNizsyKPmQCib+Wm75cIeZpEU0pEgNesBmIiTSDKqlnR83OPnd0/7mMy8+SvrsE4UAvq3c9ZyIBmZXYxeuPWxJLNqyV7pw9+FOdmgm3NhJ5yL5XoWGG8I5Bgh6ckuMTGdOR3LFmvxxWJU4RQ3zAH1vlWzA0NmaUYsicFrv2aIjv3QkJMh9xbTG9xWcLsCNd9m7AjNVrNdt4+47YiMARlCncXer4R0sn5tg0C9kTqjFN+lDLNBxxfrZ9z0Lf3ZAreXOU2TKsc8jwHf4w/VWFF55LVLOwlDNIzW0XuvVHDJTIMj9Q732tHslSJsRn2c1GjzLiyy+tM7rdvNKNhBMGPGTBmJ8TbL8aGnFpMCimJUK/cS5G4ZVNz3GsvXj2w4PV1dGbfVaUv9/KKUO2s7zKO7pRlV/LRYhVX672kaXUe2TtK9Fu511SCQs8w9gFPXqBP4fFEe0o2do9CLx0jm9bRkWSzjO5nAlNHfe2QE961Kl27+f+29a6xtWXbf9RvzsdZ+nee91bdejbsNjoUJglgR2AJBRCAxFgQh8cFRJBISFIGQeH0At/wB8SEfTFAECESwSHjJcRJCAMsSMk6IBAjSkAgwtuMm/XJVdd2quq/z2I/1mA8+jLnW3ufcc+49t7rvrdupM6Sjvc/e5+w911xrjTnmGP/x/xvFlASnjhVjIEZyH7QN3TmMCG4zo58rwAefmNWqblybcK0D2LUxsWxgOige2YCI9hdIa7CN4NaKVbCNrv62A7dKuE3ErXqkCXr9pMQFCrZn2PetEwCduMEBHNo1KRvmpuXv3tvDSOJ/ffsQxDJ76EodvVCYu0yqMqMybEYJSZ5BHzU6g1jCeMnKW0Dhayg8iNWZwW5g+kmmWkaq84Rt9SIIU0uYGrqFod/b1vNNr52O848j9cMWOVuRV2tdbZ5xAnOMSN8jyw3WW6qzWisMlXIPpJJruPJ/S4SJT7gqclA3HNXrIgXWazdf8RIz6ZhIz8xoNSBlo1WDbDmN0wvbh4hwGmYKhnpOqW/35h9W/1hq6LvRwfD8skrT8Lq3kbNuwuNuzv36gLeqUxa24U13yp7ZFEm4q7UVr7OKxL5peLM+43i+5jsHe7R7lnpR4QuqM282ZBEkRtxmH9NbkgNTR44mG/ZcMyJab2KDI5iZ4kBcoO8tce2YPjT4JdSPM67J2C7j1hHbJeyqx3QR6YJWkArC9Kb2fe0Edk0RfJEJvZJxupY8iYSZoZ+V5I0X4gTiBPI0blfJKJpcK2IRF6p9V8ylZBTnXRy8Jo3UCdgGJQhZZ/xSSUzsuoecsbXDzTyusXStLfv/4gSWieqsx55rNSCH8FwPDigmPGj4Z5uI7Qy2E0IELIi92qcBY5upsYnKxKJeG0axkss2ALQiMnI7tMmNya9hb7uMNZtYbeG/z8jAPo21f/bfxqwAmy5YdQRJMCbTR1tk0pXFeW5b+tpybJf0zqosO+HGxCoDInFmOmoXtLHMW1JlEWtG4s5ctmTEXNiIwZhMVWjiP10UsiWpjcFCvyWdqZYJ12hXrN1oc5np4s4W8sUcAPwt4AQMW+TekNXeMxsO/Zpq0dFvLJu7KmeWPDR3E3kvMD/cAMr40rWe2Blyay86g2fM5S5We8CUm06wG3BNxjUJvw7YsxZzuoS+R3LGeU81rZnOJ6SJI3mr6svrHnO6Ii/XpPOlhpvPM2XqIDctUnn8sqdaWFXgWUhhJtrJfYzhfzkAkzEu4X1k4RXmOsBwrzJV3S1c/f2MNrkR8guM8tdtdKoKXB53V/KLn1fwADtRwO7rl21wAJvO0zSe1FlypwChdVXzxM/xVeCD6SEz33N/ccDdesnfVj/m3eoRh2bNF+xS25Zv0EE5qD/t+QYmkTBxxIkB7xFXuPxzHqs2WSD5TOUjE9dfkE6/qQ1JxGEeQmtxS8vkUaY+S0wedHrj98ONP2wf07Ojxu+H6sDQC/DC/1d4+/ps6VFG3/OkUtbWJro60dyxqo5TZ6q3VhwuNry7dwJAlxyPNjPWbcVyNdFW2MZqV1jeKavtmmhZr+BbCp7fkEX3+pJUI890DrPxmKUhdxk2DbnroWkwmxbjLNk79d5tr+HlRluJr00IXrKctHIgbYdZd9hNhW13NPDGakEeHdsIJ7UZazOVU8VdZwaxkkuMSFnGOR5Kn6tQa40++PH8ueIEQgHE9NGOTsCZQhZSKgNXagw8x2LhI2gbT1x6pFXsBwKpFoLPxM7Sd45zH2mj5fFkxvl0Qj+33POnmjOgf24HZZ/1ujoNM1Z9Da3FhDxm3K+0Mteyk1954WMsW6iQLJvgoS1w5U3GbhKmDWPpT6OPPJaIrxvXsxwAvCZOIBd0Fi/gCAawRZctXgJd6etbJ8X3b1KFMRmpI92RIU8SdtHzpbuPeWd2yt8+e0BCFMRTHfK4nfOhZNre0ZiKHFQXMA85gF0bcglDL3iUAu/UvXhM0E8F1xjizGOdRURIXafhe8rIWsE/YrXtNsVIDkEdwLOA3hcmLgMlGuh7pOm1StBtIaPaUo0mAtNWuTcbHb91kcopg891q9bgAJrkWceKTdQGqC5Zmjg4AcGU+nZMCvJRdeUSNpcsfwV8Cl5azb1EQwiW0FpkY3EbQXrAKHVZ8pC9EDpDcI5HUWg6T58sU6tblTfcGRieuS0Y4MhN9pzGKau+UjWfXrECo4nZdueNj9ofobTpnw6wFIugTRvcNiHYRmyr7FgS49YZPSf5NzqA172LsM+Wh/2ewla5vpxyAVNewlslolCv3STPSZzxfnPMB+tD3S+6RD7uONjb8MXDE37s6Fv8QP2QL/kHADTZ89HskMdhwTf23uC0n3J/vc+6r9j0ni5oe0neSWIZozh376JKViVhua4Jjac1HrcZkoeGbDxuOVcVo9Uauo4cenLQSoZYexEQdIM8wAXLWR2H7THrBtMusL0SY2aXibN0gZ1GsmgU4zJmqkIgB3XD3KpQy9OCKUKTHetYF+DTjFWoWIb6QhlPz6Ou+n20NMHRB9XtU4SzkoBMfVBKMkk3dgZDInBgmJalw59pplxy4Vsp0U6SoS1X6HPFedQxTF1Pnw1vV0/AqRAtcrF5SPsFDE12vN/f4f3+mK8++BLfuX/E/DuW2cOAP2vJfb+t3mQ9d6aPuk9vVJdwHSra5JiYC4T0N7I2Oc66KaebCX4p+CW4dcKOmf+dP74JHbG1I8DsKnstnEBIhrMwUfZZs20M2rVUcPijlWaRPjtl/k2GVap5GPZ43M0572p1ApKxdeRg2vDW9JR3q0e8457wpl2Xk6774EO7AuDUT5nanpMC4WyiG2GeOg4ZE2G1DeNeViSztplmUxJWlSghaC3EiW4LxJoRijzc7E+BgD6F5Rj14ggBExIS2IKGbC6c9xT0Y9bVyqsYyLzqWfh2pCvfrQoM0VZfRFx3I4ChE+4yndg2eWfogqXvlfssuVTmrmwFLLgLmgzX9wsMczwItdhWsEWXEnbug8v3QhJyYmSICsmWVVarHDaXrUT5zg7Vklinmg/7I95vjvnkbIE59VSnmo2XJmjkNTjrlJQApi8l2g76foeNKpsXyguM8xw8bev0WNuM6ZMm/BIXyGNuZEYuwM0v22vhBNro+ObyLizgyK+xdoO5JqM8RAMJIFtS0Qtsyv7tvc0RHywPebSaEYJFgL1Zw9vzU3549jG/rfqYe3bDsbVYFDT0Rl7R5CVfdCc02fJ4tuAsTcYGmoEJaKAY6wsjrkEZcdrk+GB2yMNmwdfjXXpf0XdeE4ZB6Pc8JiRsKS19zy1p3Vpbj4NSlg9YgDphivYf6A3jXKRygf1JyzvzU+7VZxy7FfUO9Dnu5FqWJQo4C/WYfb/KAYBGSn1ZfTXhql2EwWR6nwiVJURD7QMTt52LMargoiPQz9P8Qt853SNvhgSsRgFjpCPleXF+Uil5TO0DM9eNsuexnMuEwZSmpQ7LOtV8EvZ4GPb53578IO+dHdF9a4/Fh8L+ez3VwzXmfKWOO5VkYEwIPea8oTqpqB87+n3Px0cL7k3PqU0YqdWfZ8O19LCb82A1pz+ZMD3V8rFpSyLw6Ql/+qVLq362Fl733oGQDA/Xcw6rjVKGSbgmNH36QIa69RCunnVTVl1F1zlyEsRkvFVZ6oXVevFEwJcgTXkLtcpgTKDPgbkEVgUJN0QgMRt6VEdwlaqymhRNvlIzt5K5P93nPFqS90VDT0oiUZ7rkb8ry0kZibpSKmw9plN4sgDe643vbWLqtfnnuF5zrz7jyK+ZmXbsIBxx7SUP0CbHJlZ0yRGyvdYBAGU/rPtiMQnlzi55ExF62cHFlwhqrP9n2clxbbdffVTR1tRZ3ZsH0W7GqGg4hdBqT4iUhCcZjE/UdWBv0nJQNey5Zsy8N7l0kGV93qSKkzjjg+6YD9sDvvH4LqcnM2aPhPqxNuTIpoNOG7uGbQBAjqh2xbqjPpnQnhpOz2c82FtQmTACq57Vch6zYR0rzsKUB5sFZ8sp7tTilxm/SRoJhC3TVJmcm10bVi70NVy218IJpGg4WU55OJnrzepaPAGuuOmf+t9yk66TZqxP2qn2ZQ997EYJMKa2Z7/w7nsR7E6ixOLwZGYCiVRERzsSCvSIWWXI2wyr7DhJk7Fppc+OVdJWXi+R9+ZH9NGyrittYiqkHwgK33xJOlRDlcC0aVUFDQAAH4JJREFUPW7V4zY1fSda7gQqFzicNsx9x516xZ5rOPJrjtxKkXY7Qp0a+cgoOrpJFW2yIxHK81p8RZQtzxTtPUTHkbMhJnVMMSgu3phcKLe2N70+Mv4egiHF0s7cSZFnK/j5QuxhOkYCDSnwXWsT06rneLLm2K84dqsR0LRK9Sggc54mrFLN/e6Qb6/v8MHykJNP9nCPHLP7mdnDgDvdIJuWHOIIGR7mHSJ5s8GcVUwfzekOHOdHNY+OZsxcxx2/GqOV61rOE8Iy1jzuZjxYzomnFbMToT5P+PMCBLqqMjEqJMswYZdPhm5B7WseCZCEdlXxeD5jv2rYd7XiqG+4l0p5aMm09EPnVdg2f9wkATU4BYvF5IwXSxr2rKJqxFVOmBRI0mJNKn3fMDfQ2IaIcFBtOK9r1qV5yXQZ20RMGzT7fwM48KeynHRLsFpjrGH6aAZiaO9aYpUwAkf1mjv1irfrU/ZsUyKjbuy9AAq915ZYtUkqA9fEHV7Ea5yAKT0DtY1QdyPFdgiGGNQp5yikIsARWseulNcAkdXj2T00KYhM2dKrZXUCtt120A0IzDC1xImhi8LjsO3SO+lnPA5zLOlCG+9JP2XZ1zzYzHlwsqA/rZn9lqd+klnc7/EnLeZ8Q+46BQZl7V8YE7o5k9cbxFqm9+eEyZzkLR9Nj1ge1xjJ7LmW42p1odt02JZsomeTKr55focHqznnH+4x+cgx+yhTnwSFA4ekjucFgUCwE4VeY6+JEwBaq+W56GmTf+GECpSooKDIRod4owaBi7brEAYzWRmOJ5I5l0zJFGBFSCQm0tGK33IalOPShp6sJzFf4cm/V5azOpi2RTYefx4IE49thNirs5rYwL5rOHYrZkVNyRToNZQatWzbcIeEaL9D/Pk8G7AANVssQHCGrsvEoImyHKQkKXc+7zIco+Q0xucDt17524FHz7UlwdoyUszbVrQ3wyt25LGdIZI5raY86bSpLGVDExX1uOoq1p1ntZyQTyqqU6PgnJOsiM9Vp7mcGCmZxrIl2AJ0ch+QTYM9b6hParo9z+aJY2WnfLzYo6uVPWlhTcFjpJFY9bSfct7XfLLUbYA/sVSnUJ8nRQX28WYOYId6fvxdRKPR171EKBlMY9gUIoZBiupFbAhhRyttujlpW2qbHF0u7amfAsoJaJkvX81Jp4o8pekDkF5wTenwWnbIqlG8+cuKBABSJK02SExM7i8wcc76zQlhpqQeU9tz4DaFDqwfb/7tMSgseBADGerlWg2wV2fud86TQXMBtQk4n3ASMXPd76/6mmVfcbaZsF7WKhnXmsLmdM25HqKEnUg2m0x25Tz0mcnjgG0jZhNGFGesFd7b3HG0+5b2aMGT/TkPa7RnhELIUZSgbauVhsUIzc1MP+lx6x778FyBXK1GAbkP5Jie6uvIoSe1gnl4wtQYTL8gm4r2Yc23m3u8v9dzdLDicLph6nqcREK2rPqKh8s563VNvj/BnxkW72UmJ1EVqc7b4gRueM3u3uzaHkq25rvLCYjInwb+CeCTnPNvL6/9ceCfBDrgG8A/l3M+Ke99BfgjKLL+X845//JzB54KLVPn2PSetpRybsIaOyTtdktM2yhA95RdtLRJs799NnQ5EnO+kBe4iaWSGxiSkVdZX8pnA7jEdhoFSPy0rufFbGwq2rTYTY3dZEynpB0DO291hULvYKMu387bu6CX3a3AbssvqD6AyeCsEmvOXUdlVAdg6VvqbqZ8C40n2WHTf6lXY9dk2+6cJSMltwKaFDQB7DpgNz1m2Y6dc8YqEtN0E/zSU60s3UJLtqmSYfBjctH0+lluo3Bvt4r4s1Y78kK52Y3iPpRaWBRvcaFeX7QWgpKO+DPP9LFDoiFOHP3K8LCxnM4nVFVUxaZk6FpHv6wwK8vkoaE6h8lpwp8HbBNg2Abc1HavaadS7nHmifV3lxP4z4H/EPgvd177FeArOecgIj8LfAX4N0XkR4CfAv4u4G3gL4nIb8s5P3P5MxH8uRDWjlVbsewVkuolYp9TXhkoqgcqqrhL1lGIRNvesexrTuOMs1yzl3vd72dzY0eQSPRk+qygmIg8JTMWMXRJtzW2LRfVJmlSJ2htWUTIYjSl/Gltd8yXtxc5kboes95gz2v8ao5tFVFnJY9lwOsy1VYSNqdRE3Bogrkgn7Zz8w/dfAOgylvlEBj2wF+ozvASaZPnk26PyhyzbivN26ytLvZXTcWAc8gFBFCg2kNYq5RaGXfSYJZr8um5lu6GSMta/McTqknNbDYhzWqyN6TKjvM3kLFI0Bq8BK33EyLS9XrzDU05RldUwUHf6zk0lsvw7hwCrNbYnFmIMFl4/KaiWwjtcUW/8LQVJKc6BraBeQEETR8m/DpRP2q1KWgEB91g+bh8HVtDrhxx6ukOHGH6XTiBnPP/LCJfuvTa/7jz618F/pny/J8C/mzOuQW+JSJfB/4+4H9/1neYHqYPMnFqOZ3O+NAH3piodv3uxTpkUXdtSF4NyZ0uOFXAGRiBAjSt5+FmwTcnd3nLP4EKZrJkIoYa90xHEHMmkWhypMmZdRHkGBiEhwzzmEiLijI0negK02/3jWINOX6KEqGUVQhFGF6wnEY66i3qMOnFGGLBuxvVabxhfkQjhsCs8AdMrTrikJVcNGVhXdCUm7ZS7YZkcD5S+YDZyxxU2kZ7YDdMpKM3Cro6q6ZMq57GezpbwC9yOSGwe+x5SwU3FBtCYdTZJMy60X6LzWYbpoNGDX1PbhpkvcZWFTin1G1F2OOpr9oVcXEWjNfVtOyrh049s24UNbhp9PsGx2OMAsJA5/5sg+8C8wz1zFKfqqpyqiBZg6Q8Hodr8qhIbdqA9BF5Vh7A7FSadq7fXNSs0tQT5p720LG6Zwnz68/39yIn8IeBP1eev4M6hcE+KK89ZSLyR4E/ClBPDpmcZLoDQ3/oOZ1NOT2Y4ksr5+AIBgTW7t60TY42edbBs+6Vuy9Ho/wAhR8wdJZlW/GgWfDx7IC56XjTrqDQUD0rIkgk+pxocqbJoviBnfLggGLsCuV1F62uukMZaziJQzgpgpiBj/+GHl5MCYXLRbaLNRhXq0JEuQs/TmkkUXnRfOSgOuxFMRYDIWgnyo3YdMrC26wqcmchCqGOhNqymTZKI05mIh1z0xHpWaeafbdRolE7cJ0969gvvV+iO8UFZOVp6FWgZay8DIm6VFblrkc2gjila9dHUSjtcM6t9nZgrToJY6BSrYc49WDUaUjUWr11Vj/XGOj7CzyPAzckKSFtBzHic8atPH7lCFNH8sptITkrkUyXsH3SvEZBHz6zUak4gDzQhu1cD4PI7uAAmiNDewf6+fUXwHflBETkZ1BOjZ9/0f/NOf8c8HMAB9O38uK9NRKnkC2bZs5v+Hvcma/54qIeCSFXQYkrQ7ZjiNolRx8tD1ZzNm1Fu6yhM0in3XwEISXPSZrztV4P9zvzIyLCF+w5b7tz9iTgRfCyS3aZiWSanGkzPEo1J3HG47jgLE1pC13xsM8+jVOe9HOWXU3oHT6AifkiznsAC0khyHv25Jab3iLebZuN3NOnTIImqyQONeysoarIhZto0MXTXMv1NuQMDu16JMWsi27gaQFjnTxaIEvL5BPtcrMd9HNHv5+5jxJ9vDM9AXSL4UnMTKusOU4FYDYmb0tXl1q3szBiDKSoGEmh/XZrJdQ0TdTVPoRtNHTZUlSHMEQI1pa9vZTngkxq8B4qT9qbEuc13XFFmBraPUN2bIVcElTLOa5JVKcBt+wx6w5ZbdQZ7+7fw0D2mcibDrNyOG+LyrSOgVT6/2PWlT8/o1NxuPkrR3a6tcnWkJ2M8nNxYggTw+auoTkW2ruJfK9hOrue3ORTOwER+UNowvB3563a4XeAL+782bvltWdbyphlR3XqqU8NsTYsT6ekZKhMHOv8bXSjICXo9T0gz1abmtDZrQMIhZBRIGdDMo6NwP3ZPikbvuDPOfdaLjq2a+YSqHc2pwnFZpxnR5MtH4YjTuKMj/sDzuNEEYIUMkkJI7S2i8rtP2oYDFNTyjVyTSh6aXIvOABxDrzT/x+cwE45KIOq2/Yyvi7Okp0dYbXDojckNSPPllEbiDEn0jOzrZKHkGmD8t7JuaM60VKa25QEaNQv2nTbbQNsOR+sJK1KWO1cHJVyS5i/Qx6th5il1ONLybYT7ahrygraR6VYSzvRz3U2zFUsuoJ5UAW2A5aaNJ8S9yZ0+57NsSNModsXdQIF9yEJ4kTHESuDn1j80uGsIG1AmnaLKBy+NyWkV2yB5IyEQgF+ObeTLy0aVx2GMaMDiNPCa1hYqrMpwjMTYXNX6A4z6U7P4f6a/Ul77Wd+KicgIj8B/BvAP5xzXu+89YvAnxGRP4EmBn8I+D+e+4EpYU6X1MDCG2zjiJOa9X7FNxrlkEOyMvWm7cSJQAqFubdRSKn20nNRS0BQqfLG8FE85NF0ziZ43pgu+XhxwNvVE47tkkO7Vl49Mh0l15BmnMcp32i+wKN+zoNmwaqvCdlQ28DEagPOkJjcdJ7cmQsyaNsBlxXoecnBwQFMar3pKz+Gsxf2gqA3vPfayDIyDyeYTkjzmjAV5VGwqdT8FQ6t6sphrBJc5RAGubWIwrIBTjYTVidTFu+rAzj4dqu49pSx70yRaFg36gQGGz5bBT4b9n3DWTXhkcvkXp3HQJ+9y98w0KdrGzS4tSbQ6pOMPw+YTa9t1y+y18kZciytzUbZlyYT8t6M9s05zV3P5tjQ3IU4zYRFVBjyTiemaZS9yZ0b6lODP3fMP7K4VcA/EoVvFw5CYHyUQkCSrUGS24b08Ew2YH1fo8hc6xYlzCz9nlW2rErzDLES2iMI80x4u2G21/LOwSlvTJfs+4b/5ZqPvkmJ8BeA3wXcFZEPgH8LrQbUwK8UJtO/mnP+F3LOvy4ifx74DXSb8C89rzIwHqDXodg2Ua0S1alFktC5mugSuKyrnZRHhvNpIIgy9gatNFA89mX2YJOEbBx9Z/gtc8Tj2YyzfsK7syOO/Yp7/gxf+ha6rLmGh2HBaZjy7eUdTrsJJ2sVhkhJcC5RucCs6keAUNc5CGaHxLOcPGeRutK9JXrOc7TaUrwbLYCu/tYildcb3DkoHYgXkkFFvVaBFlISbPpa2psSFp5+IcRJovYqODqIaugqryIdu+QXVzmDoYnoQbtgeTLDPfTMPslMHkfcSTvuS2MtxKlg5j2H0w0HblNaafX4hshi7lomrldBDayet1KrN4ExMlAHsC3puTX4lVJrm7bw6V1Oir6AiXdIXZMPFoTDKet7nubY0B5Bd5jIdSZPNGIRk6GUn1NRsUrekL0hTAUTHdXUYLqIWfel4y9fBIjtOINcrgsp51Ih7s8er7IdG1KlPwOfZJiq4EyYQPNGIs8Dx8dLjmcb3pguOa5W1Ob6xrWbVAd+/xUv/6ln/P0fA/7Y8z73gokoww5g24hbG6ozo/JhXttyU53JkwQuYdzWCWjcLrqKhIIrLwCUC3vMhDoQDKk1tHlKt65Yt56zvQmH9YbHszmVCSxsO3Z0fdLssQw1H5wesG4qunWllFZJaH1i7ROrOlBVyswTejtCW5XBR8jOIMHqir6b2CsMQiP8tCQAxTm9QJ+b0b5Y6trFicdZpXv0OaRpYur1IhgUeyYSSCIXyoVmTJLuVGQKhPgsTHi8mSEnnvqxMPukpzppsacr8rQmzSrCROhnMJl1HNXrUhnoB0W90cFOS/Q0pGAkCLZTfkbT6/FIuoghkgRuU8Q1NsquLCGSBwTfi5poklDqin5/QntUsblr6I6gPdIbSXzC+cJ7MFAyoSxGKRhiZUjOESeC6QzRC/68wmfIXV9ov8pFuDtGEURKhAFbB3/5MGTHMZRzm5wheZVAi7UQa4hT6OcaAcidlsWi4QcOnnBQNRz7FVPbP3Pr91ogBrO3hLsLVXqd2SL2qKuCCYUYtE5MjzZM647j2YaQNBN/sprStp5IhXQGcnEIA+02+qgLimBaVB03WrI3rNeW99cVH0/2eLw/Y+p6DuoNg8zVw82cVVtxejojb6zqCAQUg2BVVbirHF0dMS6RWqsiqJUKQXYHjuRmmKiNLpIy0hcUYdtjzlZKOdZ3hfzBIrMJ2Tty5bcr4QBaue6CHxyA0wx3v+9pDwz9QcYsehaVQoR1VZ/Qlvr/0K05kX7UcRiig1VSbMWvL9/m1x+/yccfHLH/bcP8fmLy4TnSaBQTDqc0b1ScfwnaN3v+/jfv83cuPuJt/4R56U4EIMGeaThwGx67mTrlqBTafgl2oyUz7ZTbObSytbOdgnlsnxREs0ut9SImok52f490uMfq3QnNsWHzZiYsEnkRsFUaOyHH3VdZVUzOGJ9JNhMN9LVBoiXMBNd6Jk6oQ8KGOAKIBkYpZYBFozxjwCWNEoeqxO75HBz7taxBusjECsIiEw4Cxwcr3piveHN6ztR0LFz7XLLT18MJiBCmyn6SvFEF1d1trwWpI4tpy9FEQ5yQDetQEZN2oi2bwqbTq7yzpGEChw/RBynsOjA0qxii8bRJOK9q+koVdpQcw7DuPE3nyY1F+tLFVoiK80BaIaiUeS5AJYOqCc2Eri3Hk1HnlrSXwM8cdhPxRWhUul63DdaQ5pMx+UNGiULa0kTSP505vhAhlH3msFqkSrvpKrvdlfXJEqWs+KkAhEweNQMUGGU5jTMe9gs+WB3y6GSBf+yon2hTi6xbzXo7S5grPLc7TNSHDe9MTrjrz9kzGgkMUuG24A/U2QzJPJBQSn6dQoHHROF4gMrkO1ZbPsXCv2tirRKFTmvSolIK+IUQp4lcJ4xPGKNJnasqx3pZFRoxH0lAmGpeo58Jfq1792wN0qPJy50+A0DFbMpNLwPRjIMrv7DMASWxOMzHmDuxBXxUp5EkZmpUEesmbMevhxNwQneoQ0lOtBHE60+sIe8F7t5Z8vfc/ZB79Rl3/Tlt8jwOc/b9ISfdlO+YpOG61OSNhs+my08ThZbJM0mglHzAkKJn6Sf00Y4lyQxs2oq+c0ipOuzuWdUTqDhGMoZsElJH8ImNzfR7luauUaYfVCRU/1fwS6v6BI9qXeE2KnedrNAtTCEC1RDZbxKTRz12E7CnV5Sids0IOHU8yUOqErWPI3y3L3wAAztSjx1zBYZMa/qROOU3V/f4zuqQb37jHpP7nv1vZva/1eAfr5GmI09rwht7nL/rWX5ROPryE374+AE/uvgt7oyJ1rJ6Fq/ei6M2/UjwQdKtwKCma0K+muU5MyZ8s1VADM/pjrvSSoVFFnPi0ZzmjQmbL5RM+n7AVBHr4pX34uWmNJGslVgTiXtCZy2bNzQqsJuK6ZmDpnQe7rIRgZZzrdUyYghjBShbo9u/q4YeVQbNtolsBdsZks+lbJhxdWC/btgrTFED+/Pz7LVwAlDINxgywlr3zAayy5gqMq869t1G1XCkJ4lhYnr2fUPKwnJaY0ziNFgikMXiECRkTJDx5r9gw4UVBQKkKMR4EVk3tiGPIaH+Xx6ALKI3a/YJMwlM550qEc0s3dwT1g4p358H+u8MYaZRRZgZvQlaOzq+sECx9QbcSnArSxaozi2TRolDpNnhuNOBDh+9HWf5uaqVegRciQKM+qJS0kdlFN5EzzdP7/LgdEH9kWfyACZPlOc+WyEf7xEXNeu3JqzvCe0bkR/aP+Od6QmHVrsUlbl4+909jJGAkwKzkmfoQe6cp+21UfIs3pJrD77SLdbY1KO5letME64VMqmJc0+3MKriXGfEJd0qXucAShv1hdkcBm8z2SeVLK8h+W0JcGQiinEM+Yc1RN/fubyS2TqLywMxWUmTGgsCrtWt8250dFWT17PyAfCaOIGRAluKA7AwYMeTz1RV4LDecOTX7JlGeeMLmm1uW0ylHW8TNyVn4VymBJuJSSWiNNEkTzsBtCQlSRtIYjCE3hKixZodIkzJ2sBSwraR006Gi1JZjSezjncOTpk63Wef9xOWXcW6rUh5q6mXs9C2jhgsq6XSkEkvpCqBz/i9FluSRuvzGnPmkGyJleBPPTZnpOFiNJDzReSY7Diqa2zL+29Hzv+QDCf9jCfdlPsPD+BhzdF7mdmjyORBo06g9nQHFe2RY/muZfNOZP7OOb99/0O+NHnIoVmPqsa7NjiF4dwZm4ilBHijptFyfSRvSLWDmLGTuqD5ohKo7CA0L0Csh7kyBuqaPJ/S7Tu6faGfZ9Is4VwqbEgXbXAAw3Yvxe1gpXQ6ik1kt5WYixNl89Gy4A6IKyYdG5BTAcGnTM6FNSiIJpEvI0MBJJFTwqwFiRk3d0RvMKGgY4d+jsINaXLG3EAA5bVwAk/Z7ph3Lo4mKQkkbCHEXiLJ9Bz4DZUJVDbyqOo5n9Wc2xmptXBmNSLo5WLpcHhMJRroDcklYhI9B7uMwnUiCZCV019LWCpnlqeJ+V7L3cWKH9p/wNy1zEzHWZhw0s/4uNmjjW6snacsdIVrr506XV2yMK07plXPvdk5lY04Sdxf7/Px2R7rzT7ZGmYfO8yQFBu2BaVlVOdLCnS0UEqZrAw/l+wqOuyQVD3o/mafj873kI9q6keG6RNl0u33K7p9Sz81bL4g9HvQvBk4evuUv+P4Ie/UT7hjl9c6gCgJT1C8gGupqkBXqbLPU02ZV1y32Qi5glQJ2Xpk4ahqi2kCspxjCoRYD/ASGGeAFIuQ51PCwZRuT3MBaaJbuKsiJmUR0zxTGrQc4q6z3ZYPEY1cR4n5IRIo/QU5RnVMkW3pOKadxaEvVYIEPVcyUYm1DE1PfunJzuFXhn4tdEvPo/WcnLX0O7U9c9ty4DbP5OZ4PZ0AXEgODfMwNAsNPHhDM5EziWkRlBhYhgA2dU3IWlaxSXY28hdNSoVOBUe2HtWhar3WJsQl1SGwpc6bSsRiQbzW4RdVy51K1XpnRim8axNGkQ4r1dj74EwiOaH2gYGTf79q2asa3p6eMrX9qAYUk+G9+YIwtaRqpzd8qBaY0sZq2VYJdrYDN7FRxyE5pVtvK9xK9e9so0QjsVIYbb8QmjuZsJeojhq+sFjy9vSUQ7tmbq5HplnyxS2BSWC2ugjP6yMYjkm3iwYTMyF4rFWZcFp7YfXfpdQaW4JB4cG1Am2S1xuXQoX21FZg7JBEHcAuuQloX0Hxw9rsRKGUY3vhpjzCmnPKpXekwISH8eZUnIPZOqsrtjWSM9IbsohiEjqrjWqFSq7pHStXsQoVhkxlwnMJeiRfU354lSYiD4AV8PCzHgtwl9tx7NrtOC7a9/M4fiDn/MblF18LJwAgIn8t5/w7b8dxO47bcbzacbwk/utbu7Vb+36xWydwa7f2ObfXyQn83Gc9gGK347hot+O4aH/LjeO1yQnc2q3d2mdjr1MkcGu3dmufgd06gVu7tc+5vRZOQER+QkS+JiJfF5GffkXf+UUR+Ssi8hsi8usi8q+U149F5FdE5G+Wx6NXNB4rIv+XiPxS+f3LIvLVMid/TkSqVzCGQxH5CyLymyLyN0Tkxz+L+RCRf62ck18TkV8Qkcmrmg8R+dMi8omI/NrOa1fOgaj9B2VMvyoiP/qSx/HHy7n5VRH5b0XkcOe9r5RxfE1Efu8LfVkedNU+ox8U5/YN4AeBCvh/gB95Bd/7FvCj5fke8P8BPwL8O8BPl9d/GvjZVzQP/zrwZ4BfKr//eeCnyvM/CfyLr2AM/wXwz5fnFXD4qucDZaf+FjDdmYc/9KrmA/iHgB8Ffm3ntSvnAPhJ4H9AsY4/Bnz1JY/j9wCuPP/ZnXH8SLlvauDL5X6yN/6ul31h3eBgfxz45Z3fv4IKm7zqcfz3wD8GfA14q7z2FvC1V/Dd7wJ/GfhHgF8qF9XDnRN+YY5e0hgOys0nl15/pfNRnMD7wDEKa/8l4Pe+yvkAvnTp5rtyDoD/BPj9V/3dyxjHpff+aeDny/ML9wzwy8CP3/R7XoftwHDSB7tWq+BlWRFX+R3AV4F7Oef75a2PgHuvYAj/HkrcOqDS7wAnOeeBGO5VzMmXgQfAf1a2Jf+piMx5xfORc/4O8O8C7wH3gVPgr/Pq52PXrpuDz/La/cNoFPJdj+N1cAKfqYnIAvhvgH8153y2+15Wt/pSa6giMug8/vWX+T03MIeGn/9xzvl3oL0cF/Izr2g+jlAlqy+jjNVz4Cde5ne+iL2KOXiefTd6H1fZ6+AEPp1WwffARMSjDuDnc85/sbz8sYi8Vd5/C/jkJQ/jHwB+n4h8G/iz6Jbg3wcORWTo8nwVc/IB8EHO+avl97+AOoVXPR//KPCtnPODnHMP/EV0jl71fOzadXPwyq/dHb2PP1Ac0nc9jtfBCfyfwA+V7G+FCpr+4sv+UlGu9D8F/I2c85/YeesXgT9Ynv9BNFfw0izn/JWc87s55y+hx/4/5Zz/APBX2Go8vopxfAS8LyI/XF763Sh1/CudD3Qb8GMiMivnaBjHK52PS3bdHPwi8M+WKsGPAac724bvue3offy+/LTex0+JSC0iX+ameh+DvcwkzwskQH4Szc5/A/iZV/Sd/yAa1v0q8H+Xn59E9+N/GfibwF8Cjl/hPPwuttWBHywn8uvAfw3Ur+D7/17gr5U5+e+Ao89iPoB/G/hN4NeA/wrNer+S+QB+Ac1F9Gh09EeumwM0gfsflev2/wV+50sex9fRvf9wvf7Jnb//mTKOrwH/+It81y1s+NZu7XNur8N24NZu7dY+Q7t1Ard2a59zu3UCt3Zrn3O7dQK3dmufc7t1Ard2a59zu3UCt3Zrn3O7dQK3dmufc/v/AWdKRQUy4oaTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##TROUBLESHOOTING\n",
    "\n",
    "\n",
    "for (imgs, gt) in train_loader:\n",
    "    print(imgs[0])\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[1][0])\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[1][1])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dataString = datetime.strftime(datetime.now(), '%Y_%m_%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_result = 'experiment_numairData/'\n",
    "os.mkdir(root_result)\n",
    "model_result = root_result+'model/'\n",
    "log_result = root_result+'log/'\n",
    "os.mkdir(model_result)\n",
    "os.mkdir(log_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,  batch step: 0, loss: 76.89447784423828\n",
      "epoch: 0,  batch step: 1, loss: 180.48178100585938\n",
      "epoch: 0,  batch step: 2, loss: 237.97000122070312\n",
      "epoch: 0,  batch step: 3, loss: 62.020469665527344\n",
      "epoch: 0,  batch step: 4, loss: 47.78324508666992\n",
      "epoch: 0,  batch step: 5, loss: 69.46813201904297\n",
      "epoch: 0,  batch step: 6, loss: 77.84779357910156\n",
      "epoch: 0,  batch step: 7, loss: 99.2677230834961\n",
      "epoch: 0,  batch step: 8, loss: 134.5438690185547\n",
      "epoch: 0,  batch step: 9, loss: 119.86444091796875\n",
      "epoch: 0,  batch step: 10, loss: 43.86518859863281\n",
      "epoch: 0,  batch step: 11, loss: 100.4046630859375\n",
      "epoch: 0,  batch step: 12, loss: 157.9998779296875\n",
      "epoch: 0,  batch step: 13, loss: 171.8271942138672\n",
      "epoch: 0,  batch step: 14, loss: 80.41561126708984\n",
      "epoch: 0,  batch step: 15, loss: 200.50473022460938\n",
      "epoch: 0,  batch step: 16, loss: 111.73321533203125\n",
      "epoch: 0,  batch step: 17, loss: 60.02949142456055\n",
      "epoch: 0,  batch step: 18, loss: 280.37249755859375\n",
      "epoch: 0,  batch step: 19, loss: 318.55792236328125\n",
      "epoch: 0,  batch step: 20, loss: 50.438621520996094\n",
      "epoch: 0,  batch step: 21, loss: 71.5052490234375\n",
      "epoch: 0,  batch step: 22, loss: 468.6692810058594\n",
      "epoch: 0,  batch step: 23, loss: 282.9786376953125\n",
      "epoch: 0,  batch step: 24, loss: 60.92095184326172\n",
      "epoch: 0,  batch step: 25, loss: 54.126068115234375\n",
      "epoch: 0,  batch step: 26, loss: 63.40216064453125\n",
      "epoch: 0,  batch step: 27, loss: 159.33419799804688\n",
      "epoch: 0,  batch step: 28, loss: 84.42257690429688\n",
      "epoch: 0,  batch step: 29, loss: 312.95849609375\n",
      "epoch: 0,  batch step: 30, loss: 74.76649475097656\n",
      "epoch: 0,  batch step: 31, loss: 62.483680725097656\n",
      "epoch: 0,  batch step: 32, loss: 129.2911376953125\n",
      "epoch: 0,  batch step: 33, loss: 123.7186279296875\n",
      "epoch: 0,  batch step: 34, loss: 47.646236419677734\n",
      "epoch: 0,  batch step: 35, loss: 141.26724243164062\n",
      "epoch: 0,  batch step: 36, loss: 169.56008911132812\n",
      "epoch: 0,  batch step: 37, loss: 49.57758331298828\n",
      "epoch: 0,  batch step: 38, loss: 267.45513916015625\n",
      "epoch: 0,  batch step: 39, loss: 40.332332611083984\n",
      "epoch: 0,  batch step: 40, loss: 53.87957000732422\n",
      "epoch: 0,  batch step: 41, loss: 55.48972702026367\n",
      "epoch: 0,  batch step: 42, loss: 65.45662689208984\n",
      "epoch: 0,  batch step: 43, loss: 64.45942687988281\n",
      "epoch: 0,  batch step: 44, loss: 357.4710693359375\n",
      "epoch: 0,  batch step: 45, loss: 56.83016586303711\n",
      "epoch: 0,  batch step: 46, loss: 256.6463623046875\n",
      "epoch: 0,  batch step: 47, loss: 52.93505859375\n",
      "epoch: 0,  batch step: 48, loss: 35.93355941772461\n",
      "epoch: 0,  batch step: 49, loss: 304.263916015625\n",
      "epoch: 0,  batch step: 50, loss: 47.620277404785156\n",
      "epoch: 0,  batch step: 51, loss: 80.10016632080078\n",
      "epoch: 0,  batch step: 52, loss: 245.63955688476562\n",
      "epoch: 0,  batch step: 53, loss: 72.21652221679688\n",
      "epoch: 0,  batch step: 54, loss: 99.76927947998047\n",
      "epoch: 0,  batch step: 55, loss: 71.79073333740234\n",
      "epoch: 0,  batch step: 56, loss: 66.25419616699219\n",
      "epoch: 0,  batch step: 57, loss: 37.8161735534668\n",
      "epoch: 0,  batch step: 58, loss: 287.99066162109375\n",
      "epoch: 0,  batch step: 59, loss: 210.50531005859375\n",
      "epoch: 0,  batch step: 60, loss: 220.3404998779297\n",
      "epoch: 0,  batch step: 61, loss: 188.90696716308594\n",
      "epoch: 0,  batch step: 62, loss: 65.5569839477539\n",
      "epoch: 0,  batch step: 63, loss: 36.11560821533203\n",
      "epoch: 0,  batch step: 64, loss: 233.99267578125\n",
      "epoch: 0,  batch step: 65, loss: 124.9204330444336\n",
      "epoch: 0,  batch step: 66, loss: 475.0340270996094\n",
      "epoch: 0,  batch step: 67, loss: 301.1614990234375\n",
      "epoch: 0,  batch step: 68, loss: 70.02900695800781\n",
      "epoch: 0,  batch step: 69, loss: 155.9912872314453\n",
      "epoch: 0,  batch step: 70, loss: 113.75799560546875\n",
      "epoch: 0,  batch step: 71, loss: 319.093994140625\n",
      "epoch: 0,  batch step: 72, loss: 85.40229797363281\n",
      "epoch: 0,  batch step: 73, loss: 65.26901245117188\n",
      "epoch: 0,  batch step: 74, loss: 43.39152145385742\n",
      "epoch: 0,  batch step: 75, loss: 48.92723846435547\n",
      "epoch: 0,  batch step: 76, loss: 232.83543395996094\n",
      "epoch: 0,  batch step: 77, loss: 131.36956787109375\n",
      "epoch: 0,  batch step: 78, loss: 23.3563232421875\n",
      "epoch: 0,  batch step: 79, loss: 75.89216613769531\n",
      "epoch: 0,  batch step: 80, loss: 87.91146850585938\n",
      "epoch: 0,  batch step: 81, loss: 67.03010559082031\n",
      "epoch: 0,  batch step: 82, loss: 202.82781982421875\n",
      "epoch: 0,  batch step: 83, loss: 56.87582015991211\n",
      "epoch: 0,  batch step: 84, loss: 61.25434875488281\n",
      "epoch: 0,  batch step: 85, loss: 66.88671875\n",
      "epoch: 0,  batch step: 86, loss: 98.7148666381836\n",
      "epoch: 0,  batch step: 87, loss: 80.03605651855469\n",
      "epoch: 0,  batch step: 88, loss: 66.67807006835938\n",
      "epoch: 0,  batch step: 89, loss: 47.535972595214844\n",
      "epoch: 0,  batch step: 90, loss: 80.74518585205078\n",
      "epoch: 0,  batch step: 91, loss: 43.12879180908203\n",
      "epoch: 0,  batch step: 92, loss: 114.02861785888672\n",
      "epoch: 0,  batch step: 93, loss: 65.97114562988281\n",
      "epoch: 0,  batch step: 94, loss: 137.8341522216797\n",
      "epoch: 0,  batch step: 95, loss: 74.60171508789062\n",
      "epoch: 0,  batch step: 96, loss: 244.5428924560547\n",
      "epoch: 0,  batch step: 97, loss: 67.8591537475586\n",
      "epoch: 0,  batch step: 98, loss: 130.86349487304688\n",
      "epoch: 0,  batch step: 99, loss: 123.22354125976562\n",
      "epoch: 0,  batch step: 100, loss: 53.208580017089844\n",
      "epoch: 0,  batch step: 101, loss: 54.87736129760742\n",
      "epoch: 0,  batch step: 102, loss: 87.05171966552734\n",
      "epoch: 0,  batch step: 103, loss: 268.54827880859375\n",
      "epoch: 0,  batch step: 104, loss: 52.60244369506836\n",
      "epoch: 0,  batch step: 105, loss: 67.91070556640625\n",
      "epoch: 0,  batch step: 106, loss: 50.081329345703125\n",
      "epoch: 0,  batch step: 107, loss: 41.503211975097656\n",
      "epoch: 0,  batch step: 108, loss: 40.53656005859375\n",
      "epoch: 0,  batch step: 109, loss: 53.55180358886719\n",
      "epoch: 0,  batch step: 110, loss: 59.05561447143555\n",
      "epoch: 0,  batch step: 111, loss: 41.08226013183594\n",
      "epoch: 0,  batch step: 112, loss: 57.3880615234375\n",
      "epoch: 0,  batch step: 113, loss: 50.642539978027344\n",
      "epoch: 0,  batch step: 114, loss: 49.85470199584961\n",
      "epoch: 0,  batch step: 115, loss: 207.71237182617188\n",
      "epoch: 0,  batch step: 116, loss: 72.306884765625\n",
      "epoch: 0,  batch step: 117, loss: 58.5432243347168\n",
      "epoch: 0,  batch step: 118, loss: 68.29267120361328\n",
      "epoch: 0,  batch step: 119, loss: 65.80284881591797\n",
      "epoch: 0,  batch step: 120, loss: 259.1059265136719\n",
      "epoch: 0,  batch step: 121, loss: 33.246788024902344\n",
      "epoch: 0,  batch step: 122, loss: 257.2616271972656\n",
      "epoch: 0,  batch step: 123, loss: 86.89302062988281\n",
      "epoch: 0,  batch step: 124, loss: 183.1212615966797\n",
      "epoch: 0,  batch step: 125, loss: 101.89757537841797\n",
      "epoch: 0,  batch step: 126, loss: 117.68313598632812\n",
      "epoch: 0,  batch step: 127, loss: 49.025360107421875\n",
      "epoch: 0,  batch step: 128, loss: 108.18548583984375\n",
      "epoch: 0,  batch step: 129, loss: 56.683013916015625\n",
      "epoch: 0,  batch step: 130, loss: 58.6353874206543\n",
      "epoch: 0,  batch step: 131, loss: 140.2093048095703\n",
      "epoch: 0,  batch step: 132, loss: 58.449058532714844\n",
      "epoch: 0,  batch step: 133, loss: 175.4199676513672\n",
      "epoch: 0,  batch step: 134, loss: 34.75016403198242\n",
      "epoch: 0,  batch step: 135, loss: 455.5220947265625\n",
      "epoch: 0,  batch step: 136, loss: 57.52155303955078\n",
      "epoch: 0,  batch step: 137, loss: 93.38995361328125\n",
      "epoch: 0,  batch step: 138, loss: 45.94200897216797\n",
      "epoch: 0,  batch step: 139, loss: 103.1502914428711\n",
      "epoch: 0,  batch step: 140, loss: 44.20074462890625\n",
      "epoch: 0,  batch step: 141, loss: 175.1785125732422\n",
      "epoch: 0,  batch step: 142, loss: 54.45833969116211\n",
      "epoch: 0,  batch step: 143, loss: 65.18904113769531\n",
      "epoch: 0,  batch step: 144, loss: 31.619606018066406\n",
      "epoch: 0,  batch step: 145, loss: 111.79033660888672\n",
      "epoch: 0,  batch step: 146, loss: 46.390892028808594\n",
      "epoch: 0,  batch step: 147, loss: 37.693458557128906\n",
      "epoch: 0,  batch step: 148, loss: 156.90194702148438\n",
      "epoch: 0,  batch step: 149, loss: 117.57122802734375\n",
      "epoch: 0,  batch step: 150, loss: 45.92564010620117\n",
      "epoch: 0,  batch step: 151, loss: 73.19688415527344\n",
      "epoch: 0,  batch step: 152, loss: 57.86228561401367\n",
      "epoch: 0,  batch step: 153, loss: 60.4945068359375\n",
      "epoch: 0,  batch step: 154, loss: 354.59326171875\n",
      "epoch: 0,  batch step: 155, loss: 60.48827362060547\n",
      "epoch: 0,  batch step: 156, loss: 55.28509521484375\n",
      "epoch: 0,  batch step: 157, loss: 294.9162292480469\n",
      "epoch: 0,  batch step: 158, loss: 329.0215759277344\n",
      "epoch: 0,  batch step: 159, loss: 202.52392578125\n",
      "epoch: 0,  batch step: 160, loss: 97.53536224365234\n",
      "epoch: 0,  batch step: 161, loss: 164.860595703125\n",
      "epoch: 0,  batch step: 162, loss: 323.60992431640625\n",
      "epoch: 0,  batch step: 163, loss: 346.2671813964844\n",
      "epoch: 0,  batch step: 164, loss: 46.906829833984375\n",
      "epoch: 0,  batch step: 165, loss: 302.923583984375\n",
      "epoch: 0,  batch step: 166, loss: 102.5409164428711\n",
      "epoch: 0,  batch step: 167, loss: 40.85387420654297\n",
      "epoch: 0,  batch step: 168, loss: 113.24690246582031\n",
      "epoch: 0,  batch step: 169, loss: 156.50857543945312\n",
      "epoch: 0,  batch step: 170, loss: 174.63963317871094\n",
      "epoch: 0,  batch step: 171, loss: 147.05032348632812\n",
      "epoch: 0,  batch step: 172, loss: 77.61984252929688\n",
      "epoch: 0,  batch step: 173, loss: 63.16726303100586\n",
      "epoch: 0,  batch step: 174, loss: 37.084136962890625\n",
      "epoch: 0,  batch step: 175, loss: 65.02181243896484\n",
      "epoch: 0,  batch step: 176, loss: 48.75311279296875\n",
      "epoch: 0,  batch step: 177, loss: 46.28551483154297\n",
      "epoch: 0,  batch step: 178, loss: 112.17286682128906\n",
      "epoch: 0,  batch step: 179, loss: 240.72747802734375\n",
      "epoch: 0,  batch step: 180, loss: 30.82177734375\n",
      "epoch: 0,  batch step: 181, loss: 101.49899291992188\n",
      "epoch: 0,  batch step: 182, loss: 76.95304107666016\n",
      "epoch: 0,  batch step: 183, loss: 47.616004943847656\n",
      "epoch: 0,  batch step: 184, loss: 35.85957336425781\n",
      "epoch: 0,  batch step: 185, loss: 44.355682373046875\n",
      "epoch: 0,  batch step: 186, loss: 76.95321655273438\n",
      "epoch: 0,  batch step: 187, loss: 124.1019515991211\n",
      "epoch: 0,  batch step: 188, loss: 41.83650588989258\n",
      "epoch: 0,  batch step: 189, loss: 440.34661865234375\n",
      "epoch: 0,  batch step: 190, loss: 101.59719848632812\n",
      "epoch: 0,  batch step: 191, loss: 261.4461669921875\n",
      "epoch: 0,  batch step: 192, loss: 76.63919067382812\n",
      "epoch: 0,  batch step: 193, loss: 43.44829559326172\n",
      "epoch: 0,  batch step: 194, loss: 97.99845886230469\n",
      "epoch: 0,  batch step: 195, loss: 55.945030212402344\n",
      "epoch: 0,  batch step: 196, loss: 148.90174865722656\n",
      "epoch: 0,  batch step: 197, loss: 274.2583312988281\n",
      "epoch: 0,  batch step: 198, loss: 81.7357177734375\n",
      "epoch: 0,  batch step: 199, loss: 48.054901123046875\n",
      "epoch: 0,  batch step: 200, loss: 41.420692443847656\n",
      "epoch: 0,  batch step: 201, loss: 80.79924011230469\n",
      "epoch: 0,  batch step: 202, loss: 327.5111999511719\n",
      "epoch: 0,  batch step: 203, loss: 75.90623474121094\n",
      "epoch: 0,  batch step: 204, loss: 96.6279525756836\n",
      "epoch: 0,  batch step: 205, loss: 42.95348358154297\n",
      "epoch: 0,  batch step: 206, loss: 52.14720916748047\n",
      "epoch: 0,  batch step: 207, loss: 49.55320358276367\n",
      "epoch: 0,  batch step: 208, loss: 43.56389617919922\n",
      "epoch: 0,  batch step: 209, loss: 48.83789825439453\n",
      "epoch: 0,  batch step: 210, loss: 75.31285858154297\n",
      "epoch: 0,  batch step: 211, loss: 107.98909759521484\n",
      "epoch: 0,  batch step: 212, loss: 37.75447082519531\n",
      "epoch: 0,  batch step: 213, loss: 60.64374923706055\n",
      "epoch: 0,  batch step: 214, loss: 156.58126831054688\n",
      "epoch: 0,  batch step: 215, loss: 241.52392578125\n",
      "epoch: 0,  batch step: 216, loss: 171.40997314453125\n",
      "epoch: 0,  batch step: 217, loss: 141.91964721679688\n",
      "epoch: 0,  batch step: 218, loss: 560.380615234375\n",
      "epoch: 0,  batch step: 219, loss: 41.65592575073242\n",
      "epoch: 0,  batch step: 220, loss: 315.0157470703125\n",
      "epoch: 0,  batch step: 221, loss: 629.1636962890625\n",
      "epoch: 0,  batch step: 222, loss: 46.6744384765625\n",
      "epoch: 0,  batch step: 223, loss: 71.67988586425781\n",
      "epoch: 0,  batch step: 224, loss: 535.5880126953125\n",
      "epoch: 0,  batch step: 225, loss: 339.00048828125\n",
      "epoch: 0,  batch step: 226, loss: 77.97823333740234\n",
      "epoch: 0,  batch step: 227, loss: 64.77572631835938\n",
      "epoch: 0,  batch step: 228, loss: 37.93452835083008\n",
      "epoch: 0,  batch step: 229, loss: 64.6347885131836\n",
      "epoch: 0,  batch step: 230, loss: 56.800933837890625\n",
      "epoch: 0,  batch step: 231, loss: 89.6328353881836\n",
      "epoch: 0,  batch step: 232, loss: 146.72735595703125\n",
      "epoch: 0,  batch step: 233, loss: 54.80451583862305\n",
      "epoch: 0,  batch step: 234, loss: 91.13529968261719\n",
      "epoch: 0,  batch step: 235, loss: 242.81259155273438\n",
      "epoch: 0,  batch step: 236, loss: 51.96637725830078\n",
      "epoch: 0,  batch step: 237, loss: 87.8494873046875\n",
      "epoch: 0,  batch step: 238, loss: 426.0275573730469\n",
      "epoch: 0,  batch step: 239, loss: 33.74497985839844\n",
      "epoch: 0,  batch step: 240, loss: 37.88866424560547\n",
      "epoch: 0,  batch step: 241, loss: 254.08065795898438\n",
      "epoch: 0,  batch step: 242, loss: 80.12149810791016\n",
      "epoch: 0,  batch step: 243, loss: 62.41411590576172\n",
      "epoch: 0,  batch step: 244, loss: 62.92814254760742\n",
      "epoch: 0,  batch step: 245, loss: 69.98806762695312\n",
      "epoch: 0,  batch step: 246, loss: 29.66309928894043\n",
      "epoch: 0,  batch step: 247, loss: 216.64700317382812\n",
      "epoch: 0,  batch step: 248, loss: 123.27484130859375\n",
      "epoch: 0,  batch step: 249, loss: 48.42610168457031\n",
      "epoch: 0,  batch step: 250, loss: 274.51739501953125\n",
      "epoch: 0,  batch step: 251, loss: 44.30765914916992\n",
      "validation error epoch  0:    tensor(330.7137, device='cuda:0')\n",
      "316\n",
      "epoch: 1,  batch step: 0, loss: 63.687984466552734\n",
      "epoch: 1,  batch step: 1, loss: 88.13768768310547\n",
      "epoch: 1,  batch step: 2, loss: 397.5119323730469\n",
      "epoch: 1,  batch step: 3, loss: 131.2244873046875\n",
      "epoch: 1,  batch step: 4, loss: 48.88275146484375\n",
      "epoch: 1,  batch step: 5, loss: 258.2202453613281\n",
      "epoch: 1,  batch step: 6, loss: 185.10122680664062\n",
      "epoch: 1,  batch step: 7, loss: 81.38998413085938\n",
      "epoch: 1,  batch step: 8, loss: 105.52442932128906\n",
      "epoch: 1,  batch step: 9, loss: 270.94049072265625\n",
      "epoch: 1,  batch step: 10, loss: 186.52981567382812\n",
      "epoch: 1,  batch step: 11, loss: 37.083858489990234\n",
      "epoch: 1,  batch step: 12, loss: 85.06059265136719\n",
      "epoch: 1,  batch step: 13, loss: 65.15380859375\n",
      "epoch: 1,  batch step: 14, loss: 460.80126953125\n",
      "epoch: 1,  batch step: 15, loss: 73.99684143066406\n",
      "epoch: 1,  batch step: 16, loss: 80.24066162109375\n",
      "epoch: 1,  batch step: 17, loss: 77.03032684326172\n",
      "epoch: 1,  batch step: 18, loss: 59.226043701171875\n",
      "epoch: 1,  batch step: 19, loss: 190.82254028320312\n",
      "epoch: 1,  batch step: 20, loss: 61.74219512939453\n",
      "epoch: 1,  batch step: 21, loss: 110.20906829833984\n",
      "epoch: 1,  batch step: 22, loss: 74.59707641601562\n",
      "epoch: 1,  batch step: 23, loss: 63.154747009277344\n",
      "epoch: 1,  batch step: 24, loss: 40.99259567260742\n",
      "epoch: 1,  batch step: 25, loss: 63.22410202026367\n",
      "epoch: 1,  batch step: 26, loss: 32.52659225463867\n",
      "epoch: 1,  batch step: 27, loss: 38.97084426879883\n",
      "epoch: 1,  batch step: 28, loss: 73.99134063720703\n",
      "epoch: 1,  batch step: 29, loss: 289.01068115234375\n",
      "epoch: 1,  batch step: 30, loss: 76.95217895507812\n",
      "epoch: 1,  batch step: 31, loss: 156.41842651367188\n",
      "epoch: 1,  batch step: 32, loss: 135.49105834960938\n",
      "epoch: 1,  batch step: 33, loss: 72.49373626708984\n",
      "epoch: 1,  batch step: 34, loss: 94.88458251953125\n",
      "epoch: 1,  batch step: 35, loss: 42.8389892578125\n",
      "epoch: 1,  batch step: 36, loss: 31.997486114501953\n",
      "epoch: 1,  batch step: 37, loss: 274.5806579589844\n",
      "epoch: 1,  batch step: 38, loss: 322.1416320800781\n",
      "epoch: 1,  batch step: 39, loss: 72.32329559326172\n",
      "epoch: 1,  batch step: 40, loss: 442.6947021484375\n",
      "epoch: 1,  batch step: 41, loss: 146.65750122070312\n",
      "epoch: 1,  batch step: 42, loss: 504.4939880371094\n",
      "epoch: 1,  batch step: 43, loss: 289.3934326171875\n",
      "epoch: 1,  batch step: 44, loss: 177.28863525390625\n",
      "epoch: 1,  batch step: 45, loss: 193.79470825195312\n",
      "epoch: 1,  batch step: 46, loss: 220.4798583984375\n",
      "epoch: 1,  batch step: 47, loss: 85.24403381347656\n",
      "epoch: 1,  batch step: 48, loss: 31.928386688232422\n",
      "epoch: 1,  batch step: 49, loss: 211.23045349121094\n",
      "epoch: 1,  batch step: 50, loss: 80.17794036865234\n",
      "epoch: 1,  batch step: 51, loss: 51.404441833496094\n",
      "epoch: 1,  batch step: 52, loss: 30.270254135131836\n",
      "epoch: 1,  batch step: 53, loss: 92.00376892089844\n",
      "epoch: 1,  batch step: 54, loss: 211.85223388671875\n",
      "epoch: 1,  batch step: 55, loss: 55.971778869628906\n",
      "epoch: 1,  batch step: 56, loss: 49.11888122558594\n",
      "epoch: 1,  batch step: 57, loss: 72.05976867675781\n",
      "epoch: 1,  batch step: 58, loss: 72.02828979492188\n",
      "epoch: 1,  batch step: 59, loss: 64.82206726074219\n",
      "epoch: 1,  batch step: 60, loss: 51.826133728027344\n",
      "epoch: 1,  batch step: 61, loss: 69.87785339355469\n",
      "epoch: 1,  batch step: 62, loss: 251.12548828125\n",
      "epoch: 1,  batch step: 63, loss: 57.6062126159668\n",
      "epoch: 1,  batch step: 64, loss: 244.0896453857422\n",
      "epoch: 1,  batch step: 65, loss: 42.841976165771484\n",
      "epoch: 1,  batch step: 66, loss: 61.45155334472656\n",
      "epoch: 1,  batch step: 67, loss: 35.06819152832031\n",
      "epoch: 1,  batch step: 68, loss: 81.8842544555664\n",
      "epoch: 1,  batch step: 69, loss: 46.732200622558594\n",
      "epoch: 1,  batch step: 70, loss: 55.6182975769043\n",
      "epoch: 1,  batch step: 71, loss: 323.4070129394531\n",
      "epoch: 1,  batch step: 72, loss: 107.73207092285156\n",
      "epoch: 1,  batch step: 73, loss: 219.4905242919922\n",
      "epoch: 1,  batch step: 74, loss: 146.39935302734375\n",
      "epoch: 1,  batch step: 75, loss: 115.84942626953125\n",
      "epoch: 1,  batch step: 76, loss: 95.11827850341797\n",
      "epoch: 1,  batch step: 77, loss: 107.48606872558594\n",
      "epoch: 1,  batch step: 78, loss: 67.13487243652344\n",
      "epoch: 1,  batch step: 79, loss: 151.99508666992188\n",
      "epoch: 1,  batch step: 80, loss: 38.60173797607422\n",
      "epoch: 1,  batch step: 81, loss: 58.77692794799805\n",
      "epoch: 1,  batch step: 82, loss: 223.783203125\n",
      "epoch: 1,  batch step: 83, loss: 52.389278411865234\n",
      "epoch: 1,  batch step: 84, loss: 108.8497543334961\n",
      "epoch: 1,  batch step: 85, loss: 69.94807434082031\n",
      "epoch: 1,  batch step: 86, loss: 181.98277282714844\n",
      "epoch: 1,  batch step: 87, loss: 131.0459442138672\n",
      "epoch: 1,  batch step: 88, loss: 299.82879638671875\n",
      "epoch: 1,  batch step: 89, loss: 175.16943359375\n",
      "epoch: 1,  batch step: 90, loss: 145.01394653320312\n",
      "epoch: 1,  batch step: 91, loss: 44.97213363647461\n",
      "epoch: 1,  batch step: 92, loss: 311.8609313964844\n",
      "epoch: 1,  batch step: 93, loss: 436.437744140625\n",
      "epoch: 1,  batch step: 94, loss: 60.80094528198242\n",
      "epoch: 1,  batch step: 95, loss: 44.809349060058594\n",
      "epoch: 1,  batch step: 96, loss: 87.32564544677734\n",
      "epoch: 1,  batch step: 97, loss: 261.1649475097656\n",
      "epoch: 1,  batch step: 98, loss: 196.2639617919922\n",
      "epoch: 1,  batch step: 99, loss: 360.8916015625\n",
      "epoch: 1,  batch step: 100, loss: 63.40644836425781\n",
      "epoch: 1,  batch step: 101, loss: 70.33352661132812\n",
      "epoch: 1,  batch step: 102, loss: 81.29313659667969\n",
      "epoch: 1,  batch step: 103, loss: 50.11579895019531\n",
      "epoch: 1,  batch step: 104, loss: 81.07322692871094\n",
      "epoch: 1,  batch step: 105, loss: 57.879310607910156\n",
      "epoch: 1,  batch step: 106, loss: 265.61639404296875\n",
      "epoch: 1,  batch step: 107, loss: 48.45579147338867\n",
      "epoch: 1,  batch step: 108, loss: 187.19952392578125\n",
      "epoch: 1,  batch step: 109, loss: 26.490684509277344\n",
      "epoch: 1,  batch step: 110, loss: 145.86569213867188\n",
      "epoch: 1,  batch step: 111, loss: 211.0199432373047\n",
      "epoch: 1,  batch step: 112, loss: 51.19190979003906\n",
      "epoch: 1,  batch step: 113, loss: 134.07635498046875\n",
      "epoch: 1,  batch step: 114, loss: 30.476469039916992\n",
      "epoch: 1,  batch step: 115, loss: 48.109100341796875\n",
      "epoch: 1,  batch step: 116, loss: 39.88587188720703\n",
      "epoch: 1,  batch step: 117, loss: 176.8962860107422\n",
      "epoch: 1,  batch step: 118, loss: 264.48486328125\n",
      "epoch: 1,  batch step: 119, loss: 311.2488708496094\n",
      "epoch: 1,  batch step: 120, loss: 134.6145782470703\n",
      "epoch: 1,  batch step: 121, loss: 111.05570983886719\n",
      "epoch: 1,  batch step: 122, loss: 185.80313110351562\n",
      "epoch: 1,  batch step: 123, loss: 374.1707458496094\n",
      "epoch: 1,  batch step: 124, loss: 42.64677429199219\n",
      "epoch: 1,  batch step: 125, loss: 64.42556762695312\n",
      "epoch: 1,  batch step: 126, loss: 34.76251983642578\n",
      "epoch: 1,  batch step: 127, loss: 418.75775146484375\n",
      "epoch: 1,  batch step: 128, loss: 95.50187683105469\n",
      "epoch: 1,  batch step: 129, loss: 56.405029296875\n",
      "epoch: 1,  batch step: 130, loss: 47.295867919921875\n",
      "epoch: 1,  batch step: 131, loss: 71.86992645263672\n",
      "epoch: 1,  batch step: 132, loss: 180.72604370117188\n",
      "epoch: 1,  batch step: 133, loss: 328.9135437011719\n",
      "epoch: 1,  batch step: 134, loss: 39.74964904785156\n",
      "epoch: 1,  batch step: 135, loss: 63.0405387878418\n",
      "epoch: 1,  batch step: 136, loss: 60.39410400390625\n",
      "epoch: 1,  batch step: 137, loss: 47.6251335144043\n",
      "epoch: 1,  batch step: 138, loss: 58.923667907714844\n",
      "epoch: 1,  batch step: 139, loss: 82.42184448242188\n",
      "epoch: 1,  batch step: 140, loss: 39.5352897644043\n",
      "epoch: 1,  batch step: 141, loss: 127.36167907714844\n",
      "epoch: 1,  batch step: 142, loss: 70.63137817382812\n",
      "epoch: 1,  batch step: 143, loss: 37.89115905761719\n",
      "epoch: 1,  batch step: 144, loss: 168.9683074951172\n",
      "epoch: 1,  batch step: 145, loss: 48.62064743041992\n",
      "epoch: 1,  batch step: 146, loss: 58.644073486328125\n",
      "epoch: 1,  batch step: 147, loss: 63.61863327026367\n",
      "epoch: 1,  batch step: 148, loss: 121.367919921875\n",
      "epoch: 1,  batch step: 149, loss: 88.26170349121094\n",
      "epoch: 1,  batch step: 150, loss: 60.70255661010742\n",
      "epoch: 1,  batch step: 151, loss: 68.15685272216797\n",
      "epoch: 1,  batch step: 152, loss: 107.57890319824219\n",
      "epoch: 1,  batch step: 153, loss: 54.315956115722656\n",
      "epoch: 1,  batch step: 154, loss: 125.56425476074219\n",
      "epoch: 1,  batch step: 155, loss: 217.48814392089844\n",
      "epoch: 1,  batch step: 156, loss: 57.135372161865234\n",
      "epoch: 1,  batch step: 157, loss: 41.20612716674805\n",
      "epoch: 1,  batch step: 158, loss: 284.53759765625\n",
      "epoch: 1,  batch step: 159, loss: 54.82047653198242\n",
      "epoch: 1,  batch step: 160, loss: 273.31683349609375\n",
      "epoch: 1,  batch step: 161, loss: 203.0523681640625\n",
      "epoch: 1,  batch step: 162, loss: 51.075923919677734\n",
      "epoch: 1,  batch step: 163, loss: 64.04591369628906\n",
      "epoch: 1,  batch step: 164, loss: 67.2197036743164\n",
      "epoch: 1,  batch step: 165, loss: 257.21795654296875\n",
      "epoch: 1,  batch step: 166, loss: 144.67852783203125\n",
      "epoch: 1,  batch step: 167, loss: 88.56401062011719\n",
      "epoch: 1,  batch step: 168, loss: 41.671016693115234\n",
      "epoch: 1,  batch step: 169, loss: 108.975830078125\n",
      "epoch: 1,  batch step: 170, loss: 76.61721801757812\n",
      "epoch: 1,  batch step: 171, loss: 102.60032653808594\n",
      "epoch: 1,  batch step: 172, loss: 705.5829467773438\n",
      "epoch: 1,  batch step: 173, loss: 39.69593811035156\n",
      "epoch: 1,  batch step: 174, loss: 56.876609802246094\n",
      "epoch: 1,  batch step: 175, loss: 57.6163330078125\n",
      "epoch: 1,  batch step: 176, loss: 64.13789367675781\n",
      "epoch: 1,  batch step: 177, loss: 52.266563415527344\n",
      "epoch: 1,  batch step: 178, loss: 259.09869384765625\n",
      "epoch: 1,  batch step: 179, loss: 27.241512298583984\n",
      "epoch: 1,  batch step: 180, loss: 147.25880432128906\n",
      "epoch: 1,  batch step: 181, loss: 87.3484115600586\n",
      "epoch: 1,  batch step: 182, loss: 65.74412536621094\n",
      "epoch: 1,  batch step: 183, loss: 136.18875122070312\n",
      "epoch: 1,  batch step: 184, loss: 93.90130615234375\n",
      "epoch: 1,  batch step: 185, loss: 42.413475036621094\n",
      "epoch: 1,  batch step: 186, loss: 76.38276672363281\n",
      "epoch: 1,  batch step: 187, loss: 59.039756774902344\n",
      "epoch: 1,  batch step: 188, loss: 113.26194763183594\n",
      "epoch: 1,  batch step: 189, loss: 61.33111572265625\n",
      "epoch: 1,  batch step: 190, loss: 94.83663940429688\n",
      "epoch: 1,  batch step: 191, loss: 61.29746627807617\n",
      "epoch: 1,  batch step: 192, loss: 80.88179779052734\n",
      "epoch: 1,  batch step: 193, loss: 135.46481323242188\n",
      "epoch: 1,  batch step: 194, loss: 41.685184478759766\n",
      "epoch: 1,  batch step: 195, loss: 37.31929016113281\n",
      "epoch: 1,  batch step: 196, loss: 102.08641815185547\n",
      "epoch: 1,  batch step: 197, loss: 564.7467041015625\n",
      "epoch: 1,  batch step: 198, loss: 59.918701171875\n",
      "epoch: 1,  batch step: 199, loss: 85.2225341796875\n",
      "epoch: 1,  batch step: 200, loss: 82.78398895263672\n",
      "epoch: 1,  batch step: 201, loss: 73.5392074584961\n",
      "epoch: 1,  batch step: 202, loss: 238.0648193359375\n",
      "epoch: 1,  batch step: 203, loss: 69.84081268310547\n",
      "epoch: 1,  batch step: 204, loss: 38.23299789428711\n",
      "epoch: 1,  batch step: 205, loss: 153.80160522460938\n",
      "epoch: 1,  batch step: 206, loss: 246.3116455078125\n",
      "epoch: 1,  batch step: 207, loss: 43.28385925292969\n",
      "epoch: 1,  batch step: 208, loss: 211.02349853515625\n",
      "epoch: 1,  batch step: 209, loss: 322.988037109375\n",
      "epoch: 1,  batch step: 210, loss: 110.99776458740234\n",
      "epoch: 1,  batch step: 211, loss: 26.140981674194336\n",
      "epoch: 1,  batch step: 212, loss: 51.95301818847656\n",
      "epoch: 1,  batch step: 213, loss: 29.592632293701172\n",
      "epoch: 1,  batch step: 214, loss: 71.8149642944336\n",
      "epoch: 1,  batch step: 215, loss: 40.6962890625\n",
      "epoch: 1,  batch step: 216, loss: 59.726009368896484\n",
      "epoch: 1,  batch step: 217, loss: 95.34806823730469\n",
      "epoch: 1,  batch step: 218, loss: 345.83013916015625\n",
      "epoch: 1,  batch step: 219, loss: 168.97604370117188\n",
      "epoch: 1,  batch step: 220, loss: 166.71499633789062\n",
      "epoch: 1,  batch step: 221, loss: 42.48224639892578\n",
      "epoch: 1,  batch step: 222, loss: 342.8314208984375\n",
      "epoch: 1,  batch step: 223, loss: 85.81399536132812\n",
      "epoch: 1,  batch step: 224, loss: 122.7508773803711\n",
      "epoch: 1,  batch step: 225, loss: 75.59927368164062\n",
      "epoch: 1,  batch step: 226, loss: 71.8251724243164\n",
      "epoch: 1,  batch step: 227, loss: 41.72477722167969\n",
      "epoch: 1,  batch step: 228, loss: 59.77747344970703\n",
      "epoch: 1,  batch step: 229, loss: 43.711181640625\n",
      "epoch: 1,  batch step: 230, loss: 47.455360412597656\n",
      "epoch: 1,  batch step: 231, loss: 47.919334411621094\n",
      "epoch: 1,  batch step: 232, loss: 215.74038696289062\n",
      "epoch: 1,  batch step: 233, loss: 42.01722717285156\n",
      "epoch: 1,  batch step: 234, loss: 56.44501495361328\n",
      "epoch: 1,  batch step: 235, loss: 39.921234130859375\n",
      "epoch: 1,  batch step: 236, loss: 62.72541427612305\n",
      "epoch: 1,  batch step: 237, loss: 35.50091552734375\n",
      "epoch: 1,  batch step: 238, loss: 39.90689468383789\n",
      "epoch: 1,  batch step: 239, loss: 42.27047348022461\n",
      "epoch: 1,  batch step: 240, loss: 122.48135375976562\n",
      "epoch: 1,  batch step: 241, loss: 202.2749481201172\n",
      "epoch: 1,  batch step: 242, loss: 70.3250961303711\n",
      "epoch: 1,  batch step: 243, loss: 58.22808074951172\n",
      "epoch: 1,  batch step: 244, loss: 52.70432662963867\n",
      "epoch: 1,  batch step: 245, loss: 56.99959945678711\n",
      "epoch: 1,  batch step: 246, loss: 45.22211837768555\n",
      "epoch: 1,  batch step: 247, loss: 19.307945251464844\n",
      "epoch: 1,  batch step: 248, loss: 73.20751190185547\n",
      "epoch: 1,  batch step: 249, loss: 228.94427490234375\n",
      "epoch: 1,  batch step: 250, loss: 38.05613327026367\n",
      "epoch: 1,  batch step: 251, loss: 145.5172119140625\n",
      "validation error epoch  1:    tensor(123.8428, device='cuda:0')\n",
      "316\n",
      "epoch: 2,  batch step: 0, loss: 84.68099975585938\n",
      "epoch: 2,  batch step: 1, loss: 295.4930419921875\n",
      "epoch: 2,  batch step: 2, loss: 104.93293762207031\n",
      "epoch: 2,  batch step: 3, loss: 265.71185302734375\n",
      "epoch: 2,  batch step: 4, loss: 66.62777709960938\n",
      "epoch: 2,  batch step: 5, loss: 47.476715087890625\n",
      "epoch: 2,  batch step: 6, loss: 54.329002380371094\n",
      "epoch: 2,  batch step: 7, loss: 257.0933837890625\n",
      "epoch: 2,  batch step: 8, loss: 48.69492721557617\n",
      "epoch: 2,  batch step: 9, loss: 58.82160949707031\n",
      "epoch: 2,  batch step: 10, loss: 74.98851776123047\n",
      "epoch: 2,  batch step: 11, loss: 57.5711784362793\n",
      "epoch: 2,  batch step: 12, loss: 57.259212493896484\n",
      "epoch: 2,  batch step: 13, loss: 60.93119430541992\n",
      "epoch: 2,  batch step: 14, loss: 43.783172607421875\n",
      "epoch: 2,  batch step: 15, loss: 223.17535400390625\n",
      "epoch: 2,  batch step: 16, loss: 53.26298522949219\n",
      "epoch: 2,  batch step: 17, loss: 36.05585479736328\n",
      "epoch: 2,  batch step: 18, loss: 65.11665344238281\n",
      "epoch: 2,  batch step: 19, loss: 80.62413024902344\n",
      "epoch: 2,  batch step: 20, loss: 44.7274284362793\n",
      "epoch: 2,  batch step: 21, loss: 145.18978881835938\n",
      "epoch: 2,  batch step: 22, loss: 59.9974479675293\n",
      "epoch: 2,  batch step: 23, loss: 499.11456298828125\n",
      "epoch: 2,  batch step: 24, loss: 42.960113525390625\n",
      "epoch: 2,  batch step: 25, loss: 64.40730285644531\n",
      "epoch: 2,  batch step: 26, loss: 75.17532348632812\n",
      "epoch: 2,  batch step: 27, loss: 285.6917419433594\n",
      "epoch: 2,  batch step: 28, loss: 63.610557556152344\n",
      "epoch: 2,  batch step: 29, loss: 47.098236083984375\n",
      "epoch: 2,  batch step: 30, loss: 45.63996124267578\n",
      "epoch: 2,  batch step: 31, loss: 53.93976974487305\n",
      "epoch: 2,  batch step: 32, loss: 223.7049560546875\n",
      "epoch: 2,  batch step: 33, loss: 91.37352752685547\n",
      "epoch: 2,  batch step: 34, loss: 63.322750091552734\n",
      "epoch: 2,  batch step: 35, loss: 38.47684860229492\n",
      "epoch: 2,  batch step: 36, loss: 315.2260437011719\n",
      "epoch: 2,  batch step: 37, loss: 79.12860107421875\n",
      "epoch: 2,  batch step: 38, loss: 219.3741455078125\n",
      "epoch: 2,  batch step: 39, loss: 93.66166687011719\n",
      "epoch: 2,  batch step: 40, loss: 37.45886993408203\n",
      "epoch: 2,  batch step: 41, loss: 141.89585876464844\n",
      "epoch: 2,  batch step: 42, loss: 33.6268424987793\n",
      "epoch: 2,  batch step: 43, loss: 202.57196044921875\n",
      "epoch: 2,  batch step: 44, loss: 76.41951751708984\n",
      "epoch: 2,  batch step: 45, loss: 76.25094604492188\n",
      "epoch: 2,  batch step: 46, loss: 48.2894287109375\n",
      "epoch: 2,  batch step: 47, loss: 28.413373947143555\n",
      "epoch: 2,  batch step: 48, loss: 219.29367065429688\n",
      "epoch: 2,  batch step: 49, loss: 53.11664962768555\n",
      "epoch: 2,  batch step: 50, loss: 66.80204772949219\n",
      "epoch: 2,  batch step: 51, loss: 51.59105682373047\n",
      "epoch: 2,  batch step: 52, loss: 321.6158752441406\n",
      "epoch: 2,  batch step: 53, loss: 55.186248779296875\n",
      "epoch: 2,  batch step: 54, loss: 90.57030487060547\n",
      "epoch: 2,  batch step: 55, loss: 261.860107421875\n",
      "epoch: 2,  batch step: 56, loss: 46.1236572265625\n",
      "epoch: 2,  batch step: 57, loss: 39.1048469543457\n",
      "epoch: 2,  batch step: 58, loss: 263.9832763671875\n",
      "epoch: 2,  batch step: 59, loss: 40.597023010253906\n",
      "epoch: 2,  batch step: 60, loss: 50.690330505371094\n",
      "epoch: 2,  batch step: 61, loss: 76.46482849121094\n",
      "epoch: 2,  batch step: 62, loss: 66.29342651367188\n",
      "epoch: 2,  batch step: 63, loss: 54.851966857910156\n",
      "epoch: 2,  batch step: 64, loss: 38.660301208496094\n",
      "epoch: 2,  batch step: 65, loss: 32.505348205566406\n",
      "epoch: 2,  batch step: 66, loss: 133.6624755859375\n",
      "epoch: 2,  batch step: 67, loss: 55.41295623779297\n",
      "epoch: 2,  batch step: 68, loss: 55.012420654296875\n",
      "epoch: 2,  batch step: 69, loss: 101.6404037475586\n",
      "epoch: 2,  batch step: 70, loss: 52.558040618896484\n",
      "epoch: 2,  batch step: 71, loss: 295.37445068359375\n",
      "epoch: 2,  batch step: 72, loss: 170.62229919433594\n",
      "epoch: 2,  batch step: 73, loss: 224.9197998046875\n",
      "epoch: 2,  batch step: 74, loss: 108.12872314453125\n",
      "epoch: 2,  batch step: 75, loss: 170.1460418701172\n",
      "epoch: 2,  batch step: 76, loss: 91.410888671875\n",
      "epoch: 2,  batch step: 77, loss: 61.653900146484375\n",
      "epoch: 2,  batch step: 78, loss: 20.135086059570312\n",
      "epoch: 2,  batch step: 79, loss: 94.9752197265625\n",
      "epoch: 2,  batch step: 80, loss: 103.77300262451172\n",
      "epoch: 2,  batch step: 81, loss: 52.99620819091797\n",
      "epoch: 2,  batch step: 82, loss: 15.349322319030762\n",
      "epoch: 2,  batch step: 83, loss: 39.78182601928711\n",
      "epoch: 2,  batch step: 84, loss: 340.51580810546875\n",
      "epoch: 2,  batch step: 85, loss: 173.53839111328125\n",
      "epoch: 2,  batch step: 86, loss: 29.305145263671875\n",
      "epoch: 2,  batch step: 87, loss: 48.38303756713867\n",
      "epoch: 2,  batch step: 88, loss: 31.338279724121094\n",
      "epoch: 2,  batch step: 89, loss: 38.065670013427734\n",
      "epoch: 2,  batch step: 90, loss: 78.08732604980469\n",
      "epoch: 2,  batch step: 91, loss: 52.82126998901367\n",
      "epoch: 2,  batch step: 92, loss: 46.896995544433594\n",
      "epoch: 2,  batch step: 93, loss: 127.68093872070312\n",
      "epoch: 2,  batch step: 94, loss: 35.03285217285156\n",
      "epoch: 2,  batch step: 95, loss: 55.46708679199219\n",
      "epoch: 2,  batch step: 96, loss: 48.562068939208984\n",
      "epoch: 2,  batch step: 97, loss: 17.65715789794922\n",
      "epoch: 2,  batch step: 98, loss: 55.74188995361328\n",
      "epoch: 2,  batch step: 99, loss: 382.01416015625\n",
      "epoch: 2,  batch step: 100, loss: 71.5643539428711\n",
      "epoch: 2,  batch step: 101, loss: 61.246761322021484\n",
      "epoch: 2,  batch step: 102, loss: 20.637434005737305\n",
      "epoch: 2,  batch step: 103, loss: 83.5860595703125\n",
      "epoch: 2,  batch step: 104, loss: 42.750762939453125\n",
      "epoch: 2,  batch step: 105, loss: 78.44642639160156\n",
      "epoch: 2,  batch step: 106, loss: 98.41046142578125\n",
      "epoch: 2,  batch step: 107, loss: 21.674657821655273\n",
      "epoch: 2,  batch step: 108, loss: 37.52967071533203\n",
      "epoch: 2,  batch step: 109, loss: 42.344783782958984\n",
      "epoch: 2,  batch step: 110, loss: 30.509511947631836\n",
      "epoch: 2,  batch step: 111, loss: 84.45085144042969\n",
      "epoch: 2,  batch step: 112, loss: 51.33475875854492\n",
      "epoch: 2,  batch step: 113, loss: 89.09417724609375\n",
      "epoch: 2,  batch step: 114, loss: 127.99349212646484\n",
      "epoch: 2,  batch step: 115, loss: 27.50387954711914\n",
      "epoch: 2,  batch step: 116, loss: 38.964324951171875\n",
      "epoch: 2,  batch step: 117, loss: 140.58877563476562\n",
      "epoch: 2,  batch step: 118, loss: 28.54133415222168\n",
      "epoch: 2,  batch step: 119, loss: 106.59941101074219\n",
      "epoch: 2,  batch step: 120, loss: 250.0418701171875\n",
      "epoch: 2,  batch step: 121, loss: 40.5448112487793\n",
      "epoch: 2,  batch step: 122, loss: 113.10225677490234\n",
      "epoch: 2,  batch step: 123, loss: 359.2984619140625\n",
      "epoch: 2,  batch step: 124, loss: 95.74577331542969\n",
      "epoch: 2,  batch step: 125, loss: 44.44475555419922\n",
      "epoch: 2,  batch step: 126, loss: 480.4527587890625\n",
      "epoch: 2,  batch step: 127, loss: 34.90741729736328\n",
      "epoch: 2,  batch step: 128, loss: 54.986549377441406\n",
      "epoch: 2,  batch step: 129, loss: 60.8184814453125\n",
      "epoch: 2,  batch step: 130, loss: 196.15904235839844\n",
      "epoch: 2,  batch step: 131, loss: 273.2362365722656\n",
      "epoch: 2,  batch step: 132, loss: 165.04092407226562\n",
      "epoch: 2,  batch step: 133, loss: 44.48268127441406\n",
      "epoch: 2,  batch step: 134, loss: 156.61477661132812\n",
      "epoch: 2,  batch step: 135, loss: 142.36654663085938\n",
      "epoch: 2,  batch step: 136, loss: 62.47315979003906\n",
      "epoch: 2,  batch step: 137, loss: 218.93490600585938\n",
      "epoch: 2,  batch step: 138, loss: 42.791290283203125\n",
      "epoch: 2,  batch step: 139, loss: 223.52975463867188\n",
      "epoch: 2,  batch step: 140, loss: 221.30242919921875\n",
      "epoch: 2,  batch step: 141, loss: 93.54045867919922\n",
      "epoch: 2,  batch step: 142, loss: 71.36009216308594\n",
      "epoch: 2,  batch step: 143, loss: 87.02523040771484\n",
      "epoch: 2,  batch step: 144, loss: 66.7019271850586\n",
      "epoch: 2,  batch step: 145, loss: 37.843170166015625\n",
      "epoch: 2,  batch step: 146, loss: 64.1024398803711\n",
      "epoch: 2,  batch step: 147, loss: 414.30377197265625\n",
      "epoch: 2,  batch step: 148, loss: 45.981292724609375\n",
      "epoch: 2,  batch step: 149, loss: 280.01092529296875\n",
      "epoch: 2,  batch step: 150, loss: 25.54682731628418\n",
      "epoch: 2,  batch step: 151, loss: 28.95758056640625\n",
      "epoch: 2,  batch step: 152, loss: 94.40380859375\n",
      "epoch: 2,  batch step: 153, loss: 256.18658447265625\n",
      "epoch: 2,  batch step: 154, loss: 109.66683197021484\n",
      "epoch: 2,  batch step: 155, loss: 93.26571655273438\n",
      "epoch: 2,  batch step: 156, loss: 273.4794616699219\n",
      "epoch: 2,  batch step: 157, loss: 49.147029876708984\n",
      "epoch: 2,  batch step: 158, loss: 34.84605407714844\n",
      "epoch: 2,  batch step: 159, loss: 249.62149047851562\n",
      "epoch: 2,  batch step: 160, loss: 46.64707565307617\n",
      "epoch: 2,  batch step: 161, loss: 54.174530029296875\n",
      "epoch: 2,  batch step: 162, loss: 98.11717987060547\n",
      "epoch: 2,  batch step: 163, loss: 16.899986267089844\n",
      "epoch: 2,  batch step: 164, loss: 37.95850372314453\n",
      "epoch: 2,  batch step: 165, loss: 194.8361053466797\n",
      "epoch: 2,  batch step: 166, loss: 20.275989532470703\n",
      "epoch: 2,  batch step: 167, loss: 173.53228759765625\n",
      "epoch: 2,  batch step: 168, loss: 27.280485153198242\n",
      "epoch: 2,  batch step: 169, loss: 132.6573944091797\n",
      "epoch: 2,  batch step: 170, loss: 129.05657958984375\n",
      "epoch: 2,  batch step: 171, loss: 69.04737091064453\n",
      "epoch: 2,  batch step: 172, loss: 20.395183563232422\n",
      "epoch: 2,  batch step: 173, loss: 147.47772216796875\n",
      "epoch: 2,  batch step: 174, loss: 398.5199890136719\n",
      "epoch: 2,  batch step: 175, loss: 33.813045501708984\n",
      "epoch: 2,  batch step: 176, loss: 32.00981140136719\n",
      "epoch: 2,  batch step: 177, loss: 67.77311706542969\n",
      "epoch: 2,  batch step: 178, loss: 160.80223083496094\n",
      "epoch: 2,  batch step: 179, loss: 30.592327117919922\n",
      "epoch: 2,  batch step: 180, loss: 19.051054000854492\n",
      "epoch: 2,  batch step: 181, loss: 49.04615783691406\n",
      "epoch: 2,  batch step: 182, loss: 215.77749633789062\n",
      "epoch: 2,  batch step: 183, loss: 27.468128204345703\n",
      "epoch: 2,  batch step: 184, loss: 267.4364318847656\n",
      "epoch: 2,  batch step: 185, loss: 21.335830688476562\n",
      "epoch: 2,  batch step: 186, loss: 237.8861541748047\n",
      "epoch: 2,  batch step: 187, loss: 89.31724548339844\n",
      "epoch: 2,  batch step: 188, loss: 49.019065856933594\n",
      "epoch: 2,  batch step: 189, loss: 19.105926513671875\n",
      "epoch: 2,  batch step: 190, loss: 192.73301696777344\n",
      "epoch: 2,  batch step: 191, loss: 24.082489013671875\n",
      "epoch: 2,  batch step: 192, loss: 21.648963928222656\n",
      "epoch: 2,  batch step: 193, loss: 383.15350341796875\n",
      "epoch: 2,  batch step: 194, loss: 135.29324340820312\n",
      "epoch: 2,  batch step: 195, loss: 34.16975402832031\n",
      "epoch: 2,  batch step: 196, loss: 227.47882080078125\n",
      "epoch: 2,  batch step: 197, loss: 32.96217346191406\n",
      "epoch: 2,  batch step: 198, loss: 27.286422729492188\n",
      "epoch: 2,  batch step: 199, loss: 284.7779541015625\n",
      "epoch: 2,  batch step: 200, loss: 63.29819869995117\n",
      "epoch: 2,  batch step: 201, loss: 29.878942489624023\n",
      "epoch: 2,  batch step: 202, loss: 55.225914001464844\n",
      "epoch: 2,  batch step: 203, loss: 51.21381759643555\n",
      "epoch: 2,  batch step: 204, loss: 35.49787902832031\n",
      "epoch: 2,  batch step: 205, loss: 96.13165283203125\n",
      "epoch: 2,  batch step: 206, loss: 21.861080169677734\n",
      "epoch: 2,  batch step: 207, loss: 220.390380859375\n",
      "epoch: 2,  batch step: 208, loss: 398.4036560058594\n",
      "epoch: 2,  batch step: 209, loss: 244.2891082763672\n",
      "epoch: 2,  batch step: 210, loss: 31.020477294921875\n",
      "epoch: 2,  batch step: 211, loss: 19.931421279907227\n",
      "epoch: 2,  batch step: 212, loss: 30.434139251708984\n",
      "epoch: 2,  batch step: 213, loss: 225.4656219482422\n",
      "epoch: 2,  batch step: 214, loss: 36.69065856933594\n",
      "epoch: 2,  batch step: 215, loss: 27.811681747436523\n",
      "epoch: 2,  batch step: 216, loss: 52.10294723510742\n",
      "epoch: 2,  batch step: 217, loss: 48.84955978393555\n",
      "epoch: 2,  batch step: 218, loss: 33.763328552246094\n",
      "epoch: 2,  batch step: 219, loss: 57.306640625\n",
      "epoch: 2,  batch step: 220, loss: 388.1072692871094\n",
      "epoch: 2,  batch step: 221, loss: 60.20953369140625\n",
      "epoch: 2,  batch step: 222, loss: 37.42285919189453\n",
      "epoch: 2,  batch step: 223, loss: 30.395172119140625\n",
      "epoch: 2,  batch step: 224, loss: 24.166561126708984\n",
      "epoch: 2,  batch step: 225, loss: 432.30596923828125\n",
      "epoch: 2,  batch step: 226, loss: 93.57452392578125\n",
      "epoch: 2,  batch step: 227, loss: 96.7037353515625\n",
      "epoch: 2,  batch step: 228, loss: 37.22075653076172\n",
      "epoch: 2,  batch step: 229, loss: 34.61109924316406\n",
      "epoch: 2,  batch step: 230, loss: 202.39215087890625\n",
      "epoch: 2,  batch step: 231, loss: 132.81700134277344\n",
      "epoch: 2,  batch step: 232, loss: 14.549551010131836\n",
      "epoch: 2,  batch step: 233, loss: 16.98623275756836\n",
      "epoch: 2,  batch step: 234, loss: 29.656835556030273\n",
      "epoch: 2,  batch step: 235, loss: 125.27780151367188\n",
      "epoch: 2,  batch step: 236, loss: 251.39756774902344\n",
      "epoch: 2,  batch step: 237, loss: 47.50016403198242\n",
      "epoch: 2,  batch step: 238, loss: 46.13695526123047\n",
      "epoch: 2,  batch step: 239, loss: 14.616781234741211\n",
      "epoch: 2,  batch step: 240, loss: 24.448171615600586\n",
      "epoch: 2,  batch step: 241, loss: 16.926170349121094\n",
      "epoch: 2,  batch step: 242, loss: 28.443876266479492\n",
      "epoch: 2,  batch step: 243, loss: 20.176738739013672\n",
      "epoch: 2,  batch step: 244, loss: 14.849593162536621\n",
      "epoch: 2,  batch step: 245, loss: 22.308666229248047\n",
      "epoch: 2,  batch step: 246, loss: 33.35929870605469\n",
      "epoch: 2,  batch step: 247, loss: 18.569278717041016\n",
      "epoch: 2,  batch step: 248, loss: 11.583839416503906\n",
      "epoch: 2,  batch step: 249, loss: 563.5574951171875\n",
      "epoch: 2,  batch step: 250, loss: 24.61520004272461\n",
      "epoch: 2,  batch step: 251, loss: 109.83283233642578\n",
      "validation error epoch  2:    tensor(107.8431, device='cuda:0')\n",
      "316\n",
      "epoch: 3,  batch step: 0, loss: 16.17188262939453\n",
      "epoch: 3,  batch step: 1, loss: 20.141357421875\n",
      "epoch: 3,  batch step: 2, loss: 65.91270446777344\n",
      "epoch: 3,  batch step: 3, loss: 30.759929656982422\n",
      "epoch: 3,  batch step: 4, loss: 278.691650390625\n",
      "epoch: 3,  batch step: 5, loss: 14.775710105895996\n",
      "epoch: 3,  batch step: 6, loss: 37.809505462646484\n",
      "epoch: 3,  batch step: 7, loss: 85.50057220458984\n",
      "epoch: 3,  batch step: 8, loss: 11.034854888916016\n",
      "epoch: 3,  batch step: 9, loss: 18.316238403320312\n",
      "epoch: 3,  batch step: 10, loss: 58.33993148803711\n",
      "epoch: 3,  batch step: 11, loss: 18.286043167114258\n",
      "epoch: 3,  batch step: 12, loss: 40.11302185058594\n",
      "epoch: 3,  batch step: 13, loss: 57.93208312988281\n",
      "epoch: 3,  batch step: 14, loss: 201.48056030273438\n",
      "epoch: 3,  batch step: 15, loss: 56.75321578979492\n",
      "epoch: 3,  batch step: 16, loss: 661.4266357421875\n",
      "epoch: 3,  batch step: 17, loss: 112.47406005859375\n",
      "epoch: 3,  batch step: 18, loss: 11.966060638427734\n",
      "epoch: 3,  batch step: 19, loss: 14.043821334838867\n",
      "epoch: 3,  batch step: 20, loss: 90.02656555175781\n",
      "epoch: 3,  batch step: 21, loss: 67.03755950927734\n",
      "epoch: 3,  batch step: 22, loss: 173.99417114257812\n",
      "epoch: 3,  batch step: 23, loss: 18.642955780029297\n",
      "epoch: 3,  batch step: 24, loss: 25.065427780151367\n",
      "epoch: 3,  batch step: 25, loss: 334.12939453125\n",
      "epoch: 3,  batch step: 26, loss: 216.3104705810547\n",
      "epoch: 3,  batch step: 27, loss: 300.171142578125\n",
      "epoch: 3,  batch step: 28, loss: 29.336027145385742\n",
      "epoch: 3,  batch step: 29, loss: 159.94683837890625\n",
      "epoch: 3,  batch step: 30, loss: 49.334808349609375\n",
      "epoch: 3,  batch step: 31, loss: 250.52557373046875\n",
      "epoch: 3,  batch step: 32, loss: 54.306304931640625\n",
      "epoch: 3,  batch step: 33, loss: 200.40853881835938\n",
      "epoch: 3,  batch step: 34, loss: 79.66070556640625\n",
      "epoch: 3,  batch step: 35, loss: 52.88273239135742\n",
      "epoch: 3,  batch step: 36, loss: 34.04069519042969\n",
      "epoch: 3,  batch step: 37, loss: 26.50469207763672\n",
      "epoch: 3,  batch step: 38, loss: 76.60967254638672\n",
      "epoch: 3,  batch step: 39, loss: 15.714129447937012\n",
      "epoch: 3,  batch step: 40, loss: 8.17805004119873\n",
      "epoch: 3,  batch step: 41, loss: 83.72210693359375\n",
      "epoch: 3,  batch step: 42, loss: 33.37538528442383\n",
      "epoch: 3,  batch step: 43, loss: 178.5662841796875\n",
      "epoch: 3,  batch step: 44, loss: 14.775729179382324\n",
      "epoch: 3,  batch step: 45, loss: 68.90135955810547\n",
      "epoch: 3,  batch step: 46, loss: 14.984729766845703\n",
      "epoch: 3,  batch step: 47, loss: 148.11590576171875\n",
      "epoch: 3,  batch step: 48, loss: 128.71405029296875\n",
      "epoch: 3,  batch step: 49, loss: 31.370296478271484\n",
      "epoch: 3,  batch step: 50, loss: 68.13667297363281\n",
      "epoch: 3,  batch step: 51, loss: 16.752601623535156\n",
      "epoch: 3,  batch step: 52, loss: 30.052017211914062\n",
      "epoch: 3,  batch step: 53, loss: 60.91871643066406\n",
      "epoch: 3,  batch step: 54, loss: 18.19498062133789\n",
      "epoch: 3,  batch step: 55, loss: 217.69342041015625\n",
      "epoch: 3,  batch step: 56, loss: 216.83139038085938\n",
      "epoch: 3,  batch step: 57, loss: 13.928031921386719\n",
      "epoch: 3,  batch step: 58, loss: 14.9380521774292\n",
      "epoch: 3,  batch step: 59, loss: 241.48590087890625\n",
      "epoch: 3,  batch step: 60, loss: 27.758148193359375\n",
      "epoch: 3,  batch step: 61, loss: 20.493370056152344\n",
      "epoch: 3,  batch step: 62, loss: 312.522705078125\n",
      "epoch: 3,  batch step: 63, loss: 14.712481498718262\n",
      "epoch: 3,  batch step: 64, loss: 135.73629760742188\n",
      "epoch: 3,  batch step: 65, loss: 15.613876342773438\n",
      "epoch: 3,  batch step: 66, loss: 25.904422760009766\n",
      "epoch: 3,  batch step: 67, loss: 31.817245483398438\n",
      "epoch: 3,  batch step: 68, loss: 16.21644401550293\n",
      "epoch: 3,  batch step: 69, loss: 216.88153076171875\n",
      "epoch: 3,  batch step: 70, loss: 136.2795867919922\n",
      "epoch: 3,  batch step: 71, loss: 18.81960105895996\n",
      "epoch: 3,  batch step: 72, loss: 27.433757781982422\n",
      "epoch: 3,  batch step: 73, loss: 46.714881896972656\n",
      "epoch: 3,  batch step: 74, loss: 17.85755157470703\n",
      "epoch: 3,  batch step: 75, loss: 18.716766357421875\n",
      "epoch: 3,  batch step: 76, loss: 13.824950218200684\n",
      "epoch: 3,  batch step: 77, loss: 73.00872039794922\n",
      "epoch: 3,  batch step: 78, loss: 20.493947982788086\n",
      "epoch: 3,  batch step: 79, loss: 71.41302490234375\n",
      "epoch: 3,  batch step: 80, loss: 9.820137023925781\n",
      "epoch: 3,  batch step: 81, loss: 216.96636962890625\n",
      "epoch: 3,  batch step: 82, loss: 30.023937225341797\n",
      "epoch: 3,  batch step: 83, loss: 148.5656280517578\n",
      "epoch: 3,  batch step: 84, loss: 200.22891235351562\n",
      "epoch: 3,  batch step: 85, loss: 278.1656494140625\n",
      "epoch: 3,  batch step: 86, loss: 23.366165161132812\n",
      "epoch: 3,  batch step: 87, loss: 18.217283248901367\n",
      "epoch: 3,  batch step: 88, loss: 158.4243927001953\n",
      "epoch: 3,  batch step: 89, loss: 107.6917953491211\n",
      "epoch: 3,  batch step: 90, loss: 49.68272018432617\n",
      "epoch: 3,  batch step: 91, loss: 199.48269653320312\n",
      "epoch: 3,  batch step: 92, loss: 26.570476531982422\n",
      "epoch: 3,  batch step: 93, loss: 19.886577606201172\n",
      "epoch: 3,  batch step: 94, loss: 35.06962966918945\n",
      "epoch: 3,  batch step: 95, loss: 322.441650390625\n",
      "epoch: 3,  batch step: 96, loss: 14.380419731140137\n",
      "epoch: 3,  batch step: 97, loss: 7.536471843719482\n",
      "epoch: 3,  batch step: 98, loss: 172.7494354248047\n",
      "epoch: 3,  batch step: 99, loss: 101.88890838623047\n",
      "epoch: 3,  batch step: 100, loss: 10.961935997009277\n",
      "epoch: 3,  batch step: 101, loss: 36.35234832763672\n",
      "epoch: 3,  batch step: 102, loss: 223.57144165039062\n",
      "epoch: 3,  batch step: 103, loss: 101.89012145996094\n",
      "epoch: 3,  batch step: 104, loss: 14.118766784667969\n",
      "epoch: 3,  batch step: 105, loss: 14.253924369812012\n",
      "epoch: 3,  batch step: 106, loss: 13.578849792480469\n",
      "epoch: 3,  batch step: 107, loss: 125.59190368652344\n",
      "epoch: 3,  batch step: 108, loss: 41.611358642578125\n",
      "epoch: 3,  batch step: 109, loss: 36.11441421508789\n",
      "epoch: 3,  batch step: 110, loss: 58.094390869140625\n",
      "epoch: 3,  batch step: 111, loss: 26.27058982849121\n",
      "epoch: 3,  batch step: 112, loss: 16.246694564819336\n",
      "epoch: 3,  batch step: 113, loss: 210.918212890625\n",
      "epoch: 3,  batch step: 114, loss: 16.125308990478516\n",
      "epoch: 3,  batch step: 115, loss: 29.978111267089844\n",
      "epoch: 3,  batch step: 116, loss: 240.0638885498047\n",
      "epoch: 3,  batch step: 117, loss: 156.86741638183594\n",
      "epoch: 3,  batch step: 118, loss: 163.51231384277344\n",
      "epoch: 3,  batch step: 119, loss: 17.67854118347168\n",
      "epoch: 3,  batch step: 120, loss: 406.11163330078125\n",
      "epoch: 3,  batch step: 121, loss: 19.758380889892578\n",
      "epoch: 3,  batch step: 122, loss: 57.61170196533203\n",
      "epoch: 3,  batch step: 123, loss: 214.55938720703125\n",
      "epoch: 3,  batch step: 124, loss: 83.57571411132812\n",
      "epoch: 3,  batch step: 125, loss: 18.820646286010742\n",
      "epoch: 3,  batch step: 126, loss: 142.1799774169922\n",
      "epoch: 3,  batch step: 127, loss: 32.84209442138672\n",
      "epoch: 3,  batch step: 128, loss: 26.379642486572266\n",
      "epoch: 3,  batch step: 129, loss: 30.692752838134766\n",
      "epoch: 3,  batch step: 130, loss: 34.98920440673828\n",
      "epoch: 3,  batch step: 131, loss: 24.043081283569336\n",
      "epoch: 3,  batch step: 132, loss: 49.03391647338867\n",
      "epoch: 3,  batch step: 133, loss: 88.82960510253906\n",
      "epoch: 3,  batch step: 134, loss: 27.644577026367188\n",
      "epoch: 3,  batch step: 135, loss: 10.452237129211426\n",
      "epoch: 3,  batch step: 136, loss: 23.000669479370117\n",
      "epoch: 3,  batch step: 137, loss: 52.460113525390625\n",
      "epoch: 3,  batch step: 138, loss: 77.12967681884766\n",
      "epoch: 3,  batch step: 139, loss: 16.972875595092773\n",
      "epoch: 3,  batch step: 140, loss: 15.554067611694336\n",
      "epoch: 3,  batch step: 141, loss: 10.34126091003418\n",
      "epoch: 3,  batch step: 142, loss: 33.23899459838867\n",
      "epoch: 3,  batch step: 143, loss: 13.637535095214844\n",
      "epoch: 3,  batch step: 144, loss: 21.035232543945312\n",
      "epoch: 3,  batch step: 145, loss: 314.52716064453125\n",
      "epoch: 3,  batch step: 146, loss: 162.40921020507812\n",
      "epoch: 3,  batch step: 147, loss: 20.004642486572266\n",
      "epoch: 3,  batch step: 148, loss: 133.76202392578125\n",
      "epoch: 3,  batch step: 149, loss: 21.401214599609375\n",
      "epoch: 3,  batch step: 150, loss: 12.995965957641602\n",
      "epoch: 3,  batch step: 151, loss: 358.7479248046875\n",
      "epoch: 3,  batch step: 152, loss: 14.542860984802246\n",
      "epoch: 3,  batch step: 153, loss: 13.5741605758667\n",
      "epoch: 3,  batch step: 154, loss: 13.65719985961914\n",
      "epoch: 3,  batch step: 155, loss: 23.99835777282715\n",
      "epoch: 3,  batch step: 156, loss: 29.909523010253906\n",
      "epoch: 3,  batch step: 157, loss: 41.23760986328125\n",
      "epoch: 3,  batch step: 158, loss: 11.951414108276367\n",
      "epoch: 3,  batch step: 159, loss: 14.202353477478027\n",
      "epoch: 3,  batch step: 160, loss: 235.74069213867188\n",
      "epoch: 3,  batch step: 161, loss: 167.7299346923828\n",
      "epoch: 3,  batch step: 162, loss: 25.663925170898438\n",
      "epoch: 3,  batch step: 163, loss: 16.283370971679688\n",
      "epoch: 3,  batch step: 164, loss: 131.44424438476562\n",
      "epoch: 3,  batch step: 165, loss: 13.225189208984375\n",
      "epoch: 3,  batch step: 166, loss: 404.17987060546875\n",
      "epoch: 3,  batch step: 167, loss: 22.29021453857422\n",
      "epoch: 3,  batch step: 168, loss: 122.22039031982422\n",
      "epoch: 3,  batch step: 169, loss: 15.460601806640625\n",
      "epoch: 3,  batch step: 170, loss: 211.1766815185547\n",
      "epoch: 3,  batch step: 171, loss: 10.834189414978027\n",
      "epoch: 3,  batch step: 172, loss: 171.66490173339844\n",
      "epoch: 3,  batch step: 173, loss: 163.5806121826172\n",
      "epoch: 3,  batch step: 174, loss: 162.23751831054688\n",
      "epoch: 3,  batch step: 175, loss: 331.9764404296875\n",
      "epoch: 3,  batch step: 176, loss: 242.2938232421875\n",
      "epoch: 3,  batch step: 177, loss: 332.545654296875\n",
      "epoch: 3,  batch step: 178, loss: 12.598905563354492\n",
      "epoch: 3,  batch step: 179, loss: 34.950191497802734\n",
      "epoch: 3,  batch step: 180, loss: 25.89620590209961\n",
      "epoch: 3,  batch step: 181, loss: 73.193359375\n",
      "epoch: 3,  batch step: 182, loss: 36.850868225097656\n",
      "epoch: 3,  batch step: 183, loss: 153.83258056640625\n",
      "epoch: 3,  batch step: 184, loss: 18.7110538482666\n",
      "epoch: 3,  batch step: 185, loss: 138.42225646972656\n",
      "epoch: 3,  batch step: 186, loss: 90.03021240234375\n",
      "epoch: 3,  batch step: 187, loss: 22.586833953857422\n",
      "epoch: 3,  batch step: 188, loss: 17.298133850097656\n",
      "epoch: 3,  batch step: 189, loss: 19.02227783203125\n",
      "epoch: 3,  batch step: 190, loss: 21.536575317382812\n",
      "epoch: 3,  batch step: 191, loss: 19.261653900146484\n",
      "epoch: 3,  batch step: 192, loss: 297.3793029785156\n",
      "epoch: 3,  batch step: 193, loss: 35.33176040649414\n",
      "epoch: 3,  batch step: 194, loss: 13.523458480834961\n",
      "epoch: 3,  batch step: 195, loss: 66.95925903320312\n",
      "epoch: 3,  batch step: 196, loss: 61.57801818847656\n",
      "epoch: 3,  batch step: 197, loss: 231.98898315429688\n",
      "epoch: 3,  batch step: 198, loss: 244.80447387695312\n",
      "epoch: 3,  batch step: 199, loss: 26.767980575561523\n",
      "epoch: 3,  batch step: 200, loss: 233.86611938476562\n",
      "epoch: 3,  batch step: 201, loss: 35.451595306396484\n",
      "epoch: 3,  batch step: 202, loss: 47.001991271972656\n",
      "epoch: 3,  batch step: 203, loss: 18.36040496826172\n",
      "epoch: 3,  batch step: 204, loss: 91.90412902832031\n",
      "epoch: 3,  batch step: 205, loss: 26.50885581970215\n",
      "epoch: 3,  batch step: 206, loss: 15.917762756347656\n",
      "epoch: 3,  batch step: 207, loss: 20.420318603515625\n",
      "epoch: 3,  batch step: 208, loss: 13.913501739501953\n",
      "epoch: 3,  batch step: 209, loss: 16.417987823486328\n",
      "epoch: 3,  batch step: 210, loss: 118.80376434326172\n",
      "epoch: 3,  batch step: 211, loss: 18.490982055664062\n",
      "epoch: 3,  batch step: 212, loss: 11.85232162475586\n",
      "epoch: 3,  batch step: 213, loss: 59.272544860839844\n",
      "epoch: 3,  batch step: 214, loss: 30.42718505859375\n",
      "epoch: 3,  batch step: 215, loss: 368.810791015625\n",
      "epoch: 3,  batch step: 216, loss: 8.701570510864258\n",
      "epoch: 3,  batch step: 217, loss: 12.654494285583496\n",
      "epoch: 3,  batch step: 218, loss: 19.334230422973633\n",
      "epoch: 3,  batch step: 219, loss: 11.380350112915039\n",
      "epoch: 3,  batch step: 220, loss: 14.248245239257812\n",
      "epoch: 3,  batch step: 221, loss: 13.020488739013672\n",
      "epoch: 3,  batch step: 222, loss: 11.705734252929688\n",
      "epoch: 3,  batch step: 223, loss: 13.68034553527832\n",
      "epoch: 3,  batch step: 224, loss: 11.154945373535156\n",
      "epoch: 3,  batch step: 225, loss: 16.752666473388672\n",
      "epoch: 3,  batch step: 226, loss: 5.360276699066162\n",
      "epoch: 3,  batch step: 227, loss: 11.514799118041992\n",
      "epoch: 3,  batch step: 228, loss: 123.35380554199219\n",
      "epoch: 3,  batch step: 229, loss: 15.784494400024414\n",
      "epoch: 3,  batch step: 230, loss: 15.204172134399414\n",
      "epoch: 3,  batch step: 231, loss: 160.84352111816406\n",
      "epoch: 3,  batch step: 232, loss: 21.60195541381836\n",
      "epoch: 3,  batch step: 233, loss: 35.5039176940918\n",
      "epoch: 3,  batch step: 234, loss: 77.85253143310547\n",
      "epoch: 3,  batch step: 235, loss: 373.95819091796875\n",
      "epoch: 3,  batch step: 236, loss: 12.407831192016602\n",
      "epoch: 3,  batch step: 237, loss: 61.65692901611328\n",
      "epoch: 3,  batch step: 238, loss: 16.644149780273438\n",
      "epoch: 3,  batch step: 239, loss: 500.0800476074219\n",
      "epoch: 3,  batch step: 240, loss: 7.856446266174316\n",
      "epoch: 3,  batch step: 241, loss: 22.961917877197266\n",
      "epoch: 3,  batch step: 242, loss: 10.218300819396973\n",
      "epoch: 3,  batch step: 243, loss: 86.91232299804688\n",
      "epoch: 3,  batch step: 244, loss: 204.58177185058594\n",
      "epoch: 3,  batch step: 245, loss: 8.536823272705078\n",
      "epoch: 3,  batch step: 246, loss: 32.97267532348633\n",
      "epoch: 3,  batch step: 247, loss: 29.67787742614746\n",
      "epoch: 3,  batch step: 248, loss: 9.991415023803711\n",
      "epoch: 3,  batch step: 249, loss: 11.370688438415527\n",
      "epoch: 3,  batch step: 250, loss: 9.606590270996094\n",
      "epoch: 3,  batch step: 251, loss: 221.68353271484375\n",
      "validation error epoch  3:    tensor(80.2298, device='cuda:0')\n",
      "316\n",
      "epoch: 4,  batch step: 0, loss: 307.9826354980469\n",
      "epoch: 4,  batch step: 1, loss: 15.316028594970703\n",
      "epoch: 4,  batch step: 2, loss: 22.610729217529297\n",
      "epoch: 4,  batch step: 3, loss: 19.609325408935547\n",
      "epoch: 4,  batch step: 4, loss: 31.83214569091797\n",
      "epoch: 4,  batch step: 5, loss: 23.50069808959961\n",
      "epoch: 4,  batch step: 6, loss: 189.9449920654297\n",
      "epoch: 4,  batch step: 7, loss: 19.95171546936035\n",
      "epoch: 4,  batch step: 8, loss: 51.88837432861328\n",
      "epoch: 4,  batch step: 9, loss: 32.12080001831055\n",
      "epoch: 4,  batch step: 10, loss: 18.37316131591797\n",
      "epoch: 4,  batch step: 11, loss: 108.26856994628906\n",
      "epoch: 4,  batch step: 12, loss: 203.31375122070312\n",
      "epoch: 4,  batch step: 13, loss: 18.552236557006836\n",
      "epoch: 4,  batch step: 14, loss: 86.08140563964844\n",
      "epoch: 4,  batch step: 15, loss: 9.68104362487793\n",
      "epoch: 4,  batch step: 16, loss: 15.107877731323242\n",
      "epoch: 4,  batch step: 17, loss: 19.234445571899414\n",
      "epoch: 4,  batch step: 18, loss: 217.77297973632812\n",
      "epoch: 4,  batch step: 19, loss: 71.9614486694336\n",
      "epoch: 4,  batch step: 20, loss: 42.755001068115234\n",
      "epoch: 4,  batch step: 21, loss: 34.16327667236328\n",
      "epoch: 4,  batch step: 22, loss: 249.7052001953125\n",
      "epoch: 4,  batch step: 23, loss: 69.71376037597656\n",
      "epoch: 4,  batch step: 24, loss: 11.68104362487793\n",
      "epoch: 4,  batch step: 25, loss: 18.338682174682617\n",
      "epoch: 4,  batch step: 26, loss: 13.37433910369873\n",
      "epoch: 4,  batch step: 27, loss: 7.94218635559082\n",
      "epoch: 4,  batch step: 28, loss: 13.624715805053711\n",
      "epoch: 4,  batch step: 29, loss: 194.96560668945312\n",
      "epoch: 4,  batch step: 30, loss: 177.92010498046875\n",
      "epoch: 4,  batch step: 31, loss: 27.644153594970703\n",
      "epoch: 4,  batch step: 32, loss: 27.344646453857422\n",
      "epoch: 4,  batch step: 33, loss: 201.5279998779297\n",
      "epoch: 4,  batch step: 34, loss: 12.363940238952637\n",
      "epoch: 4,  batch step: 35, loss: 16.96673583984375\n",
      "epoch: 4,  batch step: 36, loss: 138.79685974121094\n",
      "epoch: 4,  batch step: 37, loss: 43.17375183105469\n",
      "epoch: 4,  batch step: 38, loss: 29.380264282226562\n",
      "epoch: 4,  batch step: 39, loss: 37.86149215698242\n",
      "epoch: 4,  batch step: 40, loss: 9.90743637084961\n",
      "epoch: 4,  batch step: 41, loss: 16.998489379882812\n",
      "epoch: 4,  batch step: 42, loss: 72.78033447265625\n",
      "epoch: 4,  batch step: 43, loss: 17.857980728149414\n",
      "epoch: 4,  batch step: 44, loss: 29.639083862304688\n",
      "epoch: 4,  batch step: 45, loss: 19.697891235351562\n",
      "epoch: 4,  batch step: 46, loss: 258.98602294921875\n",
      "epoch: 4,  batch step: 47, loss: 46.3603630065918\n",
      "epoch: 4,  batch step: 48, loss: 39.0135498046875\n",
      "epoch: 4,  batch step: 49, loss: 209.19676208496094\n",
      "epoch: 4,  batch step: 50, loss: 86.97261810302734\n",
      "epoch: 4,  batch step: 51, loss: 31.072629928588867\n",
      "epoch: 4,  batch step: 52, loss: 493.94757080078125\n",
      "epoch: 4,  batch step: 53, loss: 121.25614929199219\n",
      "epoch: 4,  batch step: 54, loss: 144.51983642578125\n",
      "epoch: 4,  batch step: 55, loss: 127.97177124023438\n",
      "epoch: 4,  batch step: 56, loss: 16.51473617553711\n",
      "epoch: 4,  batch step: 57, loss: 100.07430267333984\n",
      "epoch: 4,  batch step: 58, loss: 25.80661392211914\n",
      "epoch: 4,  batch step: 59, loss: 63.0864372253418\n",
      "epoch: 4,  batch step: 60, loss: 161.96371459960938\n",
      "epoch: 4,  batch step: 61, loss: 101.42442321777344\n",
      "epoch: 4,  batch step: 62, loss: 175.32305908203125\n",
      "epoch: 4,  batch step: 63, loss: 333.8995056152344\n",
      "epoch: 4,  batch step: 64, loss: 186.23106384277344\n",
      "epoch: 4,  batch step: 65, loss: 19.80660629272461\n",
      "epoch: 4,  batch step: 66, loss: 55.2463493347168\n",
      "epoch: 4,  batch step: 67, loss: 32.74083709716797\n",
      "epoch: 4,  batch step: 68, loss: 19.46725845336914\n",
      "epoch: 4,  batch step: 69, loss: 54.12035369873047\n",
      "epoch: 4,  batch step: 70, loss: 57.24773406982422\n",
      "epoch: 4,  batch step: 71, loss: 568.6915893554688\n",
      "epoch: 4,  batch step: 72, loss: 25.230819702148438\n",
      "epoch: 4,  batch step: 73, loss: 16.24774742126465\n",
      "epoch: 4,  batch step: 74, loss: 38.828857421875\n",
      "epoch: 4,  batch step: 75, loss: 183.91954040527344\n",
      "epoch: 4,  batch step: 76, loss: 218.8092041015625\n",
      "epoch: 4,  batch step: 77, loss: 72.45620727539062\n",
      "epoch: 4,  batch step: 78, loss: 37.981422424316406\n",
      "epoch: 4,  batch step: 79, loss: 14.817129135131836\n",
      "epoch: 4,  batch step: 80, loss: 395.82806396484375\n",
      "epoch: 4,  batch step: 81, loss: 14.853084564208984\n",
      "epoch: 4,  batch step: 82, loss: 21.831165313720703\n",
      "epoch: 4,  batch step: 83, loss: 11.672475814819336\n",
      "epoch: 4,  batch step: 84, loss: 34.53412628173828\n",
      "epoch: 4,  batch step: 85, loss: 19.837425231933594\n",
      "epoch: 4,  batch step: 86, loss: 92.71601867675781\n",
      "epoch: 4,  batch step: 87, loss: 98.46293640136719\n",
      "epoch: 4,  batch step: 88, loss: 57.11424255371094\n",
      "epoch: 4,  batch step: 89, loss: 22.87140464782715\n",
      "epoch: 4,  batch step: 90, loss: 428.9267578125\n",
      "epoch: 4,  batch step: 91, loss: 83.14405822753906\n",
      "epoch: 4,  batch step: 92, loss: 127.60711669921875\n",
      "epoch: 4,  batch step: 93, loss: 70.74732208251953\n",
      "epoch: 4,  batch step: 94, loss: 146.25587463378906\n",
      "epoch: 4,  batch step: 95, loss: 347.142333984375\n",
      "epoch: 4,  batch step: 96, loss: 18.8441162109375\n",
      "epoch: 4,  batch step: 97, loss: 13.496492385864258\n",
      "epoch: 4,  batch step: 98, loss: 81.02400207519531\n",
      "epoch: 4,  batch step: 99, loss: 30.19189453125\n",
      "epoch: 4,  batch step: 100, loss: 174.7640380859375\n",
      "epoch: 4,  batch step: 101, loss: 221.504150390625\n",
      "epoch: 4,  batch step: 102, loss: 187.087890625\n",
      "epoch: 4,  batch step: 103, loss: 177.87237548828125\n",
      "epoch: 4,  batch step: 104, loss: 37.923274993896484\n",
      "epoch: 4,  batch step: 105, loss: 31.628250122070312\n",
      "epoch: 4,  batch step: 106, loss: 24.3582763671875\n",
      "epoch: 4,  batch step: 107, loss: 303.8677062988281\n",
      "epoch: 4,  batch step: 108, loss: 28.022201538085938\n",
      "epoch: 4,  batch step: 109, loss: 21.289480209350586\n",
      "epoch: 4,  batch step: 110, loss: 207.10137939453125\n",
      "epoch: 4,  batch step: 111, loss: 70.8978271484375\n",
      "epoch: 4,  batch step: 112, loss: 29.492406845092773\n",
      "epoch: 4,  batch step: 113, loss: 13.330820083618164\n",
      "epoch: 4,  batch step: 114, loss: 122.80224609375\n",
      "epoch: 4,  batch step: 115, loss: 18.8306941986084\n",
      "epoch: 4,  batch step: 116, loss: 102.97899627685547\n",
      "epoch: 4,  batch step: 117, loss: 71.13551330566406\n",
      "epoch: 4,  batch step: 118, loss: 270.0912780761719\n",
      "epoch: 4,  batch step: 119, loss: 13.032560348510742\n",
      "epoch: 4,  batch step: 120, loss: 22.57953453063965\n",
      "epoch: 4,  batch step: 121, loss: 211.5964813232422\n",
      "epoch: 4,  batch step: 122, loss: 34.697601318359375\n",
      "epoch: 4,  batch step: 123, loss: 276.7140197753906\n",
      "epoch: 4,  batch step: 124, loss: 34.118953704833984\n",
      "epoch: 4,  batch step: 125, loss: 18.72092628479004\n",
      "epoch: 4,  batch step: 126, loss: 24.808204650878906\n",
      "epoch: 4,  batch step: 127, loss: 46.45528793334961\n",
      "epoch: 4,  batch step: 128, loss: 23.198657989501953\n",
      "epoch: 4,  batch step: 129, loss: 17.499526977539062\n",
      "epoch: 4,  batch step: 130, loss: 16.682470321655273\n",
      "epoch: 4,  batch step: 131, loss: 19.007741928100586\n",
      "epoch: 4,  batch step: 132, loss: 35.720314025878906\n",
      "epoch: 4,  batch step: 133, loss: 123.86119842529297\n",
      "epoch: 4,  batch step: 134, loss: 15.728719711303711\n",
      "epoch: 4,  batch step: 135, loss: 121.6278305053711\n",
      "epoch: 4,  batch step: 136, loss: 23.28131675720215\n",
      "epoch: 4,  batch step: 137, loss: 25.423255920410156\n",
      "epoch: 4,  batch step: 138, loss: 20.87026023864746\n",
      "epoch: 4,  batch step: 139, loss: 10.795852661132812\n",
      "epoch: 4,  batch step: 140, loss: 30.22152328491211\n",
      "epoch: 4,  batch step: 141, loss: 26.581298828125\n",
      "epoch: 4,  batch step: 142, loss: 28.9222412109375\n",
      "epoch: 4,  batch step: 143, loss: 170.52029418945312\n",
      "epoch: 4,  batch step: 144, loss: 14.320438385009766\n",
      "epoch: 4,  batch step: 145, loss: 19.886333465576172\n",
      "epoch: 4,  batch step: 146, loss: 31.804685592651367\n",
      "epoch: 4,  batch step: 147, loss: 16.01143455505371\n",
      "epoch: 4,  batch step: 148, loss: 16.138874053955078\n",
      "epoch: 4,  batch step: 149, loss: 32.97651290893555\n",
      "epoch: 4,  batch step: 150, loss: 241.40940856933594\n",
      "epoch: 4,  batch step: 151, loss: 307.6557312011719\n",
      "epoch: 4,  batch step: 152, loss: 20.25336265563965\n",
      "epoch: 4,  batch step: 153, loss: 5.365384578704834\n",
      "epoch: 4,  batch step: 154, loss: 322.7049560546875\n",
      "epoch: 4,  batch step: 155, loss: 6.32211971282959\n",
      "epoch: 4,  batch step: 156, loss: 26.693008422851562\n",
      "epoch: 4,  batch step: 157, loss: 33.212642669677734\n",
      "epoch: 4,  batch step: 158, loss: 63.80424118041992\n",
      "epoch: 4,  batch step: 159, loss: 38.50172424316406\n",
      "epoch: 4,  batch step: 160, loss: 49.57845687866211\n",
      "epoch: 4,  batch step: 161, loss: 250.0730438232422\n",
      "epoch: 4,  batch step: 162, loss: 32.96430206298828\n",
      "epoch: 4,  batch step: 163, loss: 296.6827392578125\n",
      "epoch: 4,  batch step: 164, loss: 18.02313804626465\n",
      "epoch: 4,  batch step: 165, loss: 137.96014404296875\n",
      "epoch: 4,  batch step: 166, loss: 26.018341064453125\n",
      "epoch: 4,  batch step: 167, loss: 18.037946701049805\n",
      "epoch: 4,  batch step: 168, loss: 89.31084442138672\n",
      "epoch: 4,  batch step: 169, loss: 13.66523265838623\n",
      "epoch: 4,  batch step: 170, loss: 199.03111267089844\n",
      "epoch: 4,  batch step: 171, loss: 260.0720520019531\n",
      "epoch: 4,  batch step: 172, loss: 79.49124145507812\n",
      "epoch: 4,  batch step: 173, loss: 118.08692932128906\n",
      "epoch: 4,  batch step: 174, loss: 27.494375228881836\n",
      "epoch: 4,  batch step: 175, loss: 23.329940795898438\n",
      "epoch: 4,  batch step: 176, loss: 22.252063751220703\n",
      "epoch: 4,  batch step: 177, loss: 48.971763610839844\n",
      "epoch: 4,  batch step: 178, loss: 11.155553817749023\n",
      "epoch: 4,  batch step: 179, loss: 103.58708953857422\n",
      "epoch: 4,  batch step: 180, loss: 19.88942527770996\n",
      "epoch: 4,  batch step: 181, loss: 32.12193298339844\n",
      "epoch: 4,  batch step: 182, loss: 9.246003150939941\n",
      "epoch: 4,  batch step: 183, loss: 18.285968780517578\n",
      "epoch: 4,  batch step: 184, loss: 18.583593368530273\n",
      "epoch: 4,  batch step: 185, loss: 98.29756164550781\n",
      "epoch: 4,  batch step: 186, loss: 147.04339599609375\n",
      "epoch: 4,  batch step: 187, loss: 582.0955810546875\n",
      "epoch: 4,  batch step: 188, loss: 14.484138488769531\n",
      "epoch: 4,  batch step: 189, loss: 8.250833511352539\n",
      "epoch: 4,  batch step: 190, loss: 23.54281234741211\n",
      "epoch: 4,  batch step: 191, loss: 46.98159408569336\n",
      "epoch: 4,  batch step: 192, loss: 16.886884689331055\n",
      "epoch: 4,  batch step: 193, loss: 13.024606704711914\n",
      "epoch: 4,  batch step: 194, loss: 217.40353393554688\n",
      "epoch: 4,  batch step: 195, loss: 37.31433868408203\n",
      "epoch: 4,  batch step: 196, loss: 44.129722595214844\n",
      "epoch: 4,  batch step: 197, loss: 34.779327392578125\n",
      "epoch: 4,  batch step: 198, loss: 36.94941329956055\n",
      "epoch: 4,  batch step: 199, loss: 10.475960731506348\n",
      "epoch: 4,  batch step: 200, loss: 8.346490859985352\n",
      "epoch: 4,  batch step: 201, loss: 62.16619873046875\n",
      "epoch: 4,  batch step: 202, loss: 393.1119079589844\n",
      "epoch: 4,  batch step: 203, loss: 10.757720947265625\n",
      "epoch: 4,  batch step: 204, loss: 10.373918533325195\n",
      "epoch: 4,  batch step: 205, loss: 14.510455131530762\n",
      "epoch: 4,  batch step: 206, loss: 187.55258178710938\n",
      "epoch: 4,  batch step: 207, loss: 16.83789825439453\n",
      "epoch: 4,  batch step: 208, loss: 7.260776042938232\n",
      "epoch: 4,  batch step: 209, loss: 23.644336700439453\n",
      "epoch: 4,  batch step: 210, loss: 243.91111755371094\n",
      "epoch: 4,  batch step: 211, loss: 416.770263671875\n",
      "epoch: 4,  batch step: 212, loss: 82.8022232055664\n",
      "epoch: 4,  batch step: 213, loss: 10.998167037963867\n",
      "epoch: 4,  batch step: 214, loss: 29.86904525756836\n",
      "epoch: 4,  batch step: 215, loss: 19.73278045654297\n",
      "epoch: 4,  batch step: 216, loss: 14.111674308776855\n",
      "epoch: 4,  batch step: 217, loss: 192.78500366210938\n",
      "epoch: 4,  batch step: 218, loss: 27.045448303222656\n",
      "epoch: 4,  batch step: 219, loss: 17.207706451416016\n",
      "epoch: 4,  batch step: 220, loss: 14.136752128601074\n",
      "epoch: 4,  batch step: 221, loss: 282.41766357421875\n",
      "epoch: 4,  batch step: 222, loss: 15.335261344909668\n",
      "epoch: 4,  batch step: 223, loss: 7.941756248474121\n",
      "epoch: 4,  batch step: 224, loss: 17.12934112548828\n",
      "epoch: 4,  batch step: 225, loss: 23.817218780517578\n",
      "epoch: 4,  batch step: 226, loss: 21.858076095581055\n",
      "epoch: 4,  batch step: 227, loss: 12.24765396118164\n",
      "epoch: 4,  batch step: 228, loss: 27.07796859741211\n",
      "epoch: 4,  batch step: 229, loss: 35.54874801635742\n",
      "epoch: 4,  batch step: 230, loss: 12.49506664276123\n",
      "epoch: 4,  batch step: 231, loss: 8.675954818725586\n",
      "epoch: 4,  batch step: 232, loss: 30.81891632080078\n",
      "epoch: 4,  batch step: 233, loss: 14.958051681518555\n",
      "epoch: 4,  batch step: 234, loss: 9.96941089630127\n",
      "epoch: 4,  batch step: 235, loss: 16.078594207763672\n",
      "epoch: 4,  batch step: 236, loss: 48.95091247558594\n",
      "epoch: 4,  batch step: 237, loss: 8.822572708129883\n",
      "epoch: 4,  batch step: 238, loss: 9.936213493347168\n",
      "epoch: 4,  batch step: 239, loss: 95.30613708496094\n",
      "epoch: 4,  batch step: 240, loss: 7.923354148864746\n",
      "epoch: 4,  batch step: 241, loss: 80.1142807006836\n",
      "epoch: 4,  batch step: 242, loss: 56.05701446533203\n",
      "epoch: 4,  batch step: 243, loss: 15.680702209472656\n",
      "epoch: 4,  batch step: 244, loss: 172.7509002685547\n",
      "epoch: 4,  batch step: 245, loss: 37.694488525390625\n",
      "epoch: 4,  batch step: 246, loss: 25.315885543823242\n",
      "epoch: 4,  batch step: 247, loss: 7.35895299911499\n",
      "epoch: 4,  batch step: 248, loss: 232.75572204589844\n",
      "epoch: 4,  batch step: 249, loss: 14.308349609375\n",
      "epoch: 4,  batch step: 250, loss: 11.014986991882324\n",
      "epoch: 4,  batch step: 251, loss: 42.71067428588867\n",
      "validation error epoch  4:    tensor(80.4021, device='cuda:0')\n",
      "316\n",
      "epoch: 5,  batch step: 0, loss: 39.38916778564453\n",
      "epoch: 5,  batch step: 1, loss: 15.531474113464355\n",
      "epoch: 5,  batch step: 2, loss: 46.040313720703125\n",
      "epoch: 5,  batch step: 3, loss: 6.742074012756348\n",
      "epoch: 5,  batch step: 4, loss: 24.50625991821289\n",
      "epoch: 5,  batch step: 5, loss: 95.23591613769531\n",
      "epoch: 5,  batch step: 6, loss: 199.9169921875\n",
      "epoch: 5,  batch step: 7, loss: 10.19980525970459\n",
      "epoch: 5,  batch step: 8, loss: 12.635798454284668\n",
      "epoch: 5,  batch step: 9, loss: 26.743810653686523\n",
      "epoch: 5,  batch step: 10, loss: 94.1298828125\n",
      "epoch: 5,  batch step: 11, loss: 10.581145286560059\n",
      "epoch: 5,  batch step: 12, loss: 94.23260498046875\n",
      "epoch: 5,  batch step: 13, loss: 18.481651306152344\n",
      "epoch: 5,  batch step: 14, loss: 19.038393020629883\n",
      "epoch: 5,  batch step: 15, loss: 10.276407241821289\n",
      "epoch: 5,  batch step: 16, loss: 379.9287109375\n",
      "epoch: 5,  batch step: 17, loss: 261.32867431640625\n",
      "epoch: 5,  batch step: 18, loss: 22.906055450439453\n",
      "epoch: 5,  batch step: 19, loss: 12.793289184570312\n",
      "epoch: 5,  batch step: 20, loss: 9.144861221313477\n",
      "epoch: 5,  batch step: 21, loss: 41.53413391113281\n",
      "epoch: 5,  batch step: 22, loss: 20.720748901367188\n",
      "epoch: 5,  batch step: 23, loss: 11.500360488891602\n",
      "epoch: 5,  batch step: 24, loss: 71.95165252685547\n",
      "epoch: 5,  batch step: 25, loss: 84.62850952148438\n",
      "epoch: 5,  batch step: 26, loss: 69.97541809082031\n",
      "epoch: 5,  batch step: 27, loss: 43.0484733581543\n",
      "epoch: 5,  batch step: 28, loss: 57.15892791748047\n",
      "epoch: 5,  batch step: 29, loss: 8.911670684814453\n",
      "epoch: 5,  batch step: 30, loss: 33.36195755004883\n",
      "epoch: 5,  batch step: 31, loss: 15.026594161987305\n",
      "epoch: 5,  batch step: 32, loss: 36.12645721435547\n",
      "epoch: 5,  batch step: 33, loss: 10.31266975402832\n",
      "epoch: 5,  batch step: 34, loss: 236.74484252929688\n",
      "epoch: 5,  batch step: 35, loss: 18.05704116821289\n",
      "epoch: 5,  batch step: 36, loss: 95.95980072021484\n",
      "epoch: 5,  batch step: 37, loss: 85.1099853515625\n",
      "epoch: 5,  batch step: 38, loss: 19.150928497314453\n",
      "epoch: 5,  batch step: 39, loss: 402.82135009765625\n",
      "epoch: 5,  batch step: 40, loss: 53.240745544433594\n",
      "epoch: 5,  batch step: 41, loss: 5.422723770141602\n",
      "epoch: 5,  batch step: 42, loss: 15.478368759155273\n",
      "epoch: 5,  batch step: 43, loss: 55.6490592956543\n",
      "epoch: 5,  batch step: 44, loss: 159.95236206054688\n",
      "epoch: 5,  batch step: 45, loss: 29.34579086303711\n",
      "epoch: 5,  batch step: 46, loss: 18.7550106048584\n",
      "epoch: 5,  batch step: 47, loss: 30.39980125427246\n",
      "epoch: 5,  batch step: 48, loss: 16.537609100341797\n",
      "epoch: 5,  batch step: 49, loss: 75.90455627441406\n",
      "epoch: 5,  batch step: 50, loss: 16.216827392578125\n",
      "epoch: 5,  batch step: 51, loss: 186.07676696777344\n",
      "epoch: 5,  batch step: 52, loss: 15.725092887878418\n",
      "epoch: 5,  batch step: 53, loss: 237.579345703125\n",
      "epoch: 5,  batch step: 54, loss: 40.2767333984375\n",
      "epoch: 5,  batch step: 55, loss: 270.6475830078125\n",
      "epoch: 5,  batch step: 56, loss: 11.132742881774902\n",
      "epoch: 5,  batch step: 57, loss: 266.7535705566406\n",
      "epoch: 5,  batch step: 58, loss: 506.1416931152344\n",
      "epoch: 5,  batch step: 59, loss: 34.988922119140625\n",
      "epoch: 5,  batch step: 60, loss: 21.6368465423584\n",
      "epoch: 5,  batch step: 61, loss: 13.7786283493042\n",
      "epoch: 5,  batch step: 62, loss: 274.1148376464844\n",
      "epoch: 5,  batch step: 63, loss: 21.80757713317871\n",
      "epoch: 5,  batch step: 64, loss: 27.66845703125\n",
      "epoch: 5,  batch step: 65, loss: 31.684261322021484\n",
      "epoch: 5,  batch step: 66, loss: 49.20759201049805\n",
      "epoch: 5,  batch step: 67, loss: 183.79000854492188\n",
      "epoch: 5,  batch step: 68, loss: 124.30764770507812\n",
      "epoch: 5,  batch step: 69, loss: 187.95944213867188\n",
      "epoch: 5,  batch step: 70, loss: 91.32357788085938\n",
      "epoch: 5,  batch step: 71, loss: 12.056414604187012\n",
      "epoch: 5,  batch step: 72, loss: 201.71142578125\n",
      "epoch: 5,  batch step: 73, loss: 33.09092330932617\n",
      "epoch: 5,  batch step: 74, loss: 24.596405029296875\n",
      "epoch: 5,  batch step: 75, loss: 24.828777313232422\n",
      "epoch: 5,  batch step: 76, loss: 43.78588104248047\n",
      "epoch: 5,  batch step: 77, loss: 30.57112693786621\n",
      "epoch: 5,  batch step: 78, loss: 17.227489471435547\n",
      "epoch: 5,  batch step: 79, loss: 36.81977462768555\n",
      "epoch: 5,  batch step: 80, loss: 25.542362213134766\n",
      "epoch: 5,  batch step: 81, loss: 63.51646041870117\n",
      "epoch: 5,  batch step: 82, loss: 35.976707458496094\n",
      "epoch: 5,  batch step: 83, loss: 222.62747192382812\n",
      "epoch: 5,  batch step: 84, loss: 15.11420726776123\n",
      "epoch: 5,  batch step: 85, loss: 7.895514965057373\n",
      "epoch: 5,  batch step: 86, loss: 21.598651885986328\n",
      "epoch: 5,  batch step: 87, loss: 218.7095184326172\n",
      "epoch: 5,  batch step: 88, loss: 15.587007522583008\n",
      "epoch: 5,  batch step: 89, loss: 54.16932678222656\n",
      "epoch: 5,  batch step: 90, loss: 17.628475189208984\n",
      "epoch: 5,  batch step: 91, loss: 256.3533020019531\n",
      "epoch: 5,  batch step: 92, loss: 14.558149337768555\n",
      "epoch: 5,  batch step: 93, loss: 16.43523597717285\n",
      "epoch: 5,  batch step: 94, loss: 15.585253715515137\n",
      "epoch: 5,  batch step: 95, loss: 189.08309936523438\n",
      "epoch: 5,  batch step: 96, loss: 154.73663330078125\n",
      "epoch: 5,  batch step: 97, loss: 37.02032470703125\n",
      "epoch: 5,  batch step: 98, loss: 20.345457077026367\n",
      "epoch: 5,  batch step: 99, loss: 83.09284973144531\n",
      "epoch: 5,  batch step: 100, loss: 8.479022979736328\n",
      "epoch: 5,  batch step: 101, loss: 12.375345230102539\n",
      "epoch: 5,  batch step: 102, loss: 6.804032325744629\n",
      "epoch: 5,  batch step: 103, loss: 14.194674491882324\n",
      "epoch: 5,  batch step: 104, loss: 25.35005760192871\n",
      "epoch: 5,  batch step: 105, loss: 12.581335067749023\n",
      "epoch: 5,  batch step: 106, loss: 223.24407958984375\n",
      "epoch: 5,  batch step: 107, loss: 36.04338073730469\n",
      "epoch: 5,  batch step: 108, loss: 349.6341247558594\n",
      "epoch: 5,  batch step: 109, loss: 20.020299911499023\n",
      "epoch: 5,  batch step: 110, loss: 115.45109558105469\n",
      "epoch: 5,  batch step: 111, loss: 133.8289794921875\n",
      "epoch: 5,  batch step: 112, loss: 114.25471496582031\n",
      "epoch: 5,  batch step: 113, loss: 340.5370178222656\n",
      "epoch: 5,  batch step: 114, loss: 32.2203483581543\n",
      "epoch: 5,  batch step: 115, loss: 13.347660064697266\n",
      "epoch: 5,  batch step: 116, loss: 15.507457733154297\n",
      "epoch: 5,  batch step: 117, loss: 97.05525207519531\n",
      "epoch: 5,  batch step: 118, loss: 268.9345703125\n",
      "epoch: 5,  batch step: 119, loss: 23.13625717163086\n",
      "epoch: 5,  batch step: 120, loss: 193.75143432617188\n",
      "epoch: 5,  batch step: 121, loss: 26.439952850341797\n",
      "epoch: 5,  batch step: 122, loss: 19.45513916015625\n",
      "epoch: 5,  batch step: 123, loss: 97.2728500366211\n",
      "epoch: 5,  batch step: 124, loss: 200.84129333496094\n",
      "epoch: 5,  batch step: 125, loss: 118.97740173339844\n",
      "epoch: 5,  batch step: 126, loss: 25.728515625\n",
      "epoch: 5,  batch step: 127, loss: 156.9678955078125\n",
      "epoch: 5,  batch step: 128, loss: 134.5608673095703\n",
      "epoch: 5,  batch step: 129, loss: 173.56968688964844\n",
      "epoch: 5,  batch step: 130, loss: 59.28575134277344\n",
      "epoch: 5,  batch step: 131, loss: 148.70999145507812\n",
      "epoch: 5,  batch step: 132, loss: 35.457584381103516\n",
      "epoch: 5,  batch step: 133, loss: 20.42451286315918\n",
      "epoch: 5,  batch step: 134, loss: 241.14254760742188\n",
      "epoch: 5,  batch step: 135, loss: 150.1450958251953\n",
      "epoch: 5,  batch step: 136, loss: 29.445486068725586\n",
      "epoch: 5,  batch step: 137, loss: 75.93955993652344\n",
      "epoch: 5,  batch step: 138, loss: 112.35240936279297\n",
      "epoch: 5,  batch step: 139, loss: 22.785696029663086\n",
      "epoch: 5,  batch step: 140, loss: 178.98226928710938\n",
      "epoch: 5,  batch step: 141, loss: 58.66624069213867\n",
      "epoch: 5,  batch step: 142, loss: 30.90266990661621\n",
      "epoch: 5,  batch step: 143, loss: 25.003082275390625\n",
      "epoch: 5,  batch step: 144, loss: 99.24422454833984\n",
      "epoch: 5,  batch step: 145, loss: 49.58338928222656\n",
      "epoch: 5,  batch step: 146, loss: 39.62983703613281\n",
      "epoch: 5,  batch step: 147, loss: 18.15570068359375\n",
      "epoch: 5,  batch step: 148, loss: 12.986261367797852\n",
      "epoch: 5,  batch step: 149, loss: 15.086090087890625\n",
      "epoch: 5,  batch step: 150, loss: 48.18663024902344\n",
      "epoch: 5,  batch step: 151, loss: 85.47306823730469\n",
      "epoch: 5,  batch step: 152, loss: 43.268795013427734\n",
      "epoch: 5,  batch step: 153, loss: 298.89404296875\n",
      "epoch: 5,  batch step: 154, loss: 11.556025505065918\n",
      "epoch: 5,  batch step: 155, loss: 11.96603012084961\n",
      "epoch: 5,  batch step: 156, loss: 12.6353759765625\n",
      "epoch: 5,  batch step: 157, loss: 74.56526947021484\n",
      "epoch: 5,  batch step: 158, loss: 6.899213790893555\n",
      "epoch: 5,  batch step: 159, loss: 27.248395919799805\n",
      "epoch: 5,  batch step: 160, loss: 65.24726104736328\n",
      "epoch: 5,  batch step: 161, loss: 47.623680114746094\n",
      "epoch: 5,  batch step: 162, loss: 51.674400329589844\n",
      "epoch: 5,  batch step: 163, loss: 10.75922679901123\n",
      "epoch: 5,  batch step: 164, loss: 215.1655731201172\n",
      "epoch: 5,  batch step: 165, loss: 46.923072814941406\n",
      "epoch: 5,  batch step: 166, loss: 74.26084899902344\n",
      "epoch: 5,  batch step: 167, loss: 15.618247985839844\n",
      "epoch: 5,  batch step: 168, loss: 75.26875305175781\n",
      "epoch: 5,  batch step: 169, loss: 168.8843231201172\n",
      "epoch: 5,  batch step: 170, loss: 10.127599716186523\n",
      "epoch: 5,  batch step: 171, loss: 13.577239990234375\n",
      "epoch: 5,  batch step: 172, loss: 223.4895477294922\n",
      "epoch: 5,  batch step: 173, loss: 182.40765380859375\n",
      "epoch: 5,  batch step: 174, loss: 138.9270477294922\n",
      "epoch: 5,  batch step: 175, loss: 181.88540649414062\n",
      "epoch: 5,  batch step: 176, loss: 400.26153564453125\n",
      "epoch: 5,  batch step: 177, loss: 11.218339920043945\n",
      "epoch: 5,  batch step: 178, loss: 324.7237854003906\n",
      "epoch: 5,  batch step: 179, loss: 14.20996379852295\n",
      "epoch: 5,  batch step: 180, loss: 9.386186599731445\n",
      "epoch: 5,  batch step: 181, loss: 15.852752685546875\n",
      "epoch: 5,  batch step: 182, loss: 14.033766746520996\n",
      "epoch: 5,  batch step: 183, loss: 38.42816162109375\n",
      "epoch: 5,  batch step: 184, loss: 96.04801940917969\n",
      "epoch: 5,  batch step: 185, loss: 53.88561248779297\n",
      "epoch: 5,  batch step: 186, loss: 21.227746963500977\n",
      "epoch: 5,  batch step: 187, loss: 11.74726676940918\n",
      "epoch: 5,  batch step: 188, loss: 32.099082946777344\n",
      "epoch: 5,  batch step: 189, loss: 70.89076232910156\n",
      "epoch: 5,  batch step: 190, loss: 8.807501792907715\n",
      "epoch: 5,  batch step: 191, loss: 15.336637496948242\n",
      "epoch: 5,  batch step: 192, loss: 6.226382255554199\n",
      "epoch: 5,  batch step: 193, loss: 18.471435546875\n",
      "epoch: 5,  batch step: 194, loss: 32.171958923339844\n",
      "epoch: 5,  batch step: 195, loss: 34.21690368652344\n",
      "epoch: 5,  batch step: 196, loss: 70.1478042602539\n",
      "epoch: 5,  batch step: 197, loss: 76.47706604003906\n",
      "epoch: 5,  batch step: 198, loss: 11.06832504272461\n",
      "epoch: 5,  batch step: 199, loss: 112.50978088378906\n",
      "epoch: 5,  batch step: 200, loss: 8.210668563842773\n",
      "epoch: 5,  batch step: 201, loss: 177.60986328125\n",
      "epoch: 5,  batch step: 202, loss: 113.79656982421875\n",
      "epoch: 5,  batch step: 203, loss: 13.908475875854492\n",
      "epoch: 5,  batch step: 204, loss: 17.075414657592773\n",
      "epoch: 5,  batch step: 205, loss: 129.96322631835938\n",
      "epoch: 5,  batch step: 206, loss: 11.490427017211914\n",
      "epoch: 5,  batch step: 207, loss: 14.930865287780762\n",
      "epoch: 5,  batch step: 208, loss: 227.902587890625\n",
      "epoch: 5,  batch step: 209, loss: 13.120577812194824\n",
      "epoch: 5,  batch step: 210, loss: 11.340872764587402\n",
      "epoch: 5,  batch step: 211, loss: 62.844234466552734\n",
      "epoch: 5,  batch step: 212, loss: 16.60957145690918\n",
      "epoch: 5,  batch step: 213, loss: 104.32968139648438\n",
      "epoch: 5,  batch step: 214, loss: 13.27256965637207\n",
      "epoch: 5,  batch step: 215, loss: 20.471275329589844\n",
      "epoch: 5,  batch step: 216, loss: 66.2254867553711\n",
      "epoch: 5,  batch step: 217, loss: 10.81763744354248\n",
      "epoch: 5,  batch step: 218, loss: 26.712217330932617\n",
      "epoch: 5,  batch step: 219, loss: 25.814220428466797\n",
      "epoch: 5,  batch step: 220, loss: 51.52962875366211\n",
      "epoch: 5,  batch step: 221, loss: 6.6589202880859375\n",
      "epoch: 5,  batch step: 222, loss: 36.76942443847656\n",
      "epoch: 5,  batch step: 223, loss: 23.711294174194336\n",
      "epoch: 5,  batch step: 224, loss: 92.34922790527344\n",
      "epoch: 5,  batch step: 225, loss: 182.3171844482422\n",
      "epoch: 5,  batch step: 226, loss: 16.296367645263672\n",
      "epoch: 5,  batch step: 227, loss: 308.8804931640625\n",
      "epoch: 5,  batch step: 228, loss: 117.28899383544922\n",
      "epoch: 5,  batch step: 229, loss: 150.87196350097656\n",
      "epoch: 5,  batch step: 230, loss: 47.33491516113281\n",
      "epoch: 5,  batch step: 231, loss: 9.949790000915527\n",
      "epoch: 5,  batch step: 232, loss: 56.57331466674805\n",
      "epoch: 5,  batch step: 233, loss: 17.115230560302734\n",
      "epoch: 5,  batch step: 234, loss: 71.97447967529297\n",
      "epoch: 5,  batch step: 235, loss: 132.89544677734375\n",
      "epoch: 5,  batch step: 236, loss: 209.68077087402344\n",
      "epoch: 5,  batch step: 237, loss: 16.997047424316406\n",
      "epoch: 5,  batch step: 238, loss: 76.9815673828125\n",
      "epoch: 5,  batch step: 239, loss: 26.84124755859375\n",
      "epoch: 5,  batch step: 240, loss: 55.24981689453125\n",
      "epoch: 5,  batch step: 241, loss: 32.242244720458984\n",
      "epoch: 5,  batch step: 242, loss: 194.01348876953125\n",
      "epoch: 5,  batch step: 243, loss: 14.16469955444336\n",
      "epoch: 5,  batch step: 244, loss: 30.19856071472168\n",
      "epoch: 5,  batch step: 245, loss: 358.0199890136719\n",
      "epoch: 5,  batch step: 246, loss: 14.218549728393555\n",
      "epoch: 5,  batch step: 247, loss: 43.62943649291992\n",
      "epoch: 5,  batch step: 248, loss: 11.330690383911133\n",
      "epoch: 5,  batch step: 249, loss: 53.41427230834961\n",
      "epoch: 5,  batch step: 250, loss: 15.276887893676758\n",
      "epoch: 5,  batch step: 251, loss: 2044.9637451171875\n",
      "validation error epoch  5:    tensor(81.4156, device='cuda:0')\n",
      "316\n",
      "epoch: 6,  batch step: 0, loss: 52.57655334472656\n",
      "epoch: 6,  batch step: 1, loss: 199.185546875\n",
      "epoch: 6,  batch step: 2, loss: 103.92105102539062\n",
      "epoch: 6,  batch step: 3, loss: 125.67578125\n",
      "epoch: 6,  batch step: 4, loss: 40.627967834472656\n",
      "epoch: 6,  batch step: 5, loss: 222.1456756591797\n",
      "epoch: 6,  batch step: 6, loss: 36.09062957763672\n",
      "epoch: 6,  batch step: 7, loss: 20.725109100341797\n",
      "epoch: 6,  batch step: 8, loss: 20.20142364501953\n",
      "epoch: 6,  batch step: 9, loss: 23.81629753112793\n",
      "epoch: 6,  batch step: 10, loss: 52.684688568115234\n",
      "epoch: 6,  batch step: 11, loss: 149.266845703125\n",
      "epoch: 6,  batch step: 12, loss: 52.23627853393555\n",
      "epoch: 6,  batch step: 13, loss: 33.01704025268555\n",
      "epoch: 6,  batch step: 14, loss: 14.800280570983887\n",
      "epoch: 6,  batch step: 15, loss: 103.83119201660156\n",
      "epoch: 6,  batch step: 16, loss: 164.7261962890625\n",
      "epoch: 6,  batch step: 17, loss: 96.47764587402344\n",
      "epoch: 6,  batch step: 18, loss: 16.539501190185547\n",
      "epoch: 6,  batch step: 19, loss: 34.7183952331543\n",
      "epoch: 6,  batch step: 20, loss: 190.02354431152344\n",
      "epoch: 6,  batch step: 21, loss: 220.05206298828125\n",
      "epoch: 6,  batch step: 22, loss: 306.0827331542969\n",
      "epoch: 6,  batch step: 23, loss: 19.685314178466797\n",
      "epoch: 6,  batch step: 24, loss: 65.37539672851562\n",
      "epoch: 6,  batch step: 25, loss: 66.6082992553711\n",
      "epoch: 6,  batch step: 26, loss: 14.272704124450684\n",
      "epoch: 6,  batch step: 27, loss: 15.159576416015625\n",
      "epoch: 6,  batch step: 28, loss: 24.367015838623047\n",
      "epoch: 6,  batch step: 29, loss: 17.687335968017578\n",
      "epoch: 6,  batch step: 30, loss: 171.72689819335938\n",
      "epoch: 6,  batch step: 31, loss: 382.4458923339844\n",
      "epoch: 6,  batch step: 32, loss: 381.1480712890625\n",
      "epoch: 6,  batch step: 33, loss: 16.443645477294922\n",
      "epoch: 6,  batch step: 34, loss: 38.12175750732422\n",
      "epoch: 6,  batch step: 35, loss: 79.65800476074219\n",
      "epoch: 6,  batch step: 36, loss: 13.51578140258789\n",
      "epoch: 6,  batch step: 37, loss: 19.957542419433594\n",
      "epoch: 6,  batch step: 38, loss: 50.698509216308594\n",
      "epoch: 6,  batch step: 39, loss: 10.063777923583984\n",
      "epoch: 6,  batch step: 40, loss: 10.755338668823242\n",
      "epoch: 6,  batch step: 41, loss: 16.287948608398438\n",
      "epoch: 6,  batch step: 42, loss: 98.24493408203125\n",
      "epoch: 6,  batch step: 43, loss: 14.912803649902344\n",
      "epoch: 6,  batch step: 44, loss: 63.753395080566406\n",
      "epoch: 6,  batch step: 45, loss: 12.24366283416748\n",
      "epoch: 6,  batch step: 46, loss: 10.382810592651367\n",
      "epoch: 6,  batch step: 47, loss: 283.0841979980469\n",
      "epoch: 6,  batch step: 48, loss: 75.73497009277344\n",
      "epoch: 6,  batch step: 49, loss: 11.687593460083008\n",
      "epoch: 6,  batch step: 50, loss: 141.06703186035156\n",
      "epoch: 6,  batch step: 51, loss: 38.323665618896484\n",
      "epoch: 6,  batch step: 52, loss: 193.42501831054688\n",
      "epoch: 6,  batch step: 53, loss: 17.24579620361328\n",
      "epoch: 6,  batch step: 54, loss: 42.57341384887695\n",
      "epoch: 6,  batch step: 55, loss: 9.54121208190918\n",
      "epoch: 6,  batch step: 56, loss: 16.34599494934082\n",
      "epoch: 6,  batch step: 57, loss: 22.108318328857422\n",
      "epoch: 6,  batch step: 58, loss: 476.7469177246094\n",
      "epoch: 6,  batch step: 59, loss: 16.918319702148438\n",
      "epoch: 6,  batch step: 60, loss: 25.840068817138672\n",
      "epoch: 6,  batch step: 61, loss: 351.1817321777344\n",
      "epoch: 6,  batch step: 62, loss: 13.502298355102539\n",
      "epoch: 6,  batch step: 63, loss: 23.51329803466797\n",
      "epoch: 6,  batch step: 64, loss: 18.845203399658203\n",
      "epoch: 6,  batch step: 65, loss: 22.04755973815918\n",
      "epoch: 6,  batch step: 66, loss: 11.711343765258789\n",
      "epoch: 6,  batch step: 67, loss: 337.20635986328125\n",
      "epoch: 6,  batch step: 68, loss: 11.362232208251953\n",
      "epoch: 6,  batch step: 69, loss: 12.681703567504883\n",
      "epoch: 6,  batch step: 70, loss: 15.366408348083496\n",
      "epoch: 6,  batch step: 71, loss: 281.12347412109375\n",
      "epoch: 6,  batch step: 72, loss: 27.548320770263672\n",
      "epoch: 6,  batch step: 73, loss: 231.30149841308594\n",
      "epoch: 6,  batch step: 74, loss: 30.267858505249023\n",
      "epoch: 6,  batch step: 75, loss: 13.075101852416992\n",
      "epoch: 6,  batch step: 76, loss: 78.78516387939453\n",
      "epoch: 6,  batch step: 77, loss: 73.78082275390625\n",
      "epoch: 6,  batch step: 78, loss: 108.25692749023438\n",
      "epoch: 6,  batch step: 79, loss: 286.6258239746094\n",
      "epoch: 6,  batch step: 80, loss: 18.703149795532227\n",
      "epoch: 6,  batch step: 81, loss: 16.881345748901367\n",
      "epoch: 6,  batch step: 82, loss: 31.266963958740234\n",
      "epoch: 6,  batch step: 83, loss: 117.95822143554688\n",
      "epoch: 6,  batch step: 84, loss: 18.252399444580078\n",
      "epoch: 6,  batch step: 85, loss: 24.898197174072266\n",
      "epoch: 6,  batch step: 86, loss: 28.17878532409668\n",
      "epoch: 6,  batch step: 87, loss: 18.056224822998047\n",
      "epoch: 6,  batch step: 88, loss: 21.421798706054688\n",
      "epoch: 6,  batch step: 89, loss: 85.89857482910156\n",
      "epoch: 6,  batch step: 90, loss: 364.6679382324219\n",
      "epoch: 6,  batch step: 91, loss: 6.953433036804199\n",
      "epoch: 6,  batch step: 92, loss: 55.6939811706543\n",
      "epoch: 6,  batch step: 93, loss: 127.43572235107422\n",
      "epoch: 6,  batch step: 94, loss: 27.766807556152344\n",
      "epoch: 6,  batch step: 95, loss: 21.072254180908203\n",
      "epoch: 6,  batch step: 96, loss: 31.70711326599121\n",
      "epoch: 6,  batch step: 97, loss: 329.5699768066406\n",
      "epoch: 6,  batch step: 98, loss: 26.414321899414062\n",
      "epoch: 6,  batch step: 99, loss: 52.405731201171875\n",
      "epoch: 6,  batch step: 100, loss: 45.455345153808594\n",
      "epoch: 6,  batch step: 101, loss: 20.45380401611328\n",
      "epoch: 6,  batch step: 102, loss: 180.4922332763672\n",
      "epoch: 6,  batch step: 103, loss: 20.061735153198242\n",
      "epoch: 6,  batch step: 104, loss: 19.952131271362305\n",
      "epoch: 6,  batch step: 105, loss: 39.9901008605957\n",
      "epoch: 6,  batch step: 106, loss: 12.636198043823242\n",
      "epoch: 6,  batch step: 107, loss: 361.46514892578125\n",
      "epoch: 6,  batch step: 108, loss: 11.652254104614258\n",
      "epoch: 6,  batch step: 109, loss: 23.44569206237793\n",
      "epoch: 6,  batch step: 110, loss: 201.82162475585938\n",
      "epoch: 6,  batch step: 111, loss: 145.386962890625\n",
      "epoch: 6,  batch step: 112, loss: 38.98857498168945\n",
      "epoch: 6,  batch step: 113, loss: 18.094406127929688\n",
      "epoch: 6,  batch step: 114, loss: 11.687177658081055\n",
      "epoch: 6,  batch step: 115, loss: 53.83298873901367\n",
      "epoch: 6,  batch step: 116, loss: 16.218942642211914\n",
      "epoch: 6,  batch step: 117, loss: 24.284160614013672\n",
      "epoch: 6,  batch step: 118, loss: 265.33880615234375\n",
      "epoch: 6,  batch step: 119, loss: 13.010183334350586\n",
      "epoch: 6,  batch step: 120, loss: 177.3314666748047\n",
      "epoch: 6,  batch step: 121, loss: 83.71121978759766\n",
      "epoch: 6,  batch step: 122, loss: 21.340808868408203\n",
      "epoch: 6,  batch step: 123, loss: 8.331594467163086\n",
      "epoch: 6,  batch step: 124, loss: 257.80279541015625\n",
      "epoch: 6,  batch step: 125, loss: 8.846351623535156\n",
      "epoch: 6,  batch step: 126, loss: 8.173762321472168\n",
      "epoch: 6,  batch step: 127, loss: 56.737098693847656\n",
      "epoch: 6,  batch step: 128, loss: 9.602882385253906\n",
      "epoch: 6,  batch step: 129, loss: 13.402734756469727\n",
      "epoch: 6,  batch step: 130, loss: 19.086383819580078\n",
      "epoch: 6,  batch step: 131, loss: 20.55605125427246\n",
      "epoch: 6,  batch step: 132, loss: 42.72248077392578\n",
      "epoch: 6,  batch step: 133, loss: 284.9757080078125\n",
      "epoch: 6,  batch step: 134, loss: 14.205668449401855\n",
      "epoch: 6,  batch step: 135, loss: 39.081809997558594\n",
      "epoch: 6,  batch step: 136, loss: 56.41962814331055\n",
      "epoch: 6,  batch step: 137, loss: 13.647159576416016\n",
      "epoch: 6,  batch step: 138, loss: 60.88132858276367\n",
      "epoch: 6,  batch step: 139, loss: 9.284383773803711\n",
      "epoch: 6,  batch step: 140, loss: 131.92703247070312\n",
      "epoch: 6,  batch step: 141, loss: 53.46467208862305\n",
      "epoch: 6,  batch step: 142, loss: 11.436027526855469\n",
      "epoch: 6,  batch step: 143, loss: 69.92071533203125\n",
      "epoch: 6,  batch step: 144, loss: 9.283369064331055\n",
      "epoch: 6,  batch step: 145, loss: 67.76199340820312\n",
      "epoch: 6,  batch step: 146, loss: 55.1077995300293\n",
      "epoch: 6,  batch step: 147, loss: 40.71525573730469\n",
      "epoch: 6,  batch step: 148, loss: 30.1485652923584\n",
      "epoch: 6,  batch step: 149, loss: 113.66529846191406\n",
      "epoch: 6,  batch step: 150, loss: 11.544715881347656\n",
      "epoch: 6,  batch step: 151, loss: 173.0728759765625\n",
      "epoch: 6,  batch step: 152, loss: 11.281915664672852\n",
      "epoch: 6,  batch step: 153, loss: 16.045019149780273\n",
      "epoch: 6,  batch step: 154, loss: 16.239713668823242\n",
      "epoch: 6,  batch step: 155, loss: 9.833585739135742\n",
      "epoch: 6,  batch step: 156, loss: 12.012236595153809\n",
      "epoch: 6,  batch step: 157, loss: 54.87779998779297\n",
      "epoch: 6,  batch step: 158, loss: 18.748058319091797\n",
      "epoch: 6,  batch step: 159, loss: 19.29115867614746\n",
      "epoch: 6,  batch step: 160, loss: 14.219396591186523\n",
      "epoch: 6,  batch step: 161, loss: 15.641164779663086\n",
      "epoch: 6,  batch step: 162, loss: 85.16006469726562\n",
      "epoch: 6,  batch step: 163, loss: 42.7979850769043\n",
      "epoch: 6,  batch step: 164, loss: 22.818767547607422\n",
      "epoch: 6,  batch step: 165, loss: 76.25262451171875\n",
      "epoch: 6,  batch step: 166, loss: 73.4635238647461\n",
      "epoch: 6,  batch step: 167, loss: 11.427223205566406\n",
      "epoch: 6,  batch step: 168, loss: 8.326106071472168\n",
      "epoch: 6,  batch step: 169, loss: 40.61458969116211\n",
      "epoch: 6,  batch step: 170, loss: 10.438824653625488\n",
      "epoch: 6,  batch step: 171, loss: 14.532496452331543\n",
      "epoch: 6,  batch step: 172, loss: 143.77638244628906\n",
      "epoch: 6,  batch step: 173, loss: 395.310546875\n",
      "epoch: 6,  batch step: 174, loss: 25.483043670654297\n",
      "epoch: 6,  batch step: 175, loss: 172.6589813232422\n",
      "epoch: 6,  batch step: 176, loss: 55.241790771484375\n",
      "epoch: 6,  batch step: 177, loss: 9.175638198852539\n",
      "epoch: 6,  batch step: 178, loss: 23.135482788085938\n",
      "epoch: 6,  batch step: 179, loss: 496.3167724609375\n",
      "epoch: 6,  batch step: 180, loss: 26.456737518310547\n",
      "epoch: 6,  batch step: 181, loss: 59.82025146484375\n",
      "epoch: 6,  batch step: 182, loss: 21.13900375366211\n",
      "epoch: 6,  batch step: 183, loss: 22.57564926147461\n",
      "epoch: 6,  batch step: 184, loss: 32.23942184448242\n",
      "epoch: 6,  batch step: 185, loss: 411.8821716308594\n",
      "epoch: 6,  batch step: 186, loss: 17.975749969482422\n",
      "epoch: 6,  batch step: 187, loss: 22.191083908081055\n",
      "epoch: 6,  batch step: 188, loss: 93.0814208984375\n",
      "epoch: 6,  batch step: 189, loss: 19.28362274169922\n",
      "epoch: 6,  batch step: 190, loss: 19.422771453857422\n",
      "epoch: 6,  batch step: 191, loss: 16.246145248413086\n",
      "epoch: 6,  batch step: 192, loss: 14.113653182983398\n",
      "epoch: 6,  batch step: 193, loss: 21.829740524291992\n",
      "epoch: 6,  batch step: 194, loss: 9.65572738647461\n",
      "epoch: 6,  batch step: 195, loss: 17.878421783447266\n",
      "epoch: 6,  batch step: 196, loss: 35.026084899902344\n",
      "epoch: 6,  batch step: 197, loss: 14.62393856048584\n",
      "epoch: 6,  batch step: 198, loss: 7.916402339935303\n",
      "epoch: 6,  batch step: 199, loss: 239.74078369140625\n",
      "epoch: 6,  batch step: 200, loss: 14.450536727905273\n",
      "epoch: 6,  batch step: 201, loss: 14.629008293151855\n",
      "epoch: 6,  batch step: 202, loss: 7.168566703796387\n",
      "epoch: 6,  batch step: 203, loss: 45.241756439208984\n",
      "epoch: 6,  batch step: 204, loss: 23.920284271240234\n",
      "epoch: 6,  batch step: 205, loss: 214.36978149414062\n",
      "epoch: 6,  batch step: 206, loss: 59.02032470703125\n",
      "epoch: 6,  batch step: 207, loss: 202.2505340576172\n",
      "epoch: 6,  batch step: 208, loss: 268.3655700683594\n",
      "epoch: 6,  batch step: 209, loss: 15.35076904296875\n",
      "epoch: 6,  batch step: 210, loss: 14.059890747070312\n",
      "epoch: 6,  batch step: 211, loss: 192.56906127929688\n",
      "epoch: 6,  batch step: 212, loss: 29.754547119140625\n",
      "epoch: 6,  batch step: 213, loss: 57.52448272705078\n",
      "epoch: 6,  batch step: 214, loss: 383.5914001464844\n",
      "epoch: 6,  batch step: 215, loss: 18.943891525268555\n",
      "epoch: 6,  batch step: 216, loss: 12.772651672363281\n",
      "epoch: 6,  batch step: 217, loss: 365.4411315917969\n",
      "epoch: 6,  batch step: 218, loss: 12.300436973571777\n",
      "epoch: 6,  batch step: 219, loss: 83.50704193115234\n",
      "epoch: 6,  batch step: 220, loss: 21.921432495117188\n",
      "epoch: 6,  batch step: 221, loss: 61.391929626464844\n",
      "epoch: 6,  batch step: 222, loss: 20.339445114135742\n",
      "epoch: 6,  batch step: 223, loss: 174.3760986328125\n",
      "epoch: 6,  batch step: 224, loss: 20.910442352294922\n",
      "epoch: 6,  batch step: 225, loss: 100.57566833496094\n",
      "epoch: 6,  batch step: 226, loss: 25.26221466064453\n",
      "epoch: 6,  batch step: 227, loss: 172.8194580078125\n",
      "epoch: 6,  batch step: 228, loss: 18.230512619018555\n",
      "epoch: 6,  batch step: 229, loss: 179.3592529296875\n",
      "epoch: 6,  batch step: 230, loss: 38.49282455444336\n",
      "epoch: 6,  batch step: 231, loss: 134.52978515625\n",
      "epoch: 6,  batch step: 232, loss: 53.73066711425781\n",
      "epoch: 6,  batch step: 233, loss: 44.62440490722656\n",
      "epoch: 6,  batch step: 234, loss: 93.92464447021484\n",
      "epoch: 6,  batch step: 235, loss: 63.371883392333984\n",
      "epoch: 6,  batch step: 236, loss: 66.58537292480469\n",
      "epoch: 6,  batch step: 237, loss: 19.440059661865234\n",
      "epoch: 6,  batch step: 238, loss: 16.06527328491211\n",
      "epoch: 6,  batch step: 239, loss: 81.83496856689453\n",
      "epoch: 6,  batch step: 240, loss: 130.41860961914062\n",
      "epoch: 6,  batch step: 241, loss: 23.63555145263672\n",
      "epoch: 6,  batch step: 242, loss: 32.57853698730469\n",
      "epoch: 6,  batch step: 243, loss: 103.66027069091797\n",
      "epoch: 6,  batch step: 244, loss: 10.61704158782959\n",
      "epoch: 6,  batch step: 245, loss: 6.0856547355651855\n",
      "epoch: 6,  batch step: 246, loss: 27.24068260192871\n",
      "epoch: 6,  batch step: 247, loss: 111.90992736816406\n",
      "epoch: 6,  batch step: 248, loss: 13.565662384033203\n",
      "epoch: 6,  batch step: 249, loss: 277.6092529296875\n",
      "epoch: 6,  batch step: 250, loss: 114.5509262084961\n",
      "epoch: 6,  batch step: 251, loss: 161.109130859375\n",
      "validation error epoch  6:    tensor(82.5965, device='cuda:0')\n",
      "316\n",
      "epoch: 7,  batch step: 0, loss: 22.841888427734375\n",
      "epoch: 7,  batch step: 1, loss: 250.15823364257812\n",
      "epoch: 7,  batch step: 2, loss: 31.898021697998047\n",
      "epoch: 7,  batch step: 3, loss: 165.1904296875\n",
      "epoch: 7,  batch step: 4, loss: 71.57469177246094\n",
      "epoch: 7,  batch step: 5, loss: 44.64220428466797\n",
      "epoch: 7,  batch step: 6, loss: 22.420547485351562\n",
      "epoch: 7,  batch step: 7, loss: 222.05099487304688\n",
      "epoch: 7,  batch step: 8, loss: 179.60057067871094\n",
      "epoch: 7,  batch step: 9, loss: 56.1212272644043\n",
      "epoch: 7,  batch step: 10, loss: 28.83660888671875\n",
      "epoch: 7,  batch step: 11, loss: 190.75732421875\n",
      "epoch: 7,  batch step: 12, loss: 123.51124572753906\n",
      "epoch: 7,  batch step: 13, loss: 107.48361206054688\n",
      "epoch: 7,  batch step: 14, loss: 21.033708572387695\n",
      "epoch: 7,  batch step: 15, loss: 90.61854553222656\n",
      "epoch: 7,  batch step: 16, loss: 19.314451217651367\n",
      "epoch: 7,  batch step: 17, loss: 91.36491394042969\n",
      "epoch: 7,  batch step: 18, loss: 58.658935546875\n",
      "epoch: 7,  batch step: 19, loss: 29.937902450561523\n",
      "epoch: 7,  batch step: 20, loss: 47.265010833740234\n",
      "epoch: 7,  batch step: 21, loss: 33.27110290527344\n",
      "epoch: 7,  batch step: 22, loss: 11.152524948120117\n",
      "epoch: 7,  batch step: 23, loss: 41.95169448852539\n",
      "epoch: 7,  batch step: 24, loss: 34.663482666015625\n",
      "epoch: 7,  batch step: 25, loss: 178.03204345703125\n",
      "epoch: 7,  batch step: 26, loss: 133.5745849609375\n",
      "epoch: 7,  batch step: 27, loss: 14.188821792602539\n",
      "epoch: 7,  batch step: 28, loss: 20.587560653686523\n",
      "epoch: 7,  batch step: 29, loss: 23.480148315429688\n",
      "epoch: 7,  batch step: 30, loss: 27.955120086669922\n",
      "epoch: 7,  batch step: 31, loss: 35.24163818359375\n",
      "epoch: 7,  batch step: 32, loss: 48.61042785644531\n",
      "epoch: 7,  batch step: 33, loss: 12.235846519470215\n",
      "epoch: 7,  batch step: 34, loss: 21.653806686401367\n",
      "epoch: 7,  batch step: 35, loss: 10.01176929473877\n",
      "epoch: 7,  batch step: 36, loss: 13.946779251098633\n",
      "epoch: 7,  batch step: 37, loss: 17.735679626464844\n",
      "epoch: 7,  batch step: 38, loss: 191.9713592529297\n",
      "epoch: 7,  batch step: 39, loss: 14.80322265625\n",
      "epoch: 7,  batch step: 40, loss: 74.86766052246094\n",
      "epoch: 7,  batch step: 41, loss: 227.0768280029297\n",
      "epoch: 7,  batch step: 42, loss: 7.634258270263672\n",
      "epoch: 7,  batch step: 43, loss: 12.49539852142334\n",
      "epoch: 7,  batch step: 44, loss: 159.17161560058594\n",
      "epoch: 7,  batch step: 45, loss: 16.83603286743164\n",
      "epoch: 7,  batch step: 46, loss: 44.41900634765625\n",
      "epoch: 7,  batch step: 47, loss: 140.92727661132812\n",
      "epoch: 7,  batch step: 48, loss: 154.73898315429688\n",
      "epoch: 7,  batch step: 49, loss: 17.56064796447754\n",
      "epoch: 7,  batch step: 50, loss: 18.99180793762207\n",
      "epoch: 7,  batch step: 51, loss: 40.71222686767578\n",
      "epoch: 7,  batch step: 52, loss: 215.7488250732422\n",
      "epoch: 7,  batch step: 53, loss: 9.03249740600586\n",
      "epoch: 7,  batch step: 54, loss: 13.680834770202637\n",
      "epoch: 7,  batch step: 55, loss: 83.49584197998047\n",
      "epoch: 7,  batch step: 56, loss: 11.052587509155273\n",
      "epoch: 7,  batch step: 57, loss: 237.70492553710938\n",
      "epoch: 7,  batch step: 58, loss: 13.852069854736328\n",
      "epoch: 7,  batch step: 59, loss: 11.64460563659668\n",
      "epoch: 7,  batch step: 60, loss: 123.70646667480469\n",
      "epoch: 7,  batch step: 61, loss: 49.159568786621094\n",
      "epoch: 7,  batch step: 62, loss: 19.88205909729004\n",
      "epoch: 7,  batch step: 63, loss: 76.95932006835938\n",
      "epoch: 7,  batch step: 64, loss: 144.72300720214844\n",
      "epoch: 7,  batch step: 65, loss: 186.77008056640625\n",
      "epoch: 7,  batch step: 66, loss: 17.843177795410156\n",
      "epoch: 7,  batch step: 67, loss: 83.53251647949219\n",
      "epoch: 7,  batch step: 68, loss: 14.716296195983887\n",
      "epoch: 7,  batch step: 69, loss: 20.485633850097656\n",
      "epoch: 7,  batch step: 70, loss: 129.75112915039062\n",
      "epoch: 7,  batch step: 71, loss: 251.15756225585938\n",
      "epoch: 7,  batch step: 72, loss: 100.91583251953125\n",
      "epoch: 7,  batch step: 73, loss: 17.47023582458496\n",
      "epoch: 7,  batch step: 74, loss: 16.11571502685547\n",
      "epoch: 7,  batch step: 75, loss: 276.07354736328125\n",
      "epoch: 7,  batch step: 76, loss: 323.60638427734375\n",
      "epoch: 7,  batch step: 77, loss: 169.21188354492188\n",
      "epoch: 7,  batch step: 78, loss: 32.755714416503906\n",
      "epoch: 7,  batch step: 79, loss: 38.062171936035156\n",
      "epoch: 7,  batch step: 80, loss: 42.54911804199219\n",
      "epoch: 7,  batch step: 81, loss: 112.62431335449219\n",
      "epoch: 7,  batch step: 82, loss: 28.005260467529297\n",
      "epoch: 7,  batch step: 83, loss: 40.92746353149414\n",
      "epoch: 7,  batch step: 84, loss: 115.07402801513672\n",
      "epoch: 7,  batch step: 85, loss: 26.26476287841797\n",
      "epoch: 7,  batch step: 86, loss: 79.52937316894531\n",
      "epoch: 7,  batch step: 87, loss: 29.216861724853516\n",
      "epoch: 7,  batch step: 88, loss: 260.61248779296875\n",
      "epoch: 7,  batch step: 89, loss: 14.774500846862793\n",
      "epoch: 7,  batch step: 90, loss: 42.712738037109375\n",
      "epoch: 7,  batch step: 91, loss: 16.871618270874023\n",
      "epoch: 7,  batch step: 92, loss: 24.986425399780273\n",
      "epoch: 7,  batch step: 93, loss: 253.81301879882812\n",
      "epoch: 7,  batch step: 94, loss: 17.6375732421875\n",
      "epoch: 7,  batch step: 95, loss: 516.6722412109375\n",
      "epoch: 7,  batch step: 96, loss: 12.224103927612305\n",
      "epoch: 7,  batch step: 97, loss: 170.33946228027344\n",
      "epoch: 7,  batch step: 98, loss: 107.07308959960938\n",
      "epoch: 7,  batch step: 99, loss: 171.40086364746094\n",
      "epoch: 7,  batch step: 100, loss: 21.958972930908203\n",
      "epoch: 7,  batch step: 101, loss: 18.260421752929688\n",
      "epoch: 7,  batch step: 102, loss: 15.022172927856445\n",
      "epoch: 7,  batch step: 103, loss: 20.158180236816406\n",
      "epoch: 7,  batch step: 104, loss: 17.63014793395996\n",
      "epoch: 7,  batch step: 105, loss: 281.82269287109375\n",
      "epoch: 7,  batch step: 106, loss: 29.650991439819336\n",
      "epoch: 7,  batch step: 107, loss: 226.69236755371094\n",
      "epoch: 7,  batch step: 108, loss: 154.40049743652344\n",
      "epoch: 7,  batch step: 109, loss: 33.99714660644531\n",
      "epoch: 7,  batch step: 110, loss: 19.879173278808594\n",
      "epoch: 7,  batch step: 111, loss: 20.01703643798828\n",
      "epoch: 7,  batch step: 112, loss: 11.745894432067871\n",
      "epoch: 7,  batch step: 113, loss: 208.99046325683594\n",
      "epoch: 7,  batch step: 114, loss: 144.05026245117188\n",
      "epoch: 7,  batch step: 115, loss: 151.45883178710938\n",
      "epoch: 7,  batch step: 116, loss: 161.8231201171875\n",
      "epoch: 7,  batch step: 117, loss: 47.34173583984375\n",
      "epoch: 7,  batch step: 118, loss: 31.35920524597168\n",
      "epoch: 7,  batch step: 119, loss: 29.970653533935547\n",
      "epoch: 7,  batch step: 120, loss: 51.77653121948242\n",
      "epoch: 7,  batch step: 121, loss: 17.968679428100586\n",
      "epoch: 7,  batch step: 122, loss: 44.96580505371094\n",
      "epoch: 7,  batch step: 123, loss: 55.22876739501953\n",
      "epoch: 7,  batch step: 124, loss: 86.93258666992188\n",
      "epoch: 7,  batch step: 125, loss: 96.53527069091797\n",
      "epoch: 7,  batch step: 126, loss: 174.98306274414062\n",
      "epoch: 7,  batch step: 127, loss: 17.98004913330078\n",
      "epoch: 7,  batch step: 128, loss: 15.669767379760742\n",
      "epoch: 7,  batch step: 129, loss: 14.729732513427734\n",
      "epoch: 7,  batch step: 130, loss: 7.870988845825195\n",
      "epoch: 7,  batch step: 131, loss: 12.872406005859375\n",
      "epoch: 7,  batch step: 132, loss: 24.86334800720215\n",
      "epoch: 7,  batch step: 133, loss: 19.218185424804688\n",
      "epoch: 7,  batch step: 134, loss: 77.65165710449219\n",
      "epoch: 7,  batch step: 135, loss: 269.35296630859375\n",
      "epoch: 7,  batch step: 136, loss: 58.69502258300781\n",
      "epoch: 7,  batch step: 137, loss: 12.262947082519531\n",
      "epoch: 7,  batch step: 138, loss: 141.54159545898438\n",
      "epoch: 7,  batch step: 139, loss: 83.46784210205078\n",
      "epoch: 7,  batch step: 140, loss: 16.65886688232422\n",
      "epoch: 7,  batch step: 141, loss: 25.887165069580078\n",
      "epoch: 7,  batch step: 142, loss: 42.454803466796875\n",
      "epoch: 7,  batch step: 143, loss: 33.38620376586914\n",
      "epoch: 7,  batch step: 144, loss: 12.397872924804688\n",
      "epoch: 7,  batch step: 145, loss: 24.124488830566406\n",
      "epoch: 7,  batch step: 146, loss: 131.32545471191406\n",
      "epoch: 7,  batch step: 147, loss: 140.6455078125\n",
      "epoch: 7,  batch step: 148, loss: 13.848933219909668\n",
      "epoch: 7,  batch step: 149, loss: 70.25\n",
      "epoch: 7,  batch step: 150, loss: 83.96865844726562\n",
      "epoch: 7,  batch step: 151, loss: 19.991209030151367\n",
      "epoch: 7,  batch step: 152, loss: 15.75031566619873\n",
      "epoch: 7,  batch step: 153, loss: 182.4075164794922\n",
      "epoch: 7,  batch step: 154, loss: 30.975605010986328\n",
      "epoch: 7,  batch step: 155, loss: 19.445751190185547\n",
      "epoch: 7,  batch step: 156, loss: 90.68850708007812\n",
      "epoch: 7,  batch step: 157, loss: 16.577390670776367\n",
      "epoch: 7,  batch step: 158, loss: 19.50959587097168\n",
      "epoch: 7,  batch step: 159, loss: 21.953031539916992\n",
      "epoch: 7,  batch step: 160, loss: 241.6031951904297\n",
      "epoch: 7,  batch step: 161, loss: 9.45207405090332\n",
      "epoch: 7,  batch step: 162, loss: 11.95276165008545\n",
      "epoch: 7,  batch step: 163, loss: 316.62091064453125\n",
      "epoch: 7,  batch step: 164, loss: 12.449291229248047\n",
      "epoch: 7,  batch step: 165, loss: 11.416007995605469\n",
      "epoch: 7,  batch step: 166, loss: 15.192464828491211\n",
      "epoch: 7,  batch step: 167, loss: 129.5389404296875\n",
      "epoch: 7,  batch step: 168, loss: 38.576904296875\n",
      "epoch: 7,  batch step: 169, loss: 13.67233943939209\n",
      "epoch: 7,  batch step: 170, loss: 21.111927032470703\n",
      "epoch: 7,  batch step: 171, loss: 47.619285583496094\n",
      "epoch: 7,  batch step: 172, loss: 11.838117599487305\n",
      "epoch: 7,  batch step: 173, loss: 175.11328125\n",
      "epoch: 7,  batch step: 174, loss: 12.90766716003418\n",
      "epoch: 7,  batch step: 175, loss: 167.00582885742188\n",
      "epoch: 7,  batch step: 176, loss: 30.655996322631836\n",
      "epoch: 7,  batch step: 177, loss: 12.478483200073242\n",
      "epoch: 7,  batch step: 178, loss: 13.780138969421387\n",
      "epoch: 7,  batch step: 179, loss: 21.13245391845703\n",
      "epoch: 7,  batch step: 180, loss: 20.87396240234375\n",
      "epoch: 7,  batch step: 181, loss: 174.22457885742188\n",
      "epoch: 7,  batch step: 182, loss: 57.616302490234375\n",
      "epoch: 7,  batch step: 183, loss: 36.02668762207031\n",
      "epoch: 7,  batch step: 184, loss: 11.931276321411133\n",
      "epoch: 7,  batch step: 185, loss: 21.6738338470459\n",
      "epoch: 7,  batch step: 186, loss: 197.42864990234375\n",
      "epoch: 7,  batch step: 187, loss: 8.956197738647461\n",
      "epoch: 7,  batch step: 188, loss: 9.976476669311523\n",
      "epoch: 7,  batch step: 189, loss: 13.153597831726074\n",
      "epoch: 7,  batch step: 190, loss: 19.609161376953125\n",
      "epoch: 7,  batch step: 191, loss: 20.480941772460938\n",
      "epoch: 7,  batch step: 192, loss: 37.915523529052734\n",
      "epoch: 7,  batch step: 193, loss: 8.440970420837402\n",
      "epoch: 7,  batch step: 194, loss: 204.7795867919922\n",
      "epoch: 7,  batch step: 195, loss: 10.839736938476562\n",
      "epoch: 7,  batch step: 196, loss: 52.253875732421875\n",
      "epoch: 7,  batch step: 197, loss: 196.14932250976562\n",
      "epoch: 7,  batch step: 198, loss: 12.864461898803711\n",
      "epoch: 7,  batch step: 199, loss: 17.932720184326172\n",
      "epoch: 7,  batch step: 200, loss: 159.962646484375\n",
      "epoch: 7,  batch step: 201, loss: 151.87191772460938\n",
      "epoch: 7,  batch step: 202, loss: 13.626867294311523\n",
      "epoch: 7,  batch step: 203, loss: 11.441883087158203\n",
      "epoch: 7,  batch step: 204, loss: 223.3372802734375\n",
      "epoch: 7,  batch step: 205, loss: 150.0546417236328\n",
      "epoch: 7,  batch step: 206, loss: 12.072920799255371\n",
      "epoch: 7,  batch step: 207, loss: 83.89762878417969\n",
      "epoch: 7,  batch step: 208, loss: 300.19769287109375\n",
      "epoch: 7,  batch step: 209, loss: 17.021127700805664\n",
      "epoch: 7,  batch step: 210, loss: 46.32263946533203\n",
      "epoch: 7,  batch step: 211, loss: 132.6800994873047\n",
      "epoch: 7,  batch step: 212, loss: 43.75556182861328\n",
      "epoch: 7,  batch step: 213, loss: 67.91880798339844\n",
      "epoch: 7,  batch step: 214, loss: 68.31842041015625\n",
      "epoch: 7,  batch step: 215, loss: 17.716814041137695\n",
      "epoch: 7,  batch step: 216, loss: 26.44052505493164\n",
      "epoch: 7,  batch step: 217, loss: 247.4290008544922\n",
      "epoch: 7,  batch step: 218, loss: 81.76127624511719\n",
      "epoch: 7,  batch step: 219, loss: 10.75905990600586\n",
      "epoch: 7,  batch step: 220, loss: 202.53292846679688\n",
      "epoch: 7,  batch step: 221, loss: 74.86693572998047\n",
      "epoch: 7,  batch step: 222, loss: 20.64432144165039\n",
      "epoch: 7,  batch step: 223, loss: 344.55963134765625\n",
      "epoch: 7,  batch step: 224, loss: 22.75594711303711\n",
      "epoch: 7,  batch step: 225, loss: 12.072425842285156\n",
      "epoch: 7,  batch step: 226, loss: 79.60868835449219\n",
      "epoch: 7,  batch step: 227, loss: 38.115081787109375\n",
      "epoch: 7,  batch step: 228, loss: 37.151573181152344\n",
      "epoch: 7,  batch step: 229, loss: 312.7037048339844\n",
      "epoch: 7,  batch step: 230, loss: 19.344757080078125\n",
      "epoch: 7,  batch step: 231, loss: 128.22113037109375\n",
      "epoch: 7,  batch step: 232, loss: 40.59809494018555\n",
      "epoch: 7,  batch step: 233, loss: 33.496925354003906\n",
      "epoch: 7,  batch step: 234, loss: 18.765287399291992\n",
      "epoch: 7,  batch step: 235, loss: 39.60322570800781\n",
      "epoch: 7,  batch step: 236, loss: 235.52154541015625\n",
      "epoch: 7,  batch step: 237, loss: 76.11228942871094\n",
      "epoch: 7,  batch step: 238, loss: 26.879398345947266\n",
      "epoch: 7,  batch step: 239, loss: 17.109294891357422\n",
      "epoch: 7,  batch step: 240, loss: 16.553558349609375\n",
      "epoch: 7,  batch step: 241, loss: 24.491268157958984\n",
      "epoch: 7,  batch step: 242, loss: 42.058834075927734\n",
      "epoch: 7,  batch step: 243, loss: 73.4485855102539\n",
      "epoch: 7,  batch step: 244, loss: 14.835670471191406\n",
      "epoch: 7,  batch step: 245, loss: 26.09377670288086\n",
      "epoch: 7,  batch step: 246, loss: 7.2231597900390625\n",
      "epoch: 7,  batch step: 247, loss: 55.29267501831055\n",
      "epoch: 7,  batch step: 248, loss: 97.24356079101562\n",
      "epoch: 7,  batch step: 249, loss: 12.027353286743164\n",
      "epoch: 7,  batch step: 250, loss: 12.673439025878906\n",
      "epoch: 7,  batch step: 251, loss: 50.08354187011719\n",
      "validation error epoch  7:    tensor(72.0952, device='cuda:0')\n",
      "316\n",
      "epoch: 8,  batch step: 0, loss: 51.622825622558594\n",
      "epoch: 8,  batch step: 1, loss: 17.637035369873047\n",
      "epoch: 8,  batch step: 2, loss: 16.229976654052734\n",
      "epoch: 8,  batch step: 3, loss: 66.40592956542969\n",
      "epoch: 8,  batch step: 4, loss: 8.956393241882324\n",
      "epoch: 8,  batch step: 5, loss: 12.826922416687012\n",
      "epoch: 8,  batch step: 6, loss: 63.96076583862305\n",
      "epoch: 8,  batch step: 7, loss: 14.104368209838867\n",
      "epoch: 8,  batch step: 8, loss: 28.771757125854492\n",
      "epoch: 8,  batch step: 9, loss: 62.260215759277344\n",
      "epoch: 8,  batch step: 10, loss: 214.08116149902344\n",
      "epoch: 8,  batch step: 11, loss: 7.472888946533203\n",
      "epoch: 8,  batch step: 12, loss: 11.975288391113281\n",
      "epoch: 8,  batch step: 13, loss: 14.990242004394531\n",
      "epoch: 8,  batch step: 14, loss: 322.0174560546875\n",
      "epoch: 8,  batch step: 15, loss: 81.25468444824219\n",
      "epoch: 8,  batch step: 16, loss: 163.91058349609375\n",
      "epoch: 8,  batch step: 17, loss: 115.14593505859375\n",
      "epoch: 8,  batch step: 18, loss: 223.606201171875\n",
      "epoch: 8,  batch step: 19, loss: 260.58294677734375\n",
      "epoch: 8,  batch step: 20, loss: 64.44770812988281\n",
      "epoch: 8,  batch step: 21, loss: 6.446368217468262\n",
      "epoch: 8,  batch step: 22, loss: 108.56050872802734\n",
      "epoch: 8,  batch step: 23, loss: 60.07490158081055\n",
      "epoch: 8,  batch step: 24, loss: 180.98838806152344\n",
      "epoch: 8,  batch step: 25, loss: 40.93883514404297\n",
      "epoch: 8,  batch step: 26, loss: 166.23873901367188\n",
      "epoch: 8,  batch step: 27, loss: 41.139854431152344\n",
      "epoch: 8,  batch step: 28, loss: 222.7749481201172\n",
      "epoch: 8,  batch step: 29, loss: 122.68608856201172\n",
      "epoch: 8,  batch step: 30, loss: 33.04335021972656\n",
      "epoch: 8,  batch step: 31, loss: 40.66156005859375\n",
      "epoch: 8,  batch step: 32, loss: 93.90032958984375\n",
      "epoch: 8,  batch step: 33, loss: 26.342931747436523\n",
      "epoch: 8,  batch step: 34, loss: 34.876686096191406\n",
      "epoch: 8,  batch step: 35, loss: 29.0118408203125\n",
      "epoch: 8,  batch step: 36, loss: 50.40612030029297\n",
      "epoch: 8,  batch step: 37, loss: 35.30299377441406\n",
      "epoch: 8,  batch step: 38, loss: 13.2831449508667\n",
      "epoch: 8,  batch step: 39, loss: 24.785383224487305\n",
      "epoch: 8,  batch step: 40, loss: 51.90392303466797\n",
      "epoch: 8,  batch step: 41, loss: 11.51768684387207\n",
      "epoch: 8,  batch step: 42, loss: 25.096206665039062\n",
      "epoch: 8,  batch step: 43, loss: 16.41490936279297\n",
      "epoch: 8,  batch step: 44, loss: 8.995338439941406\n",
      "epoch: 8,  batch step: 45, loss: 13.934793472290039\n",
      "epoch: 8,  batch step: 46, loss: 349.1534118652344\n",
      "epoch: 8,  batch step: 47, loss: 21.000972747802734\n",
      "epoch: 8,  batch step: 48, loss: 54.29149627685547\n",
      "epoch: 8,  batch step: 49, loss: 9.059405326843262\n",
      "epoch: 8,  batch step: 50, loss: 4.468197822570801\n",
      "epoch: 8,  batch step: 51, loss: 14.344584465026855\n",
      "epoch: 8,  batch step: 52, loss: 8.581114768981934\n",
      "epoch: 8,  batch step: 53, loss: 12.861620903015137\n",
      "epoch: 8,  batch step: 54, loss: 38.60420608520508\n",
      "epoch: 8,  batch step: 55, loss: 14.631093978881836\n",
      "epoch: 8,  batch step: 56, loss: 149.71234130859375\n",
      "epoch: 8,  batch step: 57, loss: 13.54898738861084\n",
      "epoch: 8,  batch step: 58, loss: 383.0945739746094\n",
      "epoch: 8,  batch step: 59, loss: 184.54637145996094\n",
      "epoch: 8,  batch step: 60, loss: 154.95700073242188\n",
      "epoch: 8,  batch step: 61, loss: 22.273710250854492\n",
      "epoch: 8,  batch step: 62, loss: 35.17845916748047\n",
      "epoch: 8,  batch step: 63, loss: 7.7967352867126465\n",
      "epoch: 8,  batch step: 64, loss: 5.505814552307129\n",
      "epoch: 8,  batch step: 65, loss: 83.17267608642578\n",
      "epoch: 8,  batch step: 66, loss: 274.73028564453125\n",
      "epoch: 8,  batch step: 67, loss: 20.058589935302734\n",
      "epoch: 8,  batch step: 68, loss: 39.542869567871094\n",
      "epoch: 8,  batch step: 69, loss: 22.020933151245117\n",
      "epoch: 8,  batch step: 70, loss: 256.449951171875\n",
      "epoch: 8,  batch step: 71, loss: 76.73970794677734\n",
      "epoch: 8,  batch step: 72, loss: 19.894554138183594\n",
      "epoch: 8,  batch step: 73, loss: 30.455848693847656\n",
      "epoch: 8,  batch step: 74, loss: 238.3096160888672\n",
      "epoch: 8,  batch step: 75, loss: 223.79559326171875\n",
      "epoch: 8,  batch step: 76, loss: 213.9946746826172\n",
      "epoch: 8,  batch step: 77, loss: 42.77983093261719\n",
      "epoch: 8,  batch step: 78, loss: 41.57766342163086\n",
      "epoch: 8,  batch step: 79, loss: 80.41545867919922\n",
      "epoch: 8,  batch step: 80, loss: 30.57594108581543\n",
      "epoch: 8,  batch step: 81, loss: 29.973236083984375\n",
      "epoch: 8,  batch step: 82, loss: 39.576236724853516\n",
      "epoch: 8,  batch step: 83, loss: 25.205036163330078\n",
      "epoch: 8,  batch step: 84, loss: 14.093971252441406\n",
      "epoch: 8,  batch step: 85, loss: 25.409019470214844\n",
      "epoch: 8,  batch step: 86, loss: 113.00357055664062\n",
      "epoch: 8,  batch step: 87, loss: 15.551267623901367\n",
      "epoch: 8,  batch step: 88, loss: 125.50306701660156\n",
      "epoch: 8,  batch step: 89, loss: 301.13531494140625\n",
      "epoch: 8,  batch step: 90, loss: 23.84298324584961\n",
      "epoch: 8,  batch step: 91, loss: 21.418973922729492\n",
      "epoch: 8,  batch step: 92, loss: 14.442294120788574\n",
      "epoch: 8,  batch step: 93, loss: 19.856061935424805\n",
      "epoch: 8,  batch step: 94, loss: 29.043861389160156\n",
      "epoch: 8,  batch step: 95, loss: 72.05724334716797\n",
      "epoch: 8,  batch step: 96, loss: 32.285701751708984\n",
      "epoch: 8,  batch step: 97, loss: 15.366874694824219\n",
      "epoch: 8,  batch step: 98, loss: 72.34269714355469\n",
      "epoch: 8,  batch step: 99, loss: 77.90879821777344\n",
      "epoch: 8,  batch step: 100, loss: 173.47036743164062\n",
      "epoch: 8,  batch step: 101, loss: 13.67667007446289\n",
      "epoch: 8,  batch step: 102, loss: 14.375738143920898\n",
      "epoch: 8,  batch step: 103, loss: 20.129531860351562\n",
      "epoch: 8,  batch step: 104, loss: 5.2627458572387695\n",
      "epoch: 8,  batch step: 105, loss: 19.63079261779785\n",
      "epoch: 8,  batch step: 106, loss: 13.697250366210938\n",
      "epoch: 8,  batch step: 107, loss: 385.128173828125\n",
      "epoch: 8,  batch step: 108, loss: 48.94247055053711\n",
      "epoch: 8,  batch step: 109, loss: 205.40042114257812\n",
      "epoch: 8,  batch step: 110, loss: 64.33895111083984\n",
      "epoch: 8,  batch step: 111, loss: 152.73052978515625\n",
      "epoch: 8,  batch step: 112, loss: 16.91737937927246\n",
      "epoch: 8,  batch step: 113, loss: 85.68609619140625\n",
      "epoch: 8,  batch step: 114, loss: 47.1284294128418\n",
      "epoch: 8,  batch step: 115, loss: 16.177751541137695\n",
      "epoch: 8,  batch step: 116, loss: 240.5272674560547\n",
      "epoch: 8,  batch step: 117, loss: 23.21210479736328\n",
      "epoch: 8,  batch step: 118, loss: 196.99252319335938\n",
      "epoch: 8,  batch step: 119, loss: 165.07708740234375\n",
      "epoch: 8,  batch step: 120, loss: 66.91683959960938\n",
      "epoch: 8,  batch step: 121, loss: 13.93580436706543\n",
      "epoch: 8,  batch step: 122, loss: 16.684860229492188\n",
      "epoch: 8,  batch step: 123, loss: 26.865652084350586\n",
      "epoch: 8,  batch step: 124, loss: 67.24490356445312\n",
      "epoch: 8,  batch step: 125, loss: 298.83343505859375\n",
      "epoch: 8,  batch step: 126, loss: 86.09395599365234\n",
      "epoch: 8,  batch step: 127, loss: 323.85247802734375\n",
      "epoch: 8,  batch step: 128, loss: 15.942197799682617\n",
      "epoch: 8,  batch step: 129, loss: 145.29971313476562\n",
      "epoch: 8,  batch step: 130, loss: 60.54691696166992\n",
      "epoch: 8,  batch step: 131, loss: 20.40927505493164\n",
      "epoch: 8,  batch step: 132, loss: 155.4945526123047\n",
      "epoch: 8,  batch step: 133, loss: 18.18299102783203\n",
      "epoch: 8,  batch step: 134, loss: 20.375961303710938\n",
      "epoch: 8,  batch step: 135, loss: 64.84648132324219\n",
      "epoch: 8,  batch step: 136, loss: 22.606948852539062\n",
      "epoch: 8,  batch step: 137, loss: 23.83870506286621\n",
      "epoch: 8,  batch step: 138, loss: 20.555301666259766\n",
      "epoch: 8,  batch step: 139, loss: 22.743932723999023\n",
      "epoch: 8,  batch step: 140, loss: 11.938478469848633\n",
      "epoch: 8,  batch step: 141, loss: 12.239020347595215\n",
      "epoch: 8,  batch step: 142, loss: 66.06647491455078\n",
      "epoch: 8,  batch step: 143, loss: 12.518987655639648\n",
      "epoch: 8,  batch step: 144, loss: 95.84854125976562\n",
      "epoch: 8,  batch step: 145, loss: 235.177490234375\n",
      "epoch: 8,  batch step: 146, loss: 121.66451263427734\n",
      "epoch: 8,  batch step: 147, loss: 198.5154266357422\n",
      "epoch: 8,  batch step: 148, loss: 406.7176208496094\n",
      "epoch: 8,  batch step: 149, loss: 11.44092845916748\n",
      "epoch: 8,  batch step: 150, loss: 10.87330150604248\n",
      "epoch: 8,  batch step: 151, loss: 24.0004940032959\n",
      "epoch: 8,  batch step: 152, loss: 73.1632308959961\n",
      "epoch: 8,  batch step: 153, loss: 30.520177841186523\n",
      "epoch: 8,  batch step: 154, loss: 15.279257774353027\n",
      "epoch: 8,  batch step: 155, loss: 32.29275131225586\n",
      "epoch: 8,  batch step: 156, loss: 12.320030212402344\n",
      "epoch: 8,  batch step: 157, loss: 75.10265350341797\n",
      "epoch: 8,  batch step: 158, loss: 87.27123260498047\n",
      "epoch: 8,  batch step: 159, loss: 8.930810928344727\n",
      "epoch: 8,  batch step: 160, loss: 36.53199005126953\n",
      "epoch: 8,  batch step: 161, loss: 17.812543869018555\n",
      "epoch: 8,  batch step: 162, loss: 79.69160461425781\n",
      "epoch: 8,  batch step: 163, loss: 27.3382625579834\n",
      "epoch: 8,  batch step: 164, loss: 20.775724411010742\n",
      "epoch: 8,  batch step: 165, loss: 146.26651000976562\n",
      "epoch: 8,  batch step: 166, loss: 66.04283905029297\n",
      "epoch: 8,  batch step: 167, loss: 111.3525390625\n",
      "epoch: 8,  batch step: 168, loss: 18.945587158203125\n",
      "epoch: 8,  batch step: 169, loss: 35.02345275878906\n",
      "epoch: 8,  batch step: 170, loss: 28.012470245361328\n",
      "epoch: 8,  batch step: 171, loss: 13.100932121276855\n",
      "epoch: 8,  batch step: 172, loss: 86.07601928710938\n",
      "epoch: 8,  batch step: 173, loss: 350.7328796386719\n",
      "epoch: 8,  batch step: 174, loss: 18.876426696777344\n",
      "epoch: 8,  batch step: 175, loss: 11.072723388671875\n",
      "epoch: 8,  batch step: 176, loss: 203.74606323242188\n",
      "epoch: 8,  batch step: 177, loss: 113.96890258789062\n",
      "epoch: 8,  batch step: 178, loss: 153.321533203125\n",
      "epoch: 8,  batch step: 179, loss: 18.064178466796875\n",
      "epoch: 8,  batch step: 180, loss: 31.222272872924805\n",
      "epoch: 8,  batch step: 181, loss: 23.91791343688965\n",
      "epoch: 8,  batch step: 182, loss: 13.578636169433594\n",
      "epoch: 8,  batch step: 183, loss: 189.77764892578125\n",
      "epoch: 8,  batch step: 184, loss: 17.593278884887695\n",
      "epoch: 8,  batch step: 185, loss: 167.775390625\n",
      "epoch: 8,  batch step: 186, loss: 24.162473678588867\n",
      "epoch: 8,  batch step: 187, loss: 17.38918685913086\n",
      "epoch: 8,  batch step: 188, loss: 88.11361694335938\n",
      "epoch: 8,  batch step: 189, loss: 23.80987548828125\n",
      "epoch: 8,  batch step: 190, loss: 39.68360137939453\n",
      "epoch: 8,  batch step: 191, loss: 24.424800872802734\n",
      "epoch: 8,  batch step: 192, loss: 29.997455596923828\n",
      "epoch: 8,  batch step: 193, loss: 137.7682342529297\n",
      "epoch: 8,  batch step: 194, loss: 290.3856201171875\n",
      "epoch: 8,  batch step: 195, loss: 21.011999130249023\n",
      "epoch: 8,  batch step: 196, loss: 10.07919979095459\n",
      "epoch: 8,  batch step: 197, loss: 15.850482940673828\n",
      "epoch: 8,  batch step: 198, loss: 46.4738883972168\n",
      "epoch: 8,  batch step: 199, loss: 14.439128875732422\n",
      "epoch: 8,  batch step: 200, loss: 12.083674430847168\n",
      "epoch: 8,  batch step: 201, loss: 11.0149564743042\n",
      "epoch: 8,  batch step: 202, loss: 187.82000732421875\n",
      "epoch: 8,  batch step: 203, loss: 19.096343994140625\n",
      "epoch: 8,  batch step: 204, loss: 22.855926513671875\n",
      "epoch: 8,  batch step: 205, loss: 17.5728816986084\n",
      "epoch: 8,  batch step: 206, loss: 37.9427490234375\n",
      "epoch: 8,  batch step: 207, loss: 112.6019287109375\n",
      "epoch: 8,  batch step: 208, loss: 19.859066009521484\n",
      "epoch: 8,  batch step: 209, loss: 46.99468994140625\n",
      "epoch: 8,  batch step: 210, loss: 71.90299987792969\n",
      "epoch: 8,  batch step: 211, loss: 18.432600021362305\n",
      "epoch: 8,  batch step: 212, loss: 19.488067626953125\n",
      "epoch: 8,  batch step: 213, loss: 26.605915069580078\n",
      "epoch: 8,  batch step: 214, loss: 149.36685180664062\n",
      "epoch: 8,  batch step: 215, loss: 44.147865295410156\n",
      "epoch: 8,  batch step: 216, loss: 35.95886993408203\n",
      "epoch: 8,  batch step: 217, loss: 12.439291954040527\n",
      "epoch: 8,  batch step: 218, loss: 177.8568115234375\n",
      "epoch: 8,  batch step: 219, loss: 45.103904724121094\n",
      "epoch: 8,  batch step: 220, loss: 108.24327850341797\n",
      "epoch: 8,  batch step: 221, loss: 20.056705474853516\n",
      "epoch: 8,  batch step: 222, loss: 16.586994171142578\n",
      "epoch: 8,  batch step: 223, loss: 13.473734855651855\n",
      "epoch: 8,  batch step: 224, loss: 12.85920238494873\n",
      "epoch: 8,  batch step: 225, loss: 54.013885498046875\n",
      "epoch: 8,  batch step: 226, loss: 319.9291076660156\n",
      "epoch: 8,  batch step: 227, loss: 15.173397064208984\n",
      "epoch: 8,  batch step: 228, loss: 7.443321228027344\n",
      "epoch: 8,  batch step: 229, loss: 185.7362060546875\n",
      "epoch: 8,  batch step: 230, loss: 152.56744384765625\n",
      "epoch: 8,  batch step: 231, loss: 22.11408805847168\n",
      "epoch: 8,  batch step: 232, loss: 22.458736419677734\n",
      "epoch: 8,  batch step: 233, loss: 18.47311019897461\n",
      "epoch: 8,  batch step: 234, loss: 20.993106842041016\n",
      "epoch: 8,  batch step: 235, loss: 21.709823608398438\n",
      "epoch: 8,  batch step: 236, loss: 70.53705596923828\n",
      "epoch: 8,  batch step: 237, loss: 26.81941795349121\n",
      "epoch: 8,  batch step: 238, loss: 29.52806854248047\n",
      "epoch: 8,  batch step: 239, loss: 70.41703796386719\n",
      "epoch: 8,  batch step: 240, loss: 17.368648529052734\n",
      "epoch: 8,  batch step: 241, loss: 9.932836532592773\n",
      "epoch: 8,  batch step: 242, loss: 7.405351161956787\n",
      "epoch: 8,  batch step: 243, loss: 25.669235229492188\n",
      "epoch: 8,  batch step: 244, loss: 13.495468139648438\n",
      "epoch: 8,  batch step: 245, loss: 137.46588134765625\n",
      "epoch: 8,  batch step: 246, loss: 312.6160888671875\n",
      "epoch: 8,  batch step: 247, loss: 66.58381652832031\n",
      "epoch: 8,  batch step: 248, loss: 46.5517692565918\n",
      "epoch: 8,  batch step: 249, loss: 63.49774169921875\n",
      "epoch: 8,  batch step: 250, loss: 77.99971771240234\n",
      "epoch: 8,  batch step: 251, loss: 93.27674102783203\n",
      "validation error epoch  8:    tensor(80.4851, device='cuda:0')\n",
      "316\n",
      "epoch: 9,  batch step: 0, loss: 21.64486312866211\n",
      "epoch: 9,  batch step: 1, loss: 12.933338165283203\n",
      "epoch: 9,  batch step: 2, loss: 52.963706970214844\n",
      "epoch: 9,  batch step: 3, loss: 176.86819458007812\n",
      "epoch: 9,  batch step: 4, loss: 188.412841796875\n",
      "epoch: 9,  batch step: 5, loss: 128.88192749023438\n",
      "epoch: 9,  batch step: 6, loss: 21.718799591064453\n",
      "epoch: 9,  batch step: 7, loss: 34.48186492919922\n",
      "epoch: 9,  batch step: 8, loss: 26.089000701904297\n",
      "epoch: 9,  batch step: 9, loss: 25.340438842773438\n",
      "epoch: 9,  batch step: 10, loss: 85.62232971191406\n",
      "epoch: 9,  batch step: 11, loss: 174.479736328125\n",
      "epoch: 9,  batch step: 12, loss: 24.101896286010742\n",
      "epoch: 9,  batch step: 13, loss: 16.802326202392578\n",
      "epoch: 9,  batch step: 14, loss: 24.287473678588867\n",
      "epoch: 9,  batch step: 15, loss: 15.946496963500977\n",
      "epoch: 9,  batch step: 16, loss: 53.46476745605469\n",
      "epoch: 9,  batch step: 17, loss: 13.006780624389648\n",
      "epoch: 9,  batch step: 18, loss: 22.466812133789062\n",
      "epoch: 9,  batch step: 19, loss: 40.849666595458984\n",
      "epoch: 9,  batch step: 20, loss: 18.108367919921875\n",
      "epoch: 9,  batch step: 21, loss: 35.37352752685547\n",
      "epoch: 9,  batch step: 22, loss: 38.02659225463867\n",
      "epoch: 9,  batch step: 23, loss: 12.850428581237793\n",
      "epoch: 9,  batch step: 24, loss: 208.7390594482422\n",
      "epoch: 9,  batch step: 25, loss: 36.5246696472168\n",
      "epoch: 9,  batch step: 26, loss: 31.296371459960938\n",
      "epoch: 9,  batch step: 27, loss: 10.502031326293945\n",
      "epoch: 9,  batch step: 28, loss: 145.4352264404297\n",
      "epoch: 9,  batch step: 29, loss: 72.09053039550781\n",
      "epoch: 9,  batch step: 30, loss: 13.18960189819336\n",
      "epoch: 9,  batch step: 31, loss: 15.146906852722168\n",
      "epoch: 9,  batch step: 32, loss: 54.361541748046875\n",
      "epoch: 9,  batch step: 33, loss: 14.956016540527344\n",
      "epoch: 9,  batch step: 34, loss: 27.28485870361328\n",
      "epoch: 9,  batch step: 35, loss: 136.25074768066406\n",
      "epoch: 9,  batch step: 36, loss: 19.916690826416016\n",
      "epoch: 9,  batch step: 37, loss: 57.83977508544922\n",
      "epoch: 9,  batch step: 38, loss: 38.58905792236328\n",
      "epoch: 9,  batch step: 39, loss: 12.745584487915039\n",
      "epoch: 9,  batch step: 40, loss: 218.78256225585938\n",
      "epoch: 9,  batch step: 41, loss: 7.044524669647217\n",
      "epoch: 9,  batch step: 42, loss: 9.655553817749023\n",
      "epoch: 9,  batch step: 43, loss: 11.149032592773438\n",
      "epoch: 9,  batch step: 44, loss: 10.704639434814453\n",
      "epoch: 9,  batch step: 45, loss: 57.32870101928711\n",
      "epoch: 9,  batch step: 46, loss: 161.82876586914062\n",
      "epoch: 9,  batch step: 47, loss: 177.34957885742188\n",
      "epoch: 9,  batch step: 48, loss: 185.11448669433594\n",
      "epoch: 9,  batch step: 49, loss: 15.612775802612305\n",
      "epoch: 9,  batch step: 50, loss: 143.8993682861328\n",
      "epoch: 9,  batch step: 51, loss: 74.67222595214844\n",
      "epoch: 9,  batch step: 52, loss: 13.052026748657227\n",
      "epoch: 9,  batch step: 53, loss: 277.32025146484375\n",
      "epoch: 9,  batch step: 54, loss: 29.064132690429688\n",
      "epoch: 9,  batch step: 55, loss: 55.72736740112305\n",
      "epoch: 9,  batch step: 56, loss: 37.677703857421875\n",
      "epoch: 9,  batch step: 57, loss: 68.37934875488281\n",
      "epoch: 9,  batch step: 58, loss: 33.09708786010742\n",
      "epoch: 9,  batch step: 59, loss: 27.616683959960938\n",
      "epoch: 9,  batch step: 60, loss: 210.864501953125\n",
      "epoch: 9,  batch step: 61, loss: 15.395716667175293\n",
      "epoch: 9,  batch step: 62, loss: 116.65721130371094\n",
      "epoch: 9,  batch step: 63, loss: 14.89306926727295\n",
      "epoch: 9,  batch step: 64, loss: 19.64139175415039\n",
      "epoch: 9,  batch step: 65, loss: 27.05815887451172\n",
      "epoch: 9,  batch step: 66, loss: 29.43927574157715\n",
      "epoch: 9,  batch step: 67, loss: 22.404232025146484\n",
      "epoch: 9,  batch step: 68, loss: 26.699331283569336\n",
      "epoch: 9,  batch step: 69, loss: 169.63816833496094\n",
      "epoch: 9,  batch step: 70, loss: 184.97268676757812\n",
      "epoch: 9,  batch step: 71, loss: 10.362510681152344\n",
      "epoch: 9,  batch step: 72, loss: 81.92446899414062\n",
      "epoch: 9,  batch step: 73, loss: 248.3280487060547\n",
      "epoch: 9,  batch step: 74, loss: 162.21771240234375\n",
      "epoch: 9,  batch step: 75, loss: 163.99606323242188\n",
      "epoch: 9,  batch step: 76, loss: 24.4653377532959\n",
      "epoch: 9,  batch step: 77, loss: 106.34007263183594\n",
      "epoch: 9,  batch step: 78, loss: 16.658206939697266\n",
      "epoch: 9,  batch step: 79, loss: 99.5350570678711\n",
      "epoch: 9,  batch step: 80, loss: 20.226882934570312\n",
      "epoch: 9,  batch step: 81, loss: 14.6258544921875\n",
      "epoch: 9,  batch step: 82, loss: 21.32510757446289\n",
      "epoch: 9,  batch step: 83, loss: 303.7869567871094\n",
      "epoch: 9,  batch step: 84, loss: 136.144287109375\n",
      "epoch: 9,  batch step: 85, loss: 10.116947174072266\n",
      "epoch: 9,  batch step: 86, loss: 12.403399467468262\n",
      "epoch: 9,  batch step: 87, loss: 26.38014793395996\n",
      "epoch: 9,  batch step: 88, loss: 16.033981323242188\n",
      "epoch: 9,  batch step: 89, loss: 38.75698471069336\n",
      "epoch: 9,  batch step: 90, loss: 16.63941192626953\n",
      "epoch: 9,  batch step: 91, loss: 32.35990524291992\n",
      "epoch: 9,  batch step: 92, loss: 11.590381622314453\n",
      "epoch: 9,  batch step: 93, loss: 15.236976623535156\n",
      "epoch: 9,  batch step: 94, loss: 189.86117553710938\n",
      "epoch: 9,  batch step: 95, loss: 13.140264511108398\n",
      "epoch: 9,  batch step: 96, loss: 29.670150756835938\n",
      "epoch: 9,  batch step: 97, loss: 78.1378173828125\n",
      "epoch: 9,  batch step: 98, loss: 16.49224853515625\n",
      "epoch: 9,  batch step: 99, loss: 13.598974227905273\n",
      "epoch: 9,  batch step: 100, loss: 7.8147687911987305\n",
      "epoch: 9,  batch step: 101, loss: 11.184802055358887\n",
      "epoch: 9,  batch step: 102, loss: 35.42780685424805\n",
      "epoch: 9,  batch step: 103, loss: 10.855582237243652\n",
      "epoch: 9,  batch step: 104, loss: 15.009408950805664\n",
      "epoch: 9,  batch step: 105, loss: 9.070510864257812\n",
      "epoch: 9,  batch step: 106, loss: 16.589527130126953\n",
      "epoch: 9,  batch step: 107, loss: 14.64091682434082\n",
      "epoch: 9,  batch step: 108, loss: 11.092940330505371\n",
      "epoch: 9,  batch step: 109, loss: 22.952838897705078\n",
      "epoch: 9,  batch step: 110, loss: 37.30586624145508\n",
      "epoch: 9,  batch step: 111, loss: 8.724569320678711\n",
      "epoch: 9,  batch step: 112, loss: 15.732295036315918\n",
      "epoch: 9,  batch step: 113, loss: 44.94034957885742\n",
      "epoch: 9,  batch step: 114, loss: 10.095382690429688\n",
      "epoch: 9,  batch step: 115, loss: 132.9620361328125\n",
      "epoch: 9,  batch step: 116, loss: 14.198439598083496\n",
      "epoch: 9,  batch step: 117, loss: 24.555355072021484\n",
      "epoch: 9,  batch step: 118, loss: 71.94789123535156\n",
      "epoch: 9,  batch step: 119, loss: 20.430706024169922\n",
      "epoch: 9,  batch step: 120, loss: 21.538597106933594\n",
      "epoch: 9,  batch step: 121, loss: 81.51153564453125\n",
      "epoch: 9,  batch step: 122, loss: 141.5836639404297\n",
      "epoch: 9,  batch step: 123, loss: 233.60037231445312\n",
      "epoch: 9,  batch step: 124, loss: 192.31027221679688\n",
      "epoch: 9,  batch step: 125, loss: 20.67006492614746\n",
      "epoch: 9,  batch step: 126, loss: 89.2283935546875\n",
      "epoch: 9,  batch step: 127, loss: 23.56116485595703\n",
      "epoch: 9,  batch step: 128, loss: 13.686564445495605\n",
      "epoch: 9,  batch step: 129, loss: 20.98230743408203\n",
      "epoch: 9,  batch step: 130, loss: 51.87371826171875\n",
      "epoch: 9,  batch step: 131, loss: 29.659536361694336\n",
      "epoch: 9,  batch step: 132, loss: 201.68557739257812\n",
      "epoch: 9,  batch step: 133, loss: 17.391616821289062\n",
      "epoch: 9,  batch step: 134, loss: 167.70260620117188\n",
      "epoch: 9,  batch step: 135, loss: 26.846416473388672\n",
      "epoch: 9,  batch step: 136, loss: 27.315139770507812\n",
      "epoch: 9,  batch step: 137, loss: 15.76468276977539\n",
      "epoch: 9,  batch step: 138, loss: 81.64305114746094\n",
      "epoch: 9,  batch step: 139, loss: 8.702449798583984\n",
      "epoch: 9,  batch step: 140, loss: 129.6318359375\n",
      "epoch: 9,  batch step: 141, loss: 309.734130859375\n",
      "epoch: 9,  batch step: 142, loss: 133.6339569091797\n",
      "epoch: 9,  batch step: 143, loss: 16.854841232299805\n",
      "epoch: 9,  batch step: 144, loss: 35.03662109375\n",
      "epoch: 9,  batch step: 145, loss: 85.24714660644531\n",
      "epoch: 9,  batch step: 146, loss: 20.53213119506836\n",
      "epoch: 9,  batch step: 147, loss: 19.585721969604492\n",
      "epoch: 9,  batch step: 148, loss: 35.36248779296875\n",
      "epoch: 9,  batch step: 149, loss: 8.886038780212402\n",
      "epoch: 9,  batch step: 150, loss: 223.94664001464844\n",
      "epoch: 9,  batch step: 151, loss: 117.79112243652344\n",
      "epoch: 9,  batch step: 152, loss: 116.22207641601562\n",
      "epoch: 9,  batch step: 153, loss: 17.005939483642578\n",
      "epoch: 9,  batch step: 154, loss: 22.22269058227539\n",
      "epoch: 9,  batch step: 155, loss: 7.020269393920898\n",
      "epoch: 9,  batch step: 156, loss: 79.91156005859375\n",
      "epoch: 9,  batch step: 157, loss: 32.79336166381836\n",
      "epoch: 9,  batch step: 158, loss: 250.67666625976562\n",
      "epoch: 9,  batch step: 159, loss: 20.261463165283203\n",
      "epoch: 9,  batch step: 160, loss: 37.69943618774414\n",
      "epoch: 9,  batch step: 161, loss: 325.65875244140625\n",
      "epoch: 9,  batch step: 162, loss: 54.53754425048828\n",
      "epoch: 9,  batch step: 163, loss: 8.596323013305664\n",
      "epoch: 9,  batch step: 164, loss: 156.8936767578125\n",
      "epoch: 9,  batch step: 165, loss: 264.8824157714844\n",
      "epoch: 9,  batch step: 166, loss: 20.92877197265625\n",
      "epoch: 9,  batch step: 167, loss: 28.441051483154297\n",
      "epoch: 9,  batch step: 168, loss: 51.57623291015625\n",
      "epoch: 9,  batch step: 169, loss: 56.531795501708984\n",
      "epoch: 9,  batch step: 170, loss: 93.51045989990234\n",
      "epoch: 9,  batch step: 171, loss: 13.334543228149414\n",
      "epoch: 9,  batch step: 172, loss: 52.311668395996094\n",
      "epoch: 9,  batch step: 173, loss: 327.88299560546875\n",
      "epoch: 9,  batch step: 174, loss: 25.226848602294922\n",
      "epoch: 9,  batch step: 175, loss: 16.482168197631836\n",
      "epoch: 9,  batch step: 176, loss: 14.26321029663086\n",
      "epoch: 9,  batch step: 177, loss: 18.966575622558594\n",
      "epoch: 9,  batch step: 178, loss: 13.618864059448242\n",
      "epoch: 9,  batch step: 179, loss: 21.732486724853516\n",
      "epoch: 9,  batch step: 180, loss: 20.777542114257812\n",
      "epoch: 9,  batch step: 181, loss: 321.8249206542969\n",
      "epoch: 9,  batch step: 182, loss: 35.655975341796875\n",
      "epoch: 9,  batch step: 183, loss: 11.918130874633789\n",
      "epoch: 9,  batch step: 184, loss: 211.13980102539062\n",
      "epoch: 9,  batch step: 185, loss: 21.881332397460938\n",
      "epoch: 9,  batch step: 186, loss: 59.74715805053711\n",
      "epoch: 9,  batch step: 187, loss: 12.995837211608887\n",
      "epoch: 9,  batch step: 188, loss: 254.54287719726562\n",
      "epoch: 9,  batch step: 189, loss: 14.927447319030762\n",
      "epoch: 9,  batch step: 190, loss: 115.16905975341797\n",
      "epoch: 9,  batch step: 191, loss: 20.86760711669922\n",
      "epoch: 9,  batch step: 192, loss: 15.35822868347168\n",
      "epoch: 9,  batch step: 193, loss: 30.80974578857422\n",
      "epoch: 9,  batch step: 194, loss: 115.5963363647461\n",
      "epoch: 9,  batch step: 195, loss: 118.81411743164062\n",
      "epoch: 9,  batch step: 196, loss: 367.0306091308594\n",
      "epoch: 9,  batch step: 197, loss: 14.099760055541992\n",
      "epoch: 9,  batch step: 198, loss: 15.045866012573242\n",
      "epoch: 9,  batch step: 199, loss: 9.652128219604492\n",
      "epoch: 9,  batch step: 200, loss: 29.189239501953125\n",
      "epoch: 9,  batch step: 201, loss: 53.365013122558594\n",
      "epoch: 9,  batch step: 202, loss: 19.513731002807617\n",
      "epoch: 9,  batch step: 203, loss: 13.87122631072998\n",
      "epoch: 9,  batch step: 204, loss: 138.98138427734375\n",
      "epoch: 9,  batch step: 205, loss: 10.662073135375977\n",
      "epoch: 9,  batch step: 206, loss: 15.074127197265625\n",
      "epoch: 9,  batch step: 207, loss: 32.82185363769531\n",
      "epoch: 9,  batch step: 208, loss: 15.063185691833496\n",
      "epoch: 9,  batch step: 209, loss: 8.211431503295898\n",
      "epoch: 9,  batch step: 210, loss: 16.34577178955078\n",
      "epoch: 9,  batch step: 211, loss: 26.345008850097656\n",
      "epoch: 9,  batch step: 212, loss: 14.93191146850586\n",
      "epoch: 9,  batch step: 213, loss: 34.59248733520508\n",
      "epoch: 9,  batch step: 214, loss: 153.1943359375\n",
      "epoch: 9,  batch step: 215, loss: 5.840266227722168\n",
      "epoch: 9,  batch step: 216, loss: 13.536443710327148\n",
      "epoch: 9,  batch step: 217, loss: 207.8984375\n",
      "epoch: 9,  batch step: 218, loss: 8.044658660888672\n",
      "epoch: 9,  batch step: 219, loss: 233.82797241210938\n",
      "epoch: 9,  batch step: 220, loss: 66.78370666503906\n",
      "epoch: 9,  batch step: 221, loss: 19.428558349609375\n",
      "epoch: 9,  batch step: 222, loss: 316.8619384765625\n",
      "epoch: 9,  batch step: 223, loss: 143.82745361328125\n",
      "epoch: 9,  batch step: 224, loss: 16.025583267211914\n",
      "epoch: 9,  batch step: 225, loss: 119.65921020507812\n",
      "epoch: 9,  batch step: 226, loss: 38.31432342529297\n",
      "epoch: 9,  batch step: 227, loss: 302.88763427734375\n",
      "epoch: 9,  batch step: 228, loss: 57.24137496948242\n",
      "epoch: 9,  batch step: 229, loss: 72.46116638183594\n",
      "epoch: 9,  batch step: 230, loss: 64.90164947509766\n",
      "epoch: 9,  batch step: 231, loss: 152.99624633789062\n",
      "epoch: 9,  batch step: 232, loss: 18.940204620361328\n",
      "epoch: 9,  batch step: 233, loss: 170.81234741210938\n",
      "epoch: 9,  batch step: 234, loss: 54.833099365234375\n",
      "epoch: 9,  batch step: 235, loss: 447.1270751953125\n",
      "epoch: 9,  batch step: 236, loss: 157.16111755371094\n",
      "epoch: 9,  batch step: 237, loss: 59.9542350769043\n",
      "epoch: 9,  batch step: 238, loss: 77.46420288085938\n",
      "epoch: 9,  batch step: 239, loss: 42.93185043334961\n",
      "epoch: 9,  batch step: 240, loss: 226.06497192382812\n",
      "epoch: 9,  batch step: 241, loss: 90.73567962646484\n",
      "epoch: 9,  batch step: 242, loss: 164.14947509765625\n",
      "epoch: 9,  batch step: 243, loss: 25.631916046142578\n",
      "epoch: 9,  batch step: 244, loss: 44.794464111328125\n",
      "epoch: 9,  batch step: 245, loss: 37.482269287109375\n",
      "epoch: 9,  batch step: 246, loss: 16.41864776611328\n",
      "epoch: 9,  batch step: 247, loss: 34.14186477661133\n",
      "epoch: 9,  batch step: 248, loss: 42.1598014831543\n",
      "epoch: 9,  batch step: 249, loss: 112.9371337890625\n",
      "epoch: 9,  batch step: 250, loss: 74.1169662475586\n",
      "epoch: 9,  batch step: 251, loss: 233.96778869628906\n",
      "finished saving checkpoints\n",
      "validation error epoch  9:    tensor(75.8260, device='cuda:0')\n",
      "316\n",
      "epoch: 10,  batch step: 0, loss: 149.77676391601562\n",
      "epoch: 10,  batch step: 1, loss: 26.068565368652344\n",
      "epoch: 10,  batch step: 2, loss: 224.6395721435547\n",
      "epoch: 10,  batch step: 3, loss: 13.623923301696777\n",
      "epoch: 10,  batch step: 4, loss: 54.67214584350586\n",
      "epoch: 10,  batch step: 5, loss: 101.66207885742188\n",
      "epoch: 10,  batch step: 6, loss: 177.87364196777344\n",
      "epoch: 10,  batch step: 7, loss: 87.78063201904297\n",
      "epoch: 10,  batch step: 8, loss: 14.547922134399414\n",
      "epoch: 10,  batch step: 9, loss: 219.09457397460938\n",
      "epoch: 10,  batch step: 10, loss: 110.51439666748047\n",
      "epoch: 10,  batch step: 11, loss: 41.66189193725586\n",
      "epoch: 10,  batch step: 12, loss: 131.19027709960938\n",
      "epoch: 10,  batch step: 13, loss: 18.969505310058594\n",
      "epoch: 10,  batch step: 14, loss: 41.90576934814453\n",
      "epoch: 10,  batch step: 15, loss: 39.10197067260742\n",
      "epoch: 10,  batch step: 16, loss: 119.39995574951172\n",
      "epoch: 10,  batch step: 17, loss: 27.64333152770996\n",
      "epoch: 10,  batch step: 18, loss: 319.5091857910156\n",
      "epoch: 10,  batch step: 19, loss: 63.42113494873047\n",
      "epoch: 10,  batch step: 20, loss: 235.0192108154297\n",
      "epoch: 10,  batch step: 21, loss: 271.2956237792969\n",
      "epoch: 10,  batch step: 22, loss: 140.1309356689453\n",
      "epoch: 10,  batch step: 23, loss: 189.13815307617188\n",
      "epoch: 10,  batch step: 24, loss: 44.53539276123047\n",
      "epoch: 10,  batch step: 25, loss: 28.16210174560547\n",
      "epoch: 10,  batch step: 26, loss: 75.94158935546875\n",
      "epoch: 10,  batch step: 27, loss: 274.116943359375\n",
      "epoch: 10,  batch step: 28, loss: 52.13991928100586\n",
      "epoch: 10,  batch step: 29, loss: 113.32843017578125\n",
      "epoch: 10,  batch step: 30, loss: 38.099056243896484\n",
      "epoch: 10,  batch step: 31, loss: 64.55435943603516\n",
      "epoch: 10,  batch step: 32, loss: 74.44522094726562\n",
      "epoch: 10,  batch step: 33, loss: 24.129247665405273\n",
      "epoch: 10,  batch step: 34, loss: 24.314922332763672\n",
      "epoch: 10,  batch step: 35, loss: 75.13897705078125\n",
      "epoch: 10,  batch step: 36, loss: 27.61422348022461\n",
      "epoch: 10,  batch step: 37, loss: 77.0320053100586\n",
      "epoch: 10,  batch step: 38, loss: 16.445850372314453\n",
      "epoch: 10,  batch step: 39, loss: 10.172636985778809\n",
      "epoch: 10,  batch step: 40, loss: 102.51675415039062\n",
      "epoch: 10,  batch step: 41, loss: 41.05087661743164\n",
      "epoch: 10,  batch step: 42, loss: 49.83257293701172\n",
      "epoch: 10,  batch step: 43, loss: 33.94042205810547\n",
      "epoch: 10,  batch step: 44, loss: 183.9508514404297\n",
      "epoch: 10,  batch step: 45, loss: 14.400199890136719\n",
      "epoch: 10,  batch step: 46, loss: 15.113604545593262\n",
      "epoch: 10,  batch step: 47, loss: 9.629659652709961\n",
      "epoch: 10,  batch step: 48, loss: 10.631183624267578\n",
      "epoch: 10,  batch step: 49, loss: 12.185176849365234\n",
      "epoch: 10,  batch step: 50, loss: 74.32341003417969\n",
      "epoch: 10,  batch step: 51, loss: 44.082725524902344\n",
      "epoch: 10,  batch step: 52, loss: 60.18541717529297\n",
      "epoch: 10,  batch step: 53, loss: 11.865079879760742\n",
      "epoch: 10,  batch step: 54, loss: 15.650457382202148\n",
      "epoch: 10,  batch step: 55, loss: 6.884654998779297\n",
      "epoch: 10,  batch step: 56, loss: 20.076290130615234\n",
      "epoch: 10,  batch step: 57, loss: 172.67893981933594\n",
      "epoch: 10,  batch step: 58, loss: 12.516407012939453\n",
      "epoch: 10,  batch step: 59, loss: 7.682649612426758\n",
      "epoch: 10,  batch step: 60, loss: 7.75792121887207\n",
      "epoch: 10,  batch step: 61, loss: 271.2880554199219\n",
      "epoch: 10,  batch step: 62, loss: 122.60690307617188\n",
      "epoch: 10,  batch step: 63, loss: 14.355316162109375\n",
      "epoch: 10,  batch step: 64, loss: 21.809972763061523\n",
      "epoch: 10,  batch step: 65, loss: 141.41177368164062\n",
      "epoch: 10,  batch step: 66, loss: 20.807422637939453\n",
      "epoch: 10,  batch step: 67, loss: 38.4268798828125\n",
      "epoch: 10,  batch step: 68, loss: 253.09033203125\n",
      "epoch: 10,  batch step: 69, loss: 140.74276733398438\n",
      "epoch: 10,  batch step: 70, loss: 248.65684509277344\n",
      "epoch: 10,  batch step: 71, loss: 24.645076751708984\n",
      "epoch: 10,  batch step: 72, loss: 22.7522029876709\n",
      "epoch: 10,  batch step: 73, loss: 11.506522178649902\n",
      "epoch: 10,  batch step: 74, loss: 12.344503402709961\n",
      "epoch: 10,  batch step: 75, loss: 31.665084838867188\n",
      "epoch: 10,  batch step: 76, loss: 20.303199768066406\n",
      "epoch: 10,  batch step: 77, loss: 240.35470581054688\n",
      "epoch: 10,  batch step: 78, loss: 39.43638229370117\n",
      "epoch: 10,  batch step: 79, loss: 128.69775390625\n",
      "epoch: 10,  batch step: 80, loss: 11.453338623046875\n",
      "epoch: 10,  batch step: 81, loss: 15.504600524902344\n",
      "epoch: 10,  batch step: 82, loss: 12.807031631469727\n",
      "epoch: 10,  batch step: 83, loss: 28.92919158935547\n",
      "epoch: 10,  batch step: 84, loss: 12.872485160827637\n",
      "epoch: 10,  batch step: 85, loss: 20.363372802734375\n",
      "epoch: 10,  batch step: 86, loss: 16.57588005065918\n",
      "epoch: 10,  batch step: 87, loss: 76.52127838134766\n",
      "epoch: 10,  batch step: 88, loss: 31.095027923583984\n",
      "epoch: 10,  batch step: 89, loss: 148.5617218017578\n",
      "epoch: 10,  batch step: 90, loss: 18.73792839050293\n",
      "epoch: 10,  batch step: 91, loss: 13.885603904724121\n",
      "epoch: 10,  batch step: 92, loss: 61.04548263549805\n",
      "epoch: 10,  batch step: 93, loss: 20.004772186279297\n",
      "epoch: 10,  batch step: 94, loss: 99.04961395263672\n",
      "epoch: 10,  batch step: 95, loss: 76.797119140625\n",
      "epoch: 10,  batch step: 96, loss: 31.712453842163086\n",
      "epoch: 10,  batch step: 97, loss: 12.8425931930542\n",
      "epoch: 10,  batch step: 98, loss: 142.492919921875\n",
      "epoch: 10,  batch step: 99, loss: 11.39719009399414\n",
      "epoch: 10,  batch step: 100, loss: 37.34361267089844\n",
      "epoch: 10,  batch step: 101, loss: 59.55685043334961\n",
      "epoch: 10,  batch step: 102, loss: 15.383658409118652\n",
      "epoch: 10,  batch step: 103, loss: 19.131444931030273\n",
      "epoch: 10,  batch step: 104, loss: 191.79893493652344\n",
      "epoch: 10,  batch step: 105, loss: 25.325870513916016\n",
      "epoch: 10,  batch step: 106, loss: 16.22488021850586\n",
      "epoch: 10,  batch step: 107, loss: 318.6988220214844\n",
      "epoch: 10,  batch step: 108, loss: 36.20106506347656\n",
      "epoch: 10,  batch step: 109, loss: 11.977340698242188\n",
      "epoch: 10,  batch step: 110, loss: 12.306966781616211\n",
      "epoch: 10,  batch step: 111, loss: 17.68862533569336\n",
      "epoch: 10,  batch step: 112, loss: 9.631280899047852\n",
      "epoch: 10,  batch step: 113, loss: 5.984583854675293\n",
      "epoch: 10,  batch step: 114, loss: 147.7607421875\n",
      "epoch: 10,  batch step: 115, loss: 19.66773796081543\n",
      "epoch: 10,  batch step: 116, loss: 32.27155303955078\n",
      "epoch: 10,  batch step: 117, loss: 11.782295227050781\n",
      "epoch: 10,  batch step: 118, loss: 11.355810165405273\n",
      "epoch: 10,  batch step: 119, loss: 170.0824432373047\n",
      "epoch: 10,  batch step: 120, loss: 10.36977767944336\n",
      "epoch: 10,  batch step: 121, loss: 13.719759941101074\n",
      "epoch: 10,  batch step: 122, loss: 42.055850982666016\n",
      "epoch: 10,  batch step: 123, loss: 15.378523826599121\n",
      "epoch: 10,  batch step: 124, loss: 13.51880168914795\n",
      "epoch: 10,  batch step: 125, loss: 55.18316650390625\n",
      "epoch: 10,  batch step: 126, loss: 18.70850372314453\n",
      "epoch: 10,  batch step: 127, loss: 195.7863006591797\n",
      "epoch: 10,  batch step: 128, loss: 264.1824951171875\n",
      "epoch: 10,  batch step: 129, loss: 17.233121871948242\n",
      "epoch: 10,  batch step: 130, loss: 137.4904327392578\n",
      "epoch: 10,  batch step: 131, loss: 17.729124069213867\n",
      "epoch: 10,  batch step: 132, loss: 34.40022277832031\n",
      "epoch: 10,  batch step: 133, loss: 29.016448974609375\n",
      "epoch: 10,  batch step: 134, loss: 158.13739013671875\n",
      "epoch: 10,  batch step: 135, loss: 21.969100952148438\n",
      "epoch: 10,  batch step: 136, loss: 8.980262756347656\n",
      "epoch: 10,  batch step: 137, loss: 8.643694877624512\n",
      "epoch: 10,  batch step: 138, loss: 13.288414001464844\n",
      "epoch: 10,  batch step: 139, loss: 9.093624114990234\n",
      "epoch: 10,  batch step: 140, loss: 59.0080680847168\n",
      "epoch: 10,  batch step: 141, loss: 63.01073455810547\n",
      "epoch: 10,  batch step: 142, loss: 147.92318725585938\n",
      "epoch: 10,  batch step: 143, loss: 11.248404502868652\n",
      "epoch: 10,  batch step: 144, loss: 30.32942771911621\n",
      "epoch: 10,  batch step: 145, loss: 68.8367919921875\n",
      "epoch: 10,  batch step: 146, loss: 9.777592658996582\n",
      "epoch: 10,  batch step: 147, loss: 141.61740112304688\n",
      "epoch: 10,  batch step: 148, loss: 240.53826904296875\n",
      "epoch: 10,  batch step: 149, loss: 263.61053466796875\n",
      "epoch: 10,  batch step: 150, loss: 84.139404296875\n",
      "epoch: 10,  batch step: 151, loss: 597.2047729492188\n",
      "epoch: 10,  batch step: 152, loss: 69.69560241699219\n",
      "epoch: 10,  batch step: 153, loss: 20.286712646484375\n",
      "epoch: 10,  batch step: 154, loss: 44.18397521972656\n",
      "epoch: 10,  batch step: 155, loss: 26.108922958374023\n",
      "epoch: 10,  batch step: 156, loss: 22.892532348632812\n",
      "epoch: 10,  batch step: 157, loss: 25.077129364013672\n",
      "epoch: 10,  batch step: 158, loss: 27.933195114135742\n",
      "epoch: 10,  batch step: 159, loss: 33.73918914794922\n",
      "epoch: 10,  batch step: 160, loss: 152.22647094726562\n",
      "epoch: 10,  batch step: 161, loss: 30.40985679626465\n",
      "epoch: 10,  batch step: 162, loss: 35.92375183105469\n",
      "epoch: 10,  batch step: 163, loss: 13.202963829040527\n",
      "epoch: 10,  batch step: 164, loss: 30.672042846679688\n",
      "epoch: 10,  batch step: 165, loss: 71.27244567871094\n",
      "epoch: 10,  batch step: 166, loss: 18.81095314025879\n",
      "epoch: 10,  batch step: 167, loss: 98.0914535522461\n",
      "epoch: 10,  batch step: 168, loss: 66.00587463378906\n",
      "epoch: 10,  batch step: 169, loss: 241.61509704589844\n",
      "epoch: 10,  batch step: 170, loss: 23.24481201171875\n",
      "epoch: 10,  batch step: 171, loss: 222.0299530029297\n",
      "epoch: 10,  batch step: 172, loss: 14.016464233398438\n",
      "epoch: 10,  batch step: 173, loss: 15.027480125427246\n",
      "epoch: 10,  batch step: 174, loss: 30.263166427612305\n",
      "epoch: 10,  batch step: 175, loss: 95.27328491210938\n",
      "epoch: 10,  batch step: 176, loss: 115.24492645263672\n",
      "epoch: 10,  batch step: 177, loss: 12.511286735534668\n",
      "epoch: 10,  batch step: 178, loss: 200.45542907714844\n",
      "epoch: 10,  batch step: 179, loss: 66.42716979980469\n",
      "epoch: 10,  batch step: 180, loss: 58.32954025268555\n",
      "epoch: 10,  batch step: 181, loss: 293.76861572265625\n",
      "epoch: 10,  batch step: 182, loss: 16.693246841430664\n",
      "epoch: 10,  batch step: 183, loss: 41.98298645019531\n",
      "epoch: 10,  batch step: 184, loss: 11.271785736083984\n",
      "epoch: 10,  batch step: 185, loss: 22.58680534362793\n",
      "epoch: 10,  batch step: 186, loss: 90.9840087890625\n",
      "epoch: 10,  batch step: 187, loss: 32.60092544555664\n",
      "epoch: 10,  batch step: 188, loss: 15.938944816589355\n",
      "epoch: 10,  batch step: 189, loss: 44.93096923828125\n",
      "epoch: 10,  batch step: 190, loss: 126.0846176147461\n",
      "epoch: 10,  batch step: 191, loss: 102.19457244873047\n",
      "epoch: 10,  batch step: 192, loss: 43.65355682373047\n",
      "epoch: 10,  batch step: 193, loss: 40.70993423461914\n",
      "epoch: 10,  batch step: 194, loss: 23.374197006225586\n",
      "epoch: 10,  batch step: 195, loss: 185.50991821289062\n",
      "epoch: 10,  batch step: 196, loss: 21.057411193847656\n",
      "epoch: 10,  batch step: 197, loss: 276.0308532714844\n",
      "epoch: 10,  batch step: 198, loss: 15.37238597869873\n",
      "epoch: 10,  batch step: 199, loss: 22.770322799682617\n",
      "epoch: 10,  batch step: 200, loss: 27.81485366821289\n",
      "epoch: 10,  batch step: 201, loss: 24.17786979675293\n",
      "epoch: 10,  batch step: 202, loss: 125.16280364990234\n",
      "epoch: 10,  batch step: 203, loss: 8.134326934814453\n",
      "epoch: 10,  batch step: 204, loss: 15.163439750671387\n",
      "epoch: 10,  batch step: 205, loss: 159.66171264648438\n",
      "epoch: 10,  batch step: 206, loss: 144.21324157714844\n",
      "epoch: 10,  batch step: 207, loss: 297.9676513671875\n",
      "epoch: 10,  batch step: 208, loss: 43.19519805908203\n",
      "epoch: 10,  batch step: 209, loss: 31.025604248046875\n",
      "epoch: 10,  batch step: 210, loss: 12.353567123413086\n",
      "epoch: 10,  batch step: 211, loss: 17.123384475708008\n",
      "epoch: 10,  batch step: 212, loss: 24.396528244018555\n",
      "epoch: 10,  batch step: 213, loss: 57.247215270996094\n",
      "epoch: 10,  batch step: 214, loss: 10.749507904052734\n",
      "epoch: 10,  batch step: 215, loss: 15.074762344360352\n",
      "epoch: 10,  batch step: 216, loss: 20.7596435546875\n",
      "epoch: 10,  batch step: 217, loss: 16.214309692382812\n",
      "epoch: 10,  batch step: 218, loss: 64.35222625732422\n",
      "epoch: 10,  batch step: 219, loss: 64.87124633789062\n",
      "epoch: 10,  batch step: 220, loss: 16.347076416015625\n",
      "epoch: 10,  batch step: 221, loss: 187.19219970703125\n",
      "epoch: 10,  batch step: 222, loss: 6.702983379364014\n",
      "epoch: 10,  batch step: 223, loss: 32.874755859375\n",
      "epoch: 10,  batch step: 224, loss: 13.261737823486328\n",
      "epoch: 10,  batch step: 225, loss: 208.83123779296875\n",
      "epoch: 10,  batch step: 226, loss: 75.88555908203125\n",
      "epoch: 10,  batch step: 227, loss: 149.0133819580078\n",
      "epoch: 10,  batch step: 228, loss: 51.8670768737793\n",
      "epoch: 10,  batch step: 229, loss: 16.90601921081543\n",
      "epoch: 10,  batch step: 230, loss: 37.300323486328125\n",
      "epoch: 10,  batch step: 231, loss: 139.1677703857422\n",
      "epoch: 10,  batch step: 232, loss: 119.19801330566406\n",
      "epoch: 10,  batch step: 233, loss: 21.938220977783203\n",
      "epoch: 10,  batch step: 234, loss: 23.53290557861328\n",
      "epoch: 10,  batch step: 235, loss: 17.58749771118164\n",
      "epoch: 10,  batch step: 236, loss: 21.247554779052734\n",
      "epoch: 10,  batch step: 237, loss: 99.22590637207031\n",
      "epoch: 10,  batch step: 238, loss: 27.160240173339844\n",
      "epoch: 10,  batch step: 239, loss: 18.197223663330078\n",
      "epoch: 10,  batch step: 240, loss: 151.1162872314453\n",
      "epoch: 10,  batch step: 241, loss: 38.17953109741211\n",
      "epoch: 10,  batch step: 242, loss: 27.27297019958496\n",
      "epoch: 10,  batch step: 243, loss: 40.82166290283203\n",
      "epoch: 10,  batch step: 244, loss: 21.97895050048828\n",
      "epoch: 10,  batch step: 245, loss: 45.585723876953125\n",
      "epoch: 10,  batch step: 246, loss: 41.51993179321289\n",
      "epoch: 10,  batch step: 247, loss: 89.06725311279297\n",
      "epoch: 10,  batch step: 248, loss: 332.39373779296875\n",
      "epoch: 10,  batch step: 249, loss: 8.88797378540039\n",
      "epoch: 10,  batch step: 250, loss: 85.5847396850586\n",
      "epoch: 10,  batch step: 251, loss: 83.53905487060547\n",
      "validation error epoch  10:    tensor(70.9778, device='cuda:0')\n",
      "316\n",
      "epoch: 11,  batch step: 0, loss: 425.824951171875\n",
      "epoch: 11,  batch step: 1, loss: 87.07662200927734\n",
      "epoch: 11,  batch step: 2, loss: 7.811744689941406\n",
      "epoch: 11,  batch step: 3, loss: 10.177581787109375\n",
      "epoch: 11,  batch step: 4, loss: 31.409034729003906\n",
      "epoch: 11,  batch step: 5, loss: 18.0522403717041\n",
      "epoch: 11,  batch step: 6, loss: 71.23235321044922\n",
      "epoch: 11,  batch step: 7, loss: 126.23008728027344\n",
      "epoch: 11,  batch step: 8, loss: 112.60808563232422\n",
      "epoch: 11,  batch step: 9, loss: 170.78790283203125\n",
      "epoch: 11,  batch step: 10, loss: 93.88827514648438\n",
      "epoch: 11,  batch step: 11, loss: 23.450292587280273\n",
      "epoch: 11,  batch step: 12, loss: 134.1019744873047\n",
      "epoch: 11,  batch step: 13, loss: 55.97314453125\n",
      "epoch: 11,  batch step: 14, loss: 65.7309799194336\n",
      "epoch: 11,  batch step: 15, loss: 45.563472747802734\n",
      "epoch: 11,  batch step: 16, loss: 16.010433197021484\n",
      "epoch: 11,  batch step: 17, loss: 15.735183715820312\n",
      "epoch: 11,  batch step: 18, loss: 27.674495697021484\n",
      "epoch: 11,  batch step: 19, loss: 112.46235656738281\n",
      "epoch: 11,  batch step: 20, loss: 100.47447204589844\n",
      "epoch: 11,  batch step: 21, loss: 20.038311004638672\n",
      "epoch: 11,  batch step: 22, loss: 16.061256408691406\n",
      "epoch: 11,  batch step: 23, loss: 18.454572677612305\n",
      "epoch: 11,  batch step: 24, loss: 16.24604606628418\n",
      "epoch: 11,  batch step: 25, loss: 17.102476119995117\n",
      "epoch: 11,  batch step: 26, loss: 266.2867126464844\n",
      "epoch: 11,  batch step: 27, loss: 76.43771362304688\n",
      "epoch: 11,  batch step: 28, loss: 91.86296844482422\n",
      "epoch: 11,  batch step: 29, loss: 45.905487060546875\n",
      "epoch: 11,  batch step: 30, loss: 73.18391418457031\n",
      "epoch: 11,  batch step: 31, loss: 38.65642547607422\n",
      "epoch: 11,  batch step: 32, loss: 19.7573299407959\n",
      "epoch: 11,  batch step: 33, loss: 53.56303024291992\n",
      "epoch: 11,  batch step: 34, loss: 31.63732147216797\n",
      "epoch: 11,  batch step: 35, loss: 17.710493087768555\n",
      "epoch: 11,  batch step: 36, loss: 36.65010070800781\n",
      "epoch: 11,  batch step: 37, loss: 56.76542663574219\n",
      "epoch: 11,  batch step: 38, loss: 181.59414672851562\n",
      "epoch: 11,  batch step: 39, loss: 71.43749237060547\n",
      "epoch: 11,  batch step: 40, loss: 141.2093505859375\n",
      "epoch: 11,  batch step: 41, loss: 166.2791748046875\n",
      "epoch: 11,  batch step: 42, loss: 15.477519035339355\n",
      "epoch: 11,  batch step: 43, loss: 14.82484245300293\n",
      "epoch: 11,  batch step: 44, loss: 116.83491516113281\n",
      "epoch: 11,  batch step: 45, loss: 22.463451385498047\n",
      "epoch: 11,  batch step: 46, loss: 86.70994567871094\n",
      "epoch: 11,  batch step: 47, loss: 28.678447723388672\n",
      "epoch: 11,  batch step: 48, loss: 12.552175521850586\n",
      "epoch: 11,  batch step: 49, loss: 34.6878662109375\n",
      "epoch: 11,  batch step: 50, loss: 165.1558837890625\n",
      "epoch: 11,  batch step: 51, loss: 61.347923278808594\n",
      "epoch: 11,  batch step: 52, loss: 13.971185684204102\n",
      "epoch: 11,  batch step: 53, loss: 345.5712890625\n",
      "epoch: 11,  batch step: 54, loss: 103.52806854248047\n",
      "epoch: 11,  batch step: 55, loss: 17.231138229370117\n",
      "epoch: 11,  batch step: 56, loss: 66.5108413696289\n",
      "epoch: 11,  batch step: 57, loss: 48.931846618652344\n",
      "epoch: 11,  batch step: 58, loss: 14.514719009399414\n",
      "epoch: 11,  batch step: 59, loss: 48.74253845214844\n",
      "epoch: 11,  batch step: 60, loss: 36.746826171875\n",
      "epoch: 11,  batch step: 61, loss: 99.68397521972656\n",
      "epoch: 11,  batch step: 62, loss: 15.536547660827637\n",
      "epoch: 11,  batch step: 63, loss: 81.2031021118164\n",
      "epoch: 11,  batch step: 64, loss: 28.716781616210938\n",
      "epoch: 11,  batch step: 65, loss: 7.181790828704834\n",
      "epoch: 11,  batch step: 66, loss: 17.947628021240234\n",
      "epoch: 11,  batch step: 67, loss: 8.36131477355957\n",
      "epoch: 11,  batch step: 68, loss: 68.12942504882812\n",
      "epoch: 11,  batch step: 69, loss: 240.12841796875\n",
      "epoch: 11,  batch step: 70, loss: 205.52853393554688\n",
      "epoch: 11,  batch step: 71, loss: 143.41519165039062\n",
      "epoch: 11,  batch step: 72, loss: 15.221841812133789\n",
      "epoch: 11,  batch step: 73, loss: 16.70406150817871\n",
      "epoch: 11,  batch step: 74, loss: 52.27129364013672\n",
      "epoch: 11,  batch step: 75, loss: 358.2779235839844\n",
      "epoch: 11,  batch step: 76, loss: 18.81476402282715\n",
      "epoch: 11,  batch step: 77, loss: 266.43792724609375\n",
      "epoch: 11,  batch step: 78, loss: 12.172019004821777\n",
      "epoch: 11,  batch step: 79, loss: 33.77324676513672\n",
      "epoch: 11,  batch step: 80, loss: 20.36255645751953\n",
      "epoch: 11,  batch step: 81, loss: 154.15176391601562\n",
      "epoch: 11,  batch step: 82, loss: 43.09928512573242\n",
      "epoch: 11,  batch step: 83, loss: 75.7586669921875\n",
      "epoch: 11,  batch step: 84, loss: 38.87855911254883\n",
      "epoch: 11,  batch step: 85, loss: 23.83753204345703\n",
      "epoch: 11,  batch step: 86, loss: 57.98680877685547\n",
      "epoch: 11,  batch step: 87, loss: 45.39104461669922\n",
      "epoch: 11,  batch step: 88, loss: 13.316131591796875\n",
      "epoch: 11,  batch step: 89, loss: 13.247912406921387\n",
      "epoch: 11,  batch step: 90, loss: 45.0098876953125\n",
      "epoch: 11,  batch step: 91, loss: 7.868619918823242\n",
      "epoch: 11,  batch step: 92, loss: 103.13066101074219\n",
      "epoch: 11,  batch step: 93, loss: 15.031883239746094\n",
      "epoch: 11,  batch step: 94, loss: 134.28164672851562\n",
      "epoch: 11,  batch step: 95, loss: 8.986842155456543\n",
      "epoch: 11,  batch step: 96, loss: 19.48253631591797\n",
      "epoch: 11,  batch step: 97, loss: 96.7379379272461\n",
      "epoch: 11,  batch step: 98, loss: 14.468206405639648\n",
      "epoch: 11,  batch step: 99, loss: 7.0620036125183105\n",
      "epoch: 11,  batch step: 100, loss: 26.560588836669922\n",
      "epoch: 11,  batch step: 101, loss: 97.55440521240234\n",
      "epoch: 11,  batch step: 102, loss: 132.39735412597656\n",
      "epoch: 11,  batch step: 103, loss: 10.35200023651123\n",
      "epoch: 11,  batch step: 104, loss: 25.701313018798828\n",
      "epoch: 11,  batch step: 105, loss: 30.823291778564453\n",
      "epoch: 11,  batch step: 106, loss: 13.780322074890137\n",
      "epoch: 11,  batch step: 107, loss: 13.155502319335938\n",
      "epoch: 11,  batch step: 108, loss: 15.175414085388184\n",
      "epoch: 11,  batch step: 109, loss: 10.588637351989746\n",
      "epoch: 11,  batch step: 110, loss: 143.01670837402344\n",
      "epoch: 11,  batch step: 111, loss: 18.405643463134766\n",
      "epoch: 11,  batch step: 112, loss: 13.716753959655762\n",
      "epoch: 11,  batch step: 113, loss: 166.48162841796875\n",
      "epoch: 11,  batch step: 114, loss: 8.232744216918945\n",
      "epoch: 11,  batch step: 115, loss: 9.709113121032715\n",
      "epoch: 11,  batch step: 116, loss: 13.359939575195312\n",
      "epoch: 11,  batch step: 117, loss: 24.905195236206055\n",
      "epoch: 11,  batch step: 118, loss: 23.004318237304688\n",
      "epoch: 11,  batch step: 119, loss: 12.317854881286621\n",
      "epoch: 11,  batch step: 120, loss: 162.37127685546875\n",
      "epoch: 11,  batch step: 121, loss: 235.47787475585938\n",
      "epoch: 11,  batch step: 122, loss: 106.81892395019531\n",
      "epoch: 11,  batch step: 123, loss: 126.75846862792969\n",
      "epoch: 11,  batch step: 124, loss: 16.187400817871094\n",
      "epoch: 11,  batch step: 125, loss: 43.64617919921875\n",
      "epoch: 11,  batch step: 126, loss: 24.60853385925293\n",
      "epoch: 11,  batch step: 127, loss: 135.07388305664062\n",
      "epoch: 11,  batch step: 128, loss: 36.51570129394531\n",
      "epoch: 11,  batch step: 129, loss: 182.64305114746094\n",
      "epoch: 11,  batch step: 130, loss: 30.189550399780273\n",
      "epoch: 11,  batch step: 131, loss: 115.46886444091797\n",
      "epoch: 11,  batch step: 132, loss: 199.3090057373047\n",
      "epoch: 11,  batch step: 133, loss: 11.702162742614746\n",
      "epoch: 11,  batch step: 134, loss: 59.847694396972656\n",
      "epoch: 11,  batch step: 135, loss: 20.073583602905273\n",
      "epoch: 11,  batch step: 136, loss: 42.38530349731445\n",
      "epoch: 11,  batch step: 137, loss: 15.483965873718262\n",
      "epoch: 11,  batch step: 138, loss: 45.63010787963867\n",
      "epoch: 11,  batch step: 139, loss: 441.72393798828125\n",
      "epoch: 11,  batch step: 140, loss: 57.48781967163086\n",
      "epoch: 11,  batch step: 141, loss: 21.6145076751709\n",
      "epoch: 11,  batch step: 142, loss: 49.772186279296875\n",
      "epoch: 11,  batch step: 143, loss: 17.545143127441406\n",
      "epoch: 11,  batch step: 144, loss: 67.03414154052734\n",
      "epoch: 11,  batch step: 145, loss: 19.365562438964844\n",
      "epoch: 11,  batch step: 146, loss: 409.22979736328125\n",
      "epoch: 11,  batch step: 147, loss: 21.674694061279297\n",
      "epoch: 11,  batch step: 148, loss: 25.86061668395996\n",
      "epoch: 11,  batch step: 149, loss: 40.304656982421875\n",
      "epoch: 11,  batch step: 150, loss: 39.00019836425781\n",
      "epoch: 11,  batch step: 151, loss: 39.50901412963867\n",
      "epoch: 11,  batch step: 152, loss: 24.371868133544922\n",
      "epoch: 11,  batch step: 153, loss: 39.9842529296875\n",
      "epoch: 11,  batch step: 154, loss: 286.74969482421875\n",
      "epoch: 11,  batch step: 155, loss: 38.15342712402344\n",
      "epoch: 11,  batch step: 156, loss: 35.59281539916992\n",
      "epoch: 11,  batch step: 157, loss: 49.97845458984375\n",
      "epoch: 11,  batch step: 158, loss: 240.73973083496094\n",
      "epoch: 11,  batch step: 159, loss: 151.93019104003906\n",
      "epoch: 11,  batch step: 160, loss: 174.6709442138672\n",
      "epoch: 11,  batch step: 161, loss: 91.18394470214844\n",
      "epoch: 11,  batch step: 162, loss: 14.959890365600586\n",
      "epoch: 11,  batch step: 163, loss: 30.37380599975586\n",
      "epoch: 11,  batch step: 164, loss: 21.720829010009766\n",
      "epoch: 11,  batch step: 165, loss: 31.87214469909668\n",
      "epoch: 11,  batch step: 166, loss: 57.63067626953125\n",
      "epoch: 11,  batch step: 167, loss: 16.644046783447266\n",
      "epoch: 11,  batch step: 168, loss: 71.69578552246094\n",
      "epoch: 11,  batch step: 169, loss: 104.80049896240234\n",
      "epoch: 11,  batch step: 170, loss: 49.339900970458984\n",
      "epoch: 11,  batch step: 171, loss: 125.3706283569336\n",
      "epoch: 11,  batch step: 172, loss: 33.509071350097656\n",
      "epoch: 11,  batch step: 173, loss: 97.23162841796875\n",
      "epoch: 11,  batch step: 174, loss: 143.3492431640625\n",
      "epoch: 11,  batch step: 175, loss: 19.335582733154297\n",
      "epoch: 11,  batch step: 176, loss: 12.068289756774902\n",
      "epoch: 11,  batch step: 177, loss: 28.382131576538086\n",
      "epoch: 11,  batch step: 178, loss: 370.80023193359375\n",
      "epoch: 11,  batch step: 179, loss: 69.43476867675781\n",
      "epoch: 11,  batch step: 180, loss: 12.596593856811523\n",
      "epoch: 11,  batch step: 181, loss: 47.85344696044922\n",
      "epoch: 11,  batch step: 182, loss: 241.6390838623047\n",
      "epoch: 11,  batch step: 183, loss: 13.756422996520996\n",
      "epoch: 11,  batch step: 184, loss: 33.99469757080078\n",
      "epoch: 11,  batch step: 185, loss: 17.88803482055664\n",
      "epoch: 11,  batch step: 186, loss: 27.58674430847168\n",
      "epoch: 11,  batch step: 187, loss: 28.950702667236328\n",
      "epoch: 11,  batch step: 188, loss: 28.558029174804688\n",
      "epoch: 11,  batch step: 189, loss: 44.07645797729492\n",
      "epoch: 11,  batch step: 190, loss: 30.49325180053711\n",
      "epoch: 11,  batch step: 191, loss: 91.50595092773438\n",
      "epoch: 11,  batch step: 192, loss: 37.000205993652344\n",
      "epoch: 11,  batch step: 193, loss: 24.0294189453125\n",
      "epoch: 11,  batch step: 194, loss: 18.28156089782715\n",
      "epoch: 11,  batch step: 195, loss: 21.224716186523438\n",
      "epoch: 11,  batch step: 196, loss: 23.808837890625\n",
      "epoch: 11,  batch step: 197, loss: 14.8439359664917\n",
      "epoch: 11,  batch step: 198, loss: 20.070411682128906\n",
      "epoch: 11,  batch step: 199, loss: 119.46541595458984\n",
      "epoch: 11,  batch step: 200, loss: 191.6728973388672\n",
      "epoch: 11,  batch step: 201, loss: 126.79177856445312\n",
      "epoch: 11,  batch step: 202, loss: 14.375853538513184\n",
      "epoch: 11,  batch step: 203, loss: 8.748449325561523\n",
      "epoch: 11,  batch step: 204, loss: 10.822416305541992\n",
      "epoch: 11,  batch step: 205, loss: 15.094261169433594\n",
      "epoch: 11,  batch step: 206, loss: 73.93931579589844\n",
      "epoch: 11,  batch step: 207, loss: 21.857646942138672\n",
      "epoch: 11,  batch step: 208, loss: 9.216835021972656\n",
      "epoch: 11,  batch step: 209, loss: 34.56938934326172\n",
      "epoch: 11,  batch step: 210, loss: 15.287925720214844\n",
      "epoch: 11,  batch step: 211, loss: 10.654192924499512\n",
      "epoch: 11,  batch step: 212, loss: 12.091510772705078\n",
      "epoch: 11,  batch step: 213, loss: 92.28102111816406\n",
      "epoch: 11,  batch step: 214, loss: 12.46204662322998\n",
      "epoch: 11,  batch step: 215, loss: 24.49716567993164\n",
      "epoch: 11,  batch step: 216, loss: 14.549460411071777\n",
      "epoch: 11,  batch step: 217, loss: 27.352794647216797\n",
      "epoch: 11,  batch step: 218, loss: 7.392337799072266\n",
      "epoch: 11,  batch step: 219, loss: 34.772762298583984\n",
      "epoch: 11,  batch step: 220, loss: 6.033720970153809\n",
      "epoch: 11,  batch step: 221, loss: 44.0398063659668\n",
      "epoch: 11,  batch step: 222, loss: 14.697559356689453\n",
      "epoch: 11,  batch step: 223, loss: 7.151269435882568\n",
      "epoch: 11,  batch step: 224, loss: 88.142822265625\n",
      "epoch: 11,  batch step: 225, loss: 231.83847045898438\n",
      "epoch: 11,  batch step: 226, loss: 189.0858612060547\n",
      "epoch: 11,  batch step: 227, loss: 19.237682342529297\n",
      "epoch: 11,  batch step: 228, loss: 222.4719696044922\n",
      "epoch: 11,  batch step: 229, loss: 14.839353561401367\n",
      "epoch: 11,  batch step: 230, loss: 18.74960708618164\n",
      "epoch: 11,  batch step: 231, loss: 7.761294364929199\n",
      "epoch: 11,  batch step: 232, loss: 9.332130432128906\n",
      "epoch: 11,  batch step: 233, loss: 6.67595100402832\n",
      "epoch: 11,  batch step: 234, loss: 7.878493309020996\n",
      "epoch: 11,  batch step: 235, loss: 202.59568786621094\n",
      "epoch: 11,  batch step: 236, loss: 15.677762985229492\n",
      "epoch: 11,  batch step: 237, loss: 26.83173179626465\n",
      "epoch: 11,  batch step: 238, loss: 191.5381317138672\n",
      "epoch: 11,  batch step: 239, loss: 201.28636169433594\n",
      "epoch: 11,  batch step: 240, loss: 16.688058853149414\n",
      "epoch: 11,  batch step: 241, loss: 65.43865966796875\n",
      "epoch: 11,  batch step: 242, loss: 55.94318771362305\n",
      "epoch: 11,  batch step: 243, loss: 138.78256225585938\n",
      "epoch: 11,  batch step: 244, loss: 375.88421630859375\n",
      "epoch: 11,  batch step: 245, loss: 15.345660209655762\n",
      "epoch: 11,  batch step: 246, loss: 131.64268493652344\n",
      "epoch: 11,  batch step: 247, loss: 21.876615524291992\n",
      "epoch: 11,  batch step: 248, loss: 34.072998046875\n",
      "epoch: 11,  batch step: 249, loss: 18.129011154174805\n",
      "epoch: 11,  batch step: 250, loss: 148.4109344482422\n",
      "epoch: 11,  batch step: 251, loss: 104.32945251464844\n",
      "validation error epoch  11:    tensor(83.0561, device='cuda:0')\n",
      "316\n",
      "epoch: 12,  batch step: 0, loss: 64.35386657714844\n",
      "epoch: 12,  batch step: 1, loss: 18.146625518798828\n",
      "epoch: 12,  batch step: 2, loss: 20.545751571655273\n",
      "epoch: 12,  batch step: 3, loss: 39.82923889160156\n",
      "epoch: 12,  batch step: 4, loss: 138.40187072753906\n",
      "epoch: 12,  batch step: 5, loss: 16.45343017578125\n",
      "epoch: 12,  batch step: 6, loss: 70.33273315429688\n",
      "epoch: 12,  batch step: 7, loss: 17.29941177368164\n",
      "epoch: 12,  batch step: 8, loss: 43.435157775878906\n",
      "epoch: 12,  batch step: 9, loss: 167.21743774414062\n",
      "epoch: 12,  batch step: 10, loss: 20.202056884765625\n",
      "epoch: 12,  batch step: 11, loss: 21.85187339782715\n",
      "epoch: 12,  batch step: 12, loss: 218.95697021484375\n",
      "epoch: 12,  batch step: 13, loss: 9.188178062438965\n",
      "epoch: 12,  batch step: 14, loss: 34.8720588684082\n",
      "epoch: 12,  batch step: 15, loss: 30.571693420410156\n",
      "epoch: 12,  batch step: 16, loss: 204.3931121826172\n",
      "epoch: 12,  batch step: 17, loss: 43.72338104248047\n",
      "epoch: 12,  batch step: 18, loss: 78.66017150878906\n",
      "epoch: 12,  batch step: 19, loss: 42.35298538208008\n",
      "epoch: 12,  batch step: 20, loss: 30.753753662109375\n",
      "epoch: 12,  batch step: 21, loss: 183.56417846679688\n",
      "epoch: 12,  batch step: 22, loss: 21.342506408691406\n",
      "epoch: 12,  batch step: 23, loss: 139.80999755859375\n",
      "epoch: 12,  batch step: 24, loss: 320.2396240234375\n",
      "epoch: 12,  batch step: 25, loss: 35.20620346069336\n",
      "epoch: 12,  batch step: 26, loss: 30.72548484802246\n",
      "epoch: 12,  batch step: 27, loss: 87.19414520263672\n",
      "epoch: 12,  batch step: 28, loss: 29.46114158630371\n",
      "epoch: 12,  batch step: 29, loss: 43.602439880371094\n",
      "epoch: 12,  batch step: 30, loss: 15.959497451782227\n",
      "epoch: 12,  batch step: 31, loss: 19.60751724243164\n",
      "epoch: 12,  batch step: 32, loss: 163.22964477539062\n",
      "epoch: 12,  batch step: 33, loss: 11.882213592529297\n",
      "epoch: 12,  batch step: 34, loss: 20.733837127685547\n",
      "epoch: 12,  batch step: 35, loss: 203.64573669433594\n",
      "epoch: 12,  batch step: 36, loss: 24.24002456665039\n",
      "epoch: 12,  batch step: 37, loss: 89.13746643066406\n",
      "epoch: 12,  batch step: 38, loss: 64.02867126464844\n",
      "epoch: 12,  batch step: 39, loss: 45.399837493896484\n",
      "epoch: 12,  batch step: 40, loss: 132.00999450683594\n",
      "epoch: 12,  batch step: 41, loss: 34.64047622680664\n",
      "epoch: 12,  batch step: 42, loss: 188.69601440429688\n",
      "epoch: 12,  batch step: 43, loss: 24.989133834838867\n",
      "epoch: 12,  batch step: 44, loss: 9.464668273925781\n",
      "epoch: 12,  batch step: 45, loss: 13.4608793258667\n",
      "epoch: 12,  batch step: 46, loss: 24.70016860961914\n",
      "epoch: 12,  batch step: 47, loss: 49.24347686767578\n",
      "epoch: 12,  batch step: 48, loss: 16.02791976928711\n",
      "epoch: 12,  batch step: 49, loss: 15.608996391296387\n",
      "epoch: 12,  batch step: 50, loss: 190.00381469726562\n",
      "epoch: 12,  batch step: 51, loss: 75.90115356445312\n",
      "epoch: 12,  batch step: 52, loss: 27.763580322265625\n",
      "epoch: 12,  batch step: 53, loss: 131.3281707763672\n",
      "epoch: 12,  batch step: 54, loss: 34.90839767456055\n",
      "epoch: 12,  batch step: 55, loss: 127.3069076538086\n",
      "epoch: 12,  batch step: 56, loss: 24.753376007080078\n",
      "epoch: 12,  batch step: 57, loss: 203.60826110839844\n",
      "epoch: 12,  batch step: 58, loss: 9.97134780883789\n",
      "epoch: 12,  batch step: 59, loss: 55.490875244140625\n",
      "epoch: 12,  batch step: 60, loss: 67.76332092285156\n",
      "epoch: 12,  batch step: 61, loss: 9.85710334777832\n",
      "epoch: 12,  batch step: 62, loss: 50.350154876708984\n",
      "epoch: 12,  batch step: 63, loss: 18.425329208374023\n",
      "epoch: 12,  batch step: 64, loss: 125.21697235107422\n",
      "epoch: 12,  batch step: 65, loss: 28.150238037109375\n",
      "epoch: 12,  batch step: 66, loss: 179.69175720214844\n",
      "epoch: 12,  batch step: 67, loss: 101.14228820800781\n",
      "epoch: 12,  batch step: 68, loss: 113.06319427490234\n",
      "epoch: 12,  batch step: 69, loss: 13.686827659606934\n",
      "epoch: 12,  batch step: 70, loss: 50.54771041870117\n",
      "epoch: 12,  batch step: 71, loss: 191.6296844482422\n",
      "epoch: 12,  batch step: 72, loss: 43.69913864135742\n",
      "epoch: 12,  batch step: 73, loss: 96.23164367675781\n",
      "epoch: 12,  batch step: 74, loss: 104.8756332397461\n",
      "epoch: 12,  batch step: 75, loss: 25.92462158203125\n",
      "epoch: 12,  batch step: 76, loss: 40.506324768066406\n",
      "epoch: 12,  batch step: 77, loss: 16.290721893310547\n",
      "epoch: 12,  batch step: 78, loss: 29.664718627929688\n",
      "epoch: 12,  batch step: 79, loss: 361.10699462890625\n",
      "epoch: 12,  batch step: 80, loss: 19.62607192993164\n",
      "epoch: 12,  batch step: 81, loss: 304.631103515625\n",
      "epoch: 12,  batch step: 82, loss: 133.47409057617188\n",
      "epoch: 12,  batch step: 83, loss: 17.255496978759766\n",
      "epoch: 12,  batch step: 84, loss: 106.02465057373047\n",
      "epoch: 12,  batch step: 85, loss: 57.60185241699219\n",
      "epoch: 12,  batch step: 86, loss: 10.801725387573242\n",
      "epoch: 12,  batch step: 87, loss: 279.8096923828125\n",
      "epoch: 12,  batch step: 88, loss: 45.38943099975586\n",
      "epoch: 12,  batch step: 89, loss: 35.12028503417969\n",
      "epoch: 12,  batch step: 90, loss: 26.212841033935547\n",
      "epoch: 12,  batch step: 91, loss: 28.929412841796875\n",
      "epoch: 12,  batch step: 92, loss: 101.58465576171875\n",
      "epoch: 12,  batch step: 93, loss: 26.271032333374023\n",
      "epoch: 12,  batch step: 94, loss: 14.541990280151367\n",
      "epoch: 12,  batch step: 95, loss: 79.93980407714844\n",
      "epoch: 12,  batch step: 96, loss: 26.610380172729492\n",
      "epoch: 12,  batch step: 97, loss: 23.882848739624023\n",
      "epoch: 12,  batch step: 98, loss: 46.58826446533203\n",
      "epoch: 12,  batch step: 99, loss: 43.829559326171875\n",
      "epoch: 12,  batch step: 100, loss: 28.703495025634766\n",
      "epoch: 12,  batch step: 101, loss: 36.773468017578125\n",
      "epoch: 12,  batch step: 102, loss: 122.75218200683594\n",
      "epoch: 12,  batch step: 103, loss: 9.173845291137695\n",
      "epoch: 12,  batch step: 104, loss: 26.725284576416016\n",
      "epoch: 12,  batch step: 105, loss: 47.57880401611328\n",
      "epoch: 12,  batch step: 106, loss: 94.35253143310547\n",
      "epoch: 12,  batch step: 107, loss: 263.4019470214844\n",
      "epoch: 12,  batch step: 108, loss: 11.4376220703125\n",
      "epoch: 12,  batch step: 109, loss: 288.6031188964844\n",
      "epoch: 12,  batch step: 110, loss: 22.683813095092773\n",
      "epoch: 12,  batch step: 111, loss: 18.45485496520996\n",
      "epoch: 12,  batch step: 112, loss: 35.072967529296875\n",
      "epoch: 12,  batch step: 113, loss: 23.184324264526367\n",
      "epoch: 12,  batch step: 114, loss: 28.71379852294922\n",
      "epoch: 12,  batch step: 115, loss: 44.44367218017578\n",
      "epoch: 12,  batch step: 116, loss: 171.56863403320312\n",
      "epoch: 12,  batch step: 117, loss: 60.18171310424805\n",
      "epoch: 12,  batch step: 118, loss: 39.84675979614258\n",
      "epoch: 12,  batch step: 119, loss: 146.63442993164062\n",
      "epoch: 12,  batch step: 120, loss: 12.139845848083496\n",
      "epoch: 12,  batch step: 121, loss: 272.5101318359375\n",
      "epoch: 12,  batch step: 122, loss: 69.07222747802734\n",
      "epoch: 12,  batch step: 123, loss: 27.562870025634766\n",
      "epoch: 12,  batch step: 124, loss: 16.33769416809082\n",
      "epoch: 12,  batch step: 125, loss: 41.6192626953125\n",
      "epoch: 12,  batch step: 126, loss: 7.281943321228027\n",
      "epoch: 12,  batch step: 127, loss: 21.827428817749023\n",
      "epoch: 12,  batch step: 128, loss: 54.06843566894531\n",
      "epoch: 12,  batch step: 129, loss: 11.429072380065918\n",
      "epoch: 12,  batch step: 130, loss: 11.665995597839355\n",
      "epoch: 12,  batch step: 131, loss: 77.53030395507812\n",
      "epoch: 12,  batch step: 132, loss: 35.30553436279297\n",
      "epoch: 12,  batch step: 133, loss: 19.61545753479004\n",
      "epoch: 12,  batch step: 134, loss: 20.12647247314453\n",
      "epoch: 12,  batch step: 135, loss: 189.67852783203125\n",
      "epoch: 12,  batch step: 136, loss: 93.4717025756836\n",
      "epoch: 12,  batch step: 137, loss: 12.683646202087402\n",
      "epoch: 12,  batch step: 138, loss: 16.329181671142578\n",
      "epoch: 12,  batch step: 139, loss: 13.166004180908203\n",
      "epoch: 12,  batch step: 140, loss: 8.103340148925781\n",
      "epoch: 12,  batch step: 141, loss: 7.912269592285156\n",
      "epoch: 12,  batch step: 142, loss: 14.859405517578125\n",
      "epoch: 12,  batch step: 143, loss: 60.086029052734375\n",
      "epoch: 12,  batch step: 144, loss: 5.638731002807617\n",
      "epoch: 12,  batch step: 145, loss: 25.656044006347656\n",
      "epoch: 12,  batch step: 146, loss: 8.736553192138672\n",
      "epoch: 12,  batch step: 147, loss: 9.398283004760742\n",
      "epoch: 12,  batch step: 148, loss: 18.414228439331055\n",
      "epoch: 12,  batch step: 149, loss: 192.6382293701172\n",
      "epoch: 12,  batch step: 150, loss: 9.6849365234375\n",
      "epoch: 12,  batch step: 151, loss: 86.18063354492188\n",
      "epoch: 12,  batch step: 152, loss: 16.418046951293945\n",
      "epoch: 12,  batch step: 153, loss: 11.270537376403809\n",
      "epoch: 12,  batch step: 154, loss: 16.656585693359375\n",
      "epoch: 12,  batch step: 155, loss: 63.687950134277344\n",
      "epoch: 12,  batch step: 156, loss: 6.316032409667969\n",
      "epoch: 12,  batch step: 157, loss: 54.58005905151367\n",
      "epoch: 12,  batch step: 158, loss: 7.383330821990967\n",
      "epoch: 12,  batch step: 159, loss: 14.21466064453125\n",
      "epoch: 12,  batch step: 160, loss: 20.90750503540039\n",
      "epoch: 12,  batch step: 161, loss: 20.735807418823242\n",
      "epoch: 12,  batch step: 162, loss: 9.775306701660156\n",
      "epoch: 12,  batch step: 163, loss: 27.087173461914062\n",
      "epoch: 12,  batch step: 164, loss: 62.23078155517578\n",
      "epoch: 12,  batch step: 165, loss: 224.9874267578125\n",
      "epoch: 12,  batch step: 166, loss: 15.8759183883667\n",
      "epoch: 12,  batch step: 167, loss: 52.0821647644043\n",
      "epoch: 12,  batch step: 168, loss: 9.59161376953125\n",
      "epoch: 12,  batch step: 169, loss: 52.60460662841797\n",
      "epoch: 12,  batch step: 170, loss: 213.40036010742188\n",
      "epoch: 12,  batch step: 171, loss: 400.0597839355469\n",
      "epoch: 12,  batch step: 172, loss: 335.9092712402344\n",
      "epoch: 12,  batch step: 173, loss: 23.243289947509766\n",
      "epoch: 12,  batch step: 174, loss: 20.085540771484375\n",
      "epoch: 12,  batch step: 175, loss: 15.360949516296387\n",
      "epoch: 12,  batch step: 176, loss: 135.87088012695312\n",
      "epoch: 12,  batch step: 177, loss: 134.21234130859375\n",
      "epoch: 12,  batch step: 178, loss: 27.592994689941406\n",
      "epoch: 12,  batch step: 179, loss: 30.75738525390625\n",
      "epoch: 12,  batch step: 180, loss: 137.26524353027344\n",
      "epoch: 12,  batch step: 181, loss: 69.14041137695312\n",
      "epoch: 12,  batch step: 182, loss: 45.44736862182617\n",
      "epoch: 12,  batch step: 183, loss: 170.88487243652344\n",
      "epoch: 12,  batch step: 184, loss: 155.83895874023438\n",
      "epoch: 12,  batch step: 185, loss: 31.245405197143555\n",
      "epoch: 12,  batch step: 186, loss: 85.93956756591797\n",
      "epoch: 12,  batch step: 187, loss: 50.901206970214844\n",
      "epoch: 12,  batch step: 188, loss: 137.84426879882812\n",
      "epoch: 12,  batch step: 189, loss: 53.0669059753418\n",
      "epoch: 12,  batch step: 190, loss: 53.35427474975586\n",
      "epoch: 12,  batch step: 191, loss: 54.43107223510742\n",
      "epoch: 12,  batch step: 192, loss: 14.754732131958008\n",
      "epoch: 12,  batch step: 193, loss: 129.48651123046875\n",
      "epoch: 12,  batch step: 194, loss: 12.01242733001709\n",
      "epoch: 12,  batch step: 195, loss: 158.77859497070312\n",
      "epoch: 12,  batch step: 196, loss: 175.4581298828125\n",
      "epoch: 12,  batch step: 197, loss: 490.1423034667969\n",
      "epoch: 12,  batch step: 198, loss: 22.019210815429688\n",
      "epoch: 12,  batch step: 199, loss: 72.43107604980469\n",
      "epoch: 12,  batch step: 200, loss: 147.68373107910156\n",
      "epoch: 12,  batch step: 201, loss: 16.96048355102539\n",
      "epoch: 12,  batch step: 202, loss: 25.717021942138672\n",
      "epoch: 12,  batch step: 203, loss: 17.516815185546875\n",
      "epoch: 12,  batch step: 204, loss: 47.30768966674805\n",
      "epoch: 12,  batch step: 205, loss: 15.100825309753418\n",
      "epoch: 12,  batch step: 206, loss: 11.607160568237305\n",
      "epoch: 12,  batch step: 207, loss: 29.918371200561523\n",
      "epoch: 12,  batch step: 208, loss: 42.079368591308594\n",
      "epoch: 12,  batch step: 209, loss: 46.89425277709961\n",
      "epoch: 12,  batch step: 210, loss: 228.19973754882812\n",
      "epoch: 12,  batch step: 211, loss: 69.95375061035156\n",
      "epoch: 12,  batch step: 212, loss: 59.65355682373047\n",
      "epoch: 12,  batch step: 213, loss: 54.93331527709961\n",
      "epoch: 12,  batch step: 214, loss: 72.27114868164062\n",
      "epoch: 12,  batch step: 215, loss: 89.22291564941406\n",
      "epoch: 12,  batch step: 216, loss: 12.622381210327148\n",
      "epoch: 12,  batch step: 217, loss: 256.34735107421875\n",
      "epoch: 12,  batch step: 218, loss: 292.3096923828125\n",
      "epoch: 12,  batch step: 219, loss: 148.45175170898438\n",
      "epoch: 12,  batch step: 220, loss: 30.51260757446289\n",
      "epoch: 12,  batch step: 221, loss: 33.56636047363281\n",
      "epoch: 12,  batch step: 222, loss: 9.965278625488281\n",
      "epoch: 12,  batch step: 223, loss: 27.104290008544922\n",
      "epoch: 12,  batch step: 224, loss: 35.873870849609375\n",
      "epoch: 12,  batch step: 225, loss: 9.652847290039062\n",
      "epoch: 12,  batch step: 226, loss: 24.038055419921875\n",
      "epoch: 12,  batch step: 227, loss: 49.86532211303711\n",
      "epoch: 12,  batch step: 228, loss: 14.445152282714844\n",
      "epoch: 12,  batch step: 229, loss: 20.85845184326172\n",
      "epoch: 12,  batch step: 230, loss: 55.5986442565918\n",
      "epoch: 12,  batch step: 231, loss: 246.19151306152344\n",
      "epoch: 12,  batch step: 232, loss: 27.99274253845215\n",
      "epoch: 12,  batch step: 233, loss: 10.2230863571167\n",
      "epoch: 12,  batch step: 234, loss: 108.1173324584961\n",
      "epoch: 12,  batch step: 235, loss: 19.870555877685547\n",
      "epoch: 12,  batch step: 236, loss: 10.270727157592773\n",
      "epoch: 12,  batch step: 237, loss: 36.75546646118164\n",
      "epoch: 12,  batch step: 238, loss: 6.725857734680176\n",
      "epoch: 12,  batch step: 239, loss: 8.368293762207031\n",
      "epoch: 12,  batch step: 240, loss: 8.134284019470215\n",
      "epoch: 12,  batch step: 241, loss: 69.26295471191406\n",
      "epoch: 12,  batch step: 242, loss: 7.372910976409912\n",
      "epoch: 12,  batch step: 243, loss: 13.338525772094727\n",
      "epoch: 12,  batch step: 244, loss: 177.18075561523438\n",
      "epoch: 12,  batch step: 245, loss: 126.243896484375\n",
      "epoch: 12,  batch step: 246, loss: 112.30552673339844\n",
      "epoch: 12,  batch step: 247, loss: 6.163637161254883\n",
      "epoch: 12,  batch step: 248, loss: 135.61306762695312\n",
      "epoch: 12,  batch step: 249, loss: 44.6733512878418\n",
      "epoch: 12,  batch step: 250, loss: 72.452392578125\n",
      "epoch: 12,  batch step: 251, loss: 86.38890075683594\n",
      "validation error epoch  12:    tensor(115.6289, device='cuda:0')\n",
      "316\n",
      "epoch: 13,  batch step: 0, loss: 15.886283874511719\n",
      "epoch: 13,  batch step: 1, loss: 13.644160270690918\n",
      "epoch: 13,  batch step: 2, loss: 23.898204803466797\n",
      "epoch: 13,  batch step: 3, loss: 119.75261688232422\n",
      "epoch: 13,  batch step: 4, loss: 33.943336486816406\n",
      "epoch: 13,  batch step: 5, loss: 39.079673767089844\n",
      "epoch: 13,  batch step: 6, loss: 21.60470199584961\n",
      "epoch: 13,  batch step: 7, loss: 396.928955078125\n",
      "epoch: 13,  batch step: 8, loss: 144.94940185546875\n",
      "epoch: 13,  batch step: 9, loss: 10.642958641052246\n",
      "epoch: 13,  batch step: 10, loss: 17.17963409423828\n",
      "epoch: 13,  batch step: 11, loss: 17.247257232666016\n",
      "epoch: 13,  batch step: 12, loss: 241.25738525390625\n",
      "epoch: 13,  batch step: 13, loss: 28.483718872070312\n",
      "epoch: 13,  batch step: 14, loss: 12.15735149383545\n",
      "epoch: 13,  batch step: 15, loss: 81.49240112304688\n",
      "epoch: 13,  batch step: 16, loss: 51.65354919433594\n",
      "epoch: 13,  batch step: 17, loss: 143.07630920410156\n",
      "epoch: 13,  batch step: 18, loss: 65.37115478515625\n",
      "epoch: 13,  batch step: 19, loss: 53.71629333496094\n",
      "epoch: 13,  batch step: 20, loss: 17.810405731201172\n",
      "epoch: 13,  batch step: 21, loss: 125.45922088623047\n",
      "epoch: 13,  batch step: 22, loss: 23.347564697265625\n",
      "epoch: 13,  batch step: 23, loss: 99.15777587890625\n",
      "epoch: 13,  batch step: 24, loss: 37.62507247924805\n",
      "epoch: 13,  batch step: 25, loss: 51.752960205078125\n",
      "epoch: 13,  batch step: 26, loss: 62.864559173583984\n",
      "epoch: 13,  batch step: 27, loss: 30.28929901123047\n",
      "epoch: 13,  batch step: 28, loss: 24.887557983398438\n",
      "epoch: 13,  batch step: 29, loss: 26.596725463867188\n",
      "epoch: 13,  batch step: 30, loss: 57.404022216796875\n",
      "epoch: 13,  batch step: 31, loss: 482.7552490234375\n",
      "epoch: 13,  batch step: 32, loss: 15.412322998046875\n",
      "epoch: 13,  batch step: 33, loss: 9.749415397644043\n",
      "epoch: 13,  batch step: 34, loss: 12.497904777526855\n",
      "epoch: 13,  batch step: 35, loss: 67.23487091064453\n",
      "epoch: 13,  batch step: 36, loss: 27.828453063964844\n",
      "epoch: 13,  batch step: 37, loss: 10.575288772583008\n",
      "epoch: 13,  batch step: 38, loss: 18.271724700927734\n",
      "epoch: 13,  batch step: 39, loss: 13.119593620300293\n",
      "epoch: 13,  batch step: 40, loss: 120.89202117919922\n",
      "epoch: 13,  batch step: 41, loss: 8.766992568969727\n",
      "epoch: 13,  batch step: 42, loss: 61.09320831298828\n",
      "epoch: 13,  batch step: 43, loss: 12.987255096435547\n",
      "epoch: 13,  batch step: 44, loss: 16.710948944091797\n",
      "epoch: 13,  batch step: 45, loss: 10.770380020141602\n",
      "epoch: 13,  batch step: 46, loss: 31.84447479248047\n",
      "epoch: 13,  batch step: 47, loss: 12.062244415283203\n",
      "epoch: 13,  batch step: 48, loss: 62.686912536621094\n",
      "epoch: 13,  batch step: 49, loss: 15.149942398071289\n",
      "epoch: 13,  batch step: 50, loss: 20.77802085876465\n",
      "epoch: 13,  batch step: 51, loss: 679.282958984375\n",
      "epoch: 13,  batch step: 52, loss: 6.918490409851074\n",
      "epoch: 13,  batch step: 53, loss: 130.32470703125\n",
      "epoch: 13,  batch step: 54, loss: 57.09123992919922\n",
      "epoch: 13,  batch step: 55, loss: 9.383821487426758\n",
      "epoch: 13,  batch step: 56, loss: 116.02130126953125\n",
      "epoch: 13,  batch step: 57, loss: 67.50959777832031\n",
      "epoch: 13,  batch step: 58, loss: 148.4480438232422\n",
      "epoch: 13,  batch step: 59, loss: 19.630767822265625\n",
      "epoch: 13,  batch step: 60, loss: 25.35625457763672\n",
      "epoch: 13,  batch step: 61, loss: 46.75434494018555\n",
      "epoch: 13,  batch step: 62, loss: 298.654541015625\n",
      "epoch: 13,  batch step: 63, loss: 39.759403228759766\n",
      "epoch: 13,  batch step: 64, loss: 21.24264907836914\n",
      "epoch: 13,  batch step: 65, loss: 26.872695922851562\n",
      "epoch: 13,  batch step: 66, loss: 50.6170654296875\n",
      "epoch: 13,  batch step: 67, loss: 160.67437744140625\n",
      "epoch: 13,  batch step: 68, loss: 17.016599655151367\n",
      "epoch: 13,  batch step: 69, loss: 102.22368621826172\n",
      "epoch: 13,  batch step: 70, loss: 10.993715286254883\n",
      "epoch: 13,  batch step: 71, loss: 33.090087890625\n",
      "epoch: 13,  batch step: 72, loss: 242.02349853515625\n",
      "epoch: 13,  batch step: 73, loss: 12.765159606933594\n",
      "epoch: 13,  batch step: 74, loss: 9.038603782653809\n",
      "epoch: 13,  batch step: 75, loss: 99.10716247558594\n",
      "epoch: 13,  batch step: 76, loss: 158.2782440185547\n",
      "epoch: 13,  batch step: 77, loss: 21.2785701751709\n",
      "epoch: 13,  batch step: 78, loss: 244.06236267089844\n",
      "epoch: 13,  batch step: 79, loss: 164.35821533203125\n",
      "epoch: 13,  batch step: 80, loss: 255.95762634277344\n",
      "epoch: 13,  batch step: 81, loss: 86.07103729248047\n",
      "epoch: 13,  batch step: 82, loss: 49.1905632019043\n",
      "epoch: 13,  batch step: 83, loss: 24.29499053955078\n",
      "epoch: 13,  batch step: 84, loss: 128.13909912109375\n",
      "epoch: 13,  batch step: 85, loss: 62.87700653076172\n",
      "epoch: 13,  batch step: 86, loss: 207.52964782714844\n",
      "epoch: 13,  batch step: 87, loss: 40.09507751464844\n",
      "epoch: 13,  batch step: 88, loss: 32.561012268066406\n",
      "epoch: 13,  batch step: 89, loss: 15.588225364685059\n",
      "epoch: 13,  batch step: 90, loss: 12.662615776062012\n",
      "epoch: 13,  batch step: 91, loss: 68.44444274902344\n",
      "epoch: 13,  batch step: 92, loss: 37.92384719848633\n",
      "epoch: 13,  batch step: 93, loss: 30.149444580078125\n",
      "epoch: 13,  batch step: 94, loss: 44.028778076171875\n",
      "epoch: 13,  batch step: 95, loss: 11.665082931518555\n",
      "epoch: 13,  batch step: 96, loss: 16.811904907226562\n",
      "epoch: 13,  batch step: 97, loss: 174.29454040527344\n",
      "epoch: 13,  batch step: 98, loss: 58.69846725463867\n",
      "epoch: 13,  batch step: 99, loss: 169.8334503173828\n",
      "epoch: 13,  batch step: 100, loss: 91.71663665771484\n",
      "epoch: 13,  batch step: 101, loss: 11.316276550292969\n",
      "epoch: 13,  batch step: 102, loss: 6.555251598358154\n",
      "epoch: 13,  batch step: 103, loss: 34.63069534301758\n",
      "epoch: 13,  batch step: 104, loss: 9.21511459350586\n",
      "epoch: 13,  batch step: 105, loss: 111.68560028076172\n",
      "epoch: 13,  batch step: 106, loss: 90.0038070678711\n",
      "epoch: 13,  batch step: 107, loss: 9.052879333496094\n",
      "epoch: 13,  batch step: 108, loss: 6.4360456466674805\n",
      "epoch: 13,  batch step: 109, loss: 30.983457565307617\n",
      "epoch: 13,  batch step: 110, loss: 19.821086883544922\n",
      "epoch: 13,  batch step: 111, loss: 37.93019104003906\n",
      "epoch: 13,  batch step: 112, loss: 16.828968048095703\n",
      "epoch: 13,  batch step: 113, loss: 150.67263793945312\n",
      "epoch: 13,  batch step: 114, loss: 121.68077087402344\n",
      "epoch: 13,  batch step: 115, loss: 39.56524658203125\n",
      "epoch: 13,  batch step: 116, loss: 19.04817771911621\n",
      "epoch: 13,  batch step: 117, loss: 178.1737518310547\n",
      "epoch: 13,  batch step: 118, loss: 8.993906021118164\n",
      "epoch: 13,  batch step: 119, loss: 41.378536224365234\n",
      "epoch: 13,  batch step: 120, loss: 5.861140251159668\n",
      "epoch: 13,  batch step: 121, loss: 95.93464660644531\n",
      "epoch: 13,  batch step: 122, loss: 24.81633758544922\n",
      "epoch: 13,  batch step: 123, loss: 20.060527801513672\n",
      "epoch: 13,  batch step: 124, loss: 9.116992950439453\n",
      "epoch: 13,  batch step: 125, loss: 13.421140670776367\n",
      "epoch: 13,  batch step: 126, loss: 15.424697875976562\n",
      "epoch: 13,  batch step: 127, loss: 315.083251953125\n",
      "epoch: 13,  batch step: 128, loss: 41.07206344604492\n",
      "epoch: 13,  batch step: 129, loss: 33.828975677490234\n",
      "epoch: 13,  batch step: 130, loss: 52.961299896240234\n",
      "epoch: 13,  batch step: 131, loss: 41.87049865722656\n",
      "epoch: 13,  batch step: 132, loss: 74.9548568725586\n",
      "epoch: 13,  batch step: 133, loss: 100.898681640625\n",
      "epoch: 13,  batch step: 134, loss: 7.021554470062256\n",
      "epoch: 13,  batch step: 135, loss: 18.097394943237305\n",
      "epoch: 13,  batch step: 136, loss: 221.6409149169922\n",
      "epoch: 13,  batch step: 137, loss: 16.991146087646484\n",
      "epoch: 13,  batch step: 138, loss: 30.1790828704834\n",
      "epoch: 13,  batch step: 139, loss: 20.58310890197754\n",
      "epoch: 13,  batch step: 140, loss: 9.342535018920898\n",
      "epoch: 13,  batch step: 141, loss: 154.36309814453125\n",
      "epoch: 13,  batch step: 142, loss: 7.859877586364746\n",
      "epoch: 13,  batch step: 143, loss: 26.403926849365234\n",
      "epoch: 13,  batch step: 144, loss: 17.208274841308594\n",
      "epoch: 13,  batch step: 145, loss: 30.364484786987305\n",
      "epoch: 13,  batch step: 146, loss: 51.47618865966797\n",
      "epoch: 13,  batch step: 147, loss: 213.29763793945312\n",
      "epoch: 13,  batch step: 148, loss: 148.76980590820312\n",
      "epoch: 13,  batch step: 149, loss: 74.38115692138672\n",
      "epoch: 13,  batch step: 150, loss: 74.86974334716797\n",
      "epoch: 13,  batch step: 151, loss: 6.4959716796875\n",
      "epoch: 13,  batch step: 152, loss: 300.8021240234375\n",
      "epoch: 13,  batch step: 153, loss: 43.00901794433594\n",
      "epoch: 13,  batch step: 154, loss: 59.90513229370117\n",
      "epoch: 13,  batch step: 155, loss: 47.97715377807617\n",
      "epoch: 13,  batch step: 156, loss: 34.44039535522461\n",
      "epoch: 13,  batch step: 157, loss: 81.4668960571289\n",
      "epoch: 13,  batch step: 158, loss: 70.34418487548828\n",
      "epoch: 13,  batch step: 159, loss: 26.719327926635742\n",
      "epoch: 13,  batch step: 160, loss: 9.899985313415527\n",
      "epoch: 13,  batch step: 161, loss: 12.381326675415039\n",
      "epoch: 13,  batch step: 162, loss: 23.135160446166992\n",
      "epoch: 13,  batch step: 163, loss: 9.439562797546387\n",
      "epoch: 13,  batch step: 164, loss: 46.13712692260742\n",
      "epoch: 13,  batch step: 165, loss: 86.16426849365234\n",
      "epoch: 13,  batch step: 166, loss: 10.221393585205078\n",
      "epoch: 13,  batch step: 167, loss: 128.37841796875\n",
      "epoch: 13,  batch step: 168, loss: 13.288634300231934\n",
      "epoch: 13,  batch step: 169, loss: 66.53970336914062\n",
      "epoch: 13,  batch step: 170, loss: 136.64144897460938\n",
      "epoch: 13,  batch step: 171, loss: 100.03298950195312\n",
      "epoch: 13,  batch step: 172, loss: 179.87603759765625\n",
      "epoch: 13,  batch step: 173, loss: 134.29946899414062\n",
      "epoch: 13,  batch step: 174, loss: 10.518180847167969\n",
      "epoch: 13,  batch step: 175, loss: 65.4779052734375\n",
      "epoch: 13,  batch step: 176, loss: 9.432853698730469\n",
      "epoch: 13,  batch step: 177, loss: 18.424793243408203\n",
      "epoch: 13,  batch step: 178, loss: 97.70347595214844\n",
      "epoch: 13,  batch step: 179, loss: 23.006526947021484\n",
      "epoch: 13,  batch step: 180, loss: 75.17507934570312\n",
      "epoch: 13,  batch step: 181, loss: 63.32343292236328\n",
      "epoch: 13,  batch step: 182, loss: 16.140670776367188\n",
      "epoch: 13,  batch step: 183, loss: 60.092247009277344\n",
      "epoch: 13,  batch step: 184, loss: 26.429157257080078\n",
      "epoch: 13,  batch step: 185, loss: 177.599609375\n",
      "epoch: 13,  batch step: 186, loss: 8.210636138916016\n",
      "epoch: 13,  batch step: 187, loss: 16.102252960205078\n",
      "epoch: 13,  batch step: 188, loss: 11.567296981811523\n",
      "epoch: 13,  batch step: 189, loss: 66.30839538574219\n",
      "epoch: 13,  batch step: 190, loss: 107.09001159667969\n",
      "epoch: 13,  batch step: 191, loss: 80.04061889648438\n",
      "epoch: 13,  batch step: 192, loss: 115.44050598144531\n",
      "epoch: 13,  batch step: 193, loss: 36.77681350708008\n",
      "epoch: 13,  batch step: 194, loss: 156.86929321289062\n",
      "epoch: 13,  batch step: 195, loss: 49.04575729370117\n",
      "epoch: 13,  batch step: 196, loss: 55.517173767089844\n",
      "epoch: 13,  batch step: 197, loss: 69.76007080078125\n",
      "epoch: 13,  batch step: 198, loss: 28.977783203125\n",
      "epoch: 13,  batch step: 199, loss: 50.4675407409668\n",
      "epoch: 13,  batch step: 200, loss: 57.48968505859375\n",
      "epoch: 13,  batch step: 201, loss: 25.28999137878418\n",
      "epoch: 13,  batch step: 202, loss: 9.291735649108887\n",
      "epoch: 13,  batch step: 203, loss: 7.869277000427246\n",
      "epoch: 13,  batch step: 204, loss: 37.99221420288086\n",
      "epoch: 13,  batch step: 205, loss: 16.16280174255371\n",
      "epoch: 13,  batch step: 206, loss: 13.99736213684082\n",
      "epoch: 13,  batch step: 207, loss: 16.45064926147461\n",
      "epoch: 13,  batch step: 208, loss: 7.3003034591674805\n",
      "epoch: 13,  batch step: 209, loss: 6.287057876586914\n",
      "epoch: 13,  batch step: 210, loss: 8.556877136230469\n",
      "epoch: 13,  batch step: 211, loss: 31.406551361083984\n",
      "epoch: 13,  batch step: 212, loss: 83.95452117919922\n",
      "epoch: 13,  batch step: 213, loss: 3.134054183959961\n",
      "epoch: 13,  batch step: 214, loss: 155.88665771484375\n",
      "epoch: 13,  batch step: 215, loss: 14.713915824890137\n",
      "epoch: 13,  batch step: 216, loss: 5.033032417297363\n",
      "epoch: 13,  batch step: 217, loss: 28.57998275756836\n",
      "epoch: 13,  batch step: 218, loss: 5.070643424987793\n",
      "epoch: 13,  batch step: 219, loss: 29.979650497436523\n",
      "epoch: 13,  batch step: 220, loss: 91.99156951904297\n",
      "epoch: 13,  batch step: 221, loss: 6.9002532958984375\n",
      "epoch: 13,  batch step: 222, loss: 194.92623901367188\n",
      "epoch: 13,  batch step: 223, loss: 110.41114807128906\n",
      "epoch: 13,  batch step: 224, loss: 21.94393539428711\n",
      "epoch: 13,  batch step: 225, loss: 13.4529447555542\n",
      "epoch: 13,  batch step: 226, loss: 6.723091125488281\n",
      "epoch: 13,  batch step: 227, loss: 445.89385986328125\n",
      "epoch: 13,  batch step: 228, loss: 6.344635963439941\n",
      "epoch: 13,  batch step: 229, loss: 143.57943725585938\n",
      "epoch: 13,  batch step: 230, loss: 14.732418060302734\n",
      "epoch: 13,  batch step: 231, loss: 104.52422332763672\n",
      "epoch: 13,  batch step: 232, loss: 30.506561279296875\n",
      "epoch: 13,  batch step: 233, loss: 55.61369323730469\n",
      "epoch: 13,  batch step: 234, loss: 42.01203155517578\n",
      "epoch: 13,  batch step: 235, loss: 103.5162353515625\n",
      "epoch: 13,  batch step: 236, loss: 50.3275260925293\n",
      "epoch: 13,  batch step: 237, loss: 36.946834564208984\n",
      "epoch: 13,  batch step: 238, loss: 162.94500732421875\n",
      "epoch: 13,  batch step: 239, loss: 137.81654357910156\n",
      "epoch: 13,  batch step: 240, loss: 52.44942855834961\n",
      "epoch: 13,  batch step: 241, loss: 26.008094787597656\n",
      "epoch: 13,  batch step: 242, loss: 10.120723724365234\n",
      "epoch: 13,  batch step: 243, loss: 31.98340606689453\n",
      "epoch: 13,  batch step: 244, loss: 36.3588752746582\n",
      "epoch: 13,  batch step: 245, loss: 247.0025634765625\n",
      "epoch: 13,  batch step: 246, loss: 147.09274291992188\n",
      "epoch: 13,  batch step: 247, loss: 348.30340576171875\n",
      "epoch: 13,  batch step: 248, loss: 16.49695587158203\n",
      "epoch: 13,  batch step: 249, loss: 50.0354118347168\n",
      "epoch: 13,  batch step: 250, loss: 15.827552795410156\n",
      "epoch: 13,  batch step: 251, loss: 85.7761001586914\n",
      "validation error epoch  13:    tensor(75.1620, device='cuda:0')\n",
      "316\n",
      "epoch: 14,  batch step: 0, loss: 18.705368041992188\n",
      "epoch: 14,  batch step: 1, loss: 20.149953842163086\n",
      "epoch: 14,  batch step: 2, loss: 357.02685546875\n",
      "epoch: 14,  batch step: 3, loss: 38.23152160644531\n",
      "epoch: 14,  batch step: 4, loss: 52.65914535522461\n",
      "epoch: 14,  batch step: 5, loss: 89.088623046875\n",
      "epoch: 14,  batch step: 6, loss: 212.25177001953125\n",
      "epoch: 14,  batch step: 7, loss: 43.89922332763672\n",
      "epoch: 14,  batch step: 8, loss: 129.8136444091797\n",
      "epoch: 14,  batch step: 9, loss: 20.578609466552734\n",
      "epoch: 14,  batch step: 10, loss: 87.31965637207031\n",
      "epoch: 14,  batch step: 11, loss: 16.816322326660156\n",
      "epoch: 14,  batch step: 12, loss: 11.927630424499512\n",
      "epoch: 14,  batch step: 13, loss: 24.80800437927246\n",
      "epoch: 14,  batch step: 14, loss: 168.18251037597656\n",
      "epoch: 14,  batch step: 15, loss: 14.000419616699219\n",
      "epoch: 14,  batch step: 16, loss: 194.75228881835938\n",
      "epoch: 14,  batch step: 17, loss: 123.28950500488281\n",
      "epoch: 14,  batch step: 18, loss: 10.127662658691406\n",
      "epoch: 14,  batch step: 19, loss: 26.46217918395996\n",
      "epoch: 14,  batch step: 20, loss: 28.025588989257812\n",
      "epoch: 14,  batch step: 21, loss: 39.727081298828125\n",
      "epoch: 14,  batch step: 22, loss: 90.15997314453125\n",
      "epoch: 14,  batch step: 23, loss: 14.28306770324707\n",
      "epoch: 14,  batch step: 24, loss: 22.030441284179688\n",
      "epoch: 14,  batch step: 25, loss: 80.25025939941406\n",
      "epoch: 14,  batch step: 26, loss: 43.33667755126953\n",
      "epoch: 14,  batch step: 27, loss: 11.757814407348633\n",
      "epoch: 14,  batch step: 28, loss: 388.01904296875\n",
      "epoch: 14,  batch step: 29, loss: 124.14018249511719\n",
      "epoch: 14,  batch step: 30, loss: 7.7929487228393555\n",
      "epoch: 14,  batch step: 31, loss: 11.921065330505371\n",
      "epoch: 14,  batch step: 32, loss: 10.341817855834961\n",
      "epoch: 14,  batch step: 33, loss: 17.062515258789062\n",
      "epoch: 14,  batch step: 34, loss: 82.41637420654297\n",
      "epoch: 14,  batch step: 35, loss: 255.6327362060547\n",
      "epoch: 14,  batch step: 36, loss: 53.96488571166992\n",
      "epoch: 14,  batch step: 37, loss: 11.61811637878418\n",
      "epoch: 14,  batch step: 38, loss: 202.52117919921875\n",
      "epoch: 14,  batch step: 39, loss: 33.44608688354492\n",
      "epoch: 14,  batch step: 40, loss: 22.39082908630371\n",
      "epoch: 14,  batch step: 41, loss: 23.524410247802734\n",
      "epoch: 14,  batch step: 42, loss: 8.913995742797852\n",
      "epoch: 14,  batch step: 43, loss: 13.183000564575195\n",
      "epoch: 14,  batch step: 44, loss: 42.99268341064453\n",
      "epoch: 14,  batch step: 45, loss: 71.52881622314453\n",
      "epoch: 14,  batch step: 46, loss: 346.738525390625\n",
      "epoch: 14,  batch step: 47, loss: 100.31805419921875\n",
      "epoch: 14,  batch step: 48, loss: 15.368361473083496\n",
      "epoch: 14,  batch step: 49, loss: 9.182989120483398\n",
      "epoch: 14,  batch step: 50, loss: 80.80560302734375\n",
      "epoch: 14,  batch step: 51, loss: 27.27735137939453\n",
      "epoch: 14,  batch step: 52, loss: 10.16114330291748\n",
      "epoch: 14,  batch step: 53, loss: 13.772326469421387\n",
      "epoch: 14,  batch step: 54, loss: 36.3659782409668\n",
      "epoch: 14,  batch step: 55, loss: 14.58504867553711\n",
      "epoch: 14,  batch step: 56, loss: 106.15203094482422\n",
      "epoch: 14,  batch step: 57, loss: 132.49732971191406\n",
      "epoch: 14,  batch step: 58, loss: 8.6175537109375\n",
      "epoch: 14,  batch step: 59, loss: 35.91877746582031\n",
      "epoch: 14,  batch step: 60, loss: 54.98223876953125\n",
      "epoch: 14,  batch step: 61, loss: 78.57450866699219\n",
      "epoch: 14,  batch step: 62, loss: 55.33238983154297\n",
      "epoch: 14,  batch step: 63, loss: 43.160858154296875\n",
      "epoch: 14,  batch step: 64, loss: 11.723442077636719\n",
      "epoch: 14,  batch step: 65, loss: 14.85007381439209\n",
      "epoch: 14,  batch step: 66, loss: 7.668700218200684\n",
      "epoch: 14,  batch step: 67, loss: 12.861610412597656\n",
      "epoch: 14,  batch step: 68, loss: 230.82528686523438\n",
      "epoch: 14,  batch step: 69, loss: 53.921142578125\n",
      "epoch: 14,  batch step: 70, loss: 8.153676986694336\n",
      "epoch: 14,  batch step: 71, loss: 23.628597259521484\n",
      "epoch: 14,  batch step: 72, loss: 173.43008422851562\n",
      "epoch: 14,  batch step: 73, loss: 23.528545379638672\n",
      "epoch: 14,  batch step: 74, loss: 147.9964141845703\n",
      "epoch: 14,  batch step: 75, loss: 9.790717124938965\n",
      "epoch: 14,  batch step: 76, loss: 124.36170959472656\n",
      "epoch: 14,  batch step: 77, loss: 14.697230339050293\n",
      "epoch: 14,  batch step: 78, loss: 36.86994171142578\n",
      "epoch: 14,  batch step: 79, loss: 152.89410400390625\n",
      "epoch: 14,  batch step: 80, loss: 11.450284957885742\n",
      "epoch: 14,  batch step: 81, loss: 23.906208038330078\n",
      "epoch: 14,  batch step: 82, loss: 115.41523742675781\n",
      "epoch: 14,  batch step: 83, loss: 85.18757629394531\n",
      "epoch: 14,  batch step: 84, loss: 10.133596420288086\n",
      "epoch: 14,  batch step: 85, loss: 100.44927978515625\n",
      "epoch: 14,  batch step: 86, loss: 252.07516479492188\n",
      "epoch: 14,  batch step: 87, loss: 53.95649719238281\n",
      "epoch: 14,  batch step: 88, loss: 78.04148864746094\n",
      "epoch: 14,  batch step: 89, loss: 49.0056266784668\n",
      "epoch: 14,  batch step: 90, loss: 12.244833946228027\n",
      "epoch: 14,  batch step: 91, loss: 36.711769104003906\n",
      "epoch: 14,  batch step: 92, loss: 48.237709045410156\n",
      "epoch: 14,  batch step: 93, loss: 10.326217651367188\n",
      "epoch: 14,  batch step: 94, loss: 8.987459182739258\n",
      "epoch: 14,  batch step: 95, loss: 11.05872917175293\n",
      "epoch: 14,  batch step: 96, loss: 93.87728881835938\n",
      "epoch: 14,  batch step: 97, loss: 130.61534118652344\n",
      "epoch: 14,  batch step: 98, loss: 135.47811889648438\n",
      "epoch: 14,  batch step: 99, loss: 236.1464080810547\n",
      "epoch: 14,  batch step: 100, loss: 22.633161544799805\n",
      "epoch: 14,  batch step: 101, loss: 13.598945617675781\n",
      "epoch: 14,  batch step: 102, loss: 74.19957733154297\n",
      "epoch: 14,  batch step: 103, loss: 107.023681640625\n",
      "epoch: 14,  batch step: 104, loss: 26.724897384643555\n",
      "epoch: 14,  batch step: 105, loss: 60.20769500732422\n",
      "epoch: 14,  batch step: 106, loss: 47.36896896362305\n",
      "epoch: 14,  batch step: 107, loss: 106.13259887695312\n",
      "epoch: 14,  batch step: 108, loss: 45.277252197265625\n",
      "epoch: 14,  batch step: 109, loss: 59.56542205810547\n",
      "epoch: 14,  batch step: 110, loss: 20.521432876586914\n",
      "epoch: 14,  batch step: 111, loss: 125.67048645019531\n",
      "epoch: 14,  batch step: 112, loss: 24.604351043701172\n",
      "epoch: 14,  batch step: 113, loss: 294.32769775390625\n",
      "epoch: 14,  batch step: 114, loss: 52.87333297729492\n",
      "epoch: 14,  batch step: 115, loss: 11.023425102233887\n",
      "epoch: 14,  batch step: 116, loss: 39.7762565612793\n",
      "epoch: 14,  batch step: 117, loss: 21.648155212402344\n",
      "epoch: 14,  batch step: 118, loss: 22.883071899414062\n",
      "epoch: 14,  batch step: 119, loss: 25.001258850097656\n",
      "epoch: 14,  batch step: 120, loss: 17.935874938964844\n",
      "epoch: 14,  batch step: 121, loss: 35.186012268066406\n",
      "epoch: 14,  batch step: 122, loss: 52.98241424560547\n",
      "epoch: 14,  batch step: 123, loss: 158.6814422607422\n",
      "epoch: 14,  batch step: 124, loss: 36.04534149169922\n",
      "epoch: 14,  batch step: 125, loss: 17.88034439086914\n",
      "epoch: 14,  batch step: 126, loss: 9.908761978149414\n",
      "epoch: 14,  batch step: 127, loss: 9.971293449401855\n",
      "epoch: 14,  batch step: 128, loss: 17.415647506713867\n",
      "epoch: 14,  batch step: 129, loss: 57.62384796142578\n",
      "epoch: 14,  batch step: 130, loss: 196.75193786621094\n",
      "epoch: 14,  batch step: 131, loss: 253.27554321289062\n",
      "epoch: 14,  batch step: 132, loss: 22.805416107177734\n",
      "epoch: 14,  batch step: 133, loss: 284.5296325683594\n",
      "epoch: 14,  batch step: 134, loss: 33.800926208496094\n",
      "epoch: 14,  batch step: 135, loss: 12.88525390625\n",
      "epoch: 14,  batch step: 136, loss: 17.045337677001953\n",
      "epoch: 14,  batch step: 137, loss: 13.945123672485352\n",
      "epoch: 14,  batch step: 138, loss: 31.41327476501465\n",
      "epoch: 14,  batch step: 139, loss: 52.358558654785156\n",
      "epoch: 14,  batch step: 140, loss: 14.499212265014648\n",
      "epoch: 14,  batch step: 141, loss: 12.814310073852539\n",
      "epoch: 14,  batch step: 142, loss: 8.483566284179688\n",
      "epoch: 14,  batch step: 143, loss: 21.658599853515625\n",
      "epoch: 14,  batch step: 144, loss: 686.412109375\n",
      "epoch: 14,  batch step: 145, loss: 17.32820701599121\n",
      "epoch: 14,  batch step: 146, loss: 48.42393112182617\n",
      "epoch: 14,  batch step: 147, loss: 12.7726469039917\n",
      "epoch: 14,  batch step: 148, loss: 14.065597534179688\n",
      "epoch: 14,  batch step: 149, loss: 35.229881286621094\n",
      "epoch: 14,  batch step: 150, loss: 22.049095153808594\n",
      "epoch: 14,  batch step: 151, loss: 107.9866943359375\n",
      "epoch: 14,  batch step: 152, loss: 10.872370719909668\n",
      "epoch: 14,  batch step: 153, loss: 12.721701622009277\n",
      "epoch: 14,  batch step: 154, loss: 25.956966400146484\n",
      "epoch: 14,  batch step: 155, loss: 85.92514038085938\n",
      "epoch: 14,  batch step: 156, loss: 63.495849609375\n",
      "epoch: 14,  batch step: 157, loss: 101.1610336303711\n",
      "epoch: 14,  batch step: 158, loss: 135.96356201171875\n",
      "epoch: 14,  batch step: 159, loss: 20.496509552001953\n",
      "epoch: 14,  batch step: 160, loss: 19.201045989990234\n",
      "epoch: 14,  batch step: 161, loss: 9.485754013061523\n",
      "epoch: 14,  batch step: 162, loss: 31.214923858642578\n",
      "epoch: 14,  batch step: 163, loss: 58.687469482421875\n",
      "epoch: 14,  batch step: 164, loss: 93.39082336425781\n",
      "epoch: 14,  batch step: 165, loss: 17.687416076660156\n",
      "epoch: 14,  batch step: 166, loss: 34.248046875\n",
      "epoch: 14,  batch step: 167, loss: 68.77117919921875\n",
      "epoch: 14,  batch step: 168, loss: 119.38204193115234\n",
      "epoch: 14,  batch step: 169, loss: 27.61141586303711\n",
      "epoch: 14,  batch step: 170, loss: 12.686941146850586\n",
      "epoch: 14,  batch step: 171, loss: 25.461959838867188\n",
      "epoch: 14,  batch step: 172, loss: 17.078935623168945\n",
      "epoch: 14,  batch step: 173, loss: 11.584189414978027\n",
      "epoch: 14,  batch step: 174, loss: 63.85074996948242\n",
      "epoch: 14,  batch step: 175, loss: 8.415733337402344\n",
      "epoch: 14,  batch step: 176, loss: 15.80860424041748\n",
      "epoch: 14,  batch step: 177, loss: 320.3663330078125\n",
      "epoch: 14,  batch step: 178, loss: 12.605047225952148\n",
      "epoch: 14,  batch step: 179, loss: 148.38343811035156\n",
      "epoch: 14,  batch step: 180, loss: 42.942176818847656\n",
      "epoch: 14,  batch step: 181, loss: 171.38644409179688\n",
      "epoch: 14,  batch step: 182, loss: 42.423885345458984\n",
      "epoch: 14,  batch step: 183, loss: 18.641021728515625\n",
      "epoch: 14,  batch step: 184, loss: 24.170894622802734\n",
      "epoch: 14,  batch step: 185, loss: 42.1943359375\n",
      "epoch: 14,  batch step: 186, loss: 183.49655151367188\n",
      "epoch: 14,  batch step: 187, loss: 7.2174601554870605\n",
      "epoch: 14,  batch step: 188, loss: 11.302558898925781\n",
      "epoch: 14,  batch step: 189, loss: 21.64607810974121\n",
      "epoch: 14,  batch step: 190, loss: 8.482524871826172\n",
      "epoch: 14,  batch step: 191, loss: 12.165685653686523\n",
      "epoch: 14,  batch step: 192, loss: 33.55520248413086\n",
      "epoch: 14,  batch step: 193, loss: 14.990069389343262\n",
      "epoch: 14,  batch step: 194, loss: 12.97492790222168\n",
      "epoch: 14,  batch step: 195, loss: 42.45401382446289\n",
      "epoch: 14,  batch step: 196, loss: 9.26775074005127\n",
      "epoch: 14,  batch step: 197, loss: 212.37704467773438\n",
      "epoch: 14,  batch step: 198, loss: 64.04881286621094\n",
      "epoch: 14,  batch step: 199, loss: 31.185195922851562\n",
      "epoch: 14,  batch step: 200, loss: 9.200357437133789\n",
      "epoch: 14,  batch step: 201, loss: 115.05923461914062\n",
      "epoch: 14,  batch step: 202, loss: 10.464152336120605\n",
      "epoch: 14,  batch step: 203, loss: 8.659937858581543\n",
      "epoch: 14,  batch step: 204, loss: 42.04948425292969\n",
      "epoch: 14,  batch step: 205, loss: 57.75090026855469\n",
      "epoch: 14,  batch step: 206, loss: 151.3956298828125\n",
      "epoch: 14,  batch step: 207, loss: 11.217620849609375\n",
      "epoch: 14,  batch step: 208, loss: 14.422943115234375\n",
      "epoch: 14,  batch step: 209, loss: 5.925500869750977\n",
      "epoch: 14,  batch step: 210, loss: 13.940691947937012\n",
      "epoch: 14,  batch step: 211, loss: 73.22660827636719\n",
      "epoch: 14,  batch step: 212, loss: 9.318714141845703\n",
      "epoch: 14,  batch step: 213, loss: 11.593256950378418\n",
      "epoch: 14,  batch step: 214, loss: 159.7784423828125\n",
      "epoch: 14,  batch step: 215, loss: 57.23676681518555\n",
      "epoch: 14,  batch step: 216, loss: 216.49008178710938\n",
      "epoch: 14,  batch step: 217, loss: 219.81600952148438\n",
      "epoch: 14,  batch step: 218, loss: 53.753623962402344\n",
      "epoch: 14,  batch step: 219, loss: 22.392803192138672\n",
      "epoch: 14,  batch step: 220, loss: 380.8882751464844\n",
      "epoch: 14,  batch step: 221, loss: 49.988121032714844\n",
      "epoch: 14,  batch step: 222, loss: 133.5904541015625\n",
      "epoch: 14,  batch step: 223, loss: 106.78692626953125\n",
      "epoch: 14,  batch step: 224, loss: 103.44230651855469\n",
      "epoch: 14,  batch step: 225, loss: 181.29513549804688\n",
      "epoch: 14,  batch step: 226, loss: 106.6389389038086\n",
      "epoch: 14,  batch step: 227, loss: 63.884552001953125\n",
      "epoch: 14,  batch step: 228, loss: 79.83190155029297\n",
      "epoch: 14,  batch step: 229, loss: 65.09739685058594\n",
      "epoch: 14,  batch step: 230, loss: 65.4686050415039\n",
      "epoch: 14,  batch step: 231, loss: 135.6118927001953\n",
      "epoch: 14,  batch step: 232, loss: 25.159637451171875\n",
      "epoch: 14,  batch step: 233, loss: 23.3580322265625\n",
      "epoch: 14,  batch step: 234, loss: 142.14535522460938\n",
      "epoch: 14,  batch step: 235, loss: 228.79470825195312\n",
      "epoch: 14,  batch step: 236, loss: 23.77712631225586\n",
      "epoch: 14,  batch step: 237, loss: 18.95966148376465\n",
      "epoch: 14,  batch step: 238, loss: 14.605006217956543\n",
      "epoch: 14,  batch step: 239, loss: 22.44168472290039\n",
      "epoch: 14,  batch step: 240, loss: 13.298121452331543\n",
      "epoch: 14,  batch step: 241, loss: 48.7603759765625\n",
      "epoch: 14,  batch step: 242, loss: 74.70352935791016\n",
      "epoch: 14,  batch step: 243, loss: 54.95050811767578\n",
      "epoch: 14,  batch step: 244, loss: 8.22652816772461\n",
      "epoch: 14,  batch step: 245, loss: 13.265266418457031\n",
      "epoch: 14,  batch step: 246, loss: 49.29302215576172\n",
      "epoch: 14,  batch step: 247, loss: 31.316707611083984\n",
      "epoch: 14,  batch step: 248, loss: 164.7059326171875\n",
      "epoch: 14,  batch step: 249, loss: 12.888717651367188\n",
      "epoch: 14,  batch step: 250, loss: 15.489397048950195\n",
      "epoch: 14,  batch step: 251, loss: 168.52308654785156\n",
      "validation error epoch  14:    tensor(72.3911, device='cuda:0')\n",
      "316\n",
      "epoch: 15,  batch step: 0, loss: 172.18673706054688\n",
      "epoch: 15,  batch step: 1, loss: 93.1520004272461\n",
      "epoch: 15,  batch step: 2, loss: 43.44727325439453\n",
      "epoch: 15,  batch step: 3, loss: 26.41957664489746\n",
      "epoch: 15,  batch step: 4, loss: 137.97976684570312\n",
      "epoch: 15,  batch step: 5, loss: 24.071516036987305\n",
      "epoch: 15,  batch step: 6, loss: 13.310478210449219\n",
      "epoch: 15,  batch step: 7, loss: 19.3372802734375\n",
      "epoch: 15,  batch step: 8, loss: 136.43203735351562\n",
      "epoch: 15,  batch step: 9, loss: 14.544242858886719\n",
      "epoch: 15,  batch step: 10, loss: 15.573648452758789\n",
      "epoch: 15,  batch step: 11, loss: 16.685848236083984\n",
      "epoch: 15,  batch step: 12, loss: 150.5805206298828\n",
      "epoch: 15,  batch step: 13, loss: 16.12204933166504\n",
      "epoch: 15,  batch step: 14, loss: 13.482025146484375\n",
      "epoch: 15,  batch step: 15, loss: 373.3290710449219\n",
      "epoch: 15,  batch step: 16, loss: 27.994705200195312\n",
      "epoch: 15,  batch step: 17, loss: 102.97897338867188\n",
      "epoch: 15,  batch step: 18, loss: 126.6500473022461\n",
      "epoch: 15,  batch step: 19, loss: 10.854331016540527\n",
      "epoch: 15,  batch step: 20, loss: 32.86419677734375\n",
      "epoch: 15,  batch step: 21, loss: 32.430389404296875\n",
      "epoch: 15,  batch step: 22, loss: 18.032974243164062\n",
      "epoch: 15,  batch step: 23, loss: 29.648534774780273\n",
      "epoch: 15,  batch step: 24, loss: 89.43859100341797\n",
      "epoch: 15,  batch step: 25, loss: 58.70191955566406\n",
      "epoch: 15,  batch step: 26, loss: 17.01312255859375\n",
      "epoch: 15,  batch step: 27, loss: 19.967151641845703\n",
      "epoch: 15,  batch step: 28, loss: 8.352155685424805\n",
      "epoch: 15,  batch step: 29, loss: 24.36104965209961\n",
      "epoch: 15,  batch step: 30, loss: 17.994638442993164\n",
      "epoch: 15,  batch step: 31, loss: 16.515344619750977\n",
      "epoch: 15,  batch step: 32, loss: 126.31584930419922\n",
      "epoch: 15,  batch step: 33, loss: 52.781532287597656\n",
      "epoch: 15,  batch step: 34, loss: 78.4918212890625\n",
      "epoch: 15,  batch step: 35, loss: 13.343498229980469\n",
      "epoch: 15,  batch step: 36, loss: 58.55411911010742\n",
      "epoch: 15,  batch step: 37, loss: 64.61734008789062\n",
      "epoch: 15,  batch step: 38, loss: 11.274127006530762\n",
      "epoch: 15,  batch step: 39, loss: 24.289159774780273\n",
      "epoch: 15,  batch step: 40, loss: 47.4680290222168\n",
      "epoch: 15,  batch step: 41, loss: 36.04974365234375\n",
      "epoch: 15,  batch step: 42, loss: 17.466068267822266\n",
      "epoch: 15,  batch step: 43, loss: 66.50930786132812\n",
      "epoch: 15,  batch step: 44, loss: 19.61048698425293\n",
      "epoch: 15,  batch step: 45, loss: 15.693171501159668\n",
      "epoch: 15,  batch step: 46, loss: 128.75794982910156\n",
      "epoch: 15,  batch step: 47, loss: 24.238250732421875\n",
      "epoch: 15,  batch step: 48, loss: 234.1075439453125\n",
      "epoch: 15,  batch step: 49, loss: 41.85144805908203\n",
      "epoch: 15,  batch step: 50, loss: 106.21788787841797\n",
      "epoch: 15,  batch step: 51, loss: 29.090818405151367\n",
      "epoch: 15,  batch step: 52, loss: 142.00653076171875\n",
      "epoch: 15,  batch step: 53, loss: 33.091304779052734\n",
      "epoch: 15,  batch step: 54, loss: 10.529550552368164\n",
      "epoch: 15,  batch step: 55, loss: 84.06007385253906\n",
      "epoch: 15,  batch step: 56, loss: 18.68053436279297\n",
      "epoch: 15,  batch step: 57, loss: 165.2283935546875\n",
      "epoch: 15,  batch step: 58, loss: 60.42688751220703\n",
      "epoch: 15,  batch step: 59, loss: 7.524662017822266\n",
      "epoch: 15,  batch step: 60, loss: 16.3638916015625\n",
      "epoch: 15,  batch step: 61, loss: 13.564452171325684\n",
      "epoch: 15,  batch step: 62, loss: 31.22553825378418\n",
      "epoch: 15,  batch step: 63, loss: 23.66537094116211\n",
      "epoch: 15,  batch step: 64, loss: 59.13035202026367\n",
      "epoch: 15,  batch step: 65, loss: 59.81501007080078\n",
      "epoch: 15,  batch step: 66, loss: 152.47901916503906\n",
      "epoch: 15,  batch step: 67, loss: 408.03436279296875\n",
      "epoch: 15,  batch step: 68, loss: 56.379417419433594\n",
      "epoch: 15,  batch step: 69, loss: 25.382673263549805\n",
      "epoch: 15,  batch step: 70, loss: 88.91607666015625\n",
      "epoch: 15,  batch step: 71, loss: 71.55794525146484\n",
      "epoch: 15,  batch step: 72, loss: 258.371826171875\n",
      "epoch: 15,  batch step: 73, loss: 23.97088623046875\n",
      "epoch: 15,  batch step: 74, loss: 22.03661346435547\n",
      "epoch: 15,  batch step: 75, loss: 19.351423263549805\n",
      "epoch: 15,  batch step: 76, loss: 29.06300926208496\n",
      "epoch: 15,  batch step: 77, loss: 229.84115600585938\n",
      "epoch: 15,  batch step: 78, loss: 30.17708396911621\n",
      "epoch: 15,  batch step: 79, loss: 202.98294067382812\n",
      "epoch: 15,  batch step: 80, loss: 9.150915145874023\n",
      "epoch: 15,  batch step: 81, loss: 23.033430099487305\n",
      "epoch: 15,  batch step: 82, loss: 28.237098693847656\n",
      "epoch: 15,  batch step: 83, loss: 53.239540100097656\n",
      "epoch: 15,  batch step: 84, loss: 85.76165008544922\n",
      "epoch: 15,  batch step: 85, loss: 75.46974182128906\n",
      "epoch: 15,  batch step: 86, loss: 9.604974746704102\n",
      "epoch: 15,  batch step: 87, loss: 13.76333999633789\n",
      "epoch: 15,  batch step: 88, loss: 17.80742645263672\n",
      "epoch: 15,  batch step: 89, loss: 75.61346435546875\n",
      "epoch: 15,  batch step: 90, loss: 13.720800399780273\n",
      "epoch: 15,  batch step: 91, loss: 51.68059539794922\n",
      "epoch: 15,  batch step: 92, loss: 93.4809799194336\n",
      "epoch: 15,  batch step: 93, loss: 21.261775970458984\n",
      "epoch: 15,  batch step: 94, loss: 151.29026794433594\n",
      "epoch: 15,  batch step: 95, loss: 15.064746856689453\n",
      "epoch: 15,  batch step: 96, loss: 86.55260467529297\n",
      "epoch: 15,  batch step: 97, loss: 18.806169509887695\n",
      "epoch: 15,  batch step: 98, loss: 107.39476776123047\n",
      "epoch: 15,  batch step: 99, loss: 175.1804656982422\n",
      "epoch: 15,  batch step: 100, loss: 199.2406463623047\n",
      "epoch: 15,  batch step: 101, loss: 14.147167205810547\n",
      "epoch: 15,  batch step: 102, loss: 4.435161590576172\n",
      "epoch: 15,  batch step: 103, loss: 12.31959342956543\n",
      "epoch: 15,  batch step: 104, loss: 11.35605239868164\n",
      "epoch: 15,  batch step: 105, loss: 18.907745361328125\n",
      "epoch: 15,  batch step: 106, loss: 7.111384391784668\n",
      "epoch: 15,  batch step: 107, loss: 11.057064056396484\n",
      "epoch: 15,  batch step: 108, loss: 19.04385757446289\n",
      "epoch: 15,  batch step: 109, loss: 45.33830261230469\n",
      "epoch: 15,  batch step: 110, loss: 9.990152359008789\n",
      "epoch: 15,  batch step: 111, loss: 92.11681365966797\n",
      "epoch: 15,  batch step: 112, loss: 37.140663146972656\n",
      "epoch: 15,  batch step: 113, loss: 40.251319885253906\n",
      "epoch: 15,  batch step: 114, loss: 16.182727813720703\n",
      "epoch: 15,  batch step: 115, loss: 10.440878868103027\n",
      "epoch: 15,  batch step: 116, loss: 14.06948471069336\n",
      "epoch: 15,  batch step: 117, loss: 6.436816215515137\n",
      "epoch: 15,  batch step: 118, loss: 43.13405990600586\n",
      "epoch: 15,  batch step: 119, loss: 21.379993438720703\n",
      "epoch: 15,  batch step: 120, loss: 39.407936096191406\n",
      "epoch: 15,  batch step: 121, loss: 16.032001495361328\n",
      "epoch: 15,  batch step: 122, loss: 4.9861626625061035\n",
      "epoch: 15,  batch step: 123, loss: 12.85097885131836\n",
      "epoch: 15,  batch step: 124, loss: 113.0655517578125\n",
      "epoch: 15,  batch step: 125, loss: 99.80842590332031\n",
      "epoch: 15,  batch step: 126, loss: 123.50218963623047\n",
      "epoch: 15,  batch step: 127, loss: 106.43199920654297\n",
      "epoch: 15,  batch step: 128, loss: 16.598770141601562\n",
      "epoch: 15,  batch step: 129, loss: 6.345672607421875\n",
      "epoch: 15,  batch step: 130, loss: 18.913951873779297\n",
      "epoch: 15,  batch step: 131, loss: 260.0065002441406\n",
      "epoch: 15,  batch step: 132, loss: 40.62210464477539\n",
      "epoch: 15,  batch step: 133, loss: 108.36766052246094\n",
      "epoch: 15,  batch step: 134, loss: 26.35561180114746\n",
      "epoch: 15,  batch step: 135, loss: 88.90776062011719\n",
      "epoch: 15,  batch step: 136, loss: 15.861989974975586\n",
      "epoch: 15,  batch step: 137, loss: 11.16904067993164\n",
      "epoch: 15,  batch step: 138, loss: 19.62335968017578\n",
      "epoch: 15,  batch step: 139, loss: 47.18947982788086\n",
      "epoch: 15,  batch step: 140, loss: 124.72941589355469\n",
      "epoch: 15,  batch step: 141, loss: 279.0760498046875\n",
      "epoch: 15,  batch step: 142, loss: 45.060359954833984\n",
      "epoch: 15,  batch step: 143, loss: 46.22697067260742\n",
      "epoch: 15,  batch step: 144, loss: 13.121108055114746\n",
      "epoch: 15,  batch step: 145, loss: 126.82879638671875\n",
      "epoch: 15,  batch step: 146, loss: 129.2423553466797\n",
      "epoch: 15,  batch step: 147, loss: 22.40448570251465\n",
      "epoch: 15,  batch step: 148, loss: 43.478477478027344\n",
      "epoch: 15,  batch step: 149, loss: 23.997726440429688\n",
      "epoch: 15,  batch step: 150, loss: 175.10606384277344\n",
      "epoch: 15,  batch step: 151, loss: 141.9503631591797\n",
      "epoch: 15,  batch step: 152, loss: 66.450927734375\n",
      "epoch: 15,  batch step: 153, loss: 26.189985275268555\n",
      "epoch: 15,  batch step: 154, loss: 146.49871826171875\n",
      "epoch: 15,  batch step: 155, loss: 111.63375091552734\n",
      "epoch: 15,  batch step: 156, loss: 16.091915130615234\n",
      "epoch: 15,  batch step: 157, loss: 9.833415985107422\n",
      "epoch: 15,  batch step: 158, loss: 12.9881591796875\n",
      "epoch: 15,  batch step: 159, loss: 40.38262939453125\n",
      "epoch: 15,  batch step: 160, loss: 54.579498291015625\n",
      "epoch: 15,  batch step: 161, loss: 22.40859603881836\n",
      "epoch: 15,  batch step: 162, loss: 48.46720504760742\n",
      "epoch: 15,  batch step: 163, loss: 22.63909149169922\n",
      "epoch: 15,  batch step: 164, loss: 140.79776000976562\n",
      "epoch: 15,  batch step: 165, loss: 5.675223350524902\n",
      "epoch: 15,  batch step: 166, loss: 100.57269287109375\n",
      "epoch: 15,  batch step: 167, loss: 18.837875366210938\n",
      "epoch: 15,  batch step: 168, loss: 26.199453353881836\n",
      "epoch: 15,  batch step: 169, loss: 131.10882568359375\n",
      "epoch: 15,  batch step: 170, loss: 19.178604125976562\n",
      "epoch: 15,  batch step: 171, loss: 55.62141036987305\n",
      "epoch: 15,  batch step: 172, loss: 133.7054443359375\n",
      "epoch: 15,  batch step: 173, loss: 25.1317195892334\n",
      "epoch: 15,  batch step: 174, loss: 47.6910400390625\n",
      "epoch: 15,  batch step: 175, loss: 151.72203063964844\n",
      "epoch: 15,  batch step: 176, loss: 14.293025970458984\n",
      "epoch: 15,  batch step: 177, loss: 88.06964111328125\n",
      "epoch: 15,  batch step: 178, loss: 88.95296478271484\n",
      "epoch: 15,  batch step: 179, loss: 103.6156234741211\n",
      "epoch: 15,  batch step: 180, loss: 76.86400604248047\n",
      "epoch: 15,  batch step: 181, loss: 197.46751403808594\n",
      "epoch: 15,  batch step: 182, loss: 171.99017333984375\n",
      "epoch: 15,  batch step: 183, loss: 38.70159912109375\n",
      "epoch: 15,  batch step: 184, loss: 74.03043365478516\n",
      "epoch: 15,  batch step: 185, loss: 42.1375732421875\n",
      "epoch: 15,  batch step: 186, loss: 26.14834976196289\n",
      "epoch: 15,  batch step: 187, loss: 77.8548583984375\n",
      "epoch: 15,  batch step: 188, loss: 99.84712219238281\n",
      "epoch: 15,  batch step: 189, loss: 91.12747192382812\n",
      "epoch: 15,  batch step: 190, loss: 13.94709300994873\n",
      "epoch: 15,  batch step: 191, loss: 27.385107040405273\n",
      "epoch: 15,  batch step: 192, loss: 199.971435546875\n",
      "epoch: 15,  batch step: 193, loss: 23.678396224975586\n",
      "epoch: 15,  batch step: 194, loss: 71.83402252197266\n",
      "epoch: 15,  batch step: 195, loss: 38.34226608276367\n",
      "epoch: 15,  batch step: 196, loss: 98.08976745605469\n",
      "epoch: 15,  batch step: 197, loss: 11.475814819335938\n",
      "epoch: 15,  batch step: 198, loss: 12.874305725097656\n",
      "epoch: 15,  batch step: 199, loss: 64.91775512695312\n",
      "epoch: 15,  batch step: 200, loss: 79.6922836303711\n",
      "epoch: 15,  batch step: 201, loss: 28.0701961517334\n",
      "epoch: 15,  batch step: 202, loss: 159.92462158203125\n",
      "epoch: 15,  batch step: 203, loss: 14.778909683227539\n",
      "epoch: 15,  batch step: 204, loss: 271.72418212890625\n",
      "epoch: 15,  batch step: 205, loss: 34.19374084472656\n",
      "epoch: 15,  batch step: 206, loss: 43.60022735595703\n",
      "epoch: 15,  batch step: 207, loss: 8.381139755249023\n",
      "epoch: 15,  batch step: 208, loss: 7.6065802574157715\n",
      "epoch: 15,  batch step: 209, loss: 36.6477165222168\n",
      "epoch: 15,  batch step: 210, loss: 23.96487808227539\n",
      "epoch: 15,  batch step: 211, loss: 31.67365074157715\n",
      "epoch: 15,  batch step: 212, loss: 54.64984130859375\n",
      "epoch: 15,  batch step: 213, loss: 9.29465389251709\n",
      "epoch: 15,  batch step: 214, loss: 8.758368492126465\n",
      "epoch: 15,  batch step: 215, loss: 11.291605949401855\n",
      "epoch: 15,  batch step: 216, loss: 46.82793426513672\n",
      "epoch: 15,  batch step: 217, loss: 29.31536865234375\n",
      "epoch: 15,  batch step: 218, loss: 85.97740173339844\n",
      "epoch: 15,  batch step: 219, loss: 11.650080680847168\n",
      "epoch: 15,  batch step: 220, loss: 315.2889099121094\n",
      "epoch: 15,  batch step: 221, loss: 111.78453063964844\n",
      "epoch: 15,  batch step: 222, loss: 8.223087310791016\n",
      "epoch: 15,  batch step: 223, loss: 9.530027389526367\n",
      "epoch: 15,  batch step: 224, loss: 94.89353942871094\n",
      "epoch: 15,  batch step: 225, loss: 6.107151031494141\n",
      "epoch: 15,  batch step: 226, loss: 252.52218627929688\n",
      "epoch: 15,  batch step: 227, loss: 131.346435546875\n",
      "epoch: 15,  batch step: 228, loss: 22.011688232421875\n",
      "epoch: 15,  batch step: 229, loss: 47.36265563964844\n",
      "epoch: 15,  batch step: 230, loss: 43.3811149597168\n",
      "epoch: 15,  batch step: 231, loss: 74.36780548095703\n",
      "epoch: 15,  batch step: 232, loss: 88.649169921875\n",
      "epoch: 15,  batch step: 233, loss: 40.47007751464844\n",
      "epoch: 15,  batch step: 234, loss: 20.545095443725586\n",
      "epoch: 15,  batch step: 235, loss: 9.157931327819824\n",
      "epoch: 15,  batch step: 236, loss: 68.91761779785156\n",
      "epoch: 15,  batch step: 237, loss: 49.2064323425293\n",
      "epoch: 15,  batch step: 238, loss: 19.529632568359375\n",
      "epoch: 15,  batch step: 239, loss: 23.51936912536621\n",
      "epoch: 15,  batch step: 240, loss: 7.637091636657715\n",
      "epoch: 15,  batch step: 241, loss: 57.3887939453125\n",
      "epoch: 15,  batch step: 242, loss: 10.10650634765625\n",
      "epoch: 15,  batch step: 243, loss: 21.799976348876953\n",
      "epoch: 15,  batch step: 244, loss: 175.52801513671875\n",
      "epoch: 15,  batch step: 245, loss: 59.506927490234375\n",
      "epoch: 15,  batch step: 246, loss: 9.114521026611328\n",
      "epoch: 15,  batch step: 247, loss: 25.07440757751465\n",
      "epoch: 15,  batch step: 248, loss: 28.718658447265625\n",
      "epoch: 15,  batch step: 249, loss: 445.10992431640625\n",
      "epoch: 15,  batch step: 250, loss: 51.65787887573242\n",
      "epoch: 15,  batch step: 251, loss: 49.768001556396484\n",
      "validation error epoch  15:    tensor(232.3784, device='cuda:0')\n",
      "316\n",
      "epoch: 16,  batch step: 0, loss: 19.075054168701172\n",
      "epoch: 16,  batch step: 1, loss: 109.53743743896484\n",
      "epoch: 16,  batch step: 2, loss: 58.38275146484375\n",
      "epoch: 16,  batch step: 3, loss: 53.76994323730469\n",
      "epoch: 16,  batch step: 4, loss: 53.36186981201172\n",
      "epoch: 16,  batch step: 5, loss: 82.295654296875\n",
      "epoch: 16,  batch step: 6, loss: 106.47654724121094\n",
      "epoch: 16,  batch step: 7, loss: 26.140148162841797\n",
      "epoch: 16,  batch step: 8, loss: 129.64614868164062\n",
      "epoch: 16,  batch step: 9, loss: 59.31996154785156\n",
      "epoch: 16,  batch step: 10, loss: 79.60540008544922\n",
      "epoch: 16,  batch step: 11, loss: 37.22057342529297\n",
      "epoch: 16,  batch step: 12, loss: 43.549434661865234\n",
      "epoch: 16,  batch step: 13, loss: 33.70513916015625\n",
      "epoch: 16,  batch step: 14, loss: 16.5805606842041\n",
      "epoch: 16,  batch step: 15, loss: 37.00634765625\n",
      "epoch: 16,  batch step: 16, loss: 13.91602897644043\n",
      "epoch: 16,  batch step: 17, loss: 59.309505462646484\n",
      "epoch: 16,  batch step: 18, loss: 11.459090232849121\n",
      "epoch: 16,  batch step: 19, loss: 30.411556243896484\n",
      "epoch: 16,  batch step: 20, loss: 9.680871963500977\n",
      "epoch: 16,  batch step: 21, loss: 156.87213134765625\n",
      "epoch: 16,  batch step: 22, loss: 12.620887756347656\n",
      "epoch: 16,  batch step: 23, loss: 87.66735076904297\n",
      "epoch: 16,  batch step: 24, loss: 177.4017333984375\n",
      "epoch: 16,  batch step: 25, loss: 34.46497344970703\n",
      "epoch: 16,  batch step: 26, loss: 15.212642669677734\n",
      "epoch: 16,  batch step: 27, loss: 15.089091300964355\n",
      "epoch: 16,  batch step: 28, loss: 101.39811706542969\n",
      "epoch: 16,  batch step: 29, loss: 112.44698333740234\n",
      "epoch: 16,  batch step: 30, loss: 73.55326843261719\n",
      "epoch: 16,  batch step: 31, loss: 81.36421966552734\n",
      "epoch: 16,  batch step: 32, loss: 41.80477523803711\n",
      "epoch: 16,  batch step: 33, loss: 102.13359832763672\n",
      "epoch: 16,  batch step: 34, loss: 34.52082061767578\n",
      "epoch: 16,  batch step: 35, loss: 29.71182632446289\n",
      "epoch: 16,  batch step: 36, loss: 18.59990882873535\n",
      "epoch: 16,  batch step: 37, loss: 72.18685913085938\n",
      "epoch: 16,  batch step: 38, loss: 63.69488525390625\n",
      "epoch: 16,  batch step: 39, loss: 33.39894104003906\n",
      "epoch: 16,  batch step: 40, loss: 17.218048095703125\n",
      "epoch: 16,  batch step: 41, loss: 6.915468215942383\n",
      "epoch: 16,  batch step: 42, loss: 59.346466064453125\n",
      "epoch: 16,  batch step: 43, loss: 7.756968975067139\n",
      "epoch: 16,  batch step: 44, loss: 182.91624450683594\n",
      "epoch: 16,  batch step: 45, loss: 9.004429817199707\n",
      "epoch: 16,  batch step: 46, loss: 16.379962921142578\n",
      "epoch: 16,  batch step: 47, loss: 14.6793851852417\n",
      "epoch: 16,  batch step: 48, loss: 208.04393005371094\n",
      "epoch: 16,  batch step: 49, loss: 45.6981201171875\n",
      "epoch: 16,  batch step: 50, loss: 212.5989990234375\n",
      "epoch: 16,  batch step: 51, loss: 126.07845306396484\n",
      "epoch: 16,  batch step: 52, loss: 4.646435737609863\n",
      "epoch: 16,  batch step: 53, loss: 42.057403564453125\n",
      "epoch: 16,  batch step: 54, loss: 23.201515197753906\n",
      "epoch: 16,  batch step: 55, loss: 67.8128662109375\n",
      "epoch: 16,  batch step: 56, loss: 85.05170440673828\n",
      "epoch: 16,  batch step: 57, loss: 23.199111938476562\n",
      "epoch: 16,  batch step: 58, loss: 158.30886840820312\n",
      "epoch: 16,  batch step: 59, loss: 54.654747009277344\n",
      "epoch: 16,  batch step: 60, loss: 97.54682922363281\n",
      "epoch: 16,  batch step: 61, loss: 32.06437683105469\n",
      "epoch: 16,  batch step: 62, loss: 24.738418579101562\n",
      "epoch: 16,  batch step: 63, loss: 53.00450134277344\n",
      "epoch: 16,  batch step: 64, loss: 154.35797119140625\n",
      "epoch: 16,  batch step: 65, loss: 248.57225036621094\n",
      "epoch: 16,  batch step: 66, loss: 23.51765251159668\n",
      "epoch: 16,  batch step: 67, loss: 19.465572357177734\n",
      "epoch: 16,  batch step: 68, loss: 68.18865966796875\n",
      "epoch: 16,  batch step: 69, loss: 56.739952087402344\n",
      "epoch: 16,  batch step: 70, loss: 102.3062973022461\n",
      "epoch: 16,  batch step: 71, loss: 33.88588333129883\n",
      "epoch: 16,  batch step: 72, loss: 119.35276794433594\n",
      "epoch: 16,  batch step: 73, loss: 16.027814865112305\n",
      "epoch: 16,  batch step: 74, loss: 28.419879913330078\n",
      "epoch: 16,  batch step: 75, loss: 24.369029998779297\n",
      "epoch: 16,  batch step: 76, loss: 50.99349594116211\n",
      "epoch: 16,  batch step: 77, loss: 180.15679931640625\n",
      "epoch: 16,  batch step: 78, loss: 11.688007354736328\n",
      "epoch: 16,  batch step: 79, loss: 31.415348052978516\n",
      "epoch: 16,  batch step: 80, loss: 15.177577018737793\n",
      "epoch: 16,  batch step: 81, loss: 70.09304809570312\n",
      "epoch: 16,  batch step: 82, loss: 21.953033447265625\n",
      "epoch: 16,  batch step: 83, loss: 66.59251403808594\n",
      "epoch: 16,  batch step: 84, loss: 7.349018096923828\n",
      "epoch: 16,  batch step: 85, loss: 21.03741455078125\n",
      "epoch: 16,  batch step: 86, loss: 150.69773864746094\n",
      "epoch: 16,  batch step: 87, loss: 18.375516891479492\n",
      "epoch: 16,  batch step: 88, loss: 69.03279113769531\n",
      "epoch: 16,  batch step: 89, loss: 18.280302047729492\n",
      "epoch: 16,  batch step: 90, loss: 13.400781631469727\n",
      "epoch: 16,  batch step: 91, loss: 196.354248046875\n",
      "epoch: 16,  batch step: 92, loss: 17.844940185546875\n",
      "epoch: 16,  batch step: 93, loss: 22.7291316986084\n",
      "epoch: 16,  batch step: 94, loss: 20.628759384155273\n",
      "epoch: 16,  batch step: 95, loss: 19.656450271606445\n",
      "epoch: 16,  batch step: 96, loss: 23.388324737548828\n",
      "epoch: 16,  batch step: 97, loss: 201.54623413085938\n",
      "epoch: 16,  batch step: 98, loss: 11.425875663757324\n",
      "epoch: 16,  batch step: 99, loss: 22.253414154052734\n",
      "epoch: 16,  batch step: 100, loss: 19.373186111450195\n",
      "epoch: 16,  batch step: 101, loss: 7.674117565155029\n",
      "epoch: 16,  batch step: 102, loss: 27.297645568847656\n",
      "epoch: 16,  batch step: 103, loss: 121.73226165771484\n",
      "epoch: 16,  batch step: 104, loss: 14.54937744140625\n",
      "epoch: 16,  batch step: 105, loss: 27.186477661132812\n",
      "epoch: 16,  batch step: 106, loss: 11.17733383178711\n",
      "epoch: 16,  batch step: 107, loss: 201.3569793701172\n",
      "epoch: 16,  batch step: 108, loss: 8.990138053894043\n",
      "epoch: 16,  batch step: 109, loss: 33.35478973388672\n",
      "epoch: 16,  batch step: 110, loss: 42.911842346191406\n",
      "epoch: 16,  batch step: 111, loss: 13.435415267944336\n",
      "epoch: 16,  batch step: 112, loss: 29.482990264892578\n",
      "epoch: 16,  batch step: 113, loss: 37.76288604736328\n",
      "epoch: 16,  batch step: 114, loss: 131.994140625\n",
      "epoch: 16,  batch step: 115, loss: 6.971786022186279\n",
      "epoch: 16,  batch step: 116, loss: 7.725734233856201\n",
      "epoch: 16,  batch step: 117, loss: 71.04313659667969\n",
      "epoch: 16,  batch step: 118, loss: 32.421363830566406\n",
      "epoch: 16,  batch step: 119, loss: 16.08004379272461\n",
      "epoch: 16,  batch step: 120, loss: 28.321712493896484\n",
      "epoch: 16,  batch step: 121, loss: 176.00881958007812\n",
      "epoch: 16,  batch step: 122, loss: 9.436156272888184\n",
      "epoch: 16,  batch step: 123, loss: 26.689870834350586\n",
      "epoch: 16,  batch step: 124, loss: 178.93040466308594\n",
      "epoch: 16,  batch step: 125, loss: 299.1842346191406\n",
      "epoch: 16,  batch step: 126, loss: 84.52806854248047\n",
      "epoch: 16,  batch step: 127, loss: 12.529102325439453\n",
      "epoch: 16,  batch step: 128, loss: 294.28594970703125\n",
      "epoch: 16,  batch step: 129, loss: 9.311288833618164\n",
      "epoch: 16,  batch step: 130, loss: 16.187292098999023\n",
      "epoch: 16,  batch step: 131, loss: 13.063735008239746\n",
      "epoch: 16,  batch step: 132, loss: 7.5474090576171875\n",
      "epoch: 16,  batch step: 133, loss: 39.79533386230469\n",
      "epoch: 16,  batch step: 134, loss: 161.3182830810547\n",
      "epoch: 16,  batch step: 135, loss: 35.189903259277344\n",
      "epoch: 16,  batch step: 136, loss: 15.795003890991211\n",
      "epoch: 16,  batch step: 137, loss: 17.03958511352539\n",
      "epoch: 16,  batch step: 138, loss: 137.07923889160156\n",
      "epoch: 16,  batch step: 139, loss: 149.3538818359375\n",
      "epoch: 16,  batch step: 140, loss: 25.217260360717773\n",
      "epoch: 16,  batch step: 141, loss: 16.217117309570312\n",
      "epoch: 16,  batch step: 142, loss: 75.82794189453125\n",
      "epoch: 16,  batch step: 143, loss: 67.85011291503906\n",
      "epoch: 16,  batch step: 144, loss: 17.686908721923828\n",
      "epoch: 16,  batch step: 145, loss: 20.251741409301758\n",
      "epoch: 16,  batch step: 146, loss: 202.316162109375\n",
      "epoch: 16,  batch step: 147, loss: 217.1646728515625\n",
      "epoch: 16,  batch step: 148, loss: 90.2688217163086\n",
      "epoch: 16,  batch step: 149, loss: 89.05763244628906\n",
      "epoch: 16,  batch step: 150, loss: 10.840482711791992\n",
      "epoch: 16,  batch step: 151, loss: 15.449828147888184\n",
      "epoch: 16,  batch step: 152, loss: 102.92523193359375\n",
      "epoch: 16,  batch step: 153, loss: 262.05615234375\n",
      "epoch: 16,  batch step: 154, loss: 92.83572387695312\n",
      "epoch: 16,  batch step: 155, loss: 8.268223762512207\n",
      "epoch: 16,  batch step: 156, loss: 142.6821746826172\n",
      "epoch: 16,  batch step: 157, loss: 9.413816452026367\n",
      "epoch: 16,  batch step: 158, loss: 134.3372344970703\n",
      "epoch: 16,  batch step: 159, loss: 18.571487426757812\n",
      "epoch: 16,  batch step: 160, loss: 85.3343505859375\n",
      "epoch: 16,  batch step: 161, loss: 155.18710327148438\n",
      "epoch: 16,  batch step: 162, loss: 52.60224151611328\n",
      "epoch: 16,  batch step: 163, loss: 19.40279769897461\n",
      "epoch: 16,  batch step: 164, loss: 10.299238204956055\n",
      "epoch: 16,  batch step: 165, loss: 20.923110961914062\n",
      "epoch: 16,  batch step: 166, loss: 277.97705078125\n",
      "epoch: 16,  batch step: 167, loss: 7.175446510314941\n",
      "epoch: 16,  batch step: 168, loss: 47.14860153198242\n",
      "epoch: 16,  batch step: 169, loss: 178.82989501953125\n",
      "epoch: 16,  batch step: 170, loss: 12.60901165008545\n",
      "epoch: 16,  batch step: 171, loss: 123.71766662597656\n",
      "epoch: 16,  batch step: 172, loss: 40.90882873535156\n",
      "epoch: 16,  batch step: 173, loss: 10.435041427612305\n",
      "epoch: 16,  batch step: 174, loss: 56.549644470214844\n",
      "epoch: 16,  batch step: 175, loss: 16.006351470947266\n",
      "epoch: 16,  batch step: 176, loss: 84.55965423583984\n",
      "epoch: 16,  batch step: 177, loss: 23.32026481628418\n",
      "epoch: 16,  batch step: 178, loss: 61.86079406738281\n",
      "epoch: 16,  batch step: 179, loss: 14.605547904968262\n",
      "epoch: 16,  batch step: 180, loss: 88.23210906982422\n",
      "epoch: 16,  batch step: 181, loss: 7.451211929321289\n",
      "epoch: 16,  batch step: 182, loss: 8.879073143005371\n",
      "epoch: 16,  batch step: 183, loss: 337.787109375\n",
      "epoch: 16,  batch step: 184, loss: 53.66935348510742\n",
      "epoch: 16,  batch step: 185, loss: 9.421367645263672\n",
      "epoch: 16,  batch step: 186, loss: 62.91062927246094\n",
      "epoch: 16,  batch step: 187, loss: 53.91058349609375\n",
      "epoch: 16,  batch step: 188, loss: 9.002740859985352\n",
      "epoch: 16,  batch step: 189, loss: 19.520063400268555\n",
      "epoch: 16,  batch step: 190, loss: 7.704851150512695\n",
      "epoch: 16,  batch step: 191, loss: 9.80140495300293\n",
      "epoch: 16,  batch step: 192, loss: 143.23440551757812\n",
      "epoch: 16,  batch step: 193, loss: 66.04141235351562\n",
      "epoch: 16,  batch step: 194, loss: 56.41069793701172\n",
      "epoch: 16,  batch step: 195, loss: 25.587141036987305\n",
      "epoch: 16,  batch step: 196, loss: 50.593177795410156\n",
      "epoch: 16,  batch step: 197, loss: 49.16938018798828\n",
      "epoch: 16,  batch step: 198, loss: 9.556039810180664\n",
      "epoch: 16,  batch step: 199, loss: 6.715892791748047\n",
      "epoch: 16,  batch step: 200, loss: 18.04526138305664\n",
      "epoch: 16,  batch step: 201, loss: 47.581947326660156\n",
      "epoch: 16,  batch step: 202, loss: 61.749916076660156\n",
      "epoch: 16,  batch step: 203, loss: 63.4732666015625\n",
      "epoch: 16,  batch step: 204, loss: 108.60160827636719\n",
      "epoch: 16,  batch step: 205, loss: 6.604302883148193\n",
      "epoch: 16,  batch step: 206, loss: 56.8281364440918\n",
      "epoch: 16,  batch step: 207, loss: 22.763540267944336\n",
      "epoch: 16,  batch step: 208, loss: 35.32047653198242\n",
      "epoch: 16,  batch step: 209, loss: 11.865798950195312\n",
      "epoch: 16,  batch step: 210, loss: 29.10492515563965\n",
      "epoch: 16,  batch step: 211, loss: 95.603515625\n",
      "epoch: 16,  batch step: 212, loss: 186.35763549804688\n",
      "epoch: 16,  batch step: 213, loss: 22.196455001831055\n",
      "epoch: 16,  batch step: 214, loss: 224.18008422851562\n",
      "epoch: 16,  batch step: 215, loss: 25.87457275390625\n",
      "epoch: 16,  batch step: 216, loss: 100.99874877929688\n",
      "epoch: 16,  batch step: 217, loss: 23.892301559448242\n",
      "epoch: 16,  batch step: 218, loss: 41.0223274230957\n",
      "epoch: 16,  batch step: 219, loss: 111.09283447265625\n",
      "epoch: 16,  batch step: 220, loss: 106.4436264038086\n",
      "epoch: 16,  batch step: 221, loss: 19.25983238220215\n",
      "epoch: 16,  batch step: 222, loss: 133.78378295898438\n",
      "epoch: 16,  batch step: 223, loss: 65.4245376586914\n",
      "epoch: 16,  batch step: 224, loss: 39.49877166748047\n",
      "epoch: 16,  batch step: 225, loss: 55.093955993652344\n",
      "epoch: 16,  batch step: 226, loss: 43.262611389160156\n",
      "epoch: 16,  batch step: 227, loss: 44.89481735229492\n",
      "epoch: 16,  batch step: 228, loss: 62.884090423583984\n",
      "epoch: 16,  batch step: 229, loss: 13.026209831237793\n",
      "epoch: 16,  batch step: 230, loss: 17.6004638671875\n",
      "epoch: 16,  batch step: 231, loss: 41.13686752319336\n",
      "epoch: 16,  batch step: 232, loss: 244.3046112060547\n",
      "epoch: 16,  batch step: 233, loss: 57.73684310913086\n",
      "epoch: 16,  batch step: 234, loss: 13.057212829589844\n",
      "epoch: 16,  batch step: 235, loss: 15.009403228759766\n",
      "epoch: 16,  batch step: 236, loss: 6.303075313568115\n",
      "epoch: 16,  batch step: 237, loss: 8.142287254333496\n",
      "epoch: 16,  batch step: 238, loss: 17.52754783630371\n",
      "epoch: 16,  batch step: 239, loss: 13.99690055847168\n",
      "epoch: 16,  batch step: 240, loss: 154.60702514648438\n",
      "epoch: 16,  batch step: 241, loss: 13.382375717163086\n",
      "epoch: 16,  batch step: 242, loss: 194.62957763671875\n",
      "epoch: 16,  batch step: 243, loss: 23.244869232177734\n",
      "epoch: 16,  batch step: 244, loss: 19.25048828125\n",
      "epoch: 16,  batch step: 245, loss: 103.82786560058594\n",
      "epoch: 16,  batch step: 246, loss: 321.22125244140625\n",
      "epoch: 16,  batch step: 247, loss: 13.481599807739258\n",
      "epoch: 16,  batch step: 248, loss: 12.183981895446777\n",
      "epoch: 16,  batch step: 249, loss: 22.13014030456543\n",
      "epoch: 16,  batch step: 250, loss: 32.36869430541992\n",
      "epoch: 16,  batch step: 251, loss: 320.7890625\n",
      "validation error epoch  16:    tensor(146.9130, device='cuda:0')\n",
      "316\n",
      "epoch: 17,  batch step: 0, loss: 86.69749450683594\n",
      "epoch: 17,  batch step: 1, loss: 31.10244369506836\n",
      "epoch: 17,  batch step: 2, loss: 19.880168914794922\n",
      "epoch: 17,  batch step: 3, loss: 140.82518005371094\n",
      "epoch: 17,  batch step: 4, loss: 14.604704856872559\n",
      "epoch: 17,  batch step: 5, loss: 17.647905349731445\n",
      "epoch: 17,  batch step: 6, loss: 40.967037200927734\n",
      "epoch: 17,  batch step: 7, loss: 30.797758102416992\n",
      "epoch: 17,  batch step: 8, loss: 34.13242721557617\n",
      "epoch: 17,  batch step: 9, loss: 12.474867820739746\n",
      "epoch: 17,  batch step: 10, loss: 121.40278625488281\n",
      "epoch: 17,  batch step: 11, loss: 195.8854217529297\n",
      "epoch: 17,  batch step: 12, loss: 10.820554733276367\n",
      "epoch: 17,  batch step: 13, loss: 13.601215362548828\n",
      "epoch: 17,  batch step: 14, loss: 22.86091423034668\n",
      "epoch: 17,  batch step: 15, loss: 139.33566284179688\n",
      "epoch: 17,  batch step: 16, loss: 23.293365478515625\n",
      "epoch: 17,  batch step: 17, loss: 8.744211196899414\n",
      "epoch: 17,  batch step: 18, loss: 97.7451400756836\n",
      "epoch: 17,  batch step: 19, loss: 15.40464973449707\n",
      "epoch: 17,  batch step: 20, loss: 39.057167053222656\n",
      "epoch: 17,  batch step: 21, loss: 25.175701141357422\n",
      "epoch: 17,  batch step: 22, loss: 5.665992736816406\n",
      "epoch: 17,  batch step: 23, loss: 15.731035232543945\n",
      "epoch: 17,  batch step: 24, loss: 119.5108871459961\n",
      "epoch: 17,  batch step: 25, loss: 50.63055419921875\n",
      "epoch: 17,  batch step: 26, loss: 50.8961181640625\n",
      "epoch: 17,  batch step: 27, loss: 12.099520683288574\n",
      "epoch: 17,  batch step: 28, loss: 36.62331771850586\n",
      "epoch: 17,  batch step: 29, loss: 153.376708984375\n",
      "epoch: 17,  batch step: 30, loss: 83.8702392578125\n",
      "epoch: 17,  batch step: 31, loss: 14.883467674255371\n",
      "epoch: 17,  batch step: 32, loss: 7.196285247802734\n",
      "epoch: 17,  batch step: 33, loss: 8.914104461669922\n",
      "epoch: 17,  batch step: 34, loss: 64.50127410888672\n",
      "epoch: 17,  batch step: 35, loss: 60.47639083862305\n",
      "epoch: 17,  batch step: 36, loss: 6.293842315673828\n",
      "epoch: 17,  batch step: 37, loss: 13.175573348999023\n",
      "epoch: 17,  batch step: 38, loss: 7.571897506713867\n",
      "epoch: 17,  batch step: 39, loss: 20.683025360107422\n",
      "epoch: 17,  batch step: 40, loss: 35.56654357910156\n",
      "epoch: 17,  batch step: 41, loss: 5.925515651702881\n",
      "epoch: 17,  batch step: 42, loss: 215.37815856933594\n",
      "epoch: 17,  batch step: 43, loss: 7.092007637023926\n",
      "epoch: 17,  batch step: 44, loss: 4.782407760620117\n",
      "epoch: 17,  batch step: 45, loss: 21.469623565673828\n",
      "epoch: 17,  batch step: 46, loss: 92.87371063232422\n",
      "epoch: 17,  batch step: 47, loss: 102.01423645019531\n",
      "epoch: 17,  batch step: 48, loss: 176.33425903320312\n",
      "epoch: 17,  batch step: 49, loss: 5.546022415161133\n",
      "epoch: 17,  batch step: 50, loss: 9.665141105651855\n",
      "epoch: 17,  batch step: 51, loss: 46.86583709716797\n",
      "epoch: 17,  batch step: 52, loss: 10.224666595458984\n",
      "epoch: 17,  batch step: 53, loss: 164.8302764892578\n",
      "epoch: 17,  batch step: 54, loss: 64.07695770263672\n",
      "epoch: 17,  batch step: 55, loss: 20.415863037109375\n",
      "epoch: 17,  batch step: 56, loss: 8.613901138305664\n",
      "epoch: 17,  batch step: 57, loss: 20.639728546142578\n",
      "epoch: 17,  batch step: 58, loss: 136.47952270507812\n",
      "epoch: 17,  batch step: 59, loss: 155.01795959472656\n",
      "epoch: 17,  batch step: 60, loss: 9.796419143676758\n",
      "epoch: 17,  batch step: 61, loss: 45.009368896484375\n",
      "epoch: 17,  batch step: 62, loss: 194.70162963867188\n",
      "epoch: 17,  batch step: 63, loss: 259.8101806640625\n",
      "epoch: 17,  batch step: 64, loss: 51.751426696777344\n",
      "epoch: 17,  batch step: 65, loss: 84.82568359375\n",
      "epoch: 17,  batch step: 66, loss: 80.96199035644531\n",
      "epoch: 17,  batch step: 67, loss: 49.05335998535156\n",
      "epoch: 17,  batch step: 68, loss: 29.289981842041016\n",
      "epoch: 17,  batch step: 69, loss: 122.05856323242188\n",
      "epoch: 17,  batch step: 70, loss: 89.65084838867188\n",
      "epoch: 17,  batch step: 71, loss: 22.678403854370117\n",
      "epoch: 17,  batch step: 72, loss: 174.6959228515625\n",
      "epoch: 17,  batch step: 73, loss: 33.44365692138672\n",
      "epoch: 17,  batch step: 74, loss: 27.2624568939209\n",
      "epoch: 17,  batch step: 75, loss: 64.81654357910156\n",
      "epoch: 17,  batch step: 76, loss: 10.286447525024414\n",
      "epoch: 17,  batch step: 77, loss: 55.826507568359375\n",
      "epoch: 17,  batch step: 78, loss: 17.064647674560547\n",
      "epoch: 17,  batch step: 79, loss: 50.51806640625\n",
      "epoch: 17,  batch step: 80, loss: 19.395763397216797\n",
      "epoch: 17,  batch step: 81, loss: 37.724159240722656\n",
      "epoch: 17,  batch step: 82, loss: 9.98923397064209\n",
      "epoch: 17,  batch step: 83, loss: 16.612171173095703\n",
      "epoch: 17,  batch step: 84, loss: 10.02914047241211\n",
      "epoch: 17,  batch step: 85, loss: 22.403701782226562\n",
      "epoch: 17,  batch step: 86, loss: 24.920684814453125\n",
      "epoch: 17,  batch step: 87, loss: 151.169677734375\n",
      "epoch: 17,  batch step: 88, loss: 50.616249084472656\n",
      "epoch: 17,  batch step: 89, loss: 43.42863464355469\n",
      "epoch: 17,  batch step: 90, loss: 81.99465942382812\n",
      "epoch: 17,  batch step: 91, loss: 56.64443588256836\n",
      "epoch: 17,  batch step: 92, loss: 60.44231414794922\n",
      "epoch: 17,  batch step: 93, loss: 16.482746124267578\n",
      "epoch: 17,  batch step: 94, loss: 111.0699462890625\n",
      "epoch: 17,  batch step: 95, loss: 247.04348754882812\n",
      "epoch: 17,  batch step: 96, loss: 26.975231170654297\n",
      "epoch: 17,  batch step: 97, loss: 12.489875793457031\n",
      "epoch: 17,  batch step: 98, loss: 58.88304901123047\n",
      "epoch: 17,  batch step: 99, loss: 67.73518371582031\n",
      "epoch: 17,  batch step: 100, loss: 169.00390625\n",
      "epoch: 17,  batch step: 101, loss: 155.55535888671875\n",
      "epoch: 17,  batch step: 102, loss: 243.52896118164062\n",
      "epoch: 17,  batch step: 103, loss: 43.14601135253906\n",
      "epoch: 17,  batch step: 104, loss: 77.47135162353516\n",
      "epoch: 17,  batch step: 105, loss: 10.800456047058105\n",
      "epoch: 17,  batch step: 106, loss: 11.052608489990234\n",
      "epoch: 17,  batch step: 107, loss: 27.70314598083496\n",
      "epoch: 17,  batch step: 108, loss: 12.703210830688477\n",
      "epoch: 17,  batch step: 109, loss: 117.9085922241211\n",
      "epoch: 17,  batch step: 110, loss: 30.707387924194336\n",
      "epoch: 17,  batch step: 111, loss: 107.70829772949219\n",
      "epoch: 17,  batch step: 112, loss: 14.761781692504883\n",
      "epoch: 17,  batch step: 113, loss: 87.20589447021484\n",
      "epoch: 17,  batch step: 114, loss: 15.099459648132324\n",
      "epoch: 17,  batch step: 115, loss: 87.32173919677734\n",
      "epoch: 17,  batch step: 116, loss: 142.39474487304688\n",
      "epoch: 17,  batch step: 117, loss: 261.2144470214844\n",
      "epoch: 17,  batch step: 118, loss: 93.12998962402344\n",
      "epoch: 17,  batch step: 119, loss: 168.3859405517578\n",
      "epoch: 17,  batch step: 120, loss: 13.662542343139648\n",
      "epoch: 17,  batch step: 121, loss: 136.50851440429688\n",
      "epoch: 17,  batch step: 122, loss: 12.122568130493164\n",
      "epoch: 17,  batch step: 123, loss: 209.91497802734375\n",
      "epoch: 17,  batch step: 124, loss: 40.42973327636719\n",
      "epoch: 17,  batch step: 125, loss: 106.04254150390625\n",
      "epoch: 17,  batch step: 126, loss: 125.02498626708984\n",
      "epoch: 17,  batch step: 127, loss: 85.95787048339844\n",
      "epoch: 17,  batch step: 128, loss: 118.82579040527344\n",
      "epoch: 17,  batch step: 129, loss: 84.15869903564453\n",
      "epoch: 17,  batch step: 130, loss: 82.63631439208984\n",
      "epoch: 17,  batch step: 131, loss: 284.84283447265625\n",
      "epoch: 17,  batch step: 132, loss: 94.82846069335938\n",
      "epoch: 17,  batch step: 133, loss: 45.2812614440918\n",
      "epoch: 17,  batch step: 134, loss: 64.74258422851562\n",
      "epoch: 17,  batch step: 135, loss: 26.172780990600586\n",
      "epoch: 17,  batch step: 136, loss: 58.091773986816406\n",
      "epoch: 17,  batch step: 137, loss: 201.27017211914062\n",
      "epoch: 17,  batch step: 138, loss: 14.937883377075195\n",
      "epoch: 17,  batch step: 139, loss: 615.2777709960938\n",
      "epoch: 17,  batch step: 140, loss: 74.25218200683594\n",
      "epoch: 17,  batch step: 141, loss: 13.640594482421875\n",
      "epoch: 17,  batch step: 142, loss: 17.28607749938965\n",
      "epoch: 17,  batch step: 143, loss: 78.87364196777344\n",
      "epoch: 17,  batch step: 144, loss: 92.65054321289062\n",
      "epoch: 17,  batch step: 145, loss: 59.19908905029297\n",
      "epoch: 17,  batch step: 146, loss: 144.32342529296875\n",
      "epoch: 17,  batch step: 147, loss: 62.28423309326172\n",
      "epoch: 17,  batch step: 148, loss: 46.92237854003906\n",
      "epoch: 17,  batch step: 149, loss: 26.690580368041992\n",
      "epoch: 17,  batch step: 150, loss: 17.994117736816406\n",
      "epoch: 17,  batch step: 151, loss: 13.465084075927734\n",
      "epoch: 17,  batch step: 152, loss: 18.858661651611328\n",
      "epoch: 17,  batch step: 153, loss: 17.363676071166992\n",
      "epoch: 17,  batch step: 154, loss: 14.496673583984375\n",
      "epoch: 17,  batch step: 155, loss: 17.62210464477539\n",
      "epoch: 17,  batch step: 156, loss: 62.98265838623047\n",
      "epoch: 17,  batch step: 157, loss: 9.59016227722168\n",
      "epoch: 17,  batch step: 158, loss: 92.10411071777344\n",
      "epoch: 17,  batch step: 159, loss: 18.712448120117188\n",
      "epoch: 17,  batch step: 160, loss: 9.674650192260742\n",
      "epoch: 17,  batch step: 161, loss: 50.78133773803711\n",
      "epoch: 17,  batch step: 162, loss: 8.01974868774414\n",
      "epoch: 17,  batch step: 163, loss: 18.36679458618164\n",
      "epoch: 17,  batch step: 164, loss: 81.6897964477539\n",
      "epoch: 17,  batch step: 165, loss: 67.9337158203125\n",
      "epoch: 17,  batch step: 166, loss: 24.349185943603516\n",
      "epoch: 17,  batch step: 167, loss: 38.06355285644531\n",
      "epoch: 17,  batch step: 168, loss: 22.987857818603516\n",
      "epoch: 17,  batch step: 169, loss: 341.98681640625\n",
      "epoch: 17,  batch step: 170, loss: 14.332740783691406\n",
      "epoch: 17,  batch step: 171, loss: 210.11679077148438\n",
      "epoch: 17,  batch step: 172, loss: 31.26612663269043\n",
      "epoch: 17,  batch step: 173, loss: 89.15889739990234\n",
      "epoch: 17,  batch step: 174, loss: 20.356565475463867\n",
      "epoch: 17,  batch step: 175, loss: 18.968183517456055\n",
      "epoch: 17,  batch step: 176, loss: 5.9668402671813965\n",
      "epoch: 17,  batch step: 177, loss: 44.8562126159668\n",
      "epoch: 17,  batch step: 178, loss: 80.9295654296875\n",
      "epoch: 17,  batch step: 179, loss: 6.761009216308594\n",
      "epoch: 17,  batch step: 180, loss: 52.06670379638672\n",
      "epoch: 17,  batch step: 181, loss: 13.432371139526367\n",
      "epoch: 17,  batch step: 182, loss: 102.28514862060547\n",
      "epoch: 17,  batch step: 183, loss: 20.311771392822266\n",
      "epoch: 17,  batch step: 184, loss: 11.55897045135498\n",
      "epoch: 17,  batch step: 185, loss: 42.72779846191406\n",
      "epoch: 17,  batch step: 186, loss: 8.564946174621582\n",
      "epoch: 17,  batch step: 187, loss: 53.95914840698242\n",
      "epoch: 17,  batch step: 188, loss: 10.227169036865234\n",
      "epoch: 17,  batch step: 189, loss: 175.61614990234375\n",
      "epoch: 17,  batch step: 190, loss: 194.95066833496094\n",
      "epoch: 17,  batch step: 191, loss: 38.05598449707031\n",
      "epoch: 17,  batch step: 192, loss: 147.9342041015625\n",
      "epoch: 17,  batch step: 193, loss: 6.859519004821777\n",
      "epoch: 17,  batch step: 194, loss: 9.036517143249512\n",
      "epoch: 17,  batch step: 195, loss: 11.713252067565918\n",
      "epoch: 17,  batch step: 196, loss: 54.442237854003906\n",
      "epoch: 17,  batch step: 197, loss: 22.23085594177246\n",
      "epoch: 17,  batch step: 198, loss: 168.75985717773438\n",
      "epoch: 17,  batch step: 199, loss: 98.85005187988281\n",
      "epoch: 17,  batch step: 200, loss: 61.62928009033203\n",
      "epoch: 17,  batch step: 201, loss: 12.410029411315918\n",
      "epoch: 17,  batch step: 202, loss: 7.782182216644287\n",
      "epoch: 17,  batch step: 203, loss: 10.10991096496582\n",
      "epoch: 17,  batch step: 204, loss: 17.657045364379883\n",
      "epoch: 17,  batch step: 205, loss: 25.387981414794922\n",
      "epoch: 17,  batch step: 206, loss: 11.623651504516602\n",
      "epoch: 17,  batch step: 207, loss: 54.39008331298828\n",
      "epoch: 17,  batch step: 208, loss: 10.376752853393555\n",
      "epoch: 17,  batch step: 209, loss: 9.88574504852295\n",
      "epoch: 17,  batch step: 210, loss: 148.46844482421875\n",
      "epoch: 17,  batch step: 211, loss: 9.679545402526855\n",
      "epoch: 17,  batch step: 212, loss: 9.136865615844727\n",
      "epoch: 17,  batch step: 213, loss: 14.423174858093262\n",
      "epoch: 17,  batch step: 214, loss: 17.467588424682617\n",
      "epoch: 17,  batch step: 215, loss: 10.082656860351562\n",
      "epoch: 17,  batch step: 216, loss: 49.10572052001953\n",
      "epoch: 17,  batch step: 217, loss: 49.105552673339844\n",
      "epoch: 17,  batch step: 218, loss: 7.785484313964844\n",
      "epoch: 17,  batch step: 219, loss: 46.136009216308594\n",
      "epoch: 17,  batch step: 220, loss: 4.873481750488281\n",
      "epoch: 17,  batch step: 221, loss: 16.633398056030273\n",
      "epoch: 17,  batch step: 222, loss: 11.183725357055664\n",
      "epoch: 17,  batch step: 223, loss: 8.982839584350586\n",
      "epoch: 17,  batch step: 224, loss: 66.9522705078125\n",
      "epoch: 17,  batch step: 225, loss: 59.12946701049805\n",
      "epoch: 17,  batch step: 226, loss: 14.089778900146484\n",
      "epoch: 17,  batch step: 227, loss: 120.48472595214844\n",
      "epoch: 17,  batch step: 228, loss: 19.203107833862305\n",
      "epoch: 17,  batch step: 229, loss: 7.779881954193115\n",
      "epoch: 17,  batch step: 230, loss: 13.084718704223633\n",
      "epoch: 17,  batch step: 231, loss: 174.7593231201172\n",
      "epoch: 17,  batch step: 232, loss: 81.35065460205078\n",
      "epoch: 17,  batch step: 233, loss: 65.90666961669922\n",
      "epoch: 17,  batch step: 234, loss: 203.70712280273438\n",
      "epoch: 17,  batch step: 235, loss: 9.76627254486084\n",
      "epoch: 17,  batch step: 236, loss: 30.581722259521484\n",
      "epoch: 17,  batch step: 237, loss: 47.493865966796875\n",
      "epoch: 17,  batch step: 238, loss: 5.708039283752441\n",
      "epoch: 17,  batch step: 239, loss: 78.19648742675781\n",
      "epoch: 17,  batch step: 240, loss: 26.452301025390625\n",
      "epoch: 17,  batch step: 241, loss: 6.968484401702881\n",
      "epoch: 17,  batch step: 242, loss: 116.99308776855469\n",
      "epoch: 17,  batch step: 243, loss: 34.950355529785156\n",
      "epoch: 17,  batch step: 244, loss: 144.9739990234375\n",
      "epoch: 17,  batch step: 245, loss: 217.01333618164062\n",
      "epoch: 17,  batch step: 246, loss: 41.232933044433594\n",
      "epoch: 17,  batch step: 247, loss: 23.746562957763672\n",
      "epoch: 17,  batch step: 248, loss: 46.223506927490234\n",
      "epoch: 17,  batch step: 249, loss: 151.04074096679688\n",
      "epoch: 17,  batch step: 250, loss: 27.357280731201172\n",
      "epoch: 17,  batch step: 251, loss: 225.67532348632812\n",
      "validation error epoch  17:    tensor(65.8998, device='cuda:0')\n",
      "316\n",
      "epoch: 18,  batch step: 0, loss: 43.37798309326172\n",
      "epoch: 18,  batch step: 1, loss: 10.126627922058105\n",
      "epoch: 18,  batch step: 2, loss: 6.957037925720215\n",
      "epoch: 18,  batch step: 3, loss: 18.584142684936523\n",
      "epoch: 18,  batch step: 4, loss: 18.389694213867188\n",
      "epoch: 18,  batch step: 5, loss: 42.48012161254883\n",
      "epoch: 18,  batch step: 6, loss: 25.331235885620117\n",
      "epoch: 18,  batch step: 7, loss: 6.825976848602295\n",
      "epoch: 18,  batch step: 8, loss: 16.53847312927246\n",
      "epoch: 18,  batch step: 9, loss: 19.026182174682617\n",
      "epoch: 18,  batch step: 10, loss: 40.16552734375\n",
      "epoch: 18,  batch step: 11, loss: 24.797407150268555\n",
      "epoch: 18,  batch step: 12, loss: 36.05517578125\n",
      "epoch: 18,  batch step: 13, loss: 91.9071273803711\n",
      "epoch: 18,  batch step: 14, loss: 13.045348167419434\n",
      "epoch: 18,  batch step: 15, loss: 29.359432220458984\n",
      "epoch: 18,  batch step: 16, loss: 217.50881958007812\n",
      "epoch: 18,  batch step: 17, loss: 25.64624786376953\n",
      "epoch: 18,  batch step: 18, loss: 6.725893974304199\n",
      "epoch: 18,  batch step: 19, loss: 22.085681915283203\n",
      "epoch: 18,  batch step: 20, loss: 98.05918884277344\n",
      "epoch: 18,  batch step: 21, loss: 109.44107055664062\n",
      "epoch: 18,  batch step: 22, loss: 101.78839111328125\n",
      "epoch: 18,  batch step: 23, loss: 16.076627731323242\n",
      "epoch: 18,  batch step: 24, loss: 11.008620262145996\n",
      "epoch: 18,  batch step: 25, loss: 44.11152267456055\n",
      "epoch: 18,  batch step: 26, loss: 7.427454948425293\n",
      "epoch: 18,  batch step: 27, loss: 71.50652313232422\n",
      "epoch: 18,  batch step: 28, loss: 15.624000549316406\n",
      "epoch: 18,  batch step: 29, loss: 19.218488693237305\n",
      "epoch: 18,  batch step: 30, loss: 38.62952423095703\n",
      "epoch: 18,  batch step: 31, loss: 19.254165649414062\n",
      "epoch: 18,  batch step: 32, loss: 16.741825103759766\n",
      "epoch: 18,  batch step: 33, loss: 35.551509857177734\n",
      "epoch: 18,  batch step: 34, loss: 50.38566589355469\n",
      "epoch: 18,  batch step: 35, loss: 75.80278015136719\n",
      "epoch: 18,  batch step: 36, loss: 28.872241973876953\n",
      "epoch: 18,  batch step: 37, loss: 124.77849578857422\n",
      "epoch: 18,  batch step: 38, loss: 178.6967010498047\n",
      "epoch: 18,  batch step: 39, loss: 16.972654342651367\n",
      "epoch: 18,  batch step: 40, loss: 236.9893035888672\n",
      "epoch: 18,  batch step: 41, loss: 6.6978654861450195\n",
      "epoch: 18,  batch step: 42, loss: 41.464874267578125\n",
      "epoch: 18,  batch step: 43, loss: 18.08851432800293\n",
      "epoch: 18,  batch step: 44, loss: 115.56603240966797\n",
      "epoch: 18,  batch step: 45, loss: 18.116907119750977\n",
      "epoch: 18,  batch step: 46, loss: 74.55864715576172\n",
      "epoch: 18,  batch step: 47, loss: 69.0819320678711\n",
      "epoch: 18,  batch step: 48, loss: 19.98670196533203\n",
      "epoch: 18,  batch step: 49, loss: 12.151420593261719\n",
      "epoch: 18,  batch step: 50, loss: 77.0621337890625\n",
      "epoch: 18,  batch step: 51, loss: 96.96208953857422\n",
      "epoch: 18,  batch step: 52, loss: 7.688228607177734\n",
      "epoch: 18,  batch step: 53, loss: 10.496347427368164\n",
      "epoch: 18,  batch step: 54, loss: 30.68426513671875\n",
      "epoch: 18,  batch step: 55, loss: 11.217266082763672\n",
      "epoch: 18,  batch step: 56, loss: 15.79646110534668\n",
      "epoch: 18,  batch step: 57, loss: 9.872091293334961\n",
      "epoch: 18,  batch step: 58, loss: 40.79088592529297\n",
      "epoch: 18,  batch step: 59, loss: 215.8872528076172\n",
      "epoch: 18,  batch step: 60, loss: 69.412353515625\n",
      "epoch: 18,  batch step: 61, loss: 125.84856414794922\n",
      "epoch: 18,  batch step: 62, loss: 184.3275146484375\n",
      "epoch: 18,  batch step: 63, loss: 14.321945190429688\n",
      "epoch: 18,  batch step: 64, loss: 81.20506286621094\n",
      "epoch: 18,  batch step: 65, loss: 18.30610466003418\n",
      "epoch: 18,  batch step: 66, loss: 10.496583938598633\n",
      "epoch: 18,  batch step: 67, loss: 38.19621276855469\n",
      "epoch: 18,  batch step: 68, loss: 10.41085433959961\n",
      "epoch: 18,  batch step: 69, loss: 67.74629211425781\n",
      "epoch: 18,  batch step: 70, loss: 74.97734832763672\n",
      "epoch: 18,  batch step: 71, loss: 71.42031860351562\n",
      "epoch: 18,  batch step: 72, loss: 15.234488487243652\n",
      "epoch: 18,  batch step: 73, loss: 75.493408203125\n",
      "epoch: 18,  batch step: 74, loss: 133.45130920410156\n",
      "epoch: 18,  batch step: 75, loss: 17.83380889892578\n",
      "epoch: 18,  batch step: 76, loss: 85.0802993774414\n",
      "epoch: 18,  batch step: 77, loss: 59.01707077026367\n",
      "epoch: 18,  batch step: 78, loss: 62.236717224121094\n",
      "epoch: 18,  batch step: 79, loss: 21.89654541015625\n",
      "epoch: 18,  batch step: 80, loss: 98.59066009521484\n",
      "epoch: 18,  batch step: 81, loss: 45.91376876831055\n",
      "epoch: 18,  batch step: 82, loss: 13.791357040405273\n",
      "epoch: 18,  batch step: 83, loss: 84.01329040527344\n",
      "epoch: 18,  batch step: 84, loss: 11.930822372436523\n",
      "epoch: 18,  batch step: 85, loss: 115.34222412109375\n",
      "epoch: 18,  batch step: 86, loss: 121.18328857421875\n",
      "epoch: 18,  batch step: 87, loss: 68.68941497802734\n",
      "epoch: 18,  batch step: 88, loss: 48.44700241088867\n",
      "epoch: 18,  batch step: 89, loss: 14.144533157348633\n",
      "epoch: 18,  batch step: 90, loss: 156.24832153320312\n",
      "epoch: 18,  batch step: 91, loss: 19.505029678344727\n",
      "epoch: 18,  batch step: 92, loss: 53.51825714111328\n",
      "epoch: 18,  batch step: 93, loss: 31.875938415527344\n",
      "epoch: 18,  batch step: 94, loss: 79.31201934814453\n",
      "epoch: 18,  batch step: 95, loss: 34.61405944824219\n",
      "epoch: 18,  batch step: 96, loss: 31.00009536743164\n",
      "epoch: 18,  batch step: 97, loss: 55.19541931152344\n",
      "epoch: 18,  batch step: 98, loss: 33.464447021484375\n",
      "epoch: 18,  batch step: 99, loss: 63.199317932128906\n",
      "epoch: 18,  batch step: 100, loss: 176.41464233398438\n",
      "epoch: 18,  batch step: 101, loss: 55.34272384643555\n",
      "epoch: 18,  batch step: 102, loss: 10.042585372924805\n",
      "epoch: 18,  batch step: 103, loss: 11.664240837097168\n",
      "epoch: 18,  batch step: 104, loss: 288.073974609375\n",
      "epoch: 18,  batch step: 105, loss: 24.984477996826172\n",
      "epoch: 18,  batch step: 106, loss: 11.6450834274292\n",
      "epoch: 18,  batch step: 107, loss: 19.413063049316406\n",
      "epoch: 18,  batch step: 108, loss: 36.92744445800781\n",
      "epoch: 18,  batch step: 109, loss: 11.49470043182373\n",
      "epoch: 18,  batch step: 110, loss: 10.666154861450195\n",
      "epoch: 18,  batch step: 111, loss: 170.44534301757812\n",
      "epoch: 18,  batch step: 112, loss: 15.3065824508667\n",
      "epoch: 18,  batch step: 113, loss: 19.894424438476562\n",
      "epoch: 18,  batch step: 114, loss: 27.43035888671875\n",
      "epoch: 18,  batch step: 115, loss: 17.10077667236328\n",
      "epoch: 18,  batch step: 116, loss: 125.55315399169922\n",
      "epoch: 18,  batch step: 117, loss: 30.46131706237793\n",
      "epoch: 18,  batch step: 118, loss: 92.514892578125\n",
      "epoch: 18,  batch step: 119, loss: 9.706437110900879\n",
      "epoch: 18,  batch step: 120, loss: 13.185256958007812\n",
      "epoch: 18,  batch step: 121, loss: 9.541483879089355\n",
      "epoch: 18,  batch step: 122, loss: 222.4901580810547\n",
      "epoch: 18,  batch step: 123, loss: 18.96316909790039\n",
      "epoch: 18,  batch step: 124, loss: 19.509387969970703\n",
      "epoch: 18,  batch step: 125, loss: 23.198850631713867\n",
      "epoch: 18,  batch step: 126, loss: 11.409894943237305\n",
      "epoch: 18,  batch step: 127, loss: 150.27813720703125\n",
      "epoch: 18,  batch step: 128, loss: 116.53265380859375\n",
      "epoch: 18,  batch step: 129, loss: 22.61644744873047\n",
      "epoch: 18,  batch step: 130, loss: 100.00297546386719\n",
      "epoch: 18,  batch step: 131, loss: 223.3964080810547\n",
      "epoch: 18,  batch step: 132, loss: 74.255126953125\n",
      "epoch: 18,  batch step: 133, loss: 11.844249725341797\n",
      "epoch: 18,  batch step: 134, loss: 79.05935668945312\n",
      "epoch: 18,  batch step: 135, loss: 212.63064575195312\n",
      "epoch: 18,  batch step: 136, loss: 43.25282287597656\n",
      "epoch: 18,  batch step: 137, loss: 20.369556427001953\n",
      "epoch: 18,  batch step: 138, loss: 18.481525421142578\n",
      "epoch: 18,  batch step: 139, loss: 87.0182113647461\n",
      "epoch: 18,  batch step: 140, loss: 28.938907623291016\n",
      "epoch: 18,  batch step: 141, loss: 109.20625305175781\n",
      "epoch: 18,  batch step: 142, loss: 53.24144744873047\n",
      "epoch: 18,  batch step: 143, loss: 42.64112854003906\n",
      "epoch: 18,  batch step: 144, loss: 13.241271018981934\n",
      "epoch: 18,  batch step: 145, loss: 7.38819694519043\n",
      "epoch: 18,  batch step: 146, loss: 31.12588882446289\n",
      "epoch: 18,  batch step: 147, loss: 94.6217269897461\n",
      "epoch: 18,  batch step: 148, loss: 14.578897476196289\n",
      "epoch: 18,  batch step: 149, loss: 38.27044677734375\n",
      "epoch: 18,  batch step: 150, loss: 37.7772331237793\n",
      "epoch: 18,  batch step: 151, loss: 455.1956481933594\n",
      "epoch: 18,  batch step: 152, loss: 177.03524780273438\n",
      "epoch: 18,  batch step: 153, loss: 9.672150611877441\n",
      "epoch: 18,  batch step: 154, loss: 39.37788391113281\n",
      "epoch: 18,  batch step: 155, loss: 459.80755615234375\n",
      "epoch: 18,  batch step: 156, loss: 94.38461303710938\n",
      "epoch: 18,  batch step: 157, loss: 106.04798889160156\n",
      "epoch: 18,  batch step: 158, loss: 8.663203239440918\n",
      "epoch: 18,  batch step: 159, loss: 14.884127616882324\n",
      "epoch: 18,  batch step: 160, loss: 73.86076354980469\n",
      "epoch: 18,  batch step: 161, loss: 13.333353042602539\n",
      "epoch: 18,  batch step: 162, loss: 100.01943969726562\n",
      "epoch: 18,  batch step: 163, loss: 11.051277160644531\n",
      "epoch: 18,  batch step: 164, loss: 74.53536987304688\n",
      "epoch: 18,  batch step: 165, loss: 39.04725646972656\n",
      "epoch: 18,  batch step: 166, loss: 11.279598236083984\n",
      "epoch: 18,  batch step: 167, loss: 60.211360931396484\n",
      "epoch: 18,  batch step: 168, loss: 62.87774658203125\n",
      "epoch: 18,  batch step: 169, loss: 79.37428283691406\n",
      "epoch: 18,  batch step: 170, loss: 192.19677734375\n",
      "epoch: 18,  batch step: 171, loss: 34.54676818847656\n",
      "epoch: 18,  batch step: 172, loss: 58.77704620361328\n",
      "epoch: 18,  batch step: 173, loss: 102.77754211425781\n",
      "epoch: 18,  batch step: 174, loss: 33.555633544921875\n",
      "epoch: 18,  batch step: 175, loss: 163.49069213867188\n",
      "epoch: 18,  batch step: 176, loss: 24.4353084564209\n",
      "epoch: 18,  batch step: 177, loss: 6.6244025230407715\n",
      "epoch: 18,  batch step: 178, loss: 61.37987518310547\n",
      "epoch: 18,  batch step: 179, loss: 246.82481384277344\n",
      "epoch: 18,  batch step: 180, loss: 13.880386352539062\n",
      "epoch: 18,  batch step: 181, loss: 18.655780792236328\n",
      "epoch: 18,  batch step: 182, loss: 63.10436248779297\n",
      "epoch: 18,  batch step: 183, loss: 165.751953125\n",
      "epoch: 18,  batch step: 184, loss: 23.73126983642578\n",
      "epoch: 18,  batch step: 185, loss: 9.958724021911621\n",
      "epoch: 18,  batch step: 186, loss: 102.90535736083984\n",
      "epoch: 18,  batch step: 187, loss: 45.8369140625\n",
      "epoch: 18,  batch step: 188, loss: 6.598171234130859\n",
      "epoch: 18,  batch step: 189, loss: 12.037776947021484\n",
      "epoch: 18,  batch step: 190, loss: 5.773942470550537\n",
      "epoch: 18,  batch step: 191, loss: 122.71890258789062\n",
      "epoch: 18,  batch step: 192, loss: 137.89181518554688\n",
      "epoch: 18,  batch step: 193, loss: 185.14996337890625\n",
      "epoch: 18,  batch step: 194, loss: 88.40159606933594\n",
      "epoch: 18,  batch step: 195, loss: 55.10752487182617\n",
      "epoch: 18,  batch step: 196, loss: 7.674325942993164\n",
      "epoch: 18,  batch step: 197, loss: 11.790132522583008\n",
      "epoch: 18,  batch step: 198, loss: 71.9414291381836\n",
      "epoch: 18,  batch step: 199, loss: 48.785972595214844\n",
      "epoch: 18,  batch step: 200, loss: 120.20269012451172\n",
      "epoch: 18,  batch step: 201, loss: 14.643099784851074\n",
      "epoch: 18,  batch step: 202, loss: 13.977554321289062\n",
      "epoch: 18,  batch step: 203, loss: 107.99044036865234\n",
      "epoch: 18,  batch step: 204, loss: 15.302962303161621\n",
      "epoch: 18,  batch step: 205, loss: 38.860713958740234\n",
      "epoch: 18,  batch step: 206, loss: 30.422412872314453\n",
      "epoch: 18,  batch step: 207, loss: 19.67141342163086\n",
      "epoch: 18,  batch step: 208, loss: 50.33582305908203\n",
      "epoch: 18,  batch step: 209, loss: 18.22004508972168\n",
      "epoch: 18,  batch step: 210, loss: 9.165312767028809\n",
      "epoch: 18,  batch step: 211, loss: 13.255419731140137\n",
      "epoch: 18,  batch step: 212, loss: 85.68614196777344\n",
      "epoch: 18,  batch step: 213, loss: 320.40509033203125\n",
      "epoch: 18,  batch step: 214, loss: 16.66339874267578\n",
      "epoch: 18,  batch step: 215, loss: 54.27125549316406\n",
      "epoch: 18,  batch step: 216, loss: 93.27989959716797\n",
      "epoch: 18,  batch step: 217, loss: 153.99887084960938\n",
      "epoch: 18,  batch step: 218, loss: 31.808563232421875\n",
      "epoch: 18,  batch step: 219, loss: 53.43040466308594\n",
      "epoch: 18,  batch step: 220, loss: 46.14735412597656\n",
      "epoch: 18,  batch step: 221, loss: 11.3546781539917\n",
      "epoch: 18,  batch step: 222, loss: 63.512611389160156\n",
      "epoch: 18,  batch step: 223, loss: 106.41973114013672\n",
      "epoch: 18,  batch step: 224, loss: 97.41227722167969\n",
      "epoch: 18,  batch step: 225, loss: 78.3463363647461\n",
      "epoch: 18,  batch step: 226, loss: 119.70549774169922\n",
      "epoch: 18,  batch step: 227, loss: 4.001518249511719\n",
      "epoch: 18,  batch step: 228, loss: 10.960296630859375\n",
      "epoch: 18,  batch step: 229, loss: 11.713655471801758\n",
      "epoch: 18,  batch step: 230, loss: 19.274005889892578\n",
      "epoch: 18,  batch step: 231, loss: 9.688980102539062\n",
      "epoch: 18,  batch step: 232, loss: 13.247279167175293\n",
      "epoch: 18,  batch step: 233, loss: 12.676116943359375\n",
      "epoch: 18,  batch step: 234, loss: 7.227352142333984\n",
      "epoch: 18,  batch step: 235, loss: 7.034269332885742\n",
      "epoch: 18,  batch step: 236, loss: 24.310712814331055\n",
      "epoch: 18,  batch step: 237, loss: 158.4923095703125\n",
      "epoch: 18,  batch step: 238, loss: 17.740571975708008\n",
      "epoch: 18,  batch step: 239, loss: 24.17871856689453\n",
      "epoch: 18,  batch step: 240, loss: 178.37017822265625\n",
      "epoch: 18,  batch step: 241, loss: 28.01830291748047\n",
      "epoch: 18,  batch step: 242, loss: 24.489124298095703\n",
      "epoch: 18,  batch step: 243, loss: 107.74581909179688\n",
      "epoch: 18,  batch step: 244, loss: 26.139060974121094\n",
      "epoch: 18,  batch step: 245, loss: 76.04586029052734\n",
      "epoch: 18,  batch step: 246, loss: 15.31611156463623\n",
      "epoch: 18,  batch step: 247, loss: 43.203041076660156\n",
      "epoch: 18,  batch step: 248, loss: 70.44010925292969\n",
      "epoch: 18,  batch step: 249, loss: 76.86186981201172\n",
      "epoch: 18,  batch step: 250, loss: 25.07015037536621\n",
      "epoch: 18,  batch step: 251, loss: 77.61796569824219\n",
      "validation error epoch  18:    tensor(67.5855, device='cuda:0')\n",
      "316\n",
      "epoch: 19,  batch step: 0, loss: 7.840712547302246\n",
      "epoch: 19,  batch step: 1, loss: 5.698782920837402\n",
      "epoch: 19,  batch step: 2, loss: 381.814453125\n",
      "epoch: 19,  batch step: 3, loss: 12.05551815032959\n",
      "epoch: 19,  batch step: 4, loss: 121.3535385131836\n",
      "epoch: 19,  batch step: 5, loss: 22.91181755065918\n",
      "epoch: 19,  batch step: 6, loss: 316.9794006347656\n",
      "epoch: 19,  batch step: 7, loss: 11.203710556030273\n",
      "epoch: 19,  batch step: 8, loss: 23.98402976989746\n",
      "epoch: 19,  batch step: 9, loss: 125.58135986328125\n",
      "epoch: 19,  batch step: 10, loss: 37.014312744140625\n",
      "epoch: 19,  batch step: 11, loss: 19.134601593017578\n",
      "epoch: 19,  batch step: 12, loss: 57.26216125488281\n",
      "epoch: 19,  batch step: 13, loss: 25.18730926513672\n",
      "epoch: 19,  batch step: 14, loss: 8.537969589233398\n",
      "epoch: 19,  batch step: 15, loss: 24.063358306884766\n",
      "epoch: 19,  batch step: 16, loss: 85.00305938720703\n",
      "epoch: 19,  batch step: 17, loss: 15.194072723388672\n",
      "epoch: 19,  batch step: 18, loss: 7.333531856536865\n",
      "epoch: 19,  batch step: 19, loss: 86.06706237792969\n",
      "epoch: 19,  batch step: 20, loss: 50.562217712402344\n",
      "epoch: 19,  batch step: 21, loss: 16.755752563476562\n",
      "epoch: 19,  batch step: 22, loss: 9.24842357635498\n",
      "epoch: 19,  batch step: 23, loss: 86.34080505371094\n",
      "epoch: 19,  batch step: 24, loss: 4.531973838806152\n",
      "epoch: 19,  batch step: 25, loss: 9.80293083190918\n",
      "epoch: 19,  batch step: 26, loss: 24.40224838256836\n",
      "epoch: 19,  batch step: 27, loss: 49.00374984741211\n",
      "epoch: 19,  batch step: 28, loss: 24.96021270751953\n",
      "epoch: 19,  batch step: 29, loss: 16.327577590942383\n",
      "epoch: 19,  batch step: 30, loss: 13.852649688720703\n",
      "epoch: 19,  batch step: 31, loss: 6.5676984786987305\n",
      "epoch: 19,  batch step: 32, loss: 4.556463241577148\n",
      "epoch: 19,  batch step: 33, loss: 123.23780822753906\n",
      "epoch: 19,  batch step: 34, loss: 9.871085166931152\n",
      "epoch: 19,  batch step: 35, loss: 19.896289825439453\n",
      "epoch: 19,  batch step: 36, loss: 64.02813720703125\n",
      "epoch: 19,  batch step: 37, loss: 24.87411880493164\n",
      "epoch: 19,  batch step: 38, loss: 15.011123657226562\n",
      "epoch: 19,  batch step: 39, loss: 8.85255241394043\n",
      "epoch: 19,  batch step: 40, loss: 120.6126937866211\n",
      "epoch: 19,  batch step: 41, loss: 266.902099609375\n",
      "epoch: 19,  batch step: 42, loss: 32.651302337646484\n",
      "epoch: 19,  batch step: 43, loss: 16.378238677978516\n",
      "epoch: 19,  batch step: 44, loss: 106.28026580810547\n",
      "epoch: 19,  batch step: 45, loss: 166.0311279296875\n",
      "epoch: 19,  batch step: 46, loss: 187.93954467773438\n",
      "epoch: 19,  batch step: 47, loss: 135.9078369140625\n",
      "epoch: 19,  batch step: 48, loss: 73.86529541015625\n",
      "epoch: 19,  batch step: 49, loss: 11.842880249023438\n",
      "epoch: 19,  batch step: 50, loss: 50.94749069213867\n",
      "epoch: 19,  batch step: 51, loss: 82.98175048828125\n",
      "epoch: 19,  batch step: 52, loss: 98.841552734375\n",
      "epoch: 19,  batch step: 53, loss: 29.71284294128418\n",
      "epoch: 19,  batch step: 54, loss: 124.26253509521484\n",
      "epoch: 19,  batch step: 55, loss: 75.91683197021484\n",
      "epoch: 19,  batch step: 56, loss: 23.89765167236328\n",
      "epoch: 19,  batch step: 57, loss: 69.84699249267578\n",
      "epoch: 19,  batch step: 58, loss: 21.8016357421875\n",
      "epoch: 19,  batch step: 59, loss: 89.77513885498047\n",
      "epoch: 19,  batch step: 60, loss: 18.890655517578125\n",
      "epoch: 19,  batch step: 61, loss: 31.433401107788086\n",
      "epoch: 19,  batch step: 62, loss: 21.806678771972656\n",
      "epoch: 19,  batch step: 63, loss: 33.3923225402832\n",
      "epoch: 19,  batch step: 64, loss: 56.73301696777344\n",
      "epoch: 19,  batch step: 65, loss: 45.9949951171875\n",
      "epoch: 19,  batch step: 66, loss: 26.306989669799805\n",
      "epoch: 19,  batch step: 67, loss: 58.617584228515625\n",
      "epoch: 19,  batch step: 68, loss: 49.4625358581543\n",
      "epoch: 19,  batch step: 69, loss: 234.81784057617188\n",
      "epoch: 19,  batch step: 70, loss: 150.97360229492188\n",
      "epoch: 19,  batch step: 71, loss: 135.62501525878906\n",
      "epoch: 19,  batch step: 72, loss: 55.65318298339844\n",
      "epoch: 19,  batch step: 73, loss: 14.961376190185547\n",
      "epoch: 19,  batch step: 74, loss: 54.671993255615234\n",
      "epoch: 19,  batch step: 75, loss: 6.883003234863281\n",
      "epoch: 19,  batch step: 76, loss: 101.51373291015625\n",
      "epoch: 19,  batch step: 77, loss: 212.1228485107422\n",
      "epoch: 19,  batch step: 78, loss: 87.60647583007812\n",
      "epoch: 19,  batch step: 79, loss: 12.461913108825684\n",
      "epoch: 19,  batch step: 80, loss: 15.831031799316406\n",
      "epoch: 19,  batch step: 81, loss: 5.955721855163574\n",
      "epoch: 19,  batch step: 82, loss: 180.96482849121094\n",
      "epoch: 19,  batch step: 83, loss: 10.09135627746582\n",
      "epoch: 19,  batch step: 84, loss: 9.758764266967773\n",
      "epoch: 19,  batch step: 85, loss: 9.026453971862793\n",
      "epoch: 19,  batch step: 86, loss: 4.482566833496094\n",
      "epoch: 19,  batch step: 87, loss: 12.747459411621094\n",
      "epoch: 19,  batch step: 88, loss: 12.758563995361328\n",
      "epoch: 19,  batch step: 89, loss: 17.240428924560547\n",
      "epoch: 19,  batch step: 90, loss: 39.10451126098633\n",
      "epoch: 19,  batch step: 91, loss: 11.473235130310059\n",
      "epoch: 19,  batch step: 92, loss: 25.387662887573242\n",
      "epoch: 19,  batch step: 93, loss: 12.40344524383545\n",
      "epoch: 19,  batch step: 94, loss: 109.97602844238281\n",
      "epoch: 19,  batch step: 95, loss: 127.60760498046875\n",
      "epoch: 19,  batch step: 96, loss: 189.6174774169922\n",
      "epoch: 19,  batch step: 97, loss: 109.119384765625\n",
      "epoch: 19,  batch step: 98, loss: 109.08395385742188\n",
      "epoch: 19,  batch step: 99, loss: 64.81793212890625\n",
      "epoch: 19,  batch step: 100, loss: 20.172365188598633\n",
      "epoch: 19,  batch step: 101, loss: 82.65045928955078\n",
      "epoch: 19,  batch step: 102, loss: 13.498052597045898\n",
      "epoch: 19,  batch step: 103, loss: 111.06292724609375\n",
      "epoch: 19,  batch step: 104, loss: 68.39913940429688\n",
      "epoch: 19,  batch step: 105, loss: 14.276201248168945\n",
      "epoch: 19,  batch step: 106, loss: 50.798118591308594\n",
      "epoch: 19,  batch step: 107, loss: 49.098876953125\n",
      "epoch: 19,  batch step: 108, loss: 19.965408325195312\n",
      "epoch: 19,  batch step: 109, loss: 8.441831588745117\n",
      "epoch: 19,  batch step: 110, loss: 25.801185607910156\n",
      "epoch: 19,  batch step: 111, loss: 53.09684753417969\n",
      "epoch: 19,  batch step: 112, loss: 36.33918380737305\n",
      "epoch: 19,  batch step: 113, loss: 12.5767822265625\n",
      "epoch: 19,  batch step: 114, loss: 11.37967300415039\n",
      "epoch: 19,  batch step: 115, loss: 14.564688682556152\n",
      "epoch: 19,  batch step: 116, loss: 22.806373596191406\n",
      "epoch: 19,  batch step: 117, loss: 7.80221700668335\n",
      "epoch: 19,  batch step: 118, loss: 21.552669525146484\n",
      "epoch: 19,  batch step: 119, loss: 17.14681053161621\n",
      "epoch: 19,  batch step: 120, loss: 13.68072509765625\n",
      "epoch: 19,  batch step: 121, loss: 315.5537109375\n",
      "epoch: 19,  batch step: 122, loss: 6.47132682800293\n",
      "epoch: 19,  batch step: 123, loss: 11.801990509033203\n",
      "epoch: 19,  batch step: 124, loss: 23.62830924987793\n",
      "epoch: 19,  batch step: 125, loss: 122.6529769897461\n",
      "epoch: 19,  batch step: 126, loss: 155.1552734375\n",
      "epoch: 19,  batch step: 127, loss: 229.0276336669922\n",
      "epoch: 19,  batch step: 128, loss: 12.467669486999512\n",
      "epoch: 19,  batch step: 129, loss: 8.466144561767578\n",
      "epoch: 19,  batch step: 130, loss: 16.234464645385742\n",
      "epoch: 19,  batch step: 131, loss: 48.79461669921875\n",
      "epoch: 19,  batch step: 132, loss: 8.507341384887695\n",
      "epoch: 19,  batch step: 133, loss: 99.02757263183594\n",
      "epoch: 19,  batch step: 134, loss: 33.64599609375\n",
      "epoch: 19,  batch step: 135, loss: 20.08449935913086\n",
      "epoch: 19,  batch step: 136, loss: 7.060906410217285\n",
      "epoch: 19,  batch step: 137, loss: 21.65249252319336\n",
      "epoch: 19,  batch step: 138, loss: 97.99678039550781\n",
      "epoch: 19,  batch step: 139, loss: 13.440260887145996\n",
      "epoch: 19,  batch step: 140, loss: 83.54486846923828\n",
      "epoch: 19,  batch step: 141, loss: 181.19174194335938\n",
      "epoch: 19,  batch step: 142, loss: 79.1268539428711\n",
      "epoch: 19,  batch step: 143, loss: 26.18309783935547\n",
      "epoch: 19,  batch step: 144, loss: 7.890898704528809\n",
      "epoch: 19,  batch step: 145, loss: 12.717304229736328\n",
      "epoch: 19,  batch step: 146, loss: 13.18995189666748\n",
      "epoch: 19,  batch step: 147, loss: 33.383811950683594\n",
      "epoch: 19,  batch step: 148, loss: 119.56148529052734\n",
      "epoch: 19,  batch step: 149, loss: 47.48780059814453\n",
      "epoch: 19,  batch step: 150, loss: 19.071224212646484\n",
      "epoch: 19,  batch step: 151, loss: 11.098150253295898\n",
      "epoch: 19,  batch step: 152, loss: 23.480018615722656\n",
      "epoch: 19,  batch step: 153, loss: 17.920347213745117\n",
      "epoch: 19,  batch step: 154, loss: 89.96141052246094\n",
      "epoch: 19,  batch step: 155, loss: 37.41250228881836\n",
      "epoch: 19,  batch step: 156, loss: 16.454872131347656\n",
      "epoch: 19,  batch step: 157, loss: 13.2577543258667\n",
      "epoch: 19,  batch step: 158, loss: 10.928356170654297\n",
      "epoch: 19,  batch step: 159, loss: 58.96165084838867\n",
      "epoch: 19,  batch step: 160, loss: 11.070428848266602\n",
      "epoch: 19,  batch step: 161, loss: 100.7545166015625\n",
      "epoch: 19,  batch step: 162, loss: 29.36361312866211\n",
      "epoch: 19,  batch step: 163, loss: 5.713099956512451\n",
      "epoch: 19,  batch step: 164, loss: 29.05402183532715\n",
      "epoch: 19,  batch step: 165, loss: 6.851866722106934\n",
      "epoch: 19,  batch step: 166, loss: 23.97175407409668\n",
      "epoch: 19,  batch step: 167, loss: 17.29446792602539\n",
      "epoch: 19,  batch step: 168, loss: 16.007701873779297\n",
      "epoch: 19,  batch step: 169, loss: 181.4148406982422\n",
      "epoch: 19,  batch step: 170, loss: 6.999129295349121\n",
      "epoch: 19,  batch step: 171, loss: 198.15245056152344\n",
      "epoch: 19,  batch step: 172, loss: 6.827508449554443\n",
      "epoch: 19,  batch step: 173, loss: 11.666979789733887\n",
      "epoch: 19,  batch step: 174, loss: 82.04972076416016\n",
      "epoch: 19,  batch step: 175, loss: 39.78771209716797\n",
      "epoch: 19,  batch step: 176, loss: 19.806184768676758\n",
      "epoch: 19,  batch step: 177, loss: 70.12730407714844\n",
      "epoch: 19,  batch step: 178, loss: 187.43370056152344\n",
      "epoch: 19,  batch step: 179, loss: 29.182706832885742\n",
      "epoch: 19,  batch step: 180, loss: 12.605621337890625\n",
      "epoch: 19,  batch step: 181, loss: 39.49886703491211\n",
      "epoch: 19,  batch step: 182, loss: 14.624385833740234\n",
      "epoch: 19,  batch step: 183, loss: 33.00723648071289\n",
      "epoch: 19,  batch step: 184, loss: 138.25962829589844\n",
      "epoch: 19,  batch step: 185, loss: 52.296897888183594\n",
      "epoch: 19,  batch step: 186, loss: 25.2794189453125\n",
      "epoch: 19,  batch step: 187, loss: 11.28997802734375\n",
      "epoch: 19,  batch step: 188, loss: 13.183176040649414\n",
      "epoch: 19,  batch step: 189, loss: 36.45550537109375\n",
      "epoch: 19,  batch step: 190, loss: 12.613554000854492\n",
      "epoch: 19,  batch step: 191, loss: 63.834388732910156\n",
      "epoch: 19,  batch step: 192, loss: 49.556880950927734\n",
      "epoch: 19,  batch step: 193, loss: 121.82138061523438\n",
      "epoch: 19,  batch step: 194, loss: 196.59642028808594\n",
      "epoch: 19,  batch step: 195, loss: 11.779559135437012\n",
      "epoch: 19,  batch step: 196, loss: 24.782995223999023\n",
      "epoch: 19,  batch step: 197, loss: 160.34957885742188\n",
      "epoch: 19,  batch step: 198, loss: 82.814208984375\n",
      "epoch: 19,  batch step: 199, loss: 10.541423797607422\n",
      "epoch: 19,  batch step: 200, loss: 8.689432144165039\n",
      "epoch: 19,  batch step: 201, loss: 61.358177185058594\n",
      "epoch: 19,  batch step: 202, loss: 12.254355430603027\n",
      "epoch: 19,  batch step: 203, loss: 81.57735443115234\n",
      "epoch: 19,  batch step: 204, loss: 54.60906982421875\n",
      "epoch: 19,  batch step: 205, loss: 202.69053649902344\n",
      "epoch: 19,  batch step: 206, loss: 213.0394287109375\n",
      "epoch: 19,  batch step: 207, loss: 30.974233627319336\n",
      "epoch: 19,  batch step: 208, loss: 41.60731506347656\n",
      "epoch: 19,  batch step: 209, loss: 35.48612594604492\n",
      "epoch: 19,  batch step: 210, loss: 48.27958297729492\n",
      "epoch: 19,  batch step: 211, loss: 103.20037841796875\n",
      "epoch: 19,  batch step: 212, loss: 20.149765014648438\n",
      "epoch: 19,  batch step: 213, loss: 47.17561340332031\n",
      "epoch: 19,  batch step: 214, loss: 31.42684555053711\n",
      "epoch: 19,  batch step: 215, loss: 4.608485221862793\n",
      "epoch: 19,  batch step: 216, loss: 21.027671813964844\n",
      "epoch: 19,  batch step: 217, loss: 58.964073181152344\n",
      "epoch: 19,  batch step: 218, loss: 15.913622856140137\n",
      "epoch: 19,  batch step: 219, loss: 43.485565185546875\n",
      "epoch: 19,  batch step: 220, loss: 329.39935302734375\n",
      "epoch: 19,  batch step: 221, loss: 53.539398193359375\n",
      "epoch: 19,  batch step: 222, loss: 19.419231414794922\n",
      "epoch: 19,  batch step: 223, loss: 8.547212600708008\n",
      "epoch: 19,  batch step: 224, loss: 187.14443969726562\n",
      "epoch: 19,  batch step: 225, loss: 30.45136260986328\n",
      "epoch: 19,  batch step: 226, loss: 4.970855712890625\n",
      "epoch: 19,  batch step: 227, loss: 231.96533203125\n",
      "epoch: 19,  batch step: 228, loss: 8.028642654418945\n",
      "epoch: 19,  batch step: 229, loss: 285.58587646484375\n",
      "epoch: 19,  batch step: 230, loss: 93.66645812988281\n",
      "epoch: 19,  batch step: 231, loss: 11.29568862915039\n",
      "epoch: 19,  batch step: 232, loss: 89.7555160522461\n",
      "epoch: 19,  batch step: 233, loss: 125.99298095703125\n",
      "epoch: 19,  batch step: 234, loss: 94.8374252319336\n",
      "epoch: 19,  batch step: 235, loss: 8.126361846923828\n",
      "epoch: 19,  batch step: 236, loss: 21.47035789489746\n",
      "epoch: 19,  batch step: 237, loss: 28.569313049316406\n",
      "epoch: 19,  batch step: 238, loss: 10.089594841003418\n",
      "epoch: 19,  batch step: 239, loss: 60.57477951049805\n",
      "epoch: 19,  batch step: 240, loss: 14.600871086120605\n",
      "epoch: 19,  batch step: 241, loss: 35.652591705322266\n",
      "epoch: 19,  batch step: 242, loss: 12.884249687194824\n",
      "epoch: 19,  batch step: 243, loss: 170.6944580078125\n",
      "epoch: 19,  batch step: 244, loss: 16.641294479370117\n",
      "epoch: 19,  batch step: 245, loss: 197.8712615966797\n",
      "epoch: 19,  batch step: 246, loss: 49.916725158691406\n",
      "epoch: 19,  batch step: 247, loss: 407.47039794921875\n",
      "epoch: 19,  batch step: 248, loss: 213.4889373779297\n",
      "epoch: 19,  batch step: 249, loss: 90.56680297851562\n",
      "epoch: 19,  batch step: 250, loss: 25.730331420898438\n",
      "epoch: 19,  batch step: 251, loss: 95.76182556152344\n",
      "finished saving checkpoints\n",
      "validation error epoch  19:    tensor(90.7909, device='cuda:0')\n",
      "316\n",
      "epoch: 20,  batch step: 0, loss: 65.288330078125\n",
      "epoch: 20,  batch step: 1, loss: 23.396587371826172\n",
      "epoch: 20,  batch step: 2, loss: 17.47699737548828\n",
      "epoch: 20,  batch step: 3, loss: 35.50617599487305\n",
      "epoch: 20,  batch step: 4, loss: 20.78074073791504\n",
      "epoch: 20,  batch step: 5, loss: 90.47029876708984\n",
      "epoch: 20,  batch step: 6, loss: 61.32188415527344\n",
      "epoch: 20,  batch step: 7, loss: 100.36602783203125\n",
      "epoch: 20,  batch step: 8, loss: 19.383865356445312\n",
      "epoch: 20,  batch step: 9, loss: 9.449100494384766\n",
      "epoch: 20,  batch step: 10, loss: 47.633026123046875\n",
      "epoch: 20,  batch step: 11, loss: 7.641232967376709\n",
      "epoch: 20,  batch step: 12, loss: 14.150747299194336\n",
      "epoch: 20,  batch step: 13, loss: 185.88140869140625\n",
      "epoch: 20,  batch step: 14, loss: 67.48617553710938\n",
      "epoch: 20,  batch step: 15, loss: 164.72998046875\n",
      "epoch: 20,  batch step: 16, loss: 5.4680585861206055\n",
      "epoch: 20,  batch step: 17, loss: 8.772125244140625\n",
      "epoch: 20,  batch step: 18, loss: 46.18397521972656\n",
      "epoch: 20,  batch step: 19, loss: 9.997745513916016\n",
      "epoch: 20,  batch step: 20, loss: 48.67809295654297\n",
      "epoch: 20,  batch step: 21, loss: 58.72062301635742\n",
      "epoch: 20,  batch step: 22, loss: 10.592289924621582\n",
      "epoch: 20,  batch step: 23, loss: 114.95637512207031\n",
      "epoch: 20,  batch step: 24, loss: 11.558738708496094\n",
      "epoch: 20,  batch step: 25, loss: 38.127891540527344\n",
      "epoch: 20,  batch step: 26, loss: 13.776443481445312\n",
      "epoch: 20,  batch step: 27, loss: 38.459102630615234\n",
      "epoch: 20,  batch step: 28, loss: 11.016345977783203\n",
      "epoch: 20,  batch step: 29, loss: 11.058821678161621\n",
      "epoch: 20,  batch step: 30, loss: 72.42323303222656\n",
      "epoch: 20,  batch step: 31, loss: 90.1424789428711\n",
      "epoch: 20,  batch step: 32, loss: 223.0952606201172\n",
      "epoch: 20,  batch step: 33, loss: 8.082085609436035\n",
      "epoch: 20,  batch step: 34, loss: 41.31236267089844\n",
      "epoch: 20,  batch step: 35, loss: 100.78341674804688\n",
      "epoch: 20,  batch step: 36, loss: 21.175878524780273\n",
      "epoch: 20,  batch step: 37, loss: 28.26959800720215\n",
      "epoch: 20,  batch step: 38, loss: 38.61976623535156\n",
      "epoch: 20,  batch step: 39, loss: 81.3761215209961\n",
      "epoch: 20,  batch step: 40, loss: 70.40667724609375\n",
      "epoch: 20,  batch step: 41, loss: 21.90314483642578\n",
      "epoch: 20,  batch step: 42, loss: 14.150866508483887\n",
      "epoch: 20,  batch step: 43, loss: 32.26049041748047\n",
      "epoch: 20,  batch step: 44, loss: 98.33294677734375\n",
      "epoch: 20,  batch step: 45, loss: 15.764033317565918\n",
      "epoch: 20,  batch step: 46, loss: 10.549440383911133\n",
      "epoch: 20,  batch step: 47, loss: 37.773193359375\n",
      "epoch: 20,  batch step: 48, loss: 17.7227840423584\n",
      "epoch: 20,  batch step: 49, loss: 162.34136962890625\n",
      "epoch: 20,  batch step: 50, loss: 119.38450622558594\n",
      "epoch: 20,  batch step: 51, loss: 9.832161903381348\n",
      "epoch: 20,  batch step: 52, loss: 29.313945770263672\n",
      "epoch: 20,  batch step: 53, loss: 4.655146598815918\n",
      "epoch: 20,  batch step: 54, loss: 14.095014572143555\n",
      "epoch: 20,  batch step: 55, loss: 14.532995223999023\n",
      "epoch: 20,  batch step: 56, loss: 28.81414794921875\n",
      "epoch: 20,  batch step: 57, loss: 14.548974990844727\n",
      "epoch: 20,  batch step: 58, loss: 90.76061248779297\n",
      "epoch: 20,  batch step: 59, loss: 8.130988121032715\n",
      "epoch: 20,  batch step: 60, loss: 234.1522216796875\n",
      "epoch: 20,  batch step: 61, loss: 47.843013763427734\n",
      "epoch: 20,  batch step: 62, loss: 32.70008087158203\n",
      "epoch: 20,  batch step: 63, loss: 35.423004150390625\n",
      "epoch: 20,  batch step: 64, loss: 183.5156707763672\n",
      "epoch: 20,  batch step: 65, loss: 28.756641387939453\n",
      "epoch: 20,  batch step: 66, loss: 82.86891174316406\n",
      "epoch: 20,  batch step: 67, loss: 55.712120056152344\n",
      "epoch: 20,  batch step: 68, loss: 43.466888427734375\n",
      "epoch: 20,  batch step: 69, loss: 42.23908233642578\n",
      "epoch: 20,  batch step: 70, loss: 9.544618606567383\n",
      "epoch: 20,  batch step: 71, loss: 32.90678405761719\n",
      "epoch: 20,  batch step: 72, loss: 216.87814331054688\n",
      "epoch: 20,  batch step: 73, loss: 15.590479850769043\n",
      "epoch: 20,  batch step: 74, loss: 9.819879531860352\n",
      "epoch: 20,  batch step: 75, loss: 99.51338195800781\n",
      "epoch: 20,  batch step: 76, loss: 13.266197204589844\n",
      "epoch: 20,  batch step: 77, loss: 52.24348068237305\n",
      "epoch: 20,  batch step: 78, loss: 11.027304649353027\n",
      "epoch: 20,  batch step: 79, loss: 15.515151023864746\n",
      "epoch: 20,  batch step: 80, loss: 15.301197052001953\n",
      "epoch: 20,  batch step: 81, loss: 36.68960189819336\n",
      "epoch: 20,  batch step: 82, loss: 37.07575225830078\n",
      "epoch: 20,  batch step: 83, loss: 11.219871520996094\n",
      "epoch: 20,  batch step: 84, loss: 45.05542755126953\n",
      "epoch: 20,  batch step: 85, loss: 89.94329833984375\n",
      "epoch: 20,  batch step: 86, loss: 12.51347541809082\n",
      "epoch: 20,  batch step: 87, loss: 10.855606079101562\n",
      "epoch: 20,  batch step: 88, loss: 20.131803512573242\n",
      "epoch: 20,  batch step: 89, loss: 13.699114799499512\n",
      "epoch: 20,  batch step: 90, loss: 110.23075866699219\n",
      "epoch: 20,  batch step: 91, loss: 191.61514282226562\n",
      "epoch: 20,  batch step: 92, loss: 146.01004028320312\n",
      "epoch: 20,  batch step: 93, loss: 167.81211853027344\n",
      "epoch: 20,  batch step: 94, loss: 165.29806518554688\n",
      "epoch: 20,  batch step: 95, loss: 37.453704833984375\n",
      "epoch: 20,  batch step: 96, loss: 27.401044845581055\n",
      "epoch: 20,  batch step: 97, loss: 12.09378719329834\n",
      "epoch: 20,  batch step: 98, loss: 23.19643211364746\n",
      "epoch: 20,  batch step: 99, loss: 36.377830505371094\n",
      "epoch: 20,  batch step: 100, loss: 6.922274589538574\n",
      "epoch: 20,  batch step: 101, loss: 20.337238311767578\n",
      "epoch: 20,  batch step: 102, loss: 122.15452575683594\n",
      "epoch: 20,  batch step: 103, loss: 27.121475219726562\n",
      "epoch: 20,  batch step: 104, loss: 51.49998092651367\n",
      "epoch: 20,  batch step: 105, loss: 19.18292999267578\n",
      "epoch: 20,  batch step: 106, loss: 14.359698295593262\n",
      "epoch: 20,  batch step: 107, loss: 158.307861328125\n",
      "epoch: 20,  batch step: 108, loss: 85.9835205078125\n",
      "epoch: 20,  batch step: 109, loss: 30.00482749938965\n",
      "epoch: 20,  batch step: 110, loss: 21.417083740234375\n",
      "epoch: 20,  batch step: 111, loss: 26.30477523803711\n",
      "epoch: 20,  batch step: 112, loss: 14.225980758666992\n",
      "epoch: 20,  batch step: 113, loss: 51.458229064941406\n",
      "epoch: 20,  batch step: 114, loss: 7.860504150390625\n",
      "epoch: 20,  batch step: 115, loss: 257.4733581542969\n",
      "epoch: 20,  batch step: 116, loss: 61.31268310546875\n",
      "epoch: 20,  batch step: 117, loss: 178.53372192382812\n",
      "epoch: 20,  batch step: 118, loss: 71.54364013671875\n",
      "epoch: 20,  batch step: 119, loss: 17.44366455078125\n",
      "epoch: 20,  batch step: 120, loss: 96.16146850585938\n",
      "epoch: 20,  batch step: 121, loss: 38.33938980102539\n",
      "epoch: 20,  batch step: 122, loss: 72.55026245117188\n",
      "epoch: 20,  batch step: 123, loss: 24.52472686767578\n",
      "epoch: 20,  batch step: 124, loss: 34.88206100463867\n",
      "epoch: 20,  batch step: 125, loss: 7.687825679779053\n",
      "epoch: 20,  batch step: 126, loss: 14.008273124694824\n",
      "epoch: 20,  batch step: 127, loss: 9.120063781738281\n",
      "epoch: 20,  batch step: 128, loss: 5.6079254150390625\n",
      "epoch: 20,  batch step: 129, loss: 169.28079223632812\n",
      "epoch: 20,  batch step: 130, loss: 25.10866928100586\n",
      "epoch: 20,  batch step: 131, loss: 16.057510375976562\n",
      "epoch: 20,  batch step: 132, loss: 11.755770683288574\n",
      "epoch: 20,  batch step: 133, loss: 16.279577255249023\n",
      "epoch: 20,  batch step: 134, loss: 17.990333557128906\n",
      "epoch: 20,  batch step: 135, loss: 93.10100555419922\n",
      "epoch: 20,  batch step: 136, loss: 9.331611633300781\n",
      "epoch: 20,  batch step: 137, loss: 13.794215202331543\n",
      "epoch: 20,  batch step: 138, loss: 8.868318557739258\n",
      "epoch: 20,  batch step: 139, loss: 6.279098033905029\n",
      "epoch: 20,  batch step: 140, loss: 41.64070510864258\n",
      "epoch: 20,  batch step: 141, loss: 30.889862060546875\n",
      "epoch: 20,  batch step: 142, loss: 8.379873275756836\n",
      "epoch: 20,  batch step: 143, loss: 63.76165008544922\n",
      "epoch: 20,  batch step: 144, loss: 49.09821319580078\n",
      "epoch: 20,  batch step: 145, loss: 117.92918395996094\n",
      "epoch: 20,  batch step: 146, loss: 8.136704444885254\n",
      "epoch: 20,  batch step: 147, loss: 17.04763412475586\n",
      "epoch: 20,  batch step: 148, loss: 30.887380599975586\n",
      "epoch: 20,  batch step: 149, loss: 18.902698516845703\n",
      "epoch: 20,  batch step: 150, loss: 94.78289031982422\n",
      "epoch: 20,  batch step: 151, loss: 12.496020317077637\n",
      "epoch: 20,  batch step: 152, loss: 28.280654907226562\n",
      "epoch: 20,  batch step: 153, loss: 117.6134033203125\n",
      "epoch: 20,  batch step: 154, loss: 11.486922264099121\n",
      "epoch: 20,  batch step: 155, loss: 8.506158828735352\n",
      "epoch: 20,  batch step: 156, loss: 15.710169792175293\n",
      "epoch: 20,  batch step: 157, loss: 57.59687423706055\n",
      "epoch: 20,  batch step: 158, loss: 188.0319061279297\n",
      "epoch: 20,  batch step: 159, loss: 138.1139678955078\n",
      "epoch: 20,  batch step: 160, loss: 16.242116928100586\n",
      "epoch: 20,  batch step: 161, loss: 10.316080093383789\n",
      "epoch: 20,  batch step: 162, loss: 54.61119842529297\n",
      "epoch: 20,  batch step: 163, loss: 9.021471977233887\n",
      "epoch: 20,  batch step: 164, loss: 10.851313591003418\n",
      "epoch: 20,  batch step: 165, loss: 269.63934326171875\n",
      "epoch: 20,  batch step: 166, loss: 116.28451538085938\n",
      "epoch: 20,  batch step: 167, loss: 70.73851013183594\n",
      "epoch: 20,  batch step: 168, loss: 79.73653411865234\n",
      "epoch: 20,  batch step: 169, loss: 15.573301315307617\n",
      "epoch: 20,  batch step: 170, loss: 50.687049865722656\n",
      "epoch: 20,  batch step: 171, loss: 201.52902221679688\n",
      "epoch: 20,  batch step: 172, loss: 33.535621643066406\n",
      "epoch: 20,  batch step: 173, loss: 178.62673950195312\n",
      "epoch: 20,  batch step: 174, loss: 323.3442077636719\n",
      "epoch: 20,  batch step: 175, loss: 81.74635314941406\n",
      "epoch: 20,  batch step: 176, loss: 21.87075424194336\n",
      "epoch: 20,  batch step: 177, loss: 67.98027038574219\n",
      "epoch: 20,  batch step: 178, loss: 11.911089897155762\n",
      "epoch: 20,  batch step: 179, loss: 15.127264022827148\n",
      "epoch: 20,  batch step: 180, loss: 6.316991806030273\n",
      "epoch: 20,  batch step: 181, loss: 6.905979156494141\n",
      "epoch: 20,  batch step: 182, loss: 21.482702255249023\n",
      "epoch: 20,  batch step: 183, loss: 24.98418426513672\n",
      "epoch: 20,  batch step: 184, loss: 9.152901649475098\n",
      "epoch: 20,  batch step: 185, loss: 12.92872142791748\n",
      "epoch: 20,  batch step: 186, loss: 13.170337677001953\n",
      "epoch: 20,  batch step: 187, loss: 131.19796752929688\n",
      "epoch: 20,  batch step: 188, loss: 66.9088134765625\n",
      "epoch: 20,  batch step: 189, loss: 75.16065979003906\n",
      "epoch: 20,  batch step: 190, loss: 12.141947746276855\n",
      "epoch: 20,  batch step: 191, loss: 7.504524230957031\n",
      "epoch: 20,  batch step: 192, loss: 18.99017333984375\n",
      "epoch: 20,  batch step: 193, loss: 51.80328369140625\n",
      "epoch: 20,  batch step: 194, loss: 21.733154296875\n",
      "epoch: 20,  batch step: 195, loss: 8.067543029785156\n",
      "epoch: 20,  batch step: 196, loss: 7.33920955657959\n",
      "epoch: 20,  batch step: 197, loss: 309.0894775390625\n",
      "epoch: 20,  batch step: 198, loss: 190.38998413085938\n",
      "epoch: 20,  batch step: 199, loss: 23.940898895263672\n",
      "epoch: 20,  batch step: 200, loss: 26.32430648803711\n",
      "epoch: 20,  batch step: 201, loss: 54.20388412475586\n",
      "epoch: 20,  batch step: 202, loss: 89.45628356933594\n",
      "epoch: 20,  batch step: 203, loss: 123.63510131835938\n",
      "epoch: 20,  batch step: 204, loss: 151.04074096679688\n",
      "epoch: 20,  batch step: 205, loss: 90.48504638671875\n",
      "epoch: 20,  batch step: 206, loss: 22.935468673706055\n",
      "epoch: 20,  batch step: 207, loss: 59.515045166015625\n",
      "epoch: 20,  batch step: 208, loss: 33.555091857910156\n",
      "epoch: 20,  batch step: 209, loss: 78.53325653076172\n",
      "epoch: 20,  batch step: 210, loss: 64.70623016357422\n",
      "epoch: 20,  batch step: 211, loss: 81.31771087646484\n",
      "epoch: 20,  batch step: 212, loss: 10.39852237701416\n",
      "epoch: 20,  batch step: 213, loss: 32.848716735839844\n",
      "epoch: 20,  batch step: 214, loss: 225.48876953125\n",
      "epoch: 20,  batch step: 215, loss: 91.63334655761719\n",
      "epoch: 20,  batch step: 216, loss: 34.73153305053711\n",
      "epoch: 20,  batch step: 217, loss: 19.202838897705078\n",
      "epoch: 20,  batch step: 218, loss: 50.34422302246094\n",
      "epoch: 20,  batch step: 219, loss: 8.283103942871094\n",
      "epoch: 20,  batch step: 220, loss: 134.00396728515625\n",
      "epoch: 20,  batch step: 221, loss: 39.61354064941406\n",
      "epoch: 20,  batch step: 222, loss: 12.777633666992188\n",
      "epoch: 20,  batch step: 223, loss: 47.88874435424805\n",
      "epoch: 20,  batch step: 224, loss: 16.387147903442383\n",
      "epoch: 20,  batch step: 225, loss: 106.22664642333984\n",
      "epoch: 20,  batch step: 226, loss: 12.528613090515137\n",
      "epoch: 20,  batch step: 227, loss: 169.1458282470703\n",
      "epoch: 20,  batch step: 228, loss: 11.23560905456543\n",
      "epoch: 20,  batch step: 229, loss: 79.63600158691406\n",
      "epoch: 20,  batch step: 230, loss: 12.671276092529297\n",
      "epoch: 20,  batch step: 231, loss: 53.0033073425293\n",
      "epoch: 20,  batch step: 232, loss: 13.167182922363281\n",
      "epoch: 20,  batch step: 233, loss: 10.005081176757812\n",
      "epoch: 20,  batch step: 234, loss: 30.0706844329834\n",
      "epoch: 20,  batch step: 235, loss: 10.4973783493042\n",
      "epoch: 20,  batch step: 236, loss: 135.01512145996094\n",
      "epoch: 20,  batch step: 237, loss: 42.57952880859375\n",
      "epoch: 20,  batch step: 238, loss: 74.62371063232422\n",
      "epoch: 20,  batch step: 239, loss: 17.067331314086914\n",
      "epoch: 20,  batch step: 240, loss: 132.0550079345703\n",
      "epoch: 20,  batch step: 241, loss: 206.55987548828125\n",
      "epoch: 20,  batch step: 242, loss: 56.63832092285156\n",
      "epoch: 20,  batch step: 243, loss: 149.97003173828125\n",
      "epoch: 20,  batch step: 244, loss: 111.56233215332031\n",
      "epoch: 20,  batch step: 245, loss: 23.96813201904297\n",
      "epoch: 20,  batch step: 246, loss: 72.31863403320312\n",
      "epoch: 20,  batch step: 247, loss: 82.83570098876953\n",
      "epoch: 20,  batch step: 248, loss: 193.125732421875\n",
      "epoch: 20,  batch step: 249, loss: 23.44051742553711\n",
      "epoch: 20,  batch step: 250, loss: 137.75071716308594\n",
      "epoch: 20,  batch step: 251, loss: 205.2275848388672\n",
      "validation error epoch  20:    tensor(102.3624, device='cuda:0')\n",
      "316\n",
      "epoch: 21,  batch step: 0, loss: 48.054443359375\n",
      "epoch: 21,  batch step: 1, loss: 24.931182861328125\n",
      "epoch: 21,  batch step: 2, loss: 77.00042724609375\n",
      "epoch: 21,  batch step: 3, loss: 89.39028930664062\n",
      "epoch: 21,  batch step: 4, loss: 177.72918701171875\n",
      "epoch: 21,  batch step: 5, loss: 98.70063781738281\n",
      "epoch: 21,  batch step: 6, loss: 71.61621856689453\n",
      "epoch: 21,  batch step: 7, loss: 73.3621826171875\n",
      "epoch: 21,  batch step: 8, loss: 31.867298126220703\n",
      "epoch: 21,  batch step: 9, loss: 41.97920227050781\n",
      "epoch: 21,  batch step: 10, loss: 52.47030258178711\n",
      "epoch: 21,  batch step: 11, loss: 9.822562217712402\n",
      "epoch: 21,  batch step: 12, loss: 49.46105194091797\n",
      "epoch: 21,  batch step: 13, loss: 72.30646514892578\n",
      "epoch: 21,  batch step: 14, loss: 96.28500366210938\n",
      "epoch: 21,  batch step: 15, loss: 33.99055480957031\n",
      "epoch: 21,  batch step: 16, loss: 132.7014923095703\n",
      "epoch: 21,  batch step: 17, loss: 22.565719604492188\n",
      "epoch: 21,  batch step: 18, loss: 15.655566215515137\n",
      "epoch: 21,  batch step: 19, loss: 66.3350830078125\n",
      "epoch: 21,  batch step: 20, loss: 126.38703918457031\n",
      "epoch: 21,  batch step: 21, loss: 96.0169906616211\n",
      "epoch: 21,  batch step: 22, loss: 38.06050491333008\n",
      "epoch: 21,  batch step: 23, loss: 14.408918380737305\n",
      "epoch: 21,  batch step: 24, loss: 190.20529174804688\n",
      "epoch: 21,  batch step: 25, loss: 98.24584197998047\n",
      "epoch: 21,  batch step: 26, loss: 74.95738220214844\n",
      "epoch: 21,  batch step: 27, loss: 42.64121627807617\n",
      "epoch: 21,  batch step: 28, loss: 44.57952117919922\n",
      "epoch: 21,  batch step: 29, loss: 26.071056365966797\n",
      "epoch: 21,  batch step: 30, loss: 62.280738830566406\n",
      "epoch: 21,  batch step: 31, loss: 19.703933715820312\n",
      "epoch: 21,  batch step: 32, loss: 6.103879928588867\n",
      "epoch: 21,  batch step: 33, loss: 23.123403549194336\n",
      "epoch: 21,  batch step: 34, loss: 21.20168685913086\n",
      "epoch: 21,  batch step: 35, loss: 113.11769104003906\n",
      "epoch: 21,  batch step: 36, loss: 76.23001861572266\n",
      "epoch: 21,  batch step: 37, loss: 9.98116397857666\n",
      "epoch: 21,  batch step: 38, loss: 275.37811279296875\n",
      "epoch: 21,  batch step: 39, loss: 6.0523576736450195\n",
      "epoch: 21,  batch step: 40, loss: 119.17857360839844\n",
      "epoch: 21,  batch step: 41, loss: 12.514270782470703\n",
      "epoch: 21,  batch step: 42, loss: 20.612300872802734\n",
      "epoch: 21,  batch step: 43, loss: 9.2069730758667\n",
      "epoch: 21,  batch step: 44, loss: 28.783100128173828\n",
      "epoch: 21,  batch step: 45, loss: 148.80694580078125\n",
      "epoch: 21,  batch step: 46, loss: 12.540236473083496\n",
      "epoch: 21,  batch step: 47, loss: 8.934443473815918\n",
      "epoch: 21,  batch step: 48, loss: 36.24158477783203\n",
      "epoch: 21,  batch step: 49, loss: 130.33929443359375\n",
      "epoch: 21,  batch step: 50, loss: 84.71537780761719\n",
      "epoch: 21,  batch step: 51, loss: 55.35851287841797\n",
      "epoch: 21,  batch step: 52, loss: 41.467933654785156\n",
      "epoch: 21,  batch step: 53, loss: 18.854114532470703\n",
      "epoch: 21,  batch step: 54, loss: 66.43806457519531\n",
      "epoch: 21,  batch step: 55, loss: 40.97727966308594\n",
      "epoch: 21,  batch step: 56, loss: 58.64854431152344\n",
      "epoch: 21,  batch step: 57, loss: 127.43363952636719\n",
      "epoch: 21,  batch step: 58, loss: 18.389026641845703\n",
      "epoch: 21,  batch step: 59, loss: 17.262134552001953\n",
      "epoch: 21,  batch step: 60, loss: 53.148193359375\n",
      "epoch: 21,  batch step: 61, loss: 27.785484313964844\n",
      "epoch: 21,  batch step: 62, loss: 16.10967254638672\n",
      "epoch: 21,  batch step: 63, loss: 12.349996566772461\n",
      "epoch: 21,  batch step: 64, loss: 8.349475860595703\n",
      "epoch: 21,  batch step: 65, loss: 13.218647003173828\n",
      "epoch: 21,  batch step: 66, loss: 11.247408866882324\n",
      "epoch: 21,  batch step: 67, loss: 106.21379852294922\n",
      "epoch: 21,  batch step: 68, loss: 16.598203659057617\n",
      "epoch: 21,  batch step: 69, loss: 15.609251976013184\n",
      "epoch: 21,  batch step: 70, loss: 10.347808837890625\n",
      "epoch: 21,  batch step: 71, loss: 27.3895263671875\n",
      "epoch: 21,  batch step: 72, loss: 59.336360931396484\n",
      "epoch: 21,  batch step: 73, loss: 25.809593200683594\n",
      "epoch: 21,  batch step: 74, loss: 20.444263458251953\n",
      "epoch: 21,  batch step: 75, loss: 110.01190185546875\n",
      "epoch: 21,  batch step: 76, loss: 10.104487419128418\n",
      "epoch: 21,  batch step: 77, loss: 7.67159366607666\n",
      "epoch: 21,  batch step: 78, loss: 10.127436637878418\n",
      "epoch: 21,  batch step: 79, loss: 10.318315505981445\n",
      "epoch: 21,  batch step: 80, loss: 22.694669723510742\n",
      "epoch: 21,  batch step: 81, loss: 44.120506286621094\n",
      "epoch: 21,  batch step: 82, loss: 9.520980834960938\n",
      "epoch: 21,  batch step: 83, loss: 11.120413780212402\n",
      "epoch: 21,  batch step: 84, loss: 8.899492263793945\n",
      "epoch: 21,  batch step: 85, loss: 66.68656921386719\n",
      "epoch: 21,  batch step: 86, loss: 64.03791809082031\n",
      "epoch: 21,  batch step: 87, loss: 18.17403221130371\n",
      "epoch: 21,  batch step: 88, loss: 5.748310089111328\n",
      "epoch: 21,  batch step: 89, loss: 137.50338745117188\n",
      "epoch: 21,  batch step: 90, loss: 16.475133895874023\n",
      "epoch: 21,  batch step: 91, loss: 131.1164093017578\n",
      "epoch: 21,  batch step: 92, loss: 13.614616394042969\n",
      "epoch: 21,  batch step: 93, loss: 90.917724609375\n",
      "epoch: 21,  batch step: 94, loss: 114.44223022460938\n",
      "epoch: 21,  batch step: 95, loss: 41.98155212402344\n",
      "epoch: 21,  batch step: 96, loss: 45.13166046142578\n",
      "epoch: 21,  batch step: 97, loss: 82.337890625\n",
      "epoch: 21,  batch step: 98, loss: 22.520469665527344\n",
      "epoch: 21,  batch step: 99, loss: 142.19000244140625\n",
      "epoch: 21,  batch step: 100, loss: 28.650188446044922\n",
      "epoch: 21,  batch step: 101, loss: 11.47317886352539\n",
      "epoch: 21,  batch step: 102, loss: 83.07559204101562\n",
      "epoch: 21,  batch step: 103, loss: 78.73016357421875\n",
      "epoch: 21,  batch step: 104, loss: 16.617116928100586\n",
      "epoch: 21,  batch step: 105, loss: 23.121183395385742\n",
      "epoch: 21,  batch step: 106, loss: 8.216349601745605\n",
      "epoch: 21,  batch step: 107, loss: 40.66291809082031\n",
      "epoch: 21,  batch step: 108, loss: 83.83915710449219\n",
      "epoch: 21,  batch step: 109, loss: 23.55738067626953\n",
      "epoch: 21,  batch step: 110, loss: 6.252183437347412\n",
      "epoch: 21,  batch step: 111, loss: 15.710295677185059\n",
      "epoch: 21,  batch step: 112, loss: 42.99520492553711\n",
      "epoch: 21,  batch step: 113, loss: 22.198760986328125\n",
      "epoch: 21,  batch step: 114, loss: 23.65364646911621\n",
      "epoch: 21,  batch step: 115, loss: 11.819477081298828\n",
      "epoch: 21,  batch step: 116, loss: 78.21211242675781\n",
      "epoch: 21,  batch step: 117, loss: 15.642252922058105\n",
      "epoch: 21,  batch step: 118, loss: 8.48635196685791\n",
      "epoch: 21,  batch step: 119, loss: 26.805423736572266\n",
      "epoch: 21,  batch step: 120, loss: 9.943567276000977\n",
      "epoch: 21,  batch step: 121, loss: 127.60755920410156\n",
      "epoch: 21,  batch step: 122, loss: 40.83462142944336\n",
      "epoch: 21,  batch step: 123, loss: 11.281977653503418\n",
      "epoch: 21,  batch step: 124, loss: 6.810042381286621\n",
      "epoch: 21,  batch step: 125, loss: 107.343017578125\n",
      "epoch: 21,  batch step: 126, loss: 29.40575408935547\n",
      "epoch: 21,  batch step: 127, loss: 20.53942108154297\n",
      "epoch: 21,  batch step: 128, loss: 77.37471771240234\n",
      "epoch: 21,  batch step: 129, loss: 50.99591827392578\n",
      "epoch: 21,  batch step: 130, loss: 3.543426513671875\n",
      "epoch: 21,  batch step: 131, loss: 152.81507873535156\n",
      "epoch: 21,  batch step: 132, loss: 18.75591278076172\n",
      "epoch: 21,  batch step: 133, loss: 385.49334716796875\n",
      "epoch: 21,  batch step: 134, loss: 43.06452178955078\n",
      "epoch: 21,  batch step: 135, loss: 121.79841613769531\n",
      "epoch: 21,  batch step: 136, loss: 88.74668884277344\n",
      "epoch: 21,  batch step: 137, loss: 239.47348022460938\n",
      "epoch: 21,  batch step: 138, loss: 53.93156051635742\n",
      "epoch: 21,  batch step: 139, loss: 55.44788360595703\n",
      "epoch: 21,  batch step: 140, loss: 54.85167694091797\n",
      "epoch: 21,  batch step: 141, loss: 54.702880859375\n",
      "epoch: 21,  batch step: 142, loss: 61.895965576171875\n",
      "epoch: 21,  batch step: 143, loss: 26.389984130859375\n",
      "epoch: 21,  batch step: 144, loss: 12.04061508178711\n",
      "epoch: 21,  batch step: 145, loss: 150.00778198242188\n",
      "epoch: 21,  batch step: 146, loss: 14.924301147460938\n",
      "epoch: 21,  batch step: 147, loss: 23.865825653076172\n",
      "epoch: 21,  batch step: 148, loss: 7.031487941741943\n",
      "epoch: 21,  batch step: 149, loss: 33.9406852722168\n",
      "epoch: 21,  batch step: 150, loss: 32.54487228393555\n",
      "epoch: 21,  batch step: 151, loss: 10.331457138061523\n",
      "epoch: 21,  batch step: 152, loss: 9.82573127746582\n",
      "epoch: 21,  batch step: 153, loss: 4.61582088470459\n",
      "epoch: 21,  batch step: 154, loss: 59.05485153198242\n",
      "epoch: 21,  batch step: 155, loss: 108.65328979492188\n",
      "epoch: 21,  batch step: 156, loss: 42.251312255859375\n",
      "epoch: 21,  batch step: 157, loss: 10.231168746948242\n",
      "epoch: 21,  batch step: 158, loss: 93.08302307128906\n",
      "epoch: 21,  batch step: 159, loss: 4.466419219970703\n",
      "epoch: 21,  batch step: 160, loss: 68.38447570800781\n",
      "epoch: 21,  batch step: 161, loss: 22.035987854003906\n",
      "epoch: 21,  batch step: 162, loss: 8.798319816589355\n",
      "epoch: 21,  batch step: 163, loss: 168.15170288085938\n",
      "epoch: 21,  batch step: 164, loss: 20.872066497802734\n",
      "epoch: 21,  batch step: 165, loss: 54.077598571777344\n",
      "epoch: 21,  batch step: 166, loss: 79.69868469238281\n",
      "epoch: 21,  batch step: 167, loss: 52.255714416503906\n",
      "epoch: 21,  batch step: 168, loss: 35.450477600097656\n",
      "epoch: 21,  batch step: 169, loss: 106.46180725097656\n",
      "epoch: 21,  batch step: 170, loss: 11.86591911315918\n",
      "epoch: 21,  batch step: 171, loss: 78.72808074951172\n",
      "epoch: 21,  batch step: 172, loss: 48.104217529296875\n",
      "epoch: 21,  batch step: 173, loss: 124.07335662841797\n",
      "epoch: 21,  batch step: 174, loss: 96.52440643310547\n",
      "epoch: 21,  batch step: 175, loss: 104.49278259277344\n",
      "epoch: 21,  batch step: 176, loss: 39.97107696533203\n",
      "epoch: 21,  batch step: 177, loss: 87.48729705810547\n",
      "epoch: 21,  batch step: 178, loss: 14.353994369506836\n",
      "epoch: 21,  batch step: 179, loss: 15.219890594482422\n",
      "epoch: 21,  batch step: 180, loss: 9.199888229370117\n",
      "epoch: 21,  batch step: 181, loss: 10.13522720336914\n",
      "epoch: 21,  batch step: 182, loss: 11.451650619506836\n",
      "epoch: 21,  batch step: 183, loss: 110.57064819335938\n",
      "epoch: 21,  batch step: 184, loss: 22.36067771911621\n",
      "epoch: 21,  batch step: 185, loss: 8.407756805419922\n",
      "epoch: 21,  batch step: 186, loss: 9.898115158081055\n",
      "epoch: 21,  batch step: 187, loss: 18.17879867553711\n",
      "epoch: 21,  batch step: 188, loss: 16.248958587646484\n",
      "epoch: 21,  batch step: 189, loss: 7.883269309997559\n",
      "epoch: 21,  batch step: 190, loss: 7.182742595672607\n",
      "epoch: 21,  batch step: 191, loss: 9.72885513305664\n",
      "epoch: 21,  batch step: 192, loss: 146.49575805664062\n",
      "epoch: 21,  batch step: 193, loss: 303.61376953125\n",
      "epoch: 21,  batch step: 194, loss: 58.93514633178711\n",
      "epoch: 21,  batch step: 195, loss: 49.90980529785156\n",
      "epoch: 21,  batch step: 196, loss: 9.81165599822998\n",
      "epoch: 21,  batch step: 197, loss: 70.7794418334961\n",
      "epoch: 21,  batch step: 198, loss: 67.72486114501953\n",
      "epoch: 21,  batch step: 199, loss: 32.603668212890625\n",
      "epoch: 21,  batch step: 200, loss: 49.760772705078125\n",
      "epoch: 21,  batch step: 201, loss: 8.881793022155762\n",
      "epoch: 21,  batch step: 202, loss: 93.18892669677734\n",
      "epoch: 21,  batch step: 203, loss: 13.174250602722168\n",
      "epoch: 21,  batch step: 204, loss: 24.068578720092773\n",
      "epoch: 21,  batch step: 205, loss: 68.21708679199219\n",
      "epoch: 21,  batch step: 206, loss: 10.407613754272461\n",
      "epoch: 21,  batch step: 207, loss: 48.324615478515625\n",
      "epoch: 21,  batch step: 208, loss: 14.869491577148438\n",
      "epoch: 21,  batch step: 209, loss: 75.24906158447266\n",
      "epoch: 21,  batch step: 210, loss: 150.74044799804688\n",
      "epoch: 21,  batch step: 211, loss: 11.622072219848633\n",
      "epoch: 21,  batch step: 212, loss: 8.016637802124023\n",
      "epoch: 21,  batch step: 213, loss: 10.282751083374023\n",
      "epoch: 21,  batch step: 214, loss: 49.537391662597656\n",
      "epoch: 21,  batch step: 215, loss: 18.760377883911133\n",
      "epoch: 21,  batch step: 216, loss: 9.921152114868164\n",
      "epoch: 21,  batch step: 217, loss: 109.99520874023438\n",
      "epoch: 21,  batch step: 218, loss: 26.713871002197266\n",
      "epoch: 21,  batch step: 219, loss: 42.04899978637695\n",
      "epoch: 21,  batch step: 220, loss: 40.29230499267578\n",
      "epoch: 21,  batch step: 221, loss: 11.725375175476074\n",
      "epoch: 21,  batch step: 222, loss: 116.15113830566406\n",
      "epoch: 21,  batch step: 223, loss: 28.103958129882812\n",
      "epoch: 21,  batch step: 224, loss: 123.96788024902344\n",
      "epoch: 21,  batch step: 225, loss: 24.815292358398438\n",
      "epoch: 21,  batch step: 226, loss: 27.177776336669922\n",
      "epoch: 21,  batch step: 227, loss: 52.865081787109375\n",
      "epoch: 21,  batch step: 228, loss: 87.78248596191406\n",
      "epoch: 21,  batch step: 229, loss: 14.075380325317383\n",
      "epoch: 21,  batch step: 230, loss: 7.087649345397949\n",
      "epoch: 21,  batch step: 231, loss: 365.72076416015625\n",
      "epoch: 21,  batch step: 232, loss: 12.950064659118652\n",
      "epoch: 21,  batch step: 233, loss: 12.978626251220703\n",
      "epoch: 21,  batch step: 234, loss: 156.2086181640625\n",
      "epoch: 21,  batch step: 235, loss: 29.07477378845215\n",
      "epoch: 21,  batch step: 236, loss: 79.8583984375\n",
      "epoch: 21,  batch step: 237, loss: 15.171012878417969\n",
      "epoch: 21,  batch step: 238, loss: 10.057165145874023\n",
      "epoch: 21,  batch step: 239, loss: 69.4128646850586\n",
      "epoch: 21,  batch step: 240, loss: 9.69875717163086\n",
      "epoch: 21,  batch step: 241, loss: 92.12928771972656\n",
      "epoch: 21,  batch step: 242, loss: 113.85586547851562\n",
      "epoch: 21,  batch step: 243, loss: 78.59494018554688\n",
      "epoch: 21,  batch step: 244, loss: 170.7065887451172\n",
      "epoch: 21,  batch step: 245, loss: 11.455522537231445\n",
      "epoch: 21,  batch step: 246, loss: 154.5970458984375\n",
      "epoch: 21,  batch step: 247, loss: 32.75100326538086\n",
      "epoch: 21,  batch step: 248, loss: 61.167945861816406\n",
      "epoch: 21,  batch step: 249, loss: 169.26048278808594\n",
      "epoch: 21,  batch step: 250, loss: 46.90489959716797\n",
      "epoch: 21,  batch step: 251, loss: 81.93690490722656\n",
      "validation error epoch  21:    tensor(150.2564, device='cuda:0')\n",
      "316\n",
      "epoch: 22,  batch step: 0, loss: 71.18147277832031\n",
      "epoch: 22,  batch step: 1, loss: 151.19119262695312\n",
      "epoch: 22,  batch step: 2, loss: 5.899592876434326\n",
      "epoch: 22,  batch step: 3, loss: 11.48687744140625\n",
      "epoch: 22,  batch step: 4, loss: 39.04119110107422\n",
      "epoch: 22,  batch step: 5, loss: 40.919769287109375\n",
      "epoch: 22,  batch step: 6, loss: 73.68598937988281\n",
      "epoch: 22,  batch step: 7, loss: 11.593783378601074\n",
      "epoch: 22,  batch step: 8, loss: 13.458072662353516\n",
      "epoch: 22,  batch step: 9, loss: 27.876991271972656\n",
      "epoch: 22,  batch step: 10, loss: 54.61674118041992\n",
      "epoch: 22,  batch step: 11, loss: 17.439220428466797\n",
      "epoch: 22,  batch step: 12, loss: 6.13458251953125\n",
      "epoch: 22,  batch step: 13, loss: 11.205324172973633\n",
      "epoch: 22,  batch step: 14, loss: 7.057867050170898\n",
      "epoch: 22,  batch step: 15, loss: 14.779022216796875\n",
      "epoch: 22,  batch step: 16, loss: 6.684364318847656\n",
      "epoch: 22,  batch step: 17, loss: 9.858402252197266\n",
      "epoch: 22,  batch step: 18, loss: 4.207372665405273\n",
      "epoch: 22,  batch step: 19, loss: 144.4217529296875\n",
      "epoch: 22,  batch step: 20, loss: 11.643973350524902\n",
      "epoch: 22,  batch step: 21, loss: 49.74333190917969\n",
      "epoch: 22,  batch step: 22, loss: 67.78849792480469\n",
      "epoch: 22,  batch step: 23, loss: 19.9473934173584\n",
      "epoch: 22,  batch step: 24, loss: 87.66509246826172\n",
      "epoch: 22,  batch step: 25, loss: 97.93634033203125\n",
      "epoch: 22,  batch step: 26, loss: 17.495052337646484\n",
      "epoch: 22,  batch step: 27, loss: 33.5654296875\n",
      "epoch: 22,  batch step: 28, loss: 9.260835647583008\n",
      "epoch: 22,  batch step: 29, loss: 43.349884033203125\n",
      "epoch: 22,  batch step: 30, loss: 60.15522384643555\n",
      "epoch: 22,  batch step: 31, loss: 156.42042541503906\n",
      "epoch: 22,  batch step: 32, loss: 35.107025146484375\n",
      "epoch: 22,  batch step: 33, loss: 25.212783813476562\n",
      "epoch: 22,  batch step: 34, loss: 8.756933212280273\n",
      "epoch: 22,  batch step: 35, loss: 72.53469848632812\n",
      "epoch: 22,  batch step: 36, loss: 49.16300582885742\n",
      "epoch: 22,  batch step: 37, loss: 49.98698425292969\n",
      "epoch: 22,  batch step: 38, loss: 10.204337120056152\n",
      "epoch: 22,  batch step: 39, loss: 43.35505294799805\n",
      "epoch: 22,  batch step: 40, loss: 49.92964172363281\n",
      "epoch: 22,  batch step: 41, loss: 20.966617584228516\n",
      "epoch: 22,  batch step: 42, loss: 33.96063232421875\n",
      "epoch: 22,  batch step: 43, loss: 11.593151092529297\n",
      "epoch: 22,  batch step: 44, loss: 139.178466796875\n",
      "epoch: 22,  batch step: 45, loss: 31.07274627685547\n",
      "epoch: 22,  batch step: 46, loss: 223.87890625\n",
      "epoch: 22,  batch step: 47, loss: 38.17601013183594\n",
      "epoch: 22,  batch step: 48, loss: 37.52926254272461\n",
      "epoch: 22,  batch step: 49, loss: 29.105634689331055\n",
      "epoch: 22,  batch step: 50, loss: 5.5232086181640625\n",
      "epoch: 22,  batch step: 51, loss: 166.93289184570312\n",
      "epoch: 22,  batch step: 52, loss: 9.362847328186035\n",
      "epoch: 22,  batch step: 53, loss: 9.727346420288086\n",
      "epoch: 22,  batch step: 54, loss: 40.80868911743164\n",
      "epoch: 22,  batch step: 55, loss: 54.60848617553711\n",
      "epoch: 22,  batch step: 56, loss: 25.28896141052246\n",
      "epoch: 22,  batch step: 57, loss: 84.97003173828125\n",
      "epoch: 22,  batch step: 58, loss: 6.970677852630615\n",
      "epoch: 22,  batch step: 59, loss: 16.744224548339844\n",
      "epoch: 22,  batch step: 60, loss: 19.974990844726562\n",
      "epoch: 22,  batch step: 61, loss: 49.57756805419922\n",
      "epoch: 22,  batch step: 62, loss: 17.810413360595703\n",
      "epoch: 22,  batch step: 63, loss: 10.347979545593262\n",
      "epoch: 22,  batch step: 64, loss: 43.76728057861328\n",
      "epoch: 22,  batch step: 65, loss: 23.766101837158203\n",
      "epoch: 22,  batch step: 66, loss: 15.799295425415039\n",
      "epoch: 22,  batch step: 67, loss: 9.495988845825195\n",
      "epoch: 22,  batch step: 68, loss: 15.435552597045898\n",
      "epoch: 22,  batch step: 69, loss: 48.365135192871094\n",
      "epoch: 22,  batch step: 70, loss: 12.954188346862793\n",
      "epoch: 22,  batch step: 71, loss: 13.499893188476562\n",
      "epoch: 22,  batch step: 72, loss: 14.64871883392334\n",
      "epoch: 22,  batch step: 73, loss: 33.82136917114258\n",
      "epoch: 22,  batch step: 74, loss: 5.316363334655762\n",
      "epoch: 22,  batch step: 75, loss: 13.114208221435547\n",
      "epoch: 22,  batch step: 76, loss: 30.823514938354492\n",
      "epoch: 22,  batch step: 77, loss: 28.264892578125\n",
      "epoch: 22,  batch step: 78, loss: 5.733829498291016\n",
      "epoch: 22,  batch step: 79, loss: 12.159319877624512\n",
      "epoch: 22,  batch step: 80, loss: 9.219862937927246\n",
      "epoch: 22,  batch step: 81, loss: 38.382686614990234\n",
      "epoch: 22,  batch step: 82, loss: 73.18333435058594\n",
      "epoch: 22,  batch step: 83, loss: 223.797119140625\n",
      "epoch: 22,  batch step: 84, loss: 162.4027862548828\n",
      "epoch: 22,  batch step: 85, loss: 5.545692443847656\n",
      "epoch: 22,  batch step: 86, loss: 7.837281227111816\n",
      "epoch: 22,  batch step: 87, loss: 52.66490936279297\n",
      "epoch: 22,  batch step: 88, loss: 20.798145294189453\n",
      "epoch: 22,  batch step: 89, loss: 156.16476440429688\n",
      "epoch: 22,  batch step: 90, loss: 7.885045528411865\n",
      "epoch: 22,  batch step: 91, loss: 11.192909240722656\n",
      "epoch: 22,  batch step: 92, loss: 11.829540252685547\n",
      "epoch: 22,  batch step: 93, loss: 88.38749694824219\n",
      "epoch: 22,  batch step: 94, loss: 20.84505271911621\n",
      "epoch: 22,  batch step: 95, loss: 109.81588745117188\n",
      "epoch: 22,  batch step: 96, loss: 11.276365280151367\n",
      "epoch: 22,  batch step: 97, loss: 32.404441833496094\n",
      "epoch: 22,  batch step: 98, loss: 26.125022888183594\n",
      "epoch: 22,  batch step: 99, loss: 6.253716945648193\n",
      "epoch: 22,  batch step: 100, loss: 21.00815200805664\n",
      "epoch: 22,  batch step: 101, loss: 55.910179138183594\n",
      "epoch: 22,  batch step: 102, loss: 70.16466522216797\n",
      "epoch: 22,  batch step: 103, loss: 6.7056708335876465\n",
      "epoch: 22,  batch step: 104, loss: 5.426076412200928\n",
      "epoch: 22,  batch step: 105, loss: 6.201729774475098\n",
      "epoch: 22,  batch step: 106, loss: 135.25228881835938\n",
      "epoch: 22,  batch step: 107, loss: 12.950971603393555\n",
      "epoch: 22,  batch step: 108, loss: 84.68634796142578\n",
      "epoch: 22,  batch step: 109, loss: 33.11233139038086\n",
      "epoch: 22,  batch step: 110, loss: 40.49162292480469\n",
      "epoch: 22,  batch step: 111, loss: 11.89240837097168\n",
      "epoch: 22,  batch step: 112, loss: 95.68419647216797\n",
      "epoch: 22,  batch step: 113, loss: 16.26374053955078\n",
      "epoch: 22,  batch step: 114, loss: 74.00698852539062\n",
      "epoch: 22,  batch step: 115, loss: 50.87958526611328\n",
      "epoch: 22,  batch step: 116, loss: 70.55821228027344\n",
      "epoch: 22,  batch step: 117, loss: 11.616003036499023\n",
      "epoch: 22,  batch step: 118, loss: 136.3377685546875\n",
      "epoch: 22,  batch step: 119, loss: 8.126860618591309\n",
      "epoch: 22,  batch step: 120, loss: 23.31588363647461\n",
      "epoch: 22,  batch step: 121, loss: 12.160408020019531\n",
      "epoch: 22,  batch step: 122, loss: 148.5484161376953\n",
      "epoch: 22,  batch step: 123, loss: 10.887397766113281\n",
      "epoch: 22,  batch step: 124, loss: 15.019965171813965\n",
      "epoch: 22,  batch step: 125, loss: 31.442989349365234\n",
      "epoch: 22,  batch step: 126, loss: 123.05748748779297\n",
      "epoch: 22,  batch step: 127, loss: 145.6488800048828\n",
      "epoch: 22,  batch step: 128, loss: 48.222862243652344\n",
      "epoch: 22,  batch step: 129, loss: 55.58319091796875\n",
      "epoch: 22,  batch step: 130, loss: 10.531670570373535\n",
      "epoch: 22,  batch step: 131, loss: 59.5389518737793\n",
      "epoch: 22,  batch step: 132, loss: 10.409589767456055\n",
      "epoch: 22,  batch step: 133, loss: 39.393646240234375\n",
      "epoch: 22,  batch step: 134, loss: 18.423404693603516\n",
      "epoch: 22,  batch step: 135, loss: 70.54458618164062\n",
      "epoch: 22,  batch step: 136, loss: 21.53278350830078\n",
      "epoch: 22,  batch step: 137, loss: 142.78404235839844\n",
      "epoch: 22,  batch step: 138, loss: 32.53281784057617\n",
      "epoch: 22,  batch step: 139, loss: 12.203767776489258\n",
      "epoch: 22,  batch step: 140, loss: 29.2823486328125\n",
      "epoch: 22,  batch step: 141, loss: 34.55827331542969\n",
      "epoch: 22,  batch step: 142, loss: 51.628658294677734\n",
      "epoch: 22,  batch step: 143, loss: 72.010009765625\n",
      "epoch: 22,  batch step: 144, loss: 73.50242614746094\n",
      "epoch: 22,  batch step: 145, loss: 9.853315353393555\n",
      "epoch: 22,  batch step: 146, loss: 9.759214401245117\n",
      "epoch: 22,  batch step: 147, loss: 46.83537292480469\n",
      "epoch: 22,  batch step: 148, loss: 9.950393676757812\n",
      "epoch: 22,  batch step: 149, loss: 108.33184814453125\n",
      "epoch: 22,  batch step: 150, loss: 6.770315170288086\n",
      "epoch: 22,  batch step: 151, loss: 324.3072204589844\n",
      "epoch: 22,  batch step: 152, loss: 503.7356872558594\n",
      "epoch: 22,  batch step: 153, loss: 32.305213928222656\n",
      "epoch: 22,  batch step: 154, loss: 26.973262786865234\n",
      "epoch: 22,  batch step: 155, loss: 7.149041652679443\n",
      "epoch: 22,  batch step: 156, loss: 13.664121627807617\n",
      "epoch: 22,  batch step: 157, loss: 218.50833129882812\n",
      "epoch: 22,  batch step: 158, loss: 16.819446563720703\n",
      "epoch: 22,  batch step: 159, loss: 30.43703269958496\n",
      "epoch: 22,  batch step: 160, loss: 7.950952053070068\n",
      "epoch: 22,  batch step: 161, loss: 15.509028434753418\n",
      "epoch: 22,  batch step: 162, loss: 14.63736343383789\n",
      "epoch: 22,  batch step: 163, loss: 141.4490203857422\n",
      "epoch: 22,  batch step: 164, loss: 50.604122161865234\n",
      "epoch: 22,  batch step: 165, loss: 12.162923812866211\n",
      "epoch: 22,  batch step: 166, loss: 8.06245231628418\n",
      "epoch: 22,  batch step: 167, loss: 19.936641693115234\n",
      "epoch: 22,  batch step: 168, loss: 25.033554077148438\n",
      "epoch: 22,  batch step: 169, loss: 110.92991638183594\n",
      "epoch: 22,  batch step: 170, loss: 17.04041862487793\n",
      "epoch: 22,  batch step: 171, loss: 16.942951202392578\n",
      "epoch: 22,  batch step: 172, loss: 31.476062774658203\n",
      "epoch: 22,  batch step: 173, loss: 50.49971008300781\n",
      "epoch: 22,  batch step: 174, loss: 74.96663665771484\n",
      "epoch: 22,  batch step: 175, loss: 9.807079315185547\n",
      "epoch: 22,  batch step: 176, loss: 9.095966339111328\n",
      "epoch: 22,  batch step: 177, loss: 62.05168151855469\n",
      "epoch: 22,  batch step: 178, loss: 7.312350273132324\n",
      "epoch: 22,  batch step: 179, loss: 170.3357391357422\n",
      "epoch: 22,  batch step: 180, loss: 7.945151329040527\n",
      "epoch: 22,  batch step: 181, loss: 14.981978416442871\n",
      "epoch: 22,  batch step: 182, loss: 12.600180625915527\n",
      "epoch: 22,  batch step: 183, loss: 136.52044677734375\n",
      "epoch: 22,  batch step: 184, loss: 17.802947998046875\n",
      "epoch: 22,  batch step: 185, loss: 47.48777770996094\n",
      "epoch: 22,  batch step: 186, loss: 151.6825408935547\n",
      "epoch: 22,  batch step: 187, loss: 56.21952438354492\n",
      "epoch: 22,  batch step: 188, loss: 6.871268272399902\n",
      "epoch: 22,  batch step: 189, loss: 528.3746948242188\n",
      "epoch: 22,  batch step: 190, loss: 13.982870101928711\n",
      "epoch: 22,  batch step: 191, loss: 13.34389877319336\n",
      "epoch: 22,  batch step: 192, loss: 17.46262550354004\n",
      "epoch: 22,  batch step: 193, loss: 32.0616455078125\n",
      "epoch: 22,  batch step: 194, loss: 83.08773803710938\n",
      "epoch: 22,  batch step: 195, loss: 61.390052795410156\n",
      "epoch: 22,  batch step: 196, loss: 130.276611328125\n",
      "epoch: 22,  batch step: 197, loss: 32.18785858154297\n",
      "epoch: 22,  batch step: 198, loss: 56.7461051940918\n",
      "epoch: 22,  batch step: 199, loss: 45.25767135620117\n",
      "epoch: 22,  batch step: 200, loss: 32.93529510498047\n",
      "epoch: 22,  batch step: 201, loss: 285.49346923828125\n",
      "epoch: 22,  batch step: 202, loss: 160.15029907226562\n",
      "epoch: 22,  batch step: 203, loss: 7.468339443206787\n",
      "epoch: 22,  batch step: 204, loss: 18.169055938720703\n",
      "epoch: 22,  batch step: 205, loss: 10.978208541870117\n",
      "epoch: 22,  batch step: 206, loss: 59.89814758300781\n",
      "epoch: 22,  batch step: 207, loss: 16.30147933959961\n",
      "epoch: 22,  batch step: 208, loss: 23.664236068725586\n",
      "epoch: 22,  batch step: 209, loss: 213.52011108398438\n",
      "epoch: 22,  batch step: 210, loss: 66.26868438720703\n",
      "epoch: 22,  batch step: 211, loss: 16.34762191772461\n",
      "epoch: 22,  batch step: 212, loss: 16.497146606445312\n",
      "epoch: 22,  batch step: 213, loss: 12.847877502441406\n",
      "epoch: 22,  batch step: 214, loss: 21.052610397338867\n",
      "epoch: 22,  batch step: 215, loss: 76.62684631347656\n",
      "epoch: 22,  batch step: 216, loss: 111.35238647460938\n",
      "epoch: 22,  batch step: 217, loss: 44.81925582885742\n",
      "epoch: 22,  batch step: 218, loss: 22.611303329467773\n",
      "epoch: 22,  batch step: 219, loss: 9.13088321685791\n",
      "epoch: 22,  batch step: 220, loss: 33.20998764038086\n",
      "epoch: 22,  batch step: 221, loss: 8.727631568908691\n",
      "epoch: 22,  batch step: 222, loss: 35.50524139404297\n",
      "epoch: 22,  batch step: 223, loss: 17.261272430419922\n",
      "epoch: 22,  batch step: 224, loss: 30.316450119018555\n",
      "epoch: 22,  batch step: 225, loss: 156.69700622558594\n",
      "epoch: 22,  batch step: 226, loss: 18.069385528564453\n",
      "epoch: 22,  batch step: 227, loss: 121.16280364990234\n",
      "epoch: 22,  batch step: 228, loss: 34.81578063964844\n",
      "epoch: 22,  batch step: 229, loss: 5.476715087890625\n",
      "epoch: 22,  batch step: 230, loss: 14.276708602905273\n",
      "epoch: 22,  batch step: 231, loss: 119.58979797363281\n",
      "epoch: 22,  batch step: 232, loss: 114.41671752929688\n",
      "epoch: 22,  batch step: 233, loss: 20.095205307006836\n",
      "epoch: 22,  batch step: 234, loss: 10.131378173828125\n",
      "epoch: 22,  batch step: 235, loss: 174.99705505371094\n",
      "epoch: 22,  batch step: 236, loss: 8.01372241973877\n",
      "epoch: 22,  batch step: 237, loss: 8.122650146484375\n",
      "epoch: 22,  batch step: 238, loss: 26.31165313720703\n",
      "epoch: 22,  batch step: 239, loss: 54.65439987182617\n",
      "epoch: 22,  batch step: 240, loss: 36.14984130859375\n",
      "epoch: 22,  batch step: 241, loss: 13.532439231872559\n",
      "epoch: 22,  batch step: 242, loss: 72.0302505493164\n",
      "epoch: 22,  batch step: 243, loss: 44.162715911865234\n",
      "epoch: 22,  batch step: 244, loss: 5.32003927230835\n",
      "epoch: 22,  batch step: 245, loss: 86.96504974365234\n",
      "epoch: 22,  batch step: 246, loss: 6.1417365074157715\n",
      "epoch: 22,  batch step: 247, loss: 9.872920036315918\n",
      "epoch: 22,  batch step: 248, loss: 95.19474792480469\n",
      "epoch: 22,  batch step: 249, loss: 6.432867050170898\n",
      "epoch: 22,  batch step: 250, loss: 13.293121337890625\n",
      "epoch: 22,  batch step: 251, loss: 54.34056091308594\n",
      "validation error epoch  22:    tensor(114.0391, device='cuda:0')\n",
      "316\n",
      "epoch: 23,  batch step: 0, loss: 15.444911003112793\n",
      "epoch: 23,  batch step: 1, loss: 15.528468132019043\n",
      "epoch: 23,  batch step: 2, loss: 72.11292266845703\n",
      "epoch: 23,  batch step: 3, loss: 103.11270141601562\n",
      "epoch: 23,  batch step: 4, loss: 146.67230224609375\n",
      "epoch: 23,  batch step: 5, loss: 8.960933685302734\n",
      "epoch: 23,  batch step: 6, loss: 116.14028930664062\n",
      "epoch: 23,  batch step: 7, loss: 52.635498046875\n",
      "epoch: 23,  batch step: 8, loss: 18.005828857421875\n",
      "epoch: 23,  batch step: 9, loss: 6.900238037109375\n",
      "epoch: 23,  batch step: 10, loss: 34.990577697753906\n",
      "epoch: 23,  batch step: 11, loss: 6.200856685638428\n",
      "epoch: 23,  batch step: 12, loss: 10.660694122314453\n",
      "epoch: 23,  batch step: 13, loss: 21.6778621673584\n",
      "epoch: 23,  batch step: 14, loss: 9.053586959838867\n",
      "epoch: 23,  batch step: 15, loss: 14.975504875183105\n",
      "epoch: 23,  batch step: 16, loss: 68.91366577148438\n",
      "epoch: 23,  batch step: 17, loss: 22.04903793334961\n",
      "epoch: 23,  batch step: 18, loss: 83.70925903320312\n",
      "epoch: 23,  batch step: 19, loss: 14.364794731140137\n",
      "epoch: 23,  batch step: 20, loss: 12.379152297973633\n",
      "epoch: 23,  batch step: 21, loss: 18.0261173248291\n",
      "epoch: 23,  batch step: 22, loss: 53.04796600341797\n",
      "epoch: 23,  batch step: 23, loss: 26.402868270874023\n",
      "epoch: 23,  batch step: 24, loss: 13.513786315917969\n",
      "epoch: 23,  batch step: 25, loss: 12.159683227539062\n",
      "epoch: 23,  batch step: 26, loss: 4.768950939178467\n",
      "epoch: 23,  batch step: 27, loss: 21.617483139038086\n",
      "epoch: 23,  batch step: 28, loss: 7.269540309906006\n",
      "epoch: 23,  batch step: 29, loss: 199.3413543701172\n",
      "epoch: 23,  batch step: 30, loss: 7.520525932312012\n",
      "epoch: 23,  batch step: 31, loss: 13.883200645446777\n",
      "epoch: 23,  batch step: 32, loss: 83.076416015625\n",
      "epoch: 23,  batch step: 33, loss: 52.84539794921875\n",
      "epoch: 23,  batch step: 34, loss: 14.788985252380371\n",
      "epoch: 23,  batch step: 35, loss: 10.270821571350098\n",
      "epoch: 23,  batch step: 36, loss: 14.718100547790527\n",
      "epoch: 23,  batch step: 37, loss: 32.913814544677734\n",
      "epoch: 23,  batch step: 38, loss: 195.79917907714844\n",
      "epoch: 23,  batch step: 39, loss: 17.88852310180664\n",
      "epoch: 23,  batch step: 40, loss: 23.894107818603516\n",
      "epoch: 23,  batch step: 41, loss: 18.37241554260254\n",
      "epoch: 23,  batch step: 42, loss: 6.176972389221191\n",
      "epoch: 23,  batch step: 43, loss: 58.721858978271484\n",
      "epoch: 23,  batch step: 44, loss: 9.576162338256836\n",
      "epoch: 23,  batch step: 45, loss: 57.56072998046875\n",
      "epoch: 23,  batch step: 46, loss: 65.33786010742188\n",
      "epoch: 23,  batch step: 47, loss: 6.56882381439209\n",
      "epoch: 23,  batch step: 48, loss: 52.13995361328125\n",
      "epoch: 23,  batch step: 49, loss: 10.81134033203125\n",
      "epoch: 23,  batch step: 50, loss: 55.16571807861328\n",
      "epoch: 23,  batch step: 51, loss: 12.485453605651855\n",
      "epoch: 23,  batch step: 52, loss: 89.60905456542969\n",
      "epoch: 23,  batch step: 53, loss: 10.525381088256836\n",
      "epoch: 23,  batch step: 54, loss: 14.521171569824219\n",
      "epoch: 23,  batch step: 55, loss: 9.838455200195312\n",
      "epoch: 23,  batch step: 56, loss: 6.857953071594238\n",
      "epoch: 23,  batch step: 57, loss: 9.43360424041748\n",
      "epoch: 23,  batch step: 58, loss: 86.61325073242188\n",
      "epoch: 23,  batch step: 59, loss: 16.230546951293945\n",
      "epoch: 23,  batch step: 60, loss: 191.11972045898438\n",
      "epoch: 23,  batch step: 61, loss: 9.812827110290527\n",
      "epoch: 23,  batch step: 62, loss: 153.9467315673828\n",
      "epoch: 23,  batch step: 63, loss: 7.074206352233887\n",
      "epoch: 23,  batch step: 64, loss: 93.22138977050781\n",
      "epoch: 23,  batch step: 65, loss: 6.353397369384766\n",
      "epoch: 23,  batch step: 66, loss: 193.65599060058594\n",
      "epoch: 23,  batch step: 67, loss: 11.45545768737793\n",
      "epoch: 23,  batch step: 68, loss: 142.99310302734375\n",
      "epoch: 23,  batch step: 69, loss: 13.672948837280273\n",
      "epoch: 23,  batch step: 70, loss: 57.47336959838867\n",
      "epoch: 23,  batch step: 71, loss: 17.27778434753418\n",
      "epoch: 23,  batch step: 72, loss: 29.311628341674805\n",
      "epoch: 23,  batch step: 73, loss: 9.896524429321289\n",
      "epoch: 23,  batch step: 74, loss: 55.9107666015625\n",
      "epoch: 23,  batch step: 75, loss: 79.43597412109375\n",
      "epoch: 23,  batch step: 76, loss: 44.0999755859375\n",
      "epoch: 23,  batch step: 77, loss: 17.353940963745117\n",
      "epoch: 23,  batch step: 78, loss: 174.74288940429688\n",
      "epoch: 23,  batch step: 79, loss: 69.85659790039062\n",
      "epoch: 23,  batch step: 80, loss: 13.008707046508789\n",
      "epoch: 23,  batch step: 81, loss: 15.700224876403809\n",
      "epoch: 23,  batch step: 82, loss: 264.439697265625\n",
      "epoch: 23,  batch step: 83, loss: 61.47690963745117\n",
      "epoch: 23,  batch step: 84, loss: 14.335897445678711\n",
      "epoch: 23,  batch step: 85, loss: 49.07701110839844\n",
      "epoch: 23,  batch step: 86, loss: 4.1469526290893555\n",
      "epoch: 23,  batch step: 87, loss: 36.182701110839844\n",
      "epoch: 23,  batch step: 88, loss: 10.091658592224121\n",
      "epoch: 23,  batch step: 89, loss: 8.80171012878418\n",
      "epoch: 23,  batch step: 90, loss: 47.25312423706055\n",
      "epoch: 23,  batch step: 91, loss: 7.087520599365234\n",
      "epoch: 23,  batch step: 92, loss: 106.62950134277344\n",
      "epoch: 23,  batch step: 93, loss: 142.9581756591797\n",
      "epoch: 23,  batch step: 94, loss: 44.85955810546875\n",
      "epoch: 23,  batch step: 95, loss: 82.60594177246094\n",
      "epoch: 23,  batch step: 96, loss: 19.154449462890625\n",
      "epoch: 23,  batch step: 97, loss: 14.575587272644043\n",
      "epoch: 23,  batch step: 98, loss: 75.38963317871094\n",
      "epoch: 23,  batch step: 99, loss: 12.190757751464844\n",
      "epoch: 23,  batch step: 100, loss: 59.352622985839844\n",
      "epoch: 23,  batch step: 101, loss: 26.87335968017578\n",
      "epoch: 23,  batch step: 102, loss: 58.749534606933594\n",
      "epoch: 23,  batch step: 103, loss: 18.166790008544922\n",
      "epoch: 23,  batch step: 104, loss: 24.958709716796875\n",
      "epoch: 23,  batch step: 105, loss: 45.821128845214844\n",
      "epoch: 23,  batch step: 106, loss: 13.008001327514648\n",
      "epoch: 23,  batch step: 107, loss: 102.40785217285156\n",
      "epoch: 23,  batch step: 108, loss: 74.6421890258789\n",
      "epoch: 23,  batch step: 109, loss: 47.09377670288086\n",
      "epoch: 23,  batch step: 110, loss: 4.725853443145752\n",
      "epoch: 23,  batch step: 111, loss: 34.20357894897461\n",
      "epoch: 23,  batch step: 112, loss: 19.501708984375\n",
      "epoch: 23,  batch step: 113, loss: 103.7831039428711\n",
      "epoch: 23,  batch step: 114, loss: 46.467613220214844\n",
      "epoch: 23,  batch step: 115, loss: 69.00640106201172\n",
      "epoch: 23,  batch step: 116, loss: 92.26839447021484\n",
      "epoch: 23,  batch step: 117, loss: 22.63750457763672\n",
      "epoch: 23,  batch step: 118, loss: 20.607046127319336\n",
      "epoch: 23,  batch step: 119, loss: 26.077810287475586\n",
      "epoch: 23,  batch step: 120, loss: 15.843722343444824\n",
      "epoch: 23,  batch step: 121, loss: 50.216060638427734\n",
      "epoch: 23,  batch step: 122, loss: 122.56742858886719\n",
      "epoch: 23,  batch step: 123, loss: 126.48683166503906\n",
      "epoch: 23,  batch step: 124, loss: 15.876089096069336\n",
      "epoch: 23,  batch step: 125, loss: 54.35185241699219\n",
      "epoch: 23,  batch step: 126, loss: 94.583251953125\n",
      "epoch: 23,  batch step: 127, loss: 28.424762725830078\n",
      "epoch: 23,  batch step: 128, loss: 13.559142112731934\n",
      "epoch: 23,  batch step: 129, loss: 74.43093872070312\n",
      "epoch: 23,  batch step: 130, loss: 120.74797821044922\n",
      "epoch: 23,  batch step: 131, loss: 11.702467918395996\n",
      "epoch: 23,  batch step: 132, loss: 73.26929473876953\n",
      "epoch: 23,  batch step: 133, loss: 9.480695724487305\n",
      "epoch: 23,  batch step: 134, loss: 16.003807067871094\n",
      "epoch: 23,  batch step: 135, loss: 35.17906188964844\n",
      "epoch: 23,  batch step: 136, loss: 56.1507682800293\n",
      "epoch: 23,  batch step: 137, loss: 92.77337646484375\n",
      "epoch: 23,  batch step: 138, loss: 8.298677444458008\n",
      "epoch: 23,  batch step: 139, loss: 29.37090301513672\n",
      "epoch: 23,  batch step: 140, loss: 10.333202362060547\n",
      "epoch: 23,  batch step: 141, loss: 117.83372497558594\n",
      "epoch: 23,  batch step: 142, loss: 93.58580017089844\n",
      "epoch: 23,  batch step: 143, loss: 86.81695556640625\n",
      "epoch: 23,  batch step: 144, loss: 69.23567199707031\n",
      "epoch: 23,  batch step: 145, loss: 13.240888595581055\n",
      "epoch: 23,  batch step: 146, loss: 137.21713256835938\n",
      "epoch: 23,  batch step: 147, loss: 24.543699264526367\n",
      "epoch: 23,  batch step: 148, loss: 7.407407283782959\n",
      "epoch: 23,  batch step: 149, loss: 85.83470916748047\n",
      "epoch: 23,  batch step: 150, loss: 26.90933609008789\n",
      "epoch: 23,  batch step: 151, loss: 11.64387035369873\n",
      "epoch: 23,  batch step: 152, loss: 13.497515678405762\n",
      "epoch: 23,  batch step: 153, loss: 85.2476806640625\n",
      "epoch: 23,  batch step: 154, loss: 103.20006561279297\n",
      "epoch: 23,  batch step: 155, loss: 104.09878540039062\n",
      "epoch: 23,  batch step: 156, loss: 26.586772918701172\n",
      "epoch: 23,  batch step: 157, loss: 31.890647888183594\n",
      "epoch: 23,  batch step: 158, loss: 7.275571346282959\n",
      "epoch: 23,  batch step: 159, loss: 16.496870040893555\n",
      "epoch: 23,  batch step: 160, loss: 52.479026794433594\n",
      "epoch: 23,  batch step: 161, loss: 16.868717193603516\n",
      "epoch: 23,  batch step: 162, loss: 23.7548885345459\n",
      "epoch: 23,  batch step: 163, loss: 7.975136756896973\n",
      "epoch: 23,  batch step: 164, loss: 19.289024353027344\n",
      "epoch: 23,  batch step: 165, loss: 11.074972152709961\n",
      "epoch: 23,  batch step: 166, loss: 115.67340087890625\n",
      "epoch: 23,  batch step: 167, loss: 26.971572875976562\n",
      "epoch: 23,  batch step: 168, loss: 12.00772476196289\n",
      "epoch: 23,  batch step: 169, loss: 7.452718734741211\n",
      "epoch: 23,  batch step: 170, loss: 5.91981840133667\n",
      "epoch: 23,  batch step: 171, loss: 85.18305206298828\n",
      "epoch: 23,  batch step: 172, loss: 9.607309341430664\n",
      "epoch: 23,  batch step: 173, loss: 77.16090393066406\n",
      "epoch: 23,  batch step: 174, loss: 28.74877166748047\n",
      "epoch: 23,  batch step: 175, loss: 10.779584884643555\n",
      "epoch: 23,  batch step: 176, loss: 35.812408447265625\n",
      "epoch: 23,  batch step: 177, loss: 18.498098373413086\n",
      "epoch: 23,  batch step: 178, loss: 5.867164611816406\n",
      "epoch: 23,  batch step: 179, loss: 9.628395080566406\n",
      "epoch: 23,  batch step: 180, loss: 76.51602172851562\n",
      "epoch: 23,  batch step: 181, loss: 9.001991271972656\n",
      "epoch: 23,  batch step: 182, loss: 21.429367065429688\n",
      "epoch: 23,  batch step: 183, loss: 101.98539733886719\n",
      "epoch: 23,  batch step: 184, loss: 24.859066009521484\n",
      "epoch: 23,  batch step: 185, loss: 137.03358459472656\n",
      "epoch: 23,  batch step: 186, loss: 26.112112045288086\n",
      "epoch: 23,  batch step: 187, loss: 8.842077255249023\n",
      "epoch: 23,  batch step: 188, loss: 135.43804931640625\n",
      "epoch: 23,  batch step: 189, loss: 6.70245885848999\n",
      "epoch: 23,  batch step: 190, loss: 89.92477416992188\n",
      "epoch: 23,  batch step: 191, loss: 34.00356674194336\n",
      "epoch: 23,  batch step: 192, loss: 173.65158081054688\n",
      "epoch: 23,  batch step: 193, loss: 8.63638973236084\n",
      "epoch: 23,  batch step: 194, loss: 9.034110069274902\n",
      "epoch: 23,  batch step: 195, loss: 136.81326293945312\n",
      "epoch: 23,  batch step: 196, loss: 76.87397766113281\n",
      "epoch: 23,  batch step: 197, loss: 13.92463493347168\n",
      "epoch: 23,  batch step: 198, loss: 96.59737396240234\n",
      "epoch: 23,  batch step: 199, loss: 131.86233520507812\n",
      "epoch: 23,  batch step: 200, loss: 6.036412715911865\n",
      "epoch: 23,  batch step: 201, loss: 45.34916687011719\n",
      "epoch: 23,  batch step: 202, loss: 11.14852523803711\n",
      "epoch: 23,  batch step: 203, loss: 17.276185989379883\n",
      "epoch: 23,  batch step: 204, loss: 8.910331726074219\n",
      "epoch: 23,  batch step: 205, loss: 19.075584411621094\n",
      "epoch: 23,  batch step: 206, loss: 177.06005859375\n",
      "epoch: 23,  batch step: 207, loss: 29.55837631225586\n",
      "epoch: 23,  batch step: 208, loss: 39.78380584716797\n",
      "epoch: 23,  batch step: 209, loss: 16.688020706176758\n",
      "epoch: 23,  batch step: 210, loss: 42.38908386230469\n",
      "epoch: 23,  batch step: 211, loss: 68.16915130615234\n",
      "epoch: 23,  batch step: 212, loss: 25.348440170288086\n",
      "epoch: 23,  batch step: 213, loss: 10.779138565063477\n",
      "epoch: 23,  batch step: 214, loss: 54.087852478027344\n",
      "epoch: 23,  batch step: 215, loss: 67.33728790283203\n",
      "epoch: 23,  batch step: 216, loss: 76.51639556884766\n",
      "epoch: 23,  batch step: 217, loss: 17.664539337158203\n",
      "epoch: 23,  batch step: 218, loss: 21.754255294799805\n",
      "epoch: 23,  batch step: 219, loss: 157.0943145751953\n",
      "epoch: 23,  batch step: 220, loss: 53.9481201171875\n",
      "epoch: 23,  batch step: 221, loss: 127.38802337646484\n",
      "epoch: 23,  batch step: 222, loss: 6.969934463500977\n",
      "epoch: 23,  batch step: 223, loss: 8.140546798706055\n",
      "epoch: 23,  batch step: 224, loss: 41.6496467590332\n",
      "epoch: 23,  batch step: 225, loss: 27.746566772460938\n",
      "epoch: 23,  batch step: 226, loss: 18.33475685119629\n",
      "epoch: 23,  batch step: 227, loss: 13.458888053894043\n",
      "epoch: 23,  batch step: 228, loss: 17.851028442382812\n",
      "epoch: 23,  batch step: 229, loss: 94.9345703125\n",
      "epoch: 23,  batch step: 230, loss: 49.30322265625\n",
      "epoch: 23,  batch step: 231, loss: 6.650096893310547\n",
      "epoch: 23,  batch step: 232, loss: 33.7266845703125\n",
      "epoch: 23,  batch step: 233, loss: 9.752371788024902\n",
      "epoch: 23,  batch step: 234, loss: 16.365394592285156\n",
      "epoch: 23,  batch step: 235, loss: 18.435443878173828\n",
      "epoch: 23,  batch step: 236, loss: 21.727008819580078\n",
      "epoch: 23,  batch step: 237, loss: 141.2422332763672\n",
      "epoch: 23,  batch step: 238, loss: 20.011438369750977\n",
      "epoch: 23,  batch step: 239, loss: 60.7659912109375\n",
      "epoch: 23,  batch step: 240, loss: 153.59530639648438\n",
      "epoch: 23,  batch step: 241, loss: 26.216659545898438\n",
      "epoch: 23,  batch step: 242, loss: 213.731201171875\n",
      "epoch: 23,  batch step: 243, loss: 11.507268905639648\n",
      "epoch: 23,  batch step: 244, loss: 175.83958435058594\n",
      "epoch: 23,  batch step: 245, loss: 28.323745727539062\n",
      "epoch: 23,  batch step: 246, loss: 44.06239318847656\n",
      "epoch: 23,  batch step: 247, loss: 83.1673583984375\n",
      "epoch: 23,  batch step: 248, loss: 5.070220470428467\n",
      "epoch: 23,  batch step: 249, loss: 101.00343322753906\n",
      "epoch: 23,  batch step: 250, loss: 4.682832717895508\n",
      "epoch: 23,  batch step: 251, loss: 165.5758056640625\n",
      "validation error epoch  23:    tensor(80.5146, device='cuda:0')\n",
      "316\n",
      "epoch: 24,  batch step: 0, loss: 17.722457885742188\n",
      "epoch: 24,  batch step: 1, loss: 13.299978256225586\n",
      "epoch: 24,  batch step: 2, loss: 28.992910385131836\n",
      "epoch: 24,  batch step: 3, loss: 90.76995086669922\n",
      "epoch: 24,  batch step: 4, loss: 9.392086029052734\n",
      "epoch: 24,  batch step: 5, loss: 134.43264770507812\n",
      "epoch: 24,  batch step: 6, loss: 130.35498046875\n",
      "epoch: 24,  batch step: 7, loss: 7.610228538513184\n",
      "epoch: 24,  batch step: 8, loss: 15.85919189453125\n",
      "epoch: 24,  batch step: 9, loss: 54.49043273925781\n",
      "epoch: 24,  batch step: 10, loss: 77.69511413574219\n",
      "epoch: 24,  batch step: 11, loss: 6.339189052581787\n",
      "epoch: 24,  batch step: 12, loss: 14.380217552185059\n",
      "epoch: 24,  batch step: 13, loss: 7.623645782470703\n",
      "epoch: 24,  batch step: 14, loss: 79.07703399658203\n",
      "epoch: 24,  batch step: 15, loss: 18.67165756225586\n",
      "epoch: 24,  batch step: 16, loss: 23.319141387939453\n",
      "epoch: 24,  batch step: 17, loss: 135.476806640625\n",
      "epoch: 24,  batch step: 18, loss: 149.00364685058594\n",
      "epoch: 24,  batch step: 19, loss: 96.39234924316406\n",
      "epoch: 24,  batch step: 20, loss: 13.766843795776367\n",
      "epoch: 24,  batch step: 21, loss: 14.336984634399414\n",
      "epoch: 24,  batch step: 22, loss: 21.863628387451172\n",
      "epoch: 24,  batch step: 23, loss: 19.534841537475586\n",
      "epoch: 24,  batch step: 24, loss: 7.963254451751709\n",
      "epoch: 24,  batch step: 25, loss: 61.641265869140625\n",
      "epoch: 24,  batch step: 26, loss: 7.024971961975098\n",
      "epoch: 24,  batch step: 27, loss: 75.23381805419922\n",
      "epoch: 24,  batch step: 28, loss: 12.995353698730469\n",
      "epoch: 24,  batch step: 29, loss: 91.25011444091797\n",
      "epoch: 24,  batch step: 30, loss: 64.31964111328125\n",
      "epoch: 24,  batch step: 31, loss: 25.782978057861328\n",
      "epoch: 24,  batch step: 32, loss: 137.05377197265625\n",
      "epoch: 24,  batch step: 33, loss: 11.981109619140625\n",
      "epoch: 24,  batch step: 34, loss: 30.885000228881836\n",
      "epoch: 24,  batch step: 35, loss: 187.6013641357422\n",
      "epoch: 24,  batch step: 36, loss: 22.362911224365234\n",
      "epoch: 24,  batch step: 37, loss: 46.41120529174805\n",
      "epoch: 24,  batch step: 38, loss: 80.67364501953125\n",
      "epoch: 24,  batch step: 39, loss: 17.342529296875\n",
      "epoch: 24,  batch step: 40, loss: 113.8208236694336\n",
      "epoch: 24,  batch step: 41, loss: 14.63187313079834\n",
      "epoch: 24,  batch step: 42, loss: 28.015941619873047\n",
      "epoch: 24,  batch step: 43, loss: 29.603870391845703\n",
      "epoch: 24,  batch step: 44, loss: 60.797332763671875\n",
      "epoch: 24,  batch step: 45, loss: 82.876708984375\n",
      "epoch: 24,  batch step: 46, loss: 27.889209747314453\n",
      "epoch: 24,  batch step: 47, loss: 26.03036880493164\n",
      "epoch: 24,  batch step: 48, loss: 39.48857498168945\n",
      "epoch: 24,  batch step: 49, loss: 15.921607971191406\n",
      "epoch: 24,  batch step: 50, loss: 44.01783752441406\n",
      "epoch: 24,  batch step: 51, loss: 12.87339973449707\n",
      "epoch: 24,  batch step: 52, loss: 163.8769989013672\n",
      "epoch: 24,  batch step: 53, loss: 8.612184524536133\n",
      "epoch: 24,  batch step: 54, loss: 10.9663724899292\n",
      "epoch: 24,  batch step: 55, loss: 154.22317504882812\n",
      "epoch: 24,  batch step: 56, loss: 24.164289474487305\n",
      "epoch: 24,  batch step: 57, loss: 13.964092254638672\n",
      "epoch: 24,  batch step: 58, loss: 38.68252182006836\n",
      "epoch: 24,  batch step: 59, loss: 10.828742027282715\n",
      "epoch: 24,  batch step: 60, loss: 15.664950370788574\n",
      "epoch: 24,  batch step: 61, loss: 31.59145164489746\n",
      "epoch: 24,  batch step: 62, loss: 13.088679313659668\n",
      "epoch: 24,  batch step: 63, loss: 14.264646530151367\n",
      "epoch: 24,  batch step: 64, loss: 22.91607666015625\n",
      "epoch: 24,  batch step: 65, loss: 106.18678283691406\n",
      "epoch: 24,  batch step: 66, loss: 17.11239242553711\n",
      "epoch: 24,  batch step: 67, loss: 34.32856750488281\n",
      "epoch: 24,  batch step: 68, loss: 15.481234550476074\n",
      "epoch: 24,  batch step: 69, loss: 15.682577133178711\n",
      "epoch: 24,  batch step: 70, loss: 8.081293106079102\n",
      "epoch: 24,  batch step: 71, loss: 7.009039878845215\n",
      "epoch: 24,  batch step: 72, loss: 47.197349548339844\n",
      "epoch: 24,  batch step: 73, loss: 7.055123805999756\n",
      "epoch: 24,  batch step: 74, loss: 23.54494857788086\n",
      "epoch: 24,  batch step: 75, loss: 24.37763023376465\n",
      "epoch: 24,  batch step: 76, loss: 6.163201332092285\n",
      "epoch: 24,  batch step: 77, loss: 76.36172485351562\n",
      "epoch: 24,  batch step: 78, loss: 13.115171432495117\n",
      "epoch: 24,  batch step: 79, loss: 5.179333686828613\n",
      "epoch: 24,  batch step: 80, loss: 5.155505180358887\n",
      "epoch: 24,  batch step: 81, loss: 22.381187438964844\n",
      "epoch: 24,  batch step: 82, loss: 6.506267547607422\n",
      "epoch: 24,  batch step: 83, loss: 5.908692836761475\n",
      "epoch: 24,  batch step: 84, loss: 34.08335494995117\n",
      "epoch: 24,  batch step: 85, loss: 16.050779342651367\n",
      "epoch: 24,  batch step: 86, loss: 102.99614715576172\n",
      "epoch: 24,  batch step: 87, loss: 28.381404876708984\n",
      "epoch: 24,  batch step: 88, loss: 42.610023498535156\n",
      "epoch: 24,  batch step: 89, loss: 57.266815185546875\n",
      "epoch: 24,  batch step: 90, loss: 44.59911346435547\n",
      "epoch: 24,  batch step: 91, loss: 7.069200038909912\n",
      "epoch: 24,  batch step: 92, loss: 20.373191833496094\n",
      "epoch: 24,  batch step: 93, loss: 12.062240600585938\n",
      "epoch: 24,  batch step: 94, loss: 6.829129219055176\n",
      "epoch: 24,  batch step: 95, loss: 46.63420867919922\n",
      "epoch: 24,  batch step: 96, loss: 91.0165023803711\n",
      "epoch: 24,  batch step: 97, loss: 35.8956413269043\n",
      "epoch: 24,  batch step: 98, loss: 70.38270568847656\n",
      "epoch: 24,  batch step: 99, loss: 6.694451808929443\n",
      "epoch: 24,  batch step: 100, loss: 47.199562072753906\n",
      "epoch: 24,  batch step: 101, loss: 78.17797088623047\n",
      "epoch: 24,  batch step: 102, loss: 85.53965759277344\n",
      "epoch: 24,  batch step: 103, loss: 63.57489013671875\n",
      "epoch: 24,  batch step: 104, loss: 17.998733520507812\n",
      "epoch: 24,  batch step: 105, loss: 9.768324851989746\n",
      "epoch: 24,  batch step: 106, loss: 85.90452575683594\n",
      "epoch: 24,  batch step: 107, loss: 55.85097885131836\n",
      "epoch: 24,  batch step: 108, loss: 24.025564193725586\n",
      "epoch: 24,  batch step: 109, loss: 6.016396999359131\n",
      "epoch: 24,  batch step: 110, loss: 78.68993377685547\n",
      "epoch: 24,  batch step: 111, loss: 107.27001953125\n",
      "epoch: 24,  batch step: 112, loss: 54.047279357910156\n",
      "epoch: 24,  batch step: 113, loss: 6.846949577331543\n",
      "epoch: 24,  batch step: 114, loss: 48.99602508544922\n",
      "epoch: 24,  batch step: 115, loss: 17.53406524658203\n",
      "epoch: 24,  batch step: 116, loss: 12.387248992919922\n",
      "epoch: 24,  batch step: 117, loss: 6.236968994140625\n",
      "epoch: 24,  batch step: 118, loss: 14.741477966308594\n",
      "epoch: 24,  batch step: 119, loss: 102.93968963623047\n",
      "epoch: 24,  batch step: 120, loss: 120.52874755859375\n",
      "epoch: 24,  batch step: 121, loss: 270.5064697265625\n",
      "epoch: 24,  batch step: 122, loss: 70.78921508789062\n",
      "epoch: 24,  batch step: 123, loss: 45.82125473022461\n",
      "epoch: 24,  batch step: 124, loss: 17.23695182800293\n",
      "epoch: 24,  batch step: 125, loss: 22.188011169433594\n",
      "epoch: 24,  batch step: 126, loss: 25.03804588317871\n",
      "epoch: 24,  batch step: 127, loss: 155.22677612304688\n",
      "epoch: 24,  batch step: 128, loss: 11.762374877929688\n",
      "epoch: 24,  batch step: 129, loss: 11.014811515808105\n",
      "epoch: 24,  batch step: 130, loss: 16.604766845703125\n",
      "epoch: 24,  batch step: 131, loss: 11.490819931030273\n",
      "epoch: 24,  batch step: 132, loss: 5.684190273284912\n",
      "epoch: 24,  batch step: 133, loss: 31.657196044921875\n",
      "epoch: 24,  batch step: 134, loss: 76.93632507324219\n",
      "epoch: 24,  batch step: 135, loss: 9.407453536987305\n",
      "epoch: 24,  batch step: 136, loss: 9.229415893554688\n",
      "epoch: 24,  batch step: 137, loss: 41.057003021240234\n",
      "epoch: 24,  batch step: 138, loss: 17.939613342285156\n",
      "epoch: 24,  batch step: 139, loss: 98.30413055419922\n",
      "epoch: 24,  batch step: 140, loss: 116.87407684326172\n",
      "epoch: 24,  batch step: 141, loss: 56.705162048339844\n",
      "epoch: 24,  batch step: 142, loss: 11.280230522155762\n",
      "epoch: 24,  batch step: 143, loss: 57.82183837890625\n",
      "epoch: 24,  batch step: 144, loss: 73.98175048828125\n",
      "epoch: 24,  batch step: 145, loss: 9.324868202209473\n",
      "epoch: 24,  batch step: 146, loss: 19.726980209350586\n",
      "epoch: 24,  batch step: 147, loss: 11.200926780700684\n",
      "epoch: 24,  batch step: 148, loss: 120.51310729980469\n",
      "epoch: 24,  batch step: 149, loss: 19.96337890625\n",
      "epoch: 24,  batch step: 150, loss: 64.2136459350586\n",
      "epoch: 24,  batch step: 151, loss: 54.84879684448242\n",
      "epoch: 24,  batch step: 152, loss: 104.177001953125\n",
      "epoch: 24,  batch step: 153, loss: 16.391010284423828\n",
      "epoch: 24,  batch step: 154, loss: 50.94940948486328\n",
      "epoch: 24,  batch step: 155, loss: 12.13056468963623\n",
      "epoch: 24,  batch step: 156, loss: 31.919692993164062\n",
      "epoch: 24,  batch step: 157, loss: 16.16590690612793\n",
      "epoch: 24,  batch step: 158, loss: 23.378585815429688\n",
      "epoch: 24,  batch step: 159, loss: 57.17717742919922\n",
      "epoch: 24,  batch step: 160, loss: 128.0264434814453\n",
      "epoch: 24,  batch step: 161, loss: 49.11713790893555\n",
      "epoch: 24,  batch step: 162, loss: 272.291015625\n",
      "epoch: 24,  batch step: 163, loss: 108.23784637451172\n",
      "epoch: 24,  batch step: 164, loss: 56.279396057128906\n",
      "epoch: 24,  batch step: 165, loss: 19.870643615722656\n",
      "epoch: 24,  batch step: 166, loss: 13.484861373901367\n",
      "epoch: 24,  batch step: 167, loss: 28.21526336669922\n",
      "epoch: 24,  batch step: 168, loss: 84.82105255126953\n",
      "epoch: 24,  batch step: 169, loss: 84.24274444580078\n",
      "epoch: 24,  batch step: 170, loss: 17.739686965942383\n",
      "epoch: 24,  batch step: 171, loss: 52.052223205566406\n",
      "epoch: 24,  batch step: 172, loss: 10.593984603881836\n",
      "epoch: 24,  batch step: 173, loss: 14.008773803710938\n",
      "epoch: 24,  batch step: 174, loss: 26.26512908935547\n",
      "epoch: 24,  batch step: 175, loss: 68.81695556640625\n",
      "epoch: 24,  batch step: 176, loss: 48.51559066772461\n",
      "epoch: 24,  batch step: 177, loss: 45.663787841796875\n",
      "epoch: 24,  batch step: 178, loss: 28.68262481689453\n",
      "epoch: 24,  batch step: 179, loss: 10.91989517211914\n",
      "epoch: 24,  batch step: 180, loss: 94.62081909179688\n",
      "epoch: 24,  batch step: 181, loss: 11.675905227661133\n",
      "epoch: 24,  batch step: 182, loss: 213.75579833984375\n",
      "epoch: 24,  batch step: 183, loss: 44.00596618652344\n",
      "epoch: 24,  batch step: 184, loss: 7.541558265686035\n",
      "epoch: 24,  batch step: 185, loss: 9.625542640686035\n",
      "epoch: 24,  batch step: 186, loss: 11.216421127319336\n",
      "epoch: 24,  batch step: 187, loss: 53.5321044921875\n",
      "epoch: 24,  batch step: 188, loss: 85.28045654296875\n",
      "epoch: 24,  batch step: 189, loss: 13.845344543457031\n",
      "epoch: 24,  batch step: 190, loss: 67.055908203125\n",
      "epoch: 24,  batch step: 191, loss: 6.7178874015808105\n",
      "epoch: 24,  batch step: 192, loss: 7.056832313537598\n",
      "epoch: 24,  batch step: 193, loss: 48.23995590209961\n",
      "epoch: 24,  batch step: 194, loss: 87.9032211303711\n",
      "epoch: 24,  batch step: 195, loss: 5.950813293457031\n",
      "epoch: 24,  batch step: 196, loss: 65.2718734741211\n",
      "epoch: 24,  batch step: 197, loss: 63.49607467651367\n",
      "epoch: 24,  batch step: 198, loss: 10.569498062133789\n",
      "epoch: 24,  batch step: 199, loss: 96.37680053710938\n",
      "epoch: 24,  batch step: 200, loss: 5.485926628112793\n",
      "epoch: 24,  batch step: 201, loss: 80.62423706054688\n",
      "epoch: 24,  batch step: 202, loss: 4.727283477783203\n",
      "epoch: 24,  batch step: 203, loss: 80.72511291503906\n",
      "epoch: 24,  batch step: 204, loss: 96.50563049316406\n",
      "epoch: 24,  batch step: 205, loss: 104.71806335449219\n",
      "epoch: 24,  batch step: 206, loss: 72.24485778808594\n",
      "epoch: 24,  batch step: 207, loss: 94.2353744506836\n",
      "epoch: 24,  batch step: 208, loss: 19.23531150817871\n",
      "epoch: 24,  batch step: 209, loss: 15.39031982421875\n",
      "epoch: 24,  batch step: 210, loss: 103.19439697265625\n",
      "epoch: 24,  batch step: 211, loss: 14.39411735534668\n",
      "epoch: 24,  batch step: 212, loss: 6.758206844329834\n",
      "epoch: 24,  batch step: 213, loss: 13.811847686767578\n",
      "epoch: 24,  batch step: 214, loss: 16.069976806640625\n",
      "epoch: 24,  batch step: 215, loss: 32.10117721557617\n",
      "epoch: 24,  batch step: 216, loss: 66.52223205566406\n",
      "epoch: 24,  batch step: 217, loss: 11.19861125946045\n",
      "epoch: 24,  batch step: 218, loss: 16.606197357177734\n",
      "epoch: 24,  batch step: 219, loss: 167.00927734375\n",
      "epoch: 24,  batch step: 220, loss: 10.294867515563965\n",
      "epoch: 24,  batch step: 221, loss: 26.15753746032715\n",
      "epoch: 24,  batch step: 222, loss: 103.88314056396484\n",
      "epoch: 24,  batch step: 223, loss: 18.184724807739258\n",
      "epoch: 24,  batch step: 224, loss: 53.55830383300781\n",
      "epoch: 24,  batch step: 225, loss: 96.60807800292969\n",
      "epoch: 24,  batch step: 226, loss: 7.084429740905762\n",
      "epoch: 24,  batch step: 227, loss: 19.894107818603516\n",
      "epoch: 24,  batch step: 228, loss: 37.09261703491211\n",
      "epoch: 24,  batch step: 229, loss: 20.44942855834961\n",
      "epoch: 24,  batch step: 230, loss: 96.0556640625\n",
      "epoch: 24,  batch step: 231, loss: 77.88663482666016\n",
      "epoch: 24,  batch step: 232, loss: 120.3022689819336\n",
      "epoch: 24,  batch step: 233, loss: 76.3201675415039\n",
      "epoch: 24,  batch step: 234, loss: 12.214198112487793\n",
      "epoch: 24,  batch step: 235, loss: 12.900199890136719\n",
      "epoch: 24,  batch step: 236, loss: 61.177059173583984\n",
      "epoch: 24,  batch step: 237, loss: 22.562454223632812\n",
      "epoch: 24,  batch step: 238, loss: 55.66556930541992\n",
      "epoch: 24,  batch step: 239, loss: 13.220623016357422\n",
      "epoch: 24,  batch step: 240, loss: 8.816978454589844\n",
      "epoch: 24,  batch step: 241, loss: 37.027557373046875\n",
      "epoch: 24,  batch step: 242, loss: 43.010154724121094\n",
      "epoch: 24,  batch step: 243, loss: 8.895881652832031\n",
      "epoch: 24,  batch step: 244, loss: 11.737190246582031\n",
      "epoch: 24,  batch step: 245, loss: 66.97552490234375\n",
      "epoch: 24,  batch step: 246, loss: 3.857184410095215\n",
      "epoch: 24,  batch step: 247, loss: 96.98414611816406\n",
      "epoch: 24,  batch step: 248, loss: 15.003314018249512\n",
      "epoch: 24,  batch step: 249, loss: 18.860734939575195\n",
      "epoch: 24,  batch step: 250, loss: 23.461151123046875\n",
      "epoch: 24,  batch step: 251, loss: 41.34701156616211\n",
      "validation error epoch  24:    tensor(68.9888, device='cuda:0')\n",
      "316\n",
      "epoch: 25,  batch step: 0, loss: 136.78907775878906\n",
      "epoch: 25,  batch step: 1, loss: 16.48392105102539\n",
      "epoch: 25,  batch step: 2, loss: 86.29689025878906\n",
      "epoch: 25,  batch step: 3, loss: 12.258472442626953\n",
      "epoch: 25,  batch step: 4, loss: 8.79548168182373\n",
      "epoch: 25,  batch step: 5, loss: 55.10187911987305\n",
      "epoch: 25,  batch step: 6, loss: 7.839524269104004\n",
      "epoch: 25,  batch step: 7, loss: 13.598968505859375\n",
      "epoch: 25,  batch step: 8, loss: 71.2046890258789\n",
      "epoch: 25,  batch step: 9, loss: 52.40914535522461\n",
      "epoch: 25,  batch step: 10, loss: 105.58013916015625\n",
      "epoch: 25,  batch step: 11, loss: 9.548110961914062\n",
      "epoch: 25,  batch step: 12, loss: 69.91427612304688\n",
      "epoch: 25,  batch step: 13, loss: 9.970962524414062\n",
      "epoch: 25,  batch step: 14, loss: 19.749801635742188\n",
      "epoch: 25,  batch step: 15, loss: 18.931638717651367\n",
      "epoch: 25,  batch step: 16, loss: 6.564724922180176\n",
      "epoch: 25,  batch step: 17, loss: 110.38627624511719\n",
      "epoch: 25,  batch step: 18, loss: 134.0003662109375\n",
      "epoch: 25,  batch step: 19, loss: 199.91221618652344\n",
      "epoch: 25,  batch step: 20, loss: 34.10138702392578\n",
      "epoch: 25,  batch step: 21, loss: 29.026609420776367\n",
      "epoch: 25,  batch step: 22, loss: 18.94385528564453\n",
      "epoch: 25,  batch step: 23, loss: 55.44636154174805\n",
      "epoch: 25,  batch step: 24, loss: 10.434440612792969\n",
      "epoch: 25,  batch step: 25, loss: 16.183414459228516\n",
      "epoch: 25,  batch step: 26, loss: 6.943421363830566\n",
      "epoch: 25,  batch step: 27, loss: 44.054710388183594\n",
      "epoch: 25,  batch step: 28, loss: 97.37991333007812\n",
      "epoch: 25,  batch step: 29, loss: 61.927528381347656\n",
      "epoch: 25,  batch step: 30, loss: 14.064108848571777\n",
      "epoch: 25,  batch step: 31, loss: 61.99790573120117\n",
      "epoch: 25,  batch step: 32, loss: 106.34054565429688\n",
      "epoch: 25,  batch step: 33, loss: 63.400848388671875\n",
      "epoch: 25,  batch step: 34, loss: 7.452952861785889\n",
      "epoch: 25,  batch step: 35, loss: 18.121875762939453\n",
      "epoch: 25,  batch step: 36, loss: 8.355142593383789\n",
      "epoch: 25,  batch step: 37, loss: 134.59222412109375\n",
      "epoch: 25,  batch step: 38, loss: 18.683841705322266\n",
      "epoch: 25,  batch step: 39, loss: 11.485655784606934\n",
      "epoch: 25,  batch step: 40, loss: 8.600103378295898\n",
      "epoch: 25,  batch step: 41, loss: 14.939842224121094\n",
      "epoch: 25,  batch step: 42, loss: 22.93903350830078\n",
      "epoch: 25,  batch step: 43, loss: 47.12806701660156\n",
      "epoch: 25,  batch step: 44, loss: 172.33938598632812\n",
      "epoch: 25,  batch step: 45, loss: 28.91976547241211\n",
      "epoch: 25,  batch step: 46, loss: 109.870849609375\n",
      "epoch: 25,  batch step: 47, loss: 14.533132553100586\n",
      "epoch: 25,  batch step: 48, loss: 22.84943962097168\n",
      "epoch: 25,  batch step: 49, loss: 58.787879943847656\n",
      "epoch: 25,  batch step: 50, loss: 9.22032356262207\n",
      "epoch: 25,  batch step: 51, loss: 6.506826400756836\n",
      "epoch: 25,  batch step: 52, loss: 146.2782440185547\n",
      "epoch: 25,  batch step: 53, loss: 55.94697189331055\n",
      "epoch: 25,  batch step: 54, loss: 9.173444747924805\n",
      "epoch: 25,  batch step: 55, loss: 142.87722778320312\n",
      "epoch: 25,  batch step: 56, loss: 64.76839447021484\n",
      "epoch: 25,  batch step: 57, loss: 34.71180725097656\n",
      "epoch: 25,  batch step: 58, loss: 49.67029571533203\n",
      "epoch: 25,  batch step: 59, loss: 10.423210144042969\n",
      "epoch: 25,  batch step: 60, loss: 7.4507856369018555\n",
      "epoch: 25,  batch step: 61, loss: 78.1346664428711\n",
      "epoch: 25,  batch step: 62, loss: 15.73078441619873\n",
      "epoch: 25,  batch step: 63, loss: 10.779182434082031\n",
      "epoch: 25,  batch step: 64, loss: 12.425992965698242\n",
      "epoch: 25,  batch step: 65, loss: 9.173064231872559\n",
      "epoch: 25,  batch step: 66, loss: 32.712493896484375\n",
      "epoch: 25,  batch step: 67, loss: 142.20751953125\n",
      "epoch: 25,  batch step: 68, loss: 10.072840690612793\n",
      "epoch: 25,  batch step: 69, loss: 128.57044982910156\n",
      "epoch: 25,  batch step: 70, loss: 66.2603759765625\n",
      "epoch: 25,  batch step: 71, loss: 29.717403411865234\n",
      "epoch: 25,  batch step: 72, loss: 134.0702362060547\n",
      "epoch: 25,  batch step: 73, loss: 120.40099334716797\n",
      "epoch: 25,  batch step: 74, loss: 14.786445617675781\n",
      "epoch: 25,  batch step: 75, loss: 41.497955322265625\n",
      "epoch: 25,  batch step: 76, loss: 42.892478942871094\n",
      "epoch: 25,  batch step: 77, loss: 23.830158233642578\n",
      "epoch: 25,  batch step: 78, loss: 68.14469909667969\n",
      "epoch: 25,  batch step: 79, loss: 111.7951889038086\n",
      "epoch: 25,  batch step: 80, loss: 122.08223724365234\n",
      "epoch: 25,  batch step: 81, loss: 33.0776481628418\n",
      "epoch: 25,  batch step: 82, loss: 14.512476921081543\n",
      "epoch: 25,  batch step: 83, loss: 8.06155776977539\n",
      "epoch: 25,  batch step: 84, loss: 11.877330780029297\n",
      "epoch: 25,  batch step: 85, loss: 7.512951850891113\n",
      "epoch: 25,  batch step: 86, loss: 18.035905838012695\n",
      "epoch: 25,  batch step: 87, loss: 22.125900268554688\n",
      "epoch: 25,  batch step: 88, loss: 195.0251922607422\n",
      "epoch: 25,  batch step: 89, loss: 31.923009872436523\n",
      "epoch: 25,  batch step: 90, loss: 25.22519302368164\n",
      "epoch: 25,  batch step: 91, loss: 26.85171127319336\n",
      "epoch: 25,  batch step: 92, loss: 10.10047721862793\n",
      "epoch: 25,  batch step: 93, loss: 23.954076766967773\n",
      "epoch: 25,  batch step: 94, loss: 16.93956756591797\n",
      "epoch: 25,  batch step: 95, loss: 12.33973503112793\n",
      "epoch: 25,  batch step: 96, loss: 26.430349349975586\n",
      "epoch: 25,  batch step: 97, loss: 115.10735321044922\n",
      "epoch: 25,  batch step: 98, loss: 87.58717346191406\n",
      "epoch: 25,  batch step: 99, loss: 5.406844139099121\n",
      "epoch: 25,  batch step: 100, loss: 38.438453674316406\n",
      "epoch: 25,  batch step: 101, loss: 7.353196620941162\n",
      "epoch: 25,  batch step: 102, loss: 22.343704223632812\n",
      "epoch: 25,  batch step: 103, loss: 21.966064453125\n",
      "epoch: 25,  batch step: 104, loss: 31.582611083984375\n",
      "epoch: 25,  batch step: 105, loss: 12.871749877929688\n",
      "epoch: 25,  batch step: 106, loss: 66.45356750488281\n",
      "epoch: 25,  batch step: 107, loss: 32.917320251464844\n",
      "epoch: 25,  batch step: 108, loss: 81.84561920166016\n",
      "epoch: 25,  batch step: 109, loss: 11.010858535766602\n",
      "epoch: 25,  batch step: 110, loss: 21.088470458984375\n",
      "epoch: 25,  batch step: 111, loss: 141.59934997558594\n",
      "epoch: 25,  batch step: 112, loss: 10.006769180297852\n",
      "epoch: 25,  batch step: 113, loss: 30.985754013061523\n",
      "epoch: 25,  batch step: 114, loss: 7.833896636962891\n",
      "epoch: 25,  batch step: 115, loss: 8.342260360717773\n",
      "epoch: 25,  batch step: 116, loss: 106.96288299560547\n",
      "epoch: 25,  batch step: 117, loss: 93.15153503417969\n",
      "epoch: 25,  batch step: 118, loss: 8.891566276550293\n",
      "epoch: 25,  batch step: 119, loss: 67.8739013671875\n",
      "epoch: 25,  batch step: 120, loss: 51.55204772949219\n",
      "epoch: 25,  batch step: 121, loss: 19.87077522277832\n",
      "epoch: 25,  batch step: 122, loss: 8.959393501281738\n",
      "epoch: 25,  batch step: 123, loss: 16.36724853515625\n",
      "epoch: 25,  batch step: 124, loss: 16.846899032592773\n",
      "epoch: 25,  batch step: 125, loss: 6.496098518371582\n",
      "epoch: 25,  batch step: 126, loss: 29.42146873474121\n",
      "epoch: 25,  batch step: 127, loss: 57.68180465698242\n",
      "epoch: 25,  batch step: 128, loss: 28.680561065673828\n",
      "epoch: 25,  batch step: 129, loss: 78.49430847167969\n",
      "epoch: 25,  batch step: 130, loss: 14.245840072631836\n",
      "epoch: 25,  batch step: 131, loss: 87.4023666381836\n",
      "epoch: 25,  batch step: 132, loss: 88.88726806640625\n",
      "epoch: 25,  batch step: 133, loss: 63.97466278076172\n",
      "epoch: 25,  batch step: 134, loss: 9.327997207641602\n",
      "epoch: 25,  batch step: 135, loss: 149.4168243408203\n",
      "epoch: 25,  batch step: 136, loss: 36.45573043823242\n",
      "epoch: 25,  batch step: 137, loss: 99.06431579589844\n",
      "epoch: 25,  batch step: 138, loss: 134.15457153320312\n",
      "epoch: 25,  batch step: 139, loss: 47.70156478881836\n",
      "epoch: 25,  batch step: 140, loss: 156.68856811523438\n",
      "epoch: 25,  batch step: 141, loss: 8.177225112915039\n",
      "epoch: 25,  batch step: 142, loss: 14.543550491333008\n",
      "epoch: 25,  batch step: 143, loss: 20.488157272338867\n",
      "epoch: 25,  batch step: 144, loss: 153.67999267578125\n",
      "epoch: 25,  batch step: 145, loss: 34.80946350097656\n",
      "epoch: 25,  batch step: 146, loss: 9.538921356201172\n",
      "epoch: 25,  batch step: 147, loss: 46.69286346435547\n",
      "epoch: 25,  batch step: 148, loss: 13.394149780273438\n",
      "epoch: 25,  batch step: 149, loss: 75.96096801757812\n",
      "epoch: 25,  batch step: 150, loss: 11.986509323120117\n",
      "epoch: 25,  batch step: 151, loss: 28.71902084350586\n",
      "epoch: 25,  batch step: 152, loss: 40.793357849121094\n",
      "epoch: 25,  batch step: 153, loss: 26.54188346862793\n",
      "epoch: 25,  batch step: 154, loss: 12.3638334274292\n",
      "epoch: 25,  batch step: 155, loss: 7.854723930358887\n",
      "epoch: 25,  batch step: 156, loss: 53.07106018066406\n",
      "epoch: 25,  batch step: 157, loss: 147.16172790527344\n",
      "epoch: 25,  batch step: 158, loss: 8.260211944580078\n",
      "epoch: 25,  batch step: 159, loss: 91.17701721191406\n",
      "epoch: 25,  batch step: 160, loss: 17.891145706176758\n",
      "epoch: 25,  batch step: 161, loss: 16.390317916870117\n",
      "epoch: 25,  batch step: 162, loss: 35.45911407470703\n",
      "epoch: 25,  batch step: 163, loss: 59.31559371948242\n",
      "epoch: 25,  batch step: 164, loss: 104.47264862060547\n",
      "epoch: 25,  batch step: 165, loss: 5.445446014404297\n",
      "epoch: 25,  batch step: 166, loss: 7.291314601898193\n",
      "epoch: 25,  batch step: 167, loss: 186.5532684326172\n",
      "epoch: 25,  batch step: 168, loss: 23.953990936279297\n",
      "epoch: 25,  batch step: 169, loss: 6.247575283050537\n",
      "epoch: 25,  batch step: 170, loss: 39.464176177978516\n",
      "epoch: 25,  batch step: 171, loss: 53.48887252807617\n",
      "epoch: 25,  batch step: 172, loss: 53.78838348388672\n",
      "epoch: 25,  batch step: 173, loss: 89.47338104248047\n",
      "epoch: 25,  batch step: 174, loss: 124.14550018310547\n",
      "epoch: 25,  batch step: 175, loss: 22.631521224975586\n",
      "epoch: 25,  batch step: 176, loss: 15.012321472167969\n",
      "epoch: 25,  batch step: 177, loss: 43.35185241699219\n",
      "epoch: 25,  batch step: 178, loss: 46.908687591552734\n",
      "epoch: 25,  batch step: 179, loss: 16.169408798217773\n",
      "epoch: 25,  batch step: 180, loss: 33.63258361816406\n",
      "epoch: 25,  batch step: 181, loss: 22.8823299407959\n",
      "epoch: 25,  batch step: 182, loss: 87.1085205078125\n",
      "epoch: 25,  batch step: 183, loss: 208.00531005859375\n",
      "epoch: 25,  batch step: 184, loss: 58.23345947265625\n",
      "epoch: 25,  batch step: 185, loss: 10.591588973999023\n",
      "epoch: 25,  batch step: 186, loss: 12.833120346069336\n",
      "epoch: 25,  batch step: 187, loss: 16.1889705657959\n",
      "epoch: 25,  batch step: 188, loss: 16.5155029296875\n",
      "epoch: 25,  batch step: 189, loss: 15.54123592376709\n",
      "epoch: 25,  batch step: 190, loss: 32.840972900390625\n",
      "epoch: 25,  batch step: 191, loss: 8.411556243896484\n",
      "epoch: 25,  batch step: 192, loss: 99.42952728271484\n",
      "epoch: 25,  batch step: 193, loss: 120.56026458740234\n",
      "epoch: 25,  batch step: 194, loss: 6.837289333343506\n",
      "epoch: 25,  batch step: 195, loss: 18.137287139892578\n",
      "epoch: 25,  batch step: 196, loss: 54.74840545654297\n",
      "epoch: 25,  batch step: 197, loss: 17.245521545410156\n",
      "epoch: 25,  batch step: 198, loss: 67.67570495605469\n",
      "epoch: 25,  batch step: 199, loss: 48.39509582519531\n",
      "epoch: 25,  batch step: 200, loss: 79.83348083496094\n",
      "epoch: 25,  batch step: 201, loss: 90.43663024902344\n",
      "epoch: 25,  batch step: 202, loss: 17.020740509033203\n",
      "epoch: 25,  batch step: 203, loss: 12.1539945602417\n",
      "epoch: 25,  batch step: 204, loss: 91.7906723022461\n",
      "epoch: 25,  batch step: 205, loss: 11.511443138122559\n",
      "epoch: 25,  batch step: 206, loss: 8.746011734008789\n",
      "epoch: 25,  batch step: 207, loss: 7.716724395751953\n",
      "epoch: 25,  batch step: 208, loss: 86.43535614013672\n",
      "epoch: 25,  batch step: 209, loss: 12.766843795776367\n",
      "epoch: 25,  batch step: 210, loss: 15.542596817016602\n",
      "epoch: 25,  batch step: 211, loss: 110.64470672607422\n",
      "epoch: 25,  batch step: 212, loss: 110.25425720214844\n",
      "epoch: 25,  batch step: 213, loss: 80.19861602783203\n",
      "epoch: 25,  batch step: 214, loss: 8.693443298339844\n",
      "epoch: 25,  batch step: 215, loss: 47.255943298339844\n",
      "epoch: 25,  batch step: 216, loss: 14.226950645446777\n",
      "epoch: 25,  batch step: 217, loss: 8.705657005310059\n",
      "epoch: 25,  batch step: 218, loss: 27.46480369567871\n",
      "epoch: 25,  batch step: 219, loss: 9.619260787963867\n",
      "epoch: 25,  batch step: 220, loss: 7.603471279144287\n",
      "epoch: 25,  batch step: 221, loss: 21.885574340820312\n",
      "epoch: 25,  batch step: 222, loss: 6.055498123168945\n",
      "epoch: 25,  batch step: 223, loss: 10.38395881652832\n",
      "epoch: 25,  batch step: 224, loss: 29.649185180664062\n",
      "epoch: 25,  batch step: 225, loss: 49.05425262451172\n",
      "epoch: 25,  batch step: 226, loss: 14.973464965820312\n",
      "epoch: 25,  batch step: 227, loss: 24.285367965698242\n",
      "epoch: 25,  batch step: 228, loss: 18.122177124023438\n",
      "epoch: 25,  batch step: 229, loss: 33.42739486694336\n",
      "epoch: 25,  batch step: 230, loss: 65.85015869140625\n",
      "epoch: 25,  batch step: 231, loss: 54.240596771240234\n",
      "epoch: 25,  batch step: 232, loss: 4.874917507171631\n",
      "epoch: 25,  batch step: 233, loss: 46.427242279052734\n",
      "epoch: 25,  batch step: 234, loss: 8.082548141479492\n",
      "epoch: 25,  batch step: 235, loss: 51.43224334716797\n",
      "epoch: 25,  batch step: 236, loss: 51.814857482910156\n",
      "epoch: 25,  batch step: 237, loss: 67.55423736572266\n",
      "epoch: 25,  batch step: 238, loss: 88.40980529785156\n",
      "epoch: 25,  batch step: 239, loss: 35.283443450927734\n",
      "epoch: 25,  batch step: 240, loss: 60.300132751464844\n",
      "epoch: 25,  batch step: 241, loss: 10.088868141174316\n",
      "epoch: 25,  batch step: 242, loss: 8.547446250915527\n",
      "epoch: 25,  batch step: 243, loss: 26.073095321655273\n",
      "epoch: 25,  batch step: 244, loss: 9.698307037353516\n",
      "epoch: 25,  batch step: 245, loss: 8.45991039276123\n",
      "epoch: 25,  batch step: 246, loss: 118.06550598144531\n",
      "epoch: 25,  batch step: 247, loss: 14.675024032592773\n",
      "epoch: 25,  batch step: 248, loss: 44.00934982299805\n",
      "epoch: 25,  batch step: 249, loss: 16.12514305114746\n",
      "epoch: 25,  batch step: 250, loss: 92.96356201171875\n",
      "epoch: 25,  batch step: 251, loss: 39.643333435058594\n",
      "validation error epoch  25:    tensor(70.0265, device='cuda:0')\n",
      "316\n",
      "epoch: 26,  batch step: 0, loss: 19.669933319091797\n",
      "epoch: 26,  batch step: 1, loss: 48.736629486083984\n",
      "epoch: 26,  batch step: 2, loss: 51.4654541015625\n",
      "epoch: 26,  batch step: 3, loss: 71.30525207519531\n",
      "epoch: 26,  batch step: 4, loss: 8.627056121826172\n",
      "epoch: 26,  batch step: 5, loss: 11.27994441986084\n",
      "epoch: 26,  batch step: 6, loss: 23.267744064331055\n",
      "epoch: 26,  batch step: 7, loss: 146.490478515625\n",
      "epoch: 26,  batch step: 8, loss: 7.3257155418396\n",
      "epoch: 26,  batch step: 9, loss: 39.77119445800781\n",
      "epoch: 26,  batch step: 10, loss: 16.626548767089844\n",
      "epoch: 26,  batch step: 11, loss: 72.15177154541016\n",
      "epoch: 26,  batch step: 12, loss: 22.035680770874023\n",
      "epoch: 26,  batch step: 13, loss: 12.362383842468262\n",
      "epoch: 26,  batch step: 14, loss: 27.57386016845703\n",
      "epoch: 26,  batch step: 15, loss: 72.31375122070312\n",
      "epoch: 26,  batch step: 16, loss: 15.459151268005371\n",
      "epoch: 26,  batch step: 17, loss: 115.87841033935547\n",
      "epoch: 26,  batch step: 18, loss: 6.08870267868042\n",
      "epoch: 26,  batch step: 19, loss: 22.053707122802734\n",
      "epoch: 26,  batch step: 20, loss: 8.032112121582031\n",
      "epoch: 26,  batch step: 21, loss: 51.28831481933594\n",
      "epoch: 26,  batch step: 22, loss: 44.94554901123047\n",
      "epoch: 26,  batch step: 23, loss: 18.76625633239746\n",
      "epoch: 26,  batch step: 24, loss: 46.402793884277344\n",
      "epoch: 26,  batch step: 25, loss: 46.811031341552734\n",
      "epoch: 26,  batch step: 26, loss: 36.46385955810547\n",
      "epoch: 26,  batch step: 27, loss: 17.94632911682129\n",
      "epoch: 26,  batch step: 28, loss: 64.98872375488281\n",
      "epoch: 26,  batch step: 29, loss: 62.935646057128906\n",
      "epoch: 26,  batch step: 30, loss: 36.43428421020508\n",
      "epoch: 26,  batch step: 31, loss: 80.041259765625\n",
      "epoch: 26,  batch step: 32, loss: 5.432191371917725\n",
      "epoch: 26,  batch step: 33, loss: 22.850337982177734\n",
      "epoch: 26,  batch step: 34, loss: 39.9755859375\n",
      "epoch: 26,  batch step: 35, loss: 47.95490646362305\n",
      "epoch: 26,  batch step: 36, loss: 40.791786193847656\n",
      "epoch: 26,  batch step: 37, loss: 23.597476959228516\n",
      "epoch: 26,  batch step: 38, loss: 20.69891929626465\n",
      "epoch: 26,  batch step: 39, loss: 36.445743560791016\n",
      "epoch: 26,  batch step: 40, loss: 24.83846664428711\n",
      "epoch: 26,  batch step: 41, loss: 33.56748962402344\n",
      "epoch: 26,  batch step: 42, loss: 55.051170349121094\n",
      "epoch: 26,  batch step: 43, loss: 53.8962516784668\n",
      "epoch: 26,  batch step: 44, loss: 13.146347045898438\n",
      "epoch: 26,  batch step: 45, loss: 84.611083984375\n",
      "epoch: 26,  batch step: 46, loss: 17.82077980041504\n",
      "epoch: 26,  batch step: 47, loss: 11.218894004821777\n",
      "epoch: 26,  batch step: 48, loss: 29.38735580444336\n",
      "epoch: 26,  batch step: 49, loss: 14.205804824829102\n",
      "epoch: 26,  batch step: 50, loss: 62.03757858276367\n",
      "epoch: 26,  batch step: 51, loss: 169.32659912109375\n",
      "epoch: 26,  batch step: 52, loss: 10.111247062683105\n",
      "epoch: 26,  batch step: 53, loss: 9.457635879516602\n",
      "epoch: 26,  batch step: 54, loss: 436.52960205078125\n",
      "epoch: 26,  batch step: 55, loss: 50.79615020751953\n",
      "epoch: 26,  batch step: 56, loss: 5.421779632568359\n",
      "epoch: 26,  batch step: 57, loss: 42.09535217285156\n",
      "epoch: 26,  batch step: 58, loss: 4.363537788391113\n",
      "epoch: 26,  batch step: 59, loss: 57.397239685058594\n",
      "epoch: 26,  batch step: 60, loss: 76.51162719726562\n",
      "epoch: 26,  batch step: 61, loss: 12.434099197387695\n",
      "epoch: 26,  batch step: 62, loss: 31.523292541503906\n",
      "epoch: 26,  batch step: 63, loss: 5.916327476501465\n",
      "epoch: 26,  batch step: 64, loss: 111.02678680419922\n",
      "epoch: 26,  batch step: 65, loss: 118.04521942138672\n",
      "epoch: 26,  batch step: 66, loss: 7.329892635345459\n",
      "epoch: 26,  batch step: 67, loss: 23.675186157226562\n",
      "epoch: 26,  batch step: 68, loss: 7.422517776489258\n",
      "epoch: 26,  batch step: 69, loss: 13.679780960083008\n",
      "epoch: 26,  batch step: 70, loss: 18.54098129272461\n",
      "epoch: 26,  batch step: 71, loss: 4.640686988830566\n",
      "epoch: 26,  batch step: 72, loss: 23.405277252197266\n",
      "epoch: 26,  batch step: 73, loss: 125.40512084960938\n",
      "epoch: 26,  batch step: 74, loss: 10.65972900390625\n",
      "epoch: 26,  batch step: 75, loss: 8.13986587524414\n",
      "epoch: 26,  batch step: 76, loss: 20.69537925720215\n",
      "epoch: 26,  batch step: 77, loss: 14.83032512664795\n",
      "epoch: 26,  batch step: 78, loss: 6.360939025878906\n",
      "epoch: 26,  batch step: 79, loss: 49.38863754272461\n",
      "epoch: 26,  batch step: 80, loss: 8.410152435302734\n",
      "epoch: 26,  batch step: 81, loss: 9.877045631408691\n",
      "epoch: 26,  batch step: 82, loss: 78.11612701416016\n",
      "epoch: 26,  batch step: 83, loss: 31.03639793395996\n",
      "epoch: 26,  batch step: 84, loss: 17.817626953125\n",
      "epoch: 26,  batch step: 85, loss: 39.104522705078125\n",
      "epoch: 26,  batch step: 86, loss: 9.606019973754883\n",
      "epoch: 26,  batch step: 87, loss: 12.063311576843262\n",
      "epoch: 26,  batch step: 88, loss: 112.08062744140625\n",
      "epoch: 26,  batch step: 89, loss: 5.87220573425293\n",
      "epoch: 26,  batch step: 90, loss: 6.07865047454834\n",
      "epoch: 26,  batch step: 91, loss: 153.51312255859375\n",
      "epoch: 26,  batch step: 92, loss: 11.111285209655762\n",
      "epoch: 26,  batch step: 93, loss: 82.66671752929688\n",
      "epoch: 26,  batch step: 94, loss: 35.66722869873047\n",
      "epoch: 26,  batch step: 95, loss: 44.80128479003906\n",
      "epoch: 26,  batch step: 96, loss: 29.162534713745117\n",
      "epoch: 26,  batch step: 97, loss: 197.7086181640625\n",
      "epoch: 26,  batch step: 98, loss: 5.977132797241211\n",
      "epoch: 26,  batch step: 99, loss: 86.81929779052734\n",
      "epoch: 26,  batch step: 100, loss: 47.99816131591797\n",
      "epoch: 26,  batch step: 101, loss: 35.85333251953125\n",
      "epoch: 26,  batch step: 102, loss: 9.942156791687012\n",
      "epoch: 26,  batch step: 103, loss: 67.5433120727539\n",
      "epoch: 26,  batch step: 104, loss: 21.080760955810547\n",
      "epoch: 26,  batch step: 105, loss: 11.333036422729492\n",
      "epoch: 26,  batch step: 106, loss: 13.786930084228516\n",
      "epoch: 26,  batch step: 107, loss: 271.01116943359375\n",
      "epoch: 26,  batch step: 108, loss: 90.44808197021484\n",
      "epoch: 26,  batch step: 109, loss: 38.244239807128906\n",
      "epoch: 26,  batch step: 110, loss: 11.771166801452637\n",
      "epoch: 26,  batch step: 111, loss: 9.662040710449219\n",
      "epoch: 26,  batch step: 112, loss: 36.687992095947266\n",
      "epoch: 26,  batch step: 113, loss: 48.01589584350586\n",
      "epoch: 26,  batch step: 114, loss: 23.279769897460938\n",
      "epoch: 26,  batch step: 115, loss: 149.7838592529297\n",
      "epoch: 26,  batch step: 116, loss: 239.4105224609375\n",
      "epoch: 26,  batch step: 117, loss: 83.20809173583984\n",
      "epoch: 26,  batch step: 118, loss: 53.097591400146484\n",
      "epoch: 26,  batch step: 119, loss: 126.65289306640625\n",
      "epoch: 26,  batch step: 120, loss: 12.576250076293945\n",
      "epoch: 26,  batch step: 121, loss: 5.884987831115723\n",
      "epoch: 26,  batch step: 122, loss: 7.508176326751709\n",
      "epoch: 26,  batch step: 123, loss: 21.457042694091797\n",
      "epoch: 26,  batch step: 124, loss: 50.90806198120117\n",
      "epoch: 26,  batch step: 125, loss: 117.66948699951172\n",
      "epoch: 26,  batch step: 126, loss: 35.654335021972656\n",
      "epoch: 26,  batch step: 127, loss: 54.420921325683594\n",
      "epoch: 26,  batch step: 128, loss: 9.78860092163086\n",
      "epoch: 26,  batch step: 129, loss: 52.78650665283203\n",
      "epoch: 26,  batch step: 130, loss: 24.08438491821289\n",
      "epoch: 26,  batch step: 131, loss: 49.87485885620117\n",
      "epoch: 26,  batch step: 132, loss: 113.54744720458984\n",
      "epoch: 26,  batch step: 133, loss: 26.7410888671875\n",
      "epoch: 26,  batch step: 134, loss: 94.230224609375\n",
      "epoch: 26,  batch step: 135, loss: 49.99418258666992\n",
      "epoch: 26,  batch step: 136, loss: 27.15462875366211\n",
      "epoch: 26,  batch step: 137, loss: 125.23435974121094\n",
      "epoch: 26,  batch step: 138, loss: 122.02230072021484\n",
      "epoch: 26,  batch step: 139, loss: 24.20094108581543\n",
      "epoch: 26,  batch step: 140, loss: 51.481529235839844\n",
      "epoch: 26,  batch step: 141, loss: 34.00917434692383\n",
      "epoch: 26,  batch step: 142, loss: 96.6416015625\n",
      "epoch: 26,  batch step: 143, loss: 66.83193969726562\n",
      "epoch: 26,  batch step: 144, loss: 20.46190071105957\n",
      "epoch: 26,  batch step: 145, loss: 12.254737854003906\n",
      "epoch: 26,  batch step: 146, loss: 14.095385551452637\n",
      "epoch: 26,  batch step: 147, loss: 38.455169677734375\n",
      "epoch: 26,  batch step: 148, loss: 7.442361354827881\n",
      "epoch: 26,  batch step: 149, loss: 12.289283752441406\n",
      "epoch: 26,  batch step: 150, loss: 12.567445755004883\n",
      "epoch: 26,  batch step: 151, loss: 62.065155029296875\n",
      "epoch: 26,  batch step: 152, loss: 14.242170333862305\n",
      "epoch: 26,  batch step: 153, loss: 5.533530235290527\n",
      "epoch: 26,  batch step: 154, loss: 8.572409629821777\n",
      "epoch: 26,  batch step: 155, loss: 10.53274154663086\n",
      "epoch: 26,  batch step: 156, loss: 13.411544799804688\n",
      "epoch: 26,  batch step: 157, loss: 25.225250244140625\n",
      "epoch: 26,  batch step: 158, loss: 7.454410552978516\n",
      "epoch: 26,  batch step: 159, loss: 38.11762619018555\n",
      "epoch: 26,  batch step: 160, loss: 96.84959411621094\n",
      "epoch: 26,  batch step: 161, loss: 6.637204647064209\n",
      "epoch: 26,  batch step: 162, loss: 76.3836898803711\n",
      "epoch: 26,  batch step: 163, loss: 5.126843452453613\n",
      "epoch: 26,  batch step: 164, loss: 90.43885803222656\n",
      "epoch: 26,  batch step: 165, loss: 10.230718612670898\n",
      "epoch: 26,  batch step: 166, loss: 18.31399917602539\n",
      "epoch: 26,  batch step: 167, loss: 16.947113037109375\n",
      "epoch: 26,  batch step: 168, loss: 6.089768409729004\n",
      "epoch: 26,  batch step: 169, loss: 12.746707916259766\n",
      "epoch: 26,  batch step: 170, loss: 97.44525146484375\n",
      "epoch: 26,  batch step: 171, loss: 44.66447448730469\n",
      "epoch: 26,  batch step: 172, loss: 10.25002384185791\n",
      "epoch: 26,  batch step: 173, loss: 15.913883209228516\n",
      "epoch: 26,  batch step: 174, loss: 10.642868041992188\n",
      "epoch: 26,  batch step: 175, loss: 60.95685958862305\n",
      "epoch: 26,  batch step: 176, loss: 8.499015808105469\n",
      "epoch: 26,  batch step: 177, loss: 7.090563774108887\n",
      "epoch: 26,  batch step: 178, loss: 25.62569236755371\n",
      "epoch: 26,  batch step: 179, loss: 172.83267211914062\n",
      "epoch: 26,  batch step: 180, loss: 8.505813598632812\n",
      "epoch: 26,  batch step: 181, loss: 21.21311378479004\n",
      "epoch: 26,  batch step: 182, loss: 13.338118553161621\n",
      "epoch: 26,  batch step: 183, loss: 11.956847190856934\n",
      "epoch: 26,  batch step: 184, loss: 70.71890258789062\n",
      "epoch: 26,  batch step: 185, loss: 207.62779235839844\n",
      "epoch: 26,  batch step: 186, loss: 5.789009094238281\n",
      "epoch: 26,  batch step: 187, loss: 60.43534469604492\n",
      "epoch: 26,  batch step: 188, loss: 99.07210540771484\n",
      "epoch: 26,  batch step: 189, loss: 114.80863952636719\n",
      "epoch: 26,  batch step: 190, loss: 149.2515869140625\n",
      "epoch: 26,  batch step: 191, loss: 9.313114166259766\n",
      "epoch: 26,  batch step: 192, loss: 63.97037887573242\n",
      "epoch: 26,  batch step: 193, loss: 69.6077651977539\n",
      "epoch: 26,  batch step: 194, loss: 40.72270202636719\n",
      "epoch: 26,  batch step: 195, loss: 14.552813529968262\n",
      "epoch: 26,  batch step: 196, loss: 53.727203369140625\n",
      "epoch: 26,  batch step: 197, loss: 7.697169780731201\n",
      "epoch: 26,  batch step: 198, loss: 7.752274990081787\n",
      "epoch: 26,  batch step: 199, loss: 71.55181121826172\n",
      "epoch: 26,  batch step: 200, loss: 28.508487701416016\n",
      "epoch: 26,  batch step: 201, loss: 82.66860961914062\n",
      "epoch: 26,  batch step: 202, loss: 14.257349967956543\n",
      "epoch: 26,  batch step: 203, loss: 13.988398551940918\n",
      "epoch: 26,  batch step: 204, loss: 84.95086669921875\n",
      "epoch: 26,  batch step: 205, loss: 43.463401794433594\n",
      "epoch: 26,  batch step: 206, loss: 8.740018844604492\n",
      "epoch: 26,  batch step: 207, loss: 22.58731460571289\n",
      "epoch: 26,  batch step: 208, loss: 7.310075283050537\n",
      "epoch: 26,  batch step: 209, loss: 20.651384353637695\n",
      "epoch: 26,  batch step: 210, loss: 8.747607231140137\n",
      "epoch: 26,  batch step: 211, loss: 9.988532066345215\n",
      "epoch: 26,  batch step: 212, loss: 55.56978988647461\n",
      "epoch: 26,  batch step: 213, loss: 34.938148498535156\n",
      "epoch: 26,  batch step: 214, loss: 78.56655883789062\n",
      "epoch: 26,  batch step: 215, loss: 10.49946117401123\n",
      "epoch: 26,  batch step: 216, loss: 33.8120002746582\n",
      "epoch: 26,  batch step: 217, loss: 102.20587158203125\n",
      "epoch: 26,  batch step: 218, loss: 101.91219329833984\n",
      "epoch: 26,  batch step: 219, loss: 12.323457717895508\n",
      "epoch: 26,  batch step: 220, loss: 22.371122360229492\n",
      "epoch: 26,  batch step: 221, loss: 14.42072582244873\n",
      "epoch: 26,  batch step: 222, loss: 22.74850845336914\n",
      "epoch: 26,  batch step: 223, loss: 6.2256975173950195\n",
      "epoch: 26,  batch step: 224, loss: 6.3926005363464355\n",
      "epoch: 26,  batch step: 225, loss: 51.4454345703125\n",
      "epoch: 26,  batch step: 226, loss: 4.535241603851318\n",
      "epoch: 26,  batch step: 227, loss: 5.8417887687683105\n",
      "epoch: 26,  batch step: 228, loss: 9.793977737426758\n",
      "epoch: 26,  batch step: 229, loss: 10.133342742919922\n",
      "epoch: 26,  batch step: 230, loss: 10.190122604370117\n",
      "epoch: 26,  batch step: 231, loss: 80.10731506347656\n",
      "epoch: 26,  batch step: 232, loss: 6.188291549682617\n",
      "epoch: 26,  batch step: 233, loss: 48.4726676940918\n",
      "epoch: 26,  batch step: 234, loss: 21.44440269470215\n",
      "epoch: 26,  batch step: 235, loss: 78.0052261352539\n",
      "epoch: 26,  batch step: 236, loss: 7.650551795959473\n",
      "epoch: 26,  batch step: 237, loss: 10.210333824157715\n",
      "epoch: 26,  batch step: 238, loss: 5.262998104095459\n",
      "epoch: 26,  batch step: 239, loss: 47.21722412109375\n",
      "epoch: 26,  batch step: 240, loss: 17.80896759033203\n",
      "epoch: 26,  batch step: 241, loss: 9.884214401245117\n",
      "epoch: 26,  batch step: 242, loss: 15.584785461425781\n",
      "epoch: 26,  batch step: 243, loss: 102.90721893310547\n",
      "epoch: 26,  batch step: 244, loss: 8.391775131225586\n",
      "epoch: 26,  batch step: 245, loss: 8.954188346862793\n",
      "epoch: 26,  batch step: 246, loss: 116.63753509521484\n",
      "epoch: 26,  batch step: 247, loss: 51.31303024291992\n",
      "epoch: 26,  batch step: 248, loss: 128.18658447265625\n",
      "epoch: 26,  batch step: 249, loss: 14.693873405456543\n",
      "epoch: 26,  batch step: 250, loss: 7.807744979858398\n",
      "epoch: 26,  batch step: 251, loss: 101.09209442138672\n",
      "validation error epoch  26:    tensor(73.9665, device='cuda:0')\n",
      "316\n",
      "epoch: 27,  batch step: 0, loss: 63.59075927734375\n",
      "epoch: 27,  batch step: 1, loss: 75.5621109008789\n",
      "epoch: 27,  batch step: 2, loss: 4.80574893951416\n",
      "epoch: 27,  batch step: 3, loss: 14.185088157653809\n",
      "epoch: 27,  batch step: 4, loss: 6.384604454040527\n",
      "epoch: 27,  batch step: 5, loss: 9.023693084716797\n",
      "epoch: 27,  batch step: 6, loss: 11.239866256713867\n",
      "epoch: 27,  batch step: 7, loss: 72.84709930419922\n",
      "epoch: 27,  batch step: 8, loss: 13.796586990356445\n",
      "epoch: 27,  batch step: 9, loss: 62.033843994140625\n",
      "epoch: 27,  batch step: 10, loss: 6.779425621032715\n",
      "epoch: 27,  batch step: 11, loss: 7.904916286468506\n",
      "epoch: 27,  batch step: 12, loss: 7.982611656188965\n",
      "epoch: 27,  batch step: 13, loss: 43.08254623413086\n",
      "epoch: 27,  batch step: 14, loss: 96.64867401123047\n",
      "epoch: 27,  batch step: 15, loss: 77.49815368652344\n",
      "epoch: 27,  batch step: 16, loss: 7.363254547119141\n",
      "epoch: 27,  batch step: 17, loss: 66.4396743774414\n",
      "epoch: 27,  batch step: 18, loss: 6.573108673095703\n",
      "epoch: 27,  batch step: 19, loss: 7.261300563812256\n",
      "epoch: 27,  batch step: 20, loss: 203.8897705078125\n",
      "epoch: 27,  batch step: 21, loss: 16.950092315673828\n",
      "epoch: 27,  batch step: 22, loss: 33.42085266113281\n",
      "epoch: 27,  batch step: 23, loss: 103.76873016357422\n",
      "epoch: 27,  batch step: 24, loss: 13.728572845458984\n",
      "epoch: 27,  batch step: 25, loss: 87.08055114746094\n",
      "epoch: 27,  batch step: 26, loss: 65.5194320678711\n",
      "epoch: 27,  batch step: 27, loss: 8.650813102722168\n",
      "epoch: 27,  batch step: 28, loss: 9.210271835327148\n",
      "epoch: 27,  batch step: 29, loss: 58.37986373901367\n",
      "epoch: 27,  batch step: 30, loss: 23.794401168823242\n",
      "epoch: 27,  batch step: 31, loss: 13.77945327758789\n",
      "epoch: 27,  batch step: 32, loss: 50.83587646484375\n",
      "epoch: 27,  batch step: 33, loss: 9.423352241516113\n",
      "epoch: 27,  batch step: 34, loss: 9.290267944335938\n",
      "epoch: 27,  batch step: 35, loss: 76.82828521728516\n",
      "epoch: 27,  batch step: 36, loss: 9.627727508544922\n",
      "epoch: 27,  batch step: 37, loss: 17.97124481201172\n",
      "epoch: 27,  batch step: 38, loss: 94.67805480957031\n",
      "epoch: 27,  batch step: 39, loss: 29.6250057220459\n",
      "epoch: 27,  batch step: 40, loss: 19.599750518798828\n",
      "epoch: 27,  batch step: 41, loss: 5.366466522216797\n",
      "epoch: 27,  batch step: 42, loss: 34.45774459838867\n",
      "epoch: 27,  batch step: 43, loss: 59.48072814941406\n",
      "epoch: 27,  batch step: 44, loss: 17.050518035888672\n",
      "epoch: 27,  batch step: 45, loss: 60.097164154052734\n",
      "epoch: 27,  batch step: 46, loss: 15.979009628295898\n",
      "epoch: 27,  batch step: 47, loss: 36.37752914428711\n",
      "epoch: 27,  batch step: 48, loss: 5.961465358734131\n",
      "epoch: 27,  batch step: 49, loss: 6.641063690185547\n",
      "epoch: 27,  batch step: 50, loss: 5.570003509521484\n",
      "epoch: 27,  batch step: 51, loss: 114.25886535644531\n",
      "epoch: 27,  batch step: 52, loss: 40.251033782958984\n",
      "epoch: 27,  batch step: 53, loss: 32.987117767333984\n",
      "epoch: 27,  batch step: 54, loss: 54.063270568847656\n",
      "epoch: 27,  batch step: 55, loss: 19.724884033203125\n",
      "epoch: 27,  batch step: 56, loss: 75.20896911621094\n",
      "epoch: 27,  batch step: 57, loss: 5.195220947265625\n",
      "epoch: 27,  batch step: 58, loss: 42.33201599121094\n",
      "epoch: 27,  batch step: 59, loss: 116.87303924560547\n",
      "epoch: 27,  batch step: 60, loss: 60.2971305847168\n",
      "epoch: 27,  batch step: 61, loss: 10.337753295898438\n",
      "epoch: 27,  batch step: 62, loss: 123.27168273925781\n",
      "epoch: 27,  batch step: 63, loss: 5.963937282562256\n",
      "epoch: 27,  batch step: 64, loss: 18.29605484008789\n",
      "epoch: 27,  batch step: 65, loss: 20.565746307373047\n",
      "epoch: 27,  batch step: 66, loss: 26.71973991394043\n",
      "epoch: 27,  batch step: 67, loss: 44.95904541015625\n",
      "epoch: 27,  batch step: 68, loss: 56.53940963745117\n",
      "epoch: 27,  batch step: 69, loss: 22.43141746520996\n",
      "epoch: 27,  batch step: 70, loss: 47.88427734375\n",
      "epoch: 27,  batch step: 71, loss: 14.876930236816406\n",
      "epoch: 27,  batch step: 72, loss: 14.022689819335938\n",
      "epoch: 27,  batch step: 73, loss: 53.73850631713867\n",
      "epoch: 27,  batch step: 74, loss: 103.62168884277344\n",
      "epoch: 27,  batch step: 75, loss: 57.40027618408203\n",
      "epoch: 27,  batch step: 76, loss: 35.28791046142578\n",
      "epoch: 27,  batch step: 77, loss: 180.6819305419922\n",
      "epoch: 27,  batch step: 78, loss: 39.59021759033203\n",
      "epoch: 27,  batch step: 79, loss: 16.319700241088867\n",
      "epoch: 27,  batch step: 80, loss: 59.040802001953125\n",
      "epoch: 27,  batch step: 81, loss: 53.79412841796875\n",
      "epoch: 27,  batch step: 82, loss: 72.27845764160156\n",
      "epoch: 27,  batch step: 83, loss: 7.20343542098999\n",
      "epoch: 27,  batch step: 84, loss: 9.028549194335938\n",
      "epoch: 27,  batch step: 85, loss: 17.10641098022461\n",
      "epoch: 27,  batch step: 86, loss: 66.20077514648438\n",
      "epoch: 27,  batch step: 87, loss: 190.242919921875\n",
      "epoch: 27,  batch step: 88, loss: 4.4200358390808105\n",
      "epoch: 27,  batch step: 89, loss: 43.5838623046875\n",
      "epoch: 27,  batch step: 90, loss: 97.7866439819336\n",
      "epoch: 27,  batch step: 91, loss: 5.824692726135254\n",
      "epoch: 27,  batch step: 92, loss: 28.57027244567871\n",
      "epoch: 27,  batch step: 93, loss: 8.542771339416504\n",
      "epoch: 27,  batch step: 94, loss: 9.038841247558594\n",
      "epoch: 27,  batch step: 95, loss: 10.041601181030273\n",
      "epoch: 27,  batch step: 96, loss: 12.955409049987793\n",
      "epoch: 27,  batch step: 97, loss: 69.01455688476562\n",
      "epoch: 27,  batch step: 98, loss: 42.41339111328125\n",
      "epoch: 27,  batch step: 99, loss: 16.057140350341797\n",
      "epoch: 27,  batch step: 100, loss: 10.745004653930664\n",
      "epoch: 27,  batch step: 101, loss: 48.9825439453125\n",
      "epoch: 27,  batch step: 102, loss: 13.315204620361328\n",
      "epoch: 27,  batch step: 103, loss: 78.02703857421875\n",
      "epoch: 27,  batch step: 104, loss: 18.39859962463379\n",
      "epoch: 27,  batch step: 105, loss: 87.05724334716797\n",
      "epoch: 27,  batch step: 106, loss: 83.1057357788086\n",
      "epoch: 27,  batch step: 107, loss: 13.821027755737305\n",
      "epoch: 27,  batch step: 108, loss: 7.111223220825195\n",
      "epoch: 27,  batch step: 109, loss: 11.875533103942871\n",
      "epoch: 27,  batch step: 110, loss: 12.183343887329102\n",
      "epoch: 27,  batch step: 111, loss: 58.82207489013672\n",
      "epoch: 27,  batch step: 112, loss: 61.20861053466797\n",
      "epoch: 27,  batch step: 113, loss: 42.03711700439453\n",
      "epoch: 27,  batch step: 114, loss: 12.719225883483887\n",
      "epoch: 27,  batch step: 115, loss: 8.436344146728516\n",
      "epoch: 27,  batch step: 116, loss: 25.63471794128418\n",
      "epoch: 27,  batch step: 117, loss: 91.52123260498047\n",
      "epoch: 27,  batch step: 118, loss: 17.530946731567383\n",
      "epoch: 27,  batch step: 119, loss: 90.99256896972656\n",
      "epoch: 27,  batch step: 120, loss: 8.118940353393555\n",
      "epoch: 27,  batch step: 121, loss: 34.383567810058594\n",
      "epoch: 27,  batch step: 122, loss: 26.353965759277344\n",
      "epoch: 27,  batch step: 123, loss: 6.095203399658203\n",
      "epoch: 27,  batch step: 124, loss: 5.950600624084473\n",
      "epoch: 27,  batch step: 125, loss: 26.966299057006836\n",
      "epoch: 27,  batch step: 126, loss: 86.02422332763672\n",
      "epoch: 27,  batch step: 127, loss: 6.719399929046631\n",
      "epoch: 27,  batch step: 128, loss: 61.761009216308594\n",
      "epoch: 27,  batch step: 129, loss: 70.75401306152344\n",
      "epoch: 27,  batch step: 130, loss: 21.12056541442871\n",
      "epoch: 27,  batch step: 131, loss: 11.456600189208984\n",
      "epoch: 27,  batch step: 132, loss: 73.67239379882812\n",
      "epoch: 27,  batch step: 133, loss: 12.001433372497559\n",
      "epoch: 27,  batch step: 134, loss: 9.412131309509277\n",
      "epoch: 27,  batch step: 135, loss: 66.9741439819336\n",
      "epoch: 27,  batch step: 136, loss: 69.8857192993164\n",
      "epoch: 27,  batch step: 137, loss: 10.599052429199219\n",
      "epoch: 27,  batch step: 138, loss: 10.401182174682617\n",
      "epoch: 27,  batch step: 139, loss: 14.050949096679688\n",
      "epoch: 27,  batch step: 140, loss: 142.67257690429688\n",
      "epoch: 27,  batch step: 141, loss: 24.453031539916992\n",
      "epoch: 27,  batch step: 142, loss: 10.278438568115234\n",
      "epoch: 27,  batch step: 143, loss: 50.155792236328125\n",
      "epoch: 27,  batch step: 144, loss: 10.864789962768555\n",
      "epoch: 27,  batch step: 145, loss: 14.77947998046875\n",
      "epoch: 27,  batch step: 146, loss: 5.221242904663086\n",
      "epoch: 27,  batch step: 147, loss: 9.956801414489746\n",
      "epoch: 27,  batch step: 148, loss: 4.160670757293701\n",
      "epoch: 27,  batch step: 149, loss: 11.281476974487305\n",
      "epoch: 27,  batch step: 150, loss: 26.325042724609375\n",
      "epoch: 27,  batch step: 151, loss: 9.492836952209473\n",
      "epoch: 27,  batch step: 152, loss: 39.34574890136719\n",
      "epoch: 27,  batch step: 153, loss: 42.9732780456543\n",
      "epoch: 27,  batch step: 154, loss: 12.147378921508789\n",
      "epoch: 27,  batch step: 155, loss: 10.215444564819336\n",
      "epoch: 27,  batch step: 156, loss: 11.418432235717773\n",
      "epoch: 27,  batch step: 157, loss: 97.55186462402344\n",
      "epoch: 27,  batch step: 158, loss: 44.034637451171875\n",
      "epoch: 27,  batch step: 159, loss: 68.71969604492188\n",
      "epoch: 27,  batch step: 160, loss: 24.9376220703125\n",
      "epoch: 27,  batch step: 161, loss: 5.727786064147949\n",
      "epoch: 27,  batch step: 162, loss: 6.762475490570068\n",
      "epoch: 27,  batch step: 163, loss: 69.54574584960938\n",
      "epoch: 27,  batch step: 164, loss: 34.25675582885742\n",
      "epoch: 27,  batch step: 165, loss: 11.653158187866211\n",
      "epoch: 27,  batch step: 166, loss: 22.45490074157715\n",
      "epoch: 27,  batch step: 167, loss: 22.326658248901367\n",
      "epoch: 27,  batch step: 168, loss: 97.866943359375\n",
      "epoch: 27,  batch step: 169, loss: 60.69944381713867\n",
      "epoch: 27,  batch step: 170, loss: 7.533488750457764\n",
      "epoch: 27,  batch step: 171, loss: 137.87503051757812\n",
      "epoch: 27,  batch step: 172, loss: 56.03337097167969\n",
      "epoch: 27,  batch step: 173, loss: 6.559673309326172\n",
      "epoch: 27,  batch step: 174, loss: 76.05623626708984\n",
      "epoch: 27,  batch step: 175, loss: 30.81085205078125\n",
      "epoch: 27,  batch step: 176, loss: 19.74631690979004\n",
      "epoch: 27,  batch step: 177, loss: 7.033271789550781\n",
      "epoch: 27,  batch step: 178, loss: 31.70877456665039\n",
      "epoch: 27,  batch step: 179, loss: 47.695106506347656\n",
      "epoch: 27,  batch step: 180, loss: 15.552091598510742\n",
      "epoch: 27,  batch step: 181, loss: 90.3560791015625\n",
      "epoch: 27,  batch step: 182, loss: 4.928515911102295\n",
      "epoch: 27,  batch step: 183, loss: 5.588765621185303\n",
      "epoch: 27,  batch step: 184, loss: 19.01116180419922\n",
      "epoch: 27,  batch step: 185, loss: 8.030116081237793\n",
      "epoch: 27,  batch step: 186, loss: 107.790771484375\n",
      "epoch: 27,  batch step: 187, loss: 7.9546003341674805\n",
      "epoch: 27,  batch step: 188, loss: 7.4592108726501465\n",
      "epoch: 27,  batch step: 189, loss: 177.28318786621094\n",
      "epoch: 27,  batch step: 190, loss: 7.024970531463623\n",
      "epoch: 27,  batch step: 191, loss: 16.268964767456055\n",
      "epoch: 27,  batch step: 192, loss: 53.346534729003906\n",
      "epoch: 27,  batch step: 193, loss: 25.45724105834961\n",
      "epoch: 27,  batch step: 194, loss: 40.74018478393555\n",
      "epoch: 27,  batch step: 195, loss: 194.84130859375\n",
      "epoch: 27,  batch step: 196, loss: 126.62321472167969\n",
      "epoch: 27,  batch step: 197, loss: 11.21926498413086\n",
      "epoch: 27,  batch step: 198, loss: 17.04952621459961\n",
      "epoch: 27,  batch step: 199, loss: 40.20745849609375\n",
      "epoch: 27,  batch step: 200, loss: 7.935103416442871\n",
      "epoch: 27,  batch step: 201, loss: 126.90036010742188\n",
      "epoch: 27,  batch step: 202, loss: 94.9327392578125\n",
      "epoch: 27,  batch step: 203, loss: 31.42742919921875\n",
      "epoch: 27,  batch step: 204, loss: 8.515408515930176\n",
      "epoch: 27,  batch step: 205, loss: 23.613372802734375\n",
      "epoch: 27,  batch step: 206, loss: 17.35154914855957\n",
      "epoch: 27,  batch step: 207, loss: 27.953842163085938\n",
      "epoch: 27,  batch step: 208, loss: 50.61886215209961\n",
      "epoch: 27,  batch step: 209, loss: 20.59124755859375\n",
      "epoch: 27,  batch step: 210, loss: 57.71796417236328\n",
      "epoch: 27,  batch step: 211, loss: 5.4504828453063965\n",
      "epoch: 27,  batch step: 212, loss: 113.42413330078125\n",
      "epoch: 27,  batch step: 213, loss: 101.08257293701172\n",
      "epoch: 27,  batch step: 214, loss: 14.615943908691406\n",
      "epoch: 27,  batch step: 215, loss: 20.832433700561523\n",
      "epoch: 27,  batch step: 216, loss: 73.73037719726562\n",
      "epoch: 27,  batch step: 217, loss: 20.390474319458008\n",
      "epoch: 27,  batch step: 218, loss: 127.83216857910156\n",
      "epoch: 27,  batch step: 219, loss: 11.814562797546387\n",
      "epoch: 27,  batch step: 220, loss: 57.94511032104492\n",
      "epoch: 27,  batch step: 221, loss: 16.609207153320312\n",
      "epoch: 27,  batch step: 222, loss: 37.31231689453125\n",
      "epoch: 27,  batch step: 223, loss: 225.9751434326172\n",
      "epoch: 27,  batch step: 224, loss: 15.217464447021484\n",
      "epoch: 27,  batch step: 225, loss: 7.209752082824707\n",
      "epoch: 27,  batch step: 226, loss: 3.622025966644287\n",
      "epoch: 27,  batch step: 227, loss: 11.66267204284668\n",
      "epoch: 27,  batch step: 228, loss: 6.322028160095215\n",
      "epoch: 27,  batch step: 229, loss: 29.687034606933594\n",
      "epoch: 27,  batch step: 230, loss: 59.17467498779297\n",
      "epoch: 27,  batch step: 231, loss: 8.488418579101562\n",
      "epoch: 27,  batch step: 232, loss: 33.70381164550781\n",
      "epoch: 27,  batch step: 233, loss: 9.004974365234375\n",
      "epoch: 27,  batch step: 234, loss: 30.201704025268555\n",
      "epoch: 27,  batch step: 235, loss: 78.65240478515625\n",
      "epoch: 27,  batch step: 236, loss: 88.33711242675781\n",
      "epoch: 27,  batch step: 237, loss: 20.419912338256836\n",
      "epoch: 27,  batch step: 238, loss: 22.96369171142578\n",
      "epoch: 27,  batch step: 239, loss: 79.81871032714844\n",
      "epoch: 27,  batch step: 240, loss: 92.07897186279297\n",
      "epoch: 27,  batch step: 241, loss: 5.399547100067139\n",
      "epoch: 27,  batch step: 242, loss: 14.626521110534668\n",
      "epoch: 27,  batch step: 243, loss: 32.0885009765625\n",
      "epoch: 27,  batch step: 244, loss: 29.114089965820312\n",
      "epoch: 27,  batch step: 245, loss: 50.643829345703125\n",
      "epoch: 27,  batch step: 246, loss: 7.061251640319824\n",
      "epoch: 27,  batch step: 247, loss: 7.130223751068115\n",
      "epoch: 27,  batch step: 248, loss: 11.317545890808105\n",
      "epoch: 27,  batch step: 249, loss: 44.593441009521484\n",
      "epoch: 27,  batch step: 250, loss: 194.54238891601562\n",
      "epoch: 27,  batch step: 251, loss: 519.8963012695312\n",
      "validation error epoch  27:    tensor(70.2416, device='cuda:0')\n",
      "316\n",
      "epoch: 28,  batch step: 0, loss: 10.050505638122559\n",
      "epoch: 28,  batch step: 1, loss: 13.64896297454834\n",
      "epoch: 28,  batch step: 2, loss: 9.770927429199219\n",
      "epoch: 28,  batch step: 3, loss: 13.726524353027344\n",
      "epoch: 28,  batch step: 4, loss: 102.37202453613281\n",
      "epoch: 28,  batch step: 5, loss: 55.788719177246094\n",
      "epoch: 28,  batch step: 6, loss: 36.83354187011719\n",
      "epoch: 28,  batch step: 7, loss: 34.907814025878906\n",
      "epoch: 28,  batch step: 8, loss: 11.387796401977539\n",
      "epoch: 28,  batch step: 9, loss: 4.293503284454346\n",
      "epoch: 28,  batch step: 10, loss: 13.293356895446777\n",
      "epoch: 28,  batch step: 11, loss: 38.208282470703125\n",
      "epoch: 28,  batch step: 12, loss: 32.28923034667969\n",
      "epoch: 28,  batch step: 13, loss: 31.561203002929688\n",
      "epoch: 28,  batch step: 14, loss: 73.52123260498047\n",
      "epoch: 28,  batch step: 15, loss: 23.51058006286621\n",
      "epoch: 28,  batch step: 16, loss: 63.17608642578125\n",
      "epoch: 28,  batch step: 17, loss: 16.744728088378906\n",
      "epoch: 28,  batch step: 18, loss: 16.203853607177734\n",
      "epoch: 28,  batch step: 19, loss: 5.15054988861084\n",
      "epoch: 28,  batch step: 20, loss: 86.13311767578125\n",
      "epoch: 28,  batch step: 21, loss: 7.265646934509277\n",
      "epoch: 28,  batch step: 22, loss: 22.398229598999023\n",
      "epoch: 28,  batch step: 23, loss: 17.191320419311523\n",
      "epoch: 28,  batch step: 24, loss: 15.933612823486328\n",
      "epoch: 28,  batch step: 25, loss: 59.657798767089844\n",
      "epoch: 28,  batch step: 26, loss: 16.70188331604004\n",
      "epoch: 28,  batch step: 27, loss: 7.617948055267334\n",
      "epoch: 28,  batch step: 28, loss: 12.640373229980469\n",
      "epoch: 28,  batch step: 29, loss: 8.964179039001465\n",
      "epoch: 28,  batch step: 30, loss: 6.979900360107422\n",
      "epoch: 28,  batch step: 31, loss: 13.631603240966797\n",
      "epoch: 28,  batch step: 32, loss: 11.899462699890137\n",
      "epoch: 28,  batch step: 33, loss: 47.84792709350586\n",
      "epoch: 28,  batch step: 34, loss: 7.810935974121094\n",
      "epoch: 28,  batch step: 35, loss: 5.699302673339844\n",
      "epoch: 28,  batch step: 36, loss: 6.844071388244629\n",
      "epoch: 28,  batch step: 37, loss: 5.544702053070068\n",
      "epoch: 28,  batch step: 38, loss: 4.651457786560059\n",
      "epoch: 28,  batch step: 39, loss: 27.81049346923828\n",
      "epoch: 28,  batch step: 40, loss: 4.408526420593262\n",
      "epoch: 28,  batch step: 41, loss: 11.18976879119873\n",
      "epoch: 28,  batch step: 42, loss: 139.66976928710938\n",
      "epoch: 28,  batch step: 43, loss: 6.674552917480469\n",
      "epoch: 28,  batch step: 44, loss: 13.908267974853516\n",
      "epoch: 28,  batch step: 45, loss: 16.589107513427734\n",
      "epoch: 28,  batch step: 46, loss: 78.99894714355469\n",
      "epoch: 28,  batch step: 47, loss: 12.669729232788086\n",
      "epoch: 28,  batch step: 48, loss: 109.37228393554688\n",
      "epoch: 28,  batch step: 49, loss: 75.75821685791016\n",
      "epoch: 28,  batch step: 50, loss: 7.300647735595703\n",
      "epoch: 28,  batch step: 51, loss: 4.530551433563232\n",
      "epoch: 28,  batch step: 52, loss: 11.787839889526367\n",
      "epoch: 28,  batch step: 53, loss: 44.45307922363281\n",
      "epoch: 28,  batch step: 54, loss: 5.638411998748779\n",
      "epoch: 28,  batch step: 55, loss: 5.542001247406006\n",
      "epoch: 28,  batch step: 56, loss: 13.336386680603027\n",
      "epoch: 28,  batch step: 57, loss: 79.45771789550781\n",
      "epoch: 28,  batch step: 58, loss: 134.78933715820312\n",
      "epoch: 28,  batch step: 59, loss: 8.608373641967773\n",
      "epoch: 28,  batch step: 60, loss: 70.56153106689453\n",
      "epoch: 28,  batch step: 61, loss: 41.52638626098633\n",
      "epoch: 28,  batch step: 62, loss: 57.3364372253418\n",
      "epoch: 28,  batch step: 63, loss: 78.949951171875\n",
      "epoch: 28,  batch step: 64, loss: 46.437461853027344\n",
      "epoch: 28,  batch step: 65, loss: 92.92192840576172\n",
      "epoch: 28,  batch step: 66, loss: 8.638976097106934\n",
      "epoch: 28,  batch step: 67, loss: 83.372802734375\n",
      "epoch: 28,  batch step: 68, loss: 67.84049987792969\n",
      "epoch: 28,  batch step: 69, loss: 109.36892700195312\n",
      "epoch: 28,  batch step: 70, loss: 6.253354549407959\n",
      "epoch: 28,  batch step: 71, loss: 24.91353988647461\n",
      "epoch: 28,  batch step: 72, loss: 96.88653564453125\n",
      "epoch: 28,  batch step: 73, loss: 7.721336364746094\n",
      "epoch: 28,  batch step: 74, loss: 14.084320068359375\n",
      "epoch: 28,  batch step: 75, loss: 10.035295486450195\n",
      "epoch: 28,  batch step: 76, loss: 16.151540756225586\n",
      "epoch: 28,  batch step: 77, loss: 69.01158142089844\n",
      "epoch: 28,  batch step: 78, loss: 11.643221855163574\n",
      "epoch: 28,  batch step: 79, loss: 17.65147590637207\n",
      "epoch: 28,  batch step: 80, loss: 6.166614532470703\n",
      "epoch: 28,  batch step: 81, loss: 64.72447967529297\n",
      "epoch: 28,  batch step: 82, loss: 23.996191024780273\n",
      "epoch: 28,  batch step: 83, loss: 5.130242824554443\n",
      "epoch: 28,  batch step: 84, loss: 88.05674743652344\n",
      "epoch: 28,  batch step: 85, loss: 440.9383239746094\n",
      "epoch: 28,  batch step: 86, loss: 12.652113914489746\n",
      "epoch: 28,  batch step: 87, loss: 60.437679290771484\n",
      "epoch: 28,  batch step: 88, loss: 51.23336410522461\n",
      "epoch: 28,  batch step: 89, loss: 85.48592376708984\n",
      "epoch: 28,  batch step: 90, loss: 67.18541717529297\n",
      "epoch: 28,  batch step: 91, loss: 26.869781494140625\n",
      "epoch: 28,  batch step: 92, loss: 30.747467041015625\n",
      "epoch: 28,  batch step: 93, loss: 98.33409118652344\n",
      "epoch: 28,  batch step: 94, loss: 12.730792045593262\n",
      "epoch: 28,  batch step: 95, loss: 44.78055953979492\n",
      "epoch: 28,  batch step: 96, loss: 27.739253997802734\n",
      "epoch: 28,  batch step: 97, loss: 18.544797897338867\n",
      "epoch: 28,  batch step: 98, loss: 29.418989181518555\n",
      "epoch: 28,  batch step: 99, loss: 37.3315315246582\n",
      "epoch: 28,  batch step: 100, loss: 54.0628662109375\n",
      "epoch: 28,  batch step: 101, loss: 61.00434875488281\n",
      "epoch: 28,  batch step: 102, loss: 9.563007354736328\n",
      "epoch: 28,  batch step: 103, loss: 16.075468063354492\n",
      "epoch: 28,  batch step: 104, loss: 27.72964096069336\n",
      "epoch: 28,  batch step: 105, loss: 142.45672607421875\n",
      "epoch: 28,  batch step: 106, loss: 9.970358848571777\n",
      "epoch: 28,  batch step: 107, loss: 72.381591796875\n",
      "epoch: 28,  batch step: 108, loss: 56.820396423339844\n",
      "epoch: 28,  batch step: 109, loss: 351.07318115234375\n",
      "epoch: 28,  batch step: 110, loss: 12.41989803314209\n",
      "epoch: 28,  batch step: 111, loss: 12.38492202758789\n",
      "epoch: 28,  batch step: 112, loss: 16.46906089782715\n",
      "epoch: 28,  batch step: 113, loss: 56.79777145385742\n",
      "epoch: 28,  batch step: 114, loss: 66.41558074951172\n",
      "epoch: 28,  batch step: 115, loss: 121.06826782226562\n",
      "epoch: 28,  batch step: 116, loss: 28.23226547241211\n",
      "epoch: 28,  batch step: 117, loss: 312.427734375\n",
      "epoch: 28,  batch step: 118, loss: 30.111576080322266\n",
      "epoch: 28,  batch step: 119, loss: 48.496150970458984\n",
      "epoch: 28,  batch step: 120, loss: 25.18927574157715\n",
      "epoch: 28,  batch step: 121, loss: 102.1720199584961\n",
      "epoch: 28,  batch step: 122, loss: 36.7733154296875\n",
      "epoch: 28,  batch step: 123, loss: 106.50784301757812\n",
      "epoch: 28,  batch step: 124, loss: 108.65473937988281\n",
      "epoch: 28,  batch step: 125, loss: 131.71206665039062\n",
      "epoch: 28,  batch step: 126, loss: 38.088592529296875\n",
      "epoch: 28,  batch step: 127, loss: 21.8802490234375\n",
      "epoch: 28,  batch step: 128, loss: 11.764166831970215\n",
      "epoch: 28,  batch step: 129, loss: 83.9591293334961\n",
      "epoch: 28,  batch step: 130, loss: 162.71353149414062\n",
      "epoch: 28,  batch step: 131, loss: 54.245849609375\n",
      "epoch: 28,  batch step: 132, loss: 62.295284271240234\n",
      "epoch: 28,  batch step: 133, loss: 36.45646667480469\n",
      "epoch: 28,  batch step: 134, loss: 27.85072898864746\n",
      "epoch: 28,  batch step: 135, loss: 22.110076904296875\n",
      "epoch: 28,  batch step: 136, loss: 19.025005340576172\n",
      "epoch: 28,  batch step: 137, loss: 109.2783203125\n",
      "epoch: 28,  batch step: 138, loss: 8.419124603271484\n",
      "epoch: 28,  batch step: 139, loss: 52.70966720581055\n",
      "epoch: 28,  batch step: 140, loss: 8.493929862976074\n",
      "epoch: 28,  batch step: 141, loss: 8.952766418457031\n",
      "epoch: 28,  batch step: 142, loss: 11.3329496383667\n",
      "epoch: 28,  batch step: 143, loss: 4.506896495819092\n",
      "epoch: 28,  batch step: 144, loss: 10.024796485900879\n",
      "epoch: 28,  batch step: 145, loss: 13.345108032226562\n",
      "epoch: 28,  batch step: 146, loss: 101.81197357177734\n",
      "epoch: 28,  batch step: 147, loss: 6.271915435791016\n",
      "epoch: 28,  batch step: 148, loss: 71.91397094726562\n",
      "epoch: 28,  batch step: 149, loss: 20.366825103759766\n",
      "epoch: 28,  batch step: 150, loss: 37.13930130004883\n",
      "epoch: 28,  batch step: 151, loss: 134.82742309570312\n",
      "epoch: 28,  batch step: 152, loss: 10.624560356140137\n",
      "epoch: 28,  batch step: 153, loss: 7.710818290710449\n",
      "epoch: 28,  batch step: 154, loss: 4.904248237609863\n",
      "epoch: 28,  batch step: 155, loss: 25.209083557128906\n",
      "epoch: 28,  batch step: 156, loss: 12.248576164245605\n",
      "epoch: 28,  batch step: 157, loss: 10.79295539855957\n",
      "epoch: 28,  batch step: 158, loss: 5.05174446105957\n",
      "epoch: 28,  batch step: 159, loss: 9.690502166748047\n",
      "epoch: 28,  batch step: 160, loss: 8.159585952758789\n",
      "epoch: 28,  batch step: 161, loss: 56.441070556640625\n",
      "epoch: 28,  batch step: 162, loss: 5.267551422119141\n",
      "epoch: 28,  batch step: 163, loss: 6.16605281829834\n",
      "epoch: 28,  batch step: 164, loss: 34.942176818847656\n",
      "epoch: 28,  batch step: 165, loss: 12.572436332702637\n",
      "epoch: 28,  batch step: 166, loss: 62.370025634765625\n",
      "epoch: 28,  batch step: 167, loss: 4.9333109855651855\n",
      "epoch: 28,  batch step: 168, loss: 57.04686737060547\n",
      "epoch: 28,  batch step: 169, loss: 43.88312530517578\n",
      "epoch: 28,  batch step: 170, loss: 47.127777099609375\n",
      "epoch: 28,  batch step: 171, loss: 18.591915130615234\n",
      "epoch: 28,  batch step: 172, loss: 11.386133193969727\n",
      "epoch: 28,  batch step: 173, loss: 212.9117889404297\n",
      "epoch: 28,  batch step: 174, loss: 24.258113861083984\n",
      "epoch: 28,  batch step: 175, loss: 90.38301849365234\n",
      "epoch: 28,  batch step: 176, loss: 123.65927124023438\n",
      "epoch: 28,  batch step: 177, loss: 60.9659538269043\n",
      "epoch: 28,  batch step: 178, loss: 69.13043212890625\n",
      "epoch: 28,  batch step: 179, loss: 72.07325744628906\n",
      "epoch: 28,  batch step: 180, loss: 78.5180435180664\n",
      "epoch: 28,  batch step: 181, loss: 56.04580307006836\n",
      "epoch: 28,  batch step: 182, loss: 70.02212524414062\n",
      "epoch: 28,  batch step: 183, loss: 149.79922485351562\n",
      "epoch: 28,  batch step: 184, loss: 39.69590759277344\n",
      "epoch: 28,  batch step: 185, loss: 22.709829330444336\n",
      "epoch: 28,  batch step: 186, loss: 50.044334411621094\n",
      "epoch: 28,  batch step: 187, loss: 271.8350830078125\n",
      "epoch: 28,  batch step: 188, loss: 49.1931266784668\n",
      "epoch: 28,  batch step: 189, loss: 18.59307861328125\n",
      "epoch: 28,  batch step: 190, loss: 61.18076705932617\n",
      "epoch: 28,  batch step: 191, loss: 37.91935348510742\n",
      "epoch: 28,  batch step: 192, loss: 10.05493450164795\n",
      "epoch: 28,  batch step: 193, loss: 38.09260559082031\n",
      "epoch: 28,  batch step: 194, loss: 23.47430419921875\n",
      "epoch: 28,  batch step: 195, loss: 5.497243404388428\n",
      "epoch: 28,  batch step: 196, loss: 19.43850326538086\n",
      "epoch: 28,  batch step: 197, loss: 11.41846752166748\n",
      "epoch: 28,  batch step: 198, loss: 29.099332809448242\n",
      "epoch: 28,  batch step: 199, loss: 75.3623046875\n",
      "epoch: 28,  batch step: 200, loss: 13.672018051147461\n",
      "epoch: 28,  batch step: 201, loss: 56.09149169921875\n",
      "epoch: 28,  batch step: 202, loss: 13.211222648620605\n",
      "epoch: 28,  batch step: 203, loss: 15.392142295837402\n",
      "epoch: 28,  batch step: 204, loss: 100.37773132324219\n",
      "epoch: 28,  batch step: 205, loss: 35.748653411865234\n",
      "epoch: 28,  batch step: 206, loss: 55.15920639038086\n",
      "epoch: 28,  batch step: 207, loss: 73.27967834472656\n",
      "epoch: 28,  batch step: 208, loss: 20.677703857421875\n",
      "epoch: 28,  batch step: 209, loss: 53.18891525268555\n",
      "epoch: 28,  batch step: 210, loss: 22.953710556030273\n",
      "epoch: 28,  batch step: 211, loss: 12.212774276733398\n",
      "epoch: 28,  batch step: 212, loss: 8.71353530883789\n",
      "epoch: 28,  batch step: 213, loss: 23.812881469726562\n",
      "epoch: 28,  batch step: 214, loss: 56.968040466308594\n",
      "epoch: 28,  batch step: 215, loss: 18.361013412475586\n",
      "epoch: 28,  batch step: 216, loss: 33.614044189453125\n",
      "epoch: 28,  batch step: 217, loss: 34.9860725402832\n",
      "epoch: 28,  batch step: 218, loss: 112.0775375366211\n",
      "epoch: 28,  batch step: 219, loss: 11.60724925994873\n",
      "epoch: 28,  batch step: 220, loss: 6.202740669250488\n",
      "epoch: 28,  batch step: 221, loss: 48.22660827636719\n",
      "epoch: 28,  batch step: 222, loss: 12.15491771697998\n",
      "epoch: 28,  batch step: 223, loss: 34.548641204833984\n",
      "epoch: 28,  batch step: 224, loss: 6.671748161315918\n",
      "epoch: 28,  batch step: 225, loss: 49.211448669433594\n",
      "epoch: 28,  batch step: 226, loss: 8.67564868927002\n",
      "epoch: 28,  batch step: 227, loss: 21.063194274902344\n",
      "epoch: 28,  batch step: 228, loss: 35.64299392700195\n",
      "epoch: 28,  batch step: 229, loss: 54.776580810546875\n",
      "epoch: 28,  batch step: 230, loss: 16.636608123779297\n",
      "epoch: 28,  batch step: 231, loss: 106.26461029052734\n",
      "epoch: 28,  batch step: 232, loss: 70.63311767578125\n",
      "epoch: 28,  batch step: 233, loss: 97.9903564453125\n",
      "epoch: 28,  batch step: 234, loss: 8.260543823242188\n",
      "epoch: 28,  batch step: 235, loss: 11.50687313079834\n",
      "epoch: 28,  batch step: 236, loss: 20.579710006713867\n",
      "epoch: 28,  batch step: 237, loss: 22.163944244384766\n",
      "epoch: 28,  batch step: 238, loss: 87.89009094238281\n",
      "epoch: 28,  batch step: 239, loss: 12.302824974060059\n",
      "epoch: 28,  batch step: 240, loss: 23.844371795654297\n",
      "epoch: 28,  batch step: 241, loss: 74.74323272705078\n",
      "epoch: 28,  batch step: 242, loss: 21.593189239501953\n",
      "epoch: 28,  batch step: 243, loss: 5.235495090484619\n",
      "epoch: 28,  batch step: 244, loss: 12.883353233337402\n",
      "epoch: 28,  batch step: 245, loss: 31.301353454589844\n",
      "epoch: 28,  batch step: 246, loss: 32.59539794921875\n",
      "epoch: 28,  batch step: 247, loss: 24.09224510192871\n",
      "epoch: 28,  batch step: 248, loss: 9.856010437011719\n",
      "epoch: 28,  batch step: 249, loss: 9.034015655517578\n",
      "epoch: 28,  batch step: 250, loss: 16.054170608520508\n",
      "epoch: 28,  batch step: 251, loss: 76.162353515625\n",
      "validation error epoch  28:    tensor(202.6348, device='cuda:0')\n",
      "316\n",
      "epoch: 29,  batch step: 0, loss: 8.770071029663086\n",
      "epoch: 29,  batch step: 1, loss: 143.9185791015625\n",
      "epoch: 29,  batch step: 2, loss: 51.030418395996094\n",
      "epoch: 29,  batch step: 3, loss: 135.7938232421875\n",
      "epoch: 29,  batch step: 4, loss: 17.24821662902832\n",
      "epoch: 29,  batch step: 5, loss: 97.7260971069336\n",
      "epoch: 29,  batch step: 6, loss: 64.90122985839844\n",
      "epoch: 29,  batch step: 7, loss: 82.04534912109375\n",
      "epoch: 29,  batch step: 8, loss: 11.820047378540039\n",
      "epoch: 29,  batch step: 9, loss: 60.95164108276367\n",
      "epoch: 29,  batch step: 10, loss: 9.26125431060791\n",
      "epoch: 29,  batch step: 11, loss: 18.497539520263672\n",
      "epoch: 29,  batch step: 12, loss: 23.851016998291016\n",
      "epoch: 29,  batch step: 13, loss: 5.442138671875\n",
      "epoch: 29,  batch step: 14, loss: 9.090311050415039\n",
      "epoch: 29,  batch step: 15, loss: 25.92135238647461\n",
      "epoch: 29,  batch step: 16, loss: 56.8419189453125\n",
      "epoch: 29,  batch step: 17, loss: 13.171503067016602\n",
      "epoch: 29,  batch step: 18, loss: 19.73322868347168\n",
      "epoch: 29,  batch step: 19, loss: 6.836948394775391\n",
      "epoch: 29,  batch step: 20, loss: 128.1235809326172\n",
      "epoch: 29,  batch step: 21, loss: 14.306602478027344\n",
      "epoch: 29,  batch step: 22, loss: 10.9353666305542\n",
      "epoch: 29,  batch step: 23, loss: 47.600242614746094\n",
      "epoch: 29,  batch step: 24, loss: 6.916007041931152\n",
      "epoch: 29,  batch step: 25, loss: 34.23725891113281\n",
      "epoch: 29,  batch step: 26, loss: 63.14408493041992\n",
      "epoch: 29,  batch step: 27, loss: 5.111541748046875\n",
      "epoch: 29,  batch step: 28, loss: 61.481929779052734\n",
      "epoch: 29,  batch step: 29, loss: 4.567380905151367\n",
      "epoch: 29,  batch step: 30, loss: 27.64499282836914\n",
      "epoch: 29,  batch step: 31, loss: 85.4898910522461\n",
      "epoch: 29,  batch step: 32, loss: 20.26852798461914\n",
      "epoch: 29,  batch step: 33, loss: 172.86163330078125\n",
      "epoch: 29,  batch step: 34, loss: 53.543148040771484\n",
      "epoch: 29,  batch step: 35, loss: 41.37480163574219\n",
      "epoch: 29,  batch step: 36, loss: 11.227645874023438\n",
      "epoch: 29,  batch step: 37, loss: 17.588924407958984\n",
      "epoch: 29,  batch step: 38, loss: 71.6788558959961\n",
      "epoch: 29,  batch step: 39, loss: 61.65032196044922\n",
      "epoch: 29,  batch step: 40, loss: 14.19632625579834\n",
      "epoch: 29,  batch step: 41, loss: 17.84368896484375\n",
      "epoch: 29,  batch step: 42, loss: 4.941849708557129\n",
      "epoch: 29,  batch step: 43, loss: 52.56317901611328\n",
      "epoch: 29,  batch step: 44, loss: 83.65393829345703\n",
      "epoch: 29,  batch step: 45, loss: 15.074256896972656\n",
      "epoch: 29,  batch step: 46, loss: 15.65512752532959\n",
      "epoch: 29,  batch step: 47, loss: 43.02739715576172\n",
      "epoch: 29,  batch step: 48, loss: 17.449844360351562\n",
      "epoch: 29,  batch step: 49, loss: 6.06746244430542\n",
      "epoch: 29,  batch step: 50, loss: 7.579362392425537\n",
      "epoch: 29,  batch step: 51, loss: 38.322879791259766\n",
      "epoch: 29,  batch step: 52, loss: 44.63872528076172\n",
      "epoch: 29,  batch step: 53, loss: 60.51220703125\n",
      "epoch: 29,  batch step: 54, loss: 15.5869140625\n",
      "epoch: 29,  batch step: 55, loss: 62.459529876708984\n",
      "epoch: 29,  batch step: 56, loss: 6.2913103103637695\n",
      "epoch: 29,  batch step: 57, loss: 18.927282333374023\n",
      "epoch: 29,  batch step: 58, loss: 12.290760040283203\n",
      "epoch: 29,  batch step: 59, loss: 10.658706665039062\n",
      "epoch: 29,  batch step: 60, loss: 5.042848587036133\n",
      "epoch: 29,  batch step: 61, loss: 6.87210750579834\n",
      "epoch: 29,  batch step: 62, loss: 64.32020568847656\n",
      "epoch: 29,  batch step: 63, loss: 15.433547019958496\n",
      "epoch: 29,  batch step: 64, loss: 17.401226043701172\n",
      "epoch: 29,  batch step: 65, loss: 23.70458984375\n",
      "epoch: 29,  batch step: 66, loss: 11.418414115905762\n",
      "epoch: 29,  batch step: 67, loss: 60.06431198120117\n",
      "epoch: 29,  batch step: 68, loss: 21.976177215576172\n",
      "epoch: 29,  batch step: 69, loss: 5.457000255584717\n",
      "epoch: 29,  batch step: 70, loss: 19.33248519897461\n",
      "epoch: 29,  batch step: 71, loss: 36.71644592285156\n",
      "epoch: 29,  batch step: 72, loss: 43.70818328857422\n",
      "epoch: 29,  batch step: 73, loss: 63.95288848876953\n",
      "epoch: 29,  batch step: 74, loss: 7.220732688903809\n",
      "epoch: 29,  batch step: 75, loss: 31.64459800720215\n",
      "epoch: 29,  batch step: 76, loss: 54.198020935058594\n",
      "epoch: 29,  batch step: 77, loss: 51.387752532958984\n",
      "epoch: 29,  batch step: 78, loss: 17.334060668945312\n",
      "epoch: 29,  batch step: 79, loss: 16.94786834716797\n",
      "epoch: 29,  batch step: 80, loss: 14.78921127319336\n",
      "epoch: 29,  batch step: 81, loss: 214.8329315185547\n",
      "epoch: 29,  batch step: 82, loss: 10.516337394714355\n",
      "epoch: 29,  batch step: 83, loss: 46.09731674194336\n",
      "epoch: 29,  batch step: 84, loss: 13.449934005737305\n",
      "epoch: 29,  batch step: 85, loss: 11.688278198242188\n",
      "epoch: 29,  batch step: 86, loss: 40.99003219604492\n",
      "epoch: 29,  batch step: 87, loss: 37.21580505371094\n",
      "epoch: 29,  batch step: 88, loss: 28.715110778808594\n",
      "epoch: 29,  batch step: 89, loss: 86.72991180419922\n",
      "epoch: 29,  batch step: 90, loss: 6.249874114990234\n",
      "epoch: 29,  batch step: 91, loss: 4.518100738525391\n",
      "epoch: 29,  batch step: 92, loss: 15.086721420288086\n",
      "epoch: 29,  batch step: 93, loss: 58.96116638183594\n",
      "epoch: 29,  batch step: 94, loss: 26.3585205078125\n",
      "epoch: 29,  batch step: 95, loss: 6.930202484130859\n",
      "epoch: 29,  batch step: 96, loss: 41.506492614746094\n",
      "epoch: 29,  batch step: 97, loss: 22.246715545654297\n",
      "epoch: 29,  batch step: 98, loss: 7.8554582595825195\n",
      "epoch: 29,  batch step: 99, loss: 128.44602966308594\n",
      "epoch: 29,  batch step: 100, loss: 44.611732482910156\n",
      "epoch: 29,  batch step: 101, loss: 69.60472869873047\n",
      "epoch: 29,  batch step: 102, loss: 67.32141876220703\n",
      "epoch: 29,  batch step: 103, loss: 97.54545593261719\n",
      "epoch: 29,  batch step: 104, loss: 41.887943267822266\n",
      "epoch: 29,  batch step: 105, loss: 17.090465545654297\n",
      "epoch: 29,  batch step: 106, loss: 28.5277099609375\n",
      "epoch: 29,  batch step: 107, loss: 43.49518966674805\n",
      "epoch: 29,  batch step: 108, loss: 117.27952575683594\n",
      "epoch: 29,  batch step: 109, loss: 8.919309616088867\n",
      "epoch: 29,  batch step: 110, loss: 94.30110931396484\n",
      "epoch: 29,  batch step: 111, loss: 62.1687126159668\n",
      "epoch: 29,  batch step: 112, loss: 7.4579386711120605\n",
      "epoch: 29,  batch step: 113, loss: 84.6467056274414\n",
      "epoch: 29,  batch step: 114, loss: 5.838916778564453\n",
      "epoch: 29,  batch step: 115, loss: 18.19856834411621\n",
      "epoch: 29,  batch step: 116, loss: 16.037891387939453\n",
      "epoch: 29,  batch step: 117, loss: 90.22097778320312\n",
      "epoch: 29,  batch step: 118, loss: 62.65850067138672\n",
      "epoch: 29,  batch step: 119, loss: 12.717798233032227\n",
      "epoch: 29,  batch step: 120, loss: 51.018028259277344\n",
      "epoch: 29,  batch step: 121, loss: 20.954856872558594\n",
      "epoch: 29,  batch step: 122, loss: 38.08387756347656\n",
      "epoch: 29,  batch step: 123, loss: 13.106829643249512\n",
      "epoch: 29,  batch step: 124, loss: 5.4911932945251465\n",
      "epoch: 29,  batch step: 125, loss: 12.853739738464355\n",
      "epoch: 29,  batch step: 126, loss: 9.013360977172852\n",
      "epoch: 29,  batch step: 127, loss: 51.72384262084961\n",
      "epoch: 29,  batch step: 128, loss: 25.606910705566406\n",
      "epoch: 29,  batch step: 129, loss: 83.41817474365234\n",
      "epoch: 29,  batch step: 130, loss: 70.673828125\n",
      "epoch: 29,  batch step: 131, loss: 58.22772216796875\n",
      "epoch: 29,  batch step: 132, loss: 56.436920166015625\n",
      "epoch: 29,  batch step: 133, loss: 229.5356903076172\n",
      "epoch: 29,  batch step: 134, loss: 46.86167526245117\n",
      "epoch: 29,  batch step: 135, loss: 37.12580871582031\n",
      "epoch: 29,  batch step: 136, loss: 81.5574951171875\n",
      "epoch: 29,  batch step: 137, loss: 81.87544250488281\n",
      "epoch: 29,  batch step: 138, loss: 28.080821990966797\n",
      "epoch: 29,  batch step: 139, loss: 104.43965911865234\n",
      "epoch: 29,  batch step: 140, loss: 11.997365951538086\n",
      "epoch: 29,  batch step: 141, loss: 8.259532928466797\n",
      "epoch: 29,  batch step: 142, loss: 75.04755401611328\n",
      "epoch: 29,  batch step: 143, loss: 10.982423782348633\n",
      "epoch: 29,  batch step: 144, loss: 12.29806137084961\n",
      "epoch: 29,  batch step: 145, loss: 11.257749557495117\n",
      "epoch: 29,  batch step: 146, loss: 45.761024475097656\n",
      "epoch: 29,  batch step: 147, loss: 40.264801025390625\n",
      "epoch: 29,  batch step: 148, loss: 17.096513748168945\n",
      "epoch: 29,  batch step: 149, loss: 17.047901153564453\n",
      "epoch: 29,  batch step: 150, loss: 7.326028347015381\n",
      "epoch: 29,  batch step: 151, loss: 154.5846710205078\n",
      "epoch: 29,  batch step: 152, loss: 11.425539016723633\n",
      "epoch: 29,  batch step: 153, loss: 8.959772109985352\n",
      "epoch: 29,  batch step: 154, loss: 156.10205078125\n",
      "epoch: 29,  batch step: 155, loss: 10.680160522460938\n",
      "epoch: 29,  batch step: 156, loss: 274.856689453125\n",
      "epoch: 29,  batch step: 157, loss: 8.439712524414062\n",
      "epoch: 29,  batch step: 158, loss: 8.443653106689453\n",
      "epoch: 29,  batch step: 159, loss: 19.364940643310547\n",
      "epoch: 29,  batch step: 160, loss: 52.94871520996094\n",
      "epoch: 29,  batch step: 161, loss: 30.792226791381836\n",
      "epoch: 29,  batch step: 162, loss: 69.90959930419922\n",
      "epoch: 29,  batch step: 163, loss: 97.67912292480469\n",
      "epoch: 29,  batch step: 164, loss: 9.898005485534668\n",
      "epoch: 29,  batch step: 165, loss: 126.01820373535156\n",
      "epoch: 29,  batch step: 166, loss: 11.536985397338867\n",
      "epoch: 29,  batch step: 167, loss: 7.843972682952881\n",
      "epoch: 29,  batch step: 168, loss: 90.14585876464844\n",
      "epoch: 29,  batch step: 169, loss: 6.835397243499756\n",
      "epoch: 29,  batch step: 170, loss: 11.657622337341309\n",
      "epoch: 29,  batch step: 171, loss: 38.38264465332031\n",
      "epoch: 29,  batch step: 172, loss: 20.33429718017578\n",
      "epoch: 29,  batch step: 173, loss: 39.565704345703125\n",
      "epoch: 29,  batch step: 174, loss: 40.113868713378906\n",
      "epoch: 29,  batch step: 175, loss: 13.676383972167969\n",
      "epoch: 29,  batch step: 176, loss: 11.735179901123047\n",
      "epoch: 29,  batch step: 177, loss: 44.29914093017578\n",
      "epoch: 29,  batch step: 178, loss: 72.49129486083984\n",
      "epoch: 29,  batch step: 179, loss: 10.79448127746582\n",
      "epoch: 29,  batch step: 180, loss: 47.563697814941406\n",
      "epoch: 29,  batch step: 181, loss: 76.94933319091797\n",
      "epoch: 29,  batch step: 182, loss: 48.83769226074219\n",
      "epoch: 29,  batch step: 183, loss: 22.15534210205078\n",
      "epoch: 29,  batch step: 184, loss: 20.429622650146484\n",
      "epoch: 29,  batch step: 185, loss: 72.78462219238281\n",
      "epoch: 29,  batch step: 186, loss: 69.69756317138672\n",
      "epoch: 29,  batch step: 187, loss: 39.385494232177734\n",
      "epoch: 29,  batch step: 188, loss: 15.275127410888672\n",
      "epoch: 29,  batch step: 189, loss: 71.35252380371094\n",
      "epoch: 29,  batch step: 190, loss: 113.68395233154297\n",
      "epoch: 29,  batch step: 191, loss: 64.0582275390625\n",
      "epoch: 29,  batch step: 192, loss: 87.83354949951172\n",
      "epoch: 29,  batch step: 193, loss: 5.788312911987305\n",
      "epoch: 29,  batch step: 194, loss: 42.86699676513672\n",
      "epoch: 29,  batch step: 195, loss: 5.962634086608887\n",
      "epoch: 29,  batch step: 196, loss: 5.55328893661499\n",
      "epoch: 29,  batch step: 197, loss: 8.895236015319824\n",
      "epoch: 29,  batch step: 198, loss: 17.08485984802246\n",
      "epoch: 29,  batch step: 199, loss: 297.00146484375\n",
      "epoch: 29,  batch step: 200, loss: 10.54636001586914\n",
      "epoch: 29,  batch step: 201, loss: 15.833712577819824\n",
      "epoch: 29,  batch step: 202, loss: 15.221359252929688\n",
      "epoch: 29,  batch step: 203, loss: 6.92111873626709\n",
      "epoch: 29,  batch step: 204, loss: 15.618291854858398\n",
      "epoch: 29,  batch step: 205, loss: 126.69072723388672\n",
      "epoch: 29,  batch step: 206, loss: 32.801387786865234\n",
      "epoch: 29,  batch step: 207, loss: 65.3423080444336\n",
      "epoch: 29,  batch step: 208, loss: 44.121578216552734\n",
      "epoch: 29,  batch step: 209, loss: 12.101507186889648\n",
      "epoch: 29,  batch step: 210, loss: 14.345762252807617\n",
      "epoch: 29,  batch step: 211, loss: 7.188086986541748\n",
      "epoch: 29,  batch step: 212, loss: 25.829710006713867\n",
      "epoch: 29,  batch step: 213, loss: 104.21563720703125\n",
      "epoch: 29,  batch step: 214, loss: 142.53103637695312\n",
      "epoch: 29,  batch step: 215, loss: 6.721942901611328\n",
      "epoch: 29,  batch step: 216, loss: 17.75356674194336\n",
      "epoch: 29,  batch step: 217, loss: 23.130346298217773\n",
      "epoch: 29,  batch step: 218, loss: 21.0643310546875\n",
      "epoch: 29,  batch step: 219, loss: 4.118159770965576\n",
      "epoch: 29,  batch step: 220, loss: 9.748891830444336\n",
      "epoch: 29,  batch step: 221, loss: 153.03378295898438\n",
      "epoch: 29,  batch step: 222, loss: 35.35945129394531\n",
      "epoch: 29,  batch step: 223, loss: 153.83575439453125\n",
      "epoch: 29,  batch step: 224, loss: 34.70677185058594\n",
      "epoch: 29,  batch step: 225, loss: 82.31672668457031\n",
      "epoch: 29,  batch step: 226, loss: 84.52268981933594\n",
      "epoch: 29,  batch step: 227, loss: 10.533075332641602\n",
      "epoch: 29,  batch step: 228, loss: 29.36664390563965\n",
      "epoch: 29,  batch step: 229, loss: 12.66914176940918\n",
      "epoch: 29,  batch step: 230, loss: 15.655146598815918\n",
      "epoch: 29,  batch step: 231, loss: 66.59183502197266\n",
      "epoch: 29,  batch step: 232, loss: 58.45536422729492\n",
      "epoch: 29,  batch step: 233, loss: 193.98995971679688\n",
      "epoch: 29,  batch step: 234, loss: 35.883113861083984\n",
      "epoch: 29,  batch step: 235, loss: 10.065237045288086\n",
      "epoch: 29,  batch step: 236, loss: 29.594762802124023\n",
      "epoch: 29,  batch step: 237, loss: 17.872352600097656\n",
      "epoch: 29,  batch step: 238, loss: 9.197351455688477\n",
      "epoch: 29,  batch step: 239, loss: 50.10764694213867\n",
      "epoch: 29,  batch step: 240, loss: 63.62998580932617\n",
      "epoch: 29,  batch step: 241, loss: 16.071292877197266\n",
      "epoch: 29,  batch step: 242, loss: 56.687049865722656\n",
      "epoch: 29,  batch step: 243, loss: 17.407123565673828\n",
      "epoch: 29,  batch step: 244, loss: 57.02306365966797\n",
      "epoch: 29,  batch step: 245, loss: 50.86973190307617\n",
      "epoch: 29,  batch step: 246, loss: 13.816612243652344\n",
      "epoch: 29,  batch step: 247, loss: 32.02773666381836\n",
      "epoch: 29,  batch step: 248, loss: 20.085466384887695\n",
      "epoch: 29,  batch step: 249, loss: 9.192388534545898\n",
      "epoch: 29,  batch step: 250, loss: 166.04681396484375\n",
      "epoch: 29,  batch step: 251, loss: 263.75482177734375\n",
      "finished saving checkpoints\n",
      "validation error epoch  29:    tensor(73.7476, device='cuda:0')\n",
      "316\n",
      "epoch: 30,  batch step: 0, loss: 29.442214965820312\n",
      "epoch: 30,  batch step: 1, loss: 7.970527648925781\n",
      "epoch: 30,  batch step: 2, loss: 11.729545593261719\n",
      "epoch: 30,  batch step: 3, loss: 177.76791381835938\n",
      "epoch: 30,  batch step: 4, loss: 6.330834865570068\n",
      "epoch: 30,  batch step: 5, loss: 12.16598892211914\n",
      "epoch: 30,  batch step: 6, loss: 50.99714660644531\n",
      "epoch: 30,  batch step: 7, loss: 16.225751876831055\n",
      "epoch: 30,  batch step: 8, loss: 210.19882202148438\n",
      "epoch: 30,  batch step: 9, loss: 19.526884078979492\n",
      "epoch: 30,  batch step: 10, loss: 7.689744472503662\n",
      "epoch: 30,  batch step: 11, loss: 14.774394035339355\n",
      "epoch: 30,  batch step: 12, loss: 28.22894859313965\n",
      "epoch: 30,  batch step: 13, loss: 84.33744812011719\n",
      "epoch: 30,  batch step: 14, loss: 12.711662292480469\n",
      "epoch: 30,  batch step: 15, loss: 90.49227905273438\n",
      "epoch: 30,  batch step: 16, loss: 15.31844711303711\n",
      "epoch: 30,  batch step: 17, loss: 38.28129196166992\n",
      "epoch: 30,  batch step: 18, loss: 13.444354057312012\n",
      "epoch: 30,  batch step: 19, loss: 288.17596435546875\n",
      "epoch: 30,  batch step: 20, loss: 46.26603698730469\n",
      "epoch: 30,  batch step: 21, loss: 29.06940460205078\n",
      "epoch: 30,  batch step: 22, loss: 18.706771850585938\n",
      "epoch: 30,  batch step: 23, loss: 10.717140197753906\n",
      "epoch: 30,  batch step: 24, loss: 227.39877319335938\n",
      "epoch: 30,  batch step: 25, loss: 14.382286071777344\n",
      "epoch: 30,  batch step: 26, loss: 118.91636657714844\n",
      "epoch: 30,  batch step: 27, loss: 49.914146423339844\n",
      "epoch: 30,  batch step: 28, loss: 17.7470703125\n",
      "epoch: 30,  batch step: 29, loss: 73.2055892944336\n",
      "epoch: 30,  batch step: 30, loss: 27.929603576660156\n",
      "epoch: 30,  batch step: 31, loss: 80.4691162109375\n",
      "epoch: 30,  batch step: 32, loss: 11.610885620117188\n",
      "epoch: 30,  batch step: 33, loss: 14.300078392028809\n",
      "epoch: 30,  batch step: 34, loss: 11.400505065917969\n",
      "epoch: 30,  batch step: 35, loss: 42.9774284362793\n",
      "epoch: 30,  batch step: 36, loss: 26.912572860717773\n",
      "epoch: 30,  batch step: 37, loss: 43.5026969909668\n",
      "epoch: 30,  batch step: 38, loss: 20.106103897094727\n",
      "epoch: 30,  batch step: 39, loss: 76.6926498413086\n",
      "epoch: 30,  batch step: 40, loss: 7.856656074523926\n",
      "epoch: 30,  batch step: 41, loss: 39.36103057861328\n",
      "epoch: 30,  batch step: 42, loss: 29.480831146240234\n",
      "epoch: 30,  batch step: 43, loss: 67.12138366699219\n",
      "epoch: 30,  batch step: 44, loss: 49.948604583740234\n",
      "epoch: 30,  batch step: 45, loss: 42.43430709838867\n",
      "epoch: 30,  batch step: 46, loss: 37.938846588134766\n",
      "epoch: 30,  batch step: 47, loss: 7.263488292694092\n",
      "epoch: 30,  batch step: 48, loss: 55.294677734375\n",
      "epoch: 30,  batch step: 49, loss: 11.100288391113281\n",
      "epoch: 30,  batch step: 50, loss: 19.041362762451172\n",
      "epoch: 30,  batch step: 51, loss: 64.3772964477539\n",
      "epoch: 30,  batch step: 52, loss: 11.12788200378418\n",
      "epoch: 30,  batch step: 53, loss: 32.54298400878906\n",
      "epoch: 30,  batch step: 54, loss: 73.50039672851562\n",
      "epoch: 30,  batch step: 55, loss: 77.5959243774414\n",
      "epoch: 30,  batch step: 56, loss: 8.90982437133789\n",
      "epoch: 30,  batch step: 57, loss: 47.420501708984375\n",
      "epoch: 30,  batch step: 58, loss: 11.780393600463867\n",
      "epoch: 30,  batch step: 59, loss: 11.424909591674805\n",
      "epoch: 30,  batch step: 60, loss: 18.80289649963379\n",
      "epoch: 30,  batch step: 61, loss: 53.117889404296875\n",
      "epoch: 30,  batch step: 62, loss: 11.753120422363281\n",
      "epoch: 30,  batch step: 63, loss: 54.47175216674805\n",
      "epoch: 30,  batch step: 64, loss: 59.021827697753906\n",
      "epoch: 30,  batch step: 65, loss: 5.668149471282959\n",
      "epoch: 30,  batch step: 66, loss: 5.8707170486450195\n",
      "epoch: 30,  batch step: 67, loss: 12.483327865600586\n",
      "epoch: 30,  batch step: 68, loss: 30.266212463378906\n",
      "epoch: 30,  batch step: 69, loss: 94.8655014038086\n",
      "epoch: 30,  batch step: 70, loss: 6.16609525680542\n",
      "epoch: 30,  batch step: 71, loss: 32.562652587890625\n",
      "epoch: 30,  batch step: 72, loss: 34.91912841796875\n",
      "epoch: 30,  batch step: 73, loss: 23.642072677612305\n",
      "epoch: 30,  batch step: 74, loss: 37.53831100463867\n",
      "epoch: 30,  batch step: 75, loss: 98.55137634277344\n",
      "epoch: 30,  batch step: 76, loss: 8.10232162475586\n",
      "epoch: 30,  batch step: 77, loss: 4.354955196380615\n",
      "epoch: 30,  batch step: 78, loss: 43.41432189941406\n",
      "epoch: 30,  batch step: 79, loss: 8.33405876159668\n",
      "epoch: 30,  batch step: 80, loss: 9.940923690795898\n",
      "epoch: 30,  batch step: 81, loss: 40.118309020996094\n",
      "epoch: 30,  batch step: 82, loss: 46.47410202026367\n",
      "epoch: 30,  batch step: 83, loss: 67.23600006103516\n",
      "epoch: 30,  batch step: 84, loss: 65.86382293701172\n",
      "epoch: 30,  batch step: 85, loss: 21.368282318115234\n",
      "epoch: 30,  batch step: 86, loss: 125.71995544433594\n",
      "epoch: 30,  batch step: 87, loss: 22.72853660583496\n",
      "epoch: 30,  batch step: 88, loss: 40.62665939331055\n",
      "epoch: 30,  batch step: 89, loss: 30.202457427978516\n",
      "epoch: 30,  batch step: 90, loss: 12.58387279510498\n",
      "epoch: 30,  batch step: 91, loss: 8.586692810058594\n",
      "epoch: 30,  batch step: 92, loss: 13.0559720993042\n",
      "epoch: 30,  batch step: 93, loss: 7.552947044372559\n",
      "epoch: 30,  batch step: 94, loss: 68.99922180175781\n",
      "epoch: 30,  batch step: 95, loss: 19.27799415588379\n",
      "epoch: 30,  batch step: 96, loss: 124.56480407714844\n",
      "epoch: 30,  batch step: 97, loss: 54.45304489135742\n",
      "epoch: 30,  batch step: 98, loss: 12.794475555419922\n",
      "epoch: 30,  batch step: 99, loss: 22.057783126831055\n",
      "epoch: 30,  batch step: 100, loss: 11.15164852142334\n",
      "epoch: 30,  batch step: 101, loss: 6.928793907165527\n",
      "epoch: 30,  batch step: 102, loss: 27.32770538330078\n",
      "epoch: 30,  batch step: 103, loss: 13.66065502166748\n",
      "epoch: 30,  batch step: 104, loss: 89.5570297241211\n",
      "epoch: 30,  batch step: 105, loss: 4.943255424499512\n",
      "epoch: 30,  batch step: 106, loss: 26.667598724365234\n",
      "epoch: 30,  batch step: 107, loss: 4.95979118347168\n",
      "epoch: 30,  batch step: 108, loss: 15.45505428314209\n",
      "epoch: 30,  batch step: 109, loss: 130.26260375976562\n",
      "epoch: 30,  batch step: 110, loss: 6.332568645477295\n",
      "epoch: 30,  batch step: 111, loss: 7.964317321777344\n",
      "epoch: 30,  batch step: 112, loss: 17.04344940185547\n",
      "epoch: 30,  batch step: 113, loss: 5.7062201499938965\n",
      "epoch: 30,  batch step: 114, loss: 101.12657928466797\n",
      "epoch: 30,  batch step: 115, loss: 55.58062744140625\n",
      "epoch: 30,  batch step: 116, loss: 13.847427368164062\n",
      "epoch: 30,  batch step: 117, loss: 44.196075439453125\n",
      "epoch: 30,  batch step: 118, loss: 46.30320739746094\n",
      "epoch: 30,  batch step: 119, loss: 54.067256927490234\n",
      "epoch: 30,  batch step: 120, loss: 4.854875087738037\n",
      "epoch: 30,  batch step: 121, loss: 17.50804901123047\n",
      "epoch: 30,  batch step: 122, loss: 8.885995864868164\n",
      "epoch: 30,  batch step: 123, loss: 24.016929626464844\n",
      "epoch: 30,  batch step: 124, loss: 25.61775779724121\n",
      "epoch: 30,  batch step: 125, loss: 7.47054386138916\n",
      "epoch: 30,  batch step: 126, loss: 40.26988220214844\n",
      "epoch: 30,  batch step: 127, loss: 8.337201118469238\n",
      "epoch: 30,  batch step: 128, loss: 6.836294174194336\n",
      "epoch: 30,  batch step: 129, loss: 32.39347839355469\n",
      "epoch: 30,  batch step: 130, loss: 149.23094177246094\n",
      "epoch: 30,  batch step: 131, loss: 24.48720359802246\n",
      "epoch: 30,  batch step: 132, loss: 43.79319381713867\n",
      "epoch: 30,  batch step: 133, loss: 103.3174057006836\n",
      "epoch: 30,  batch step: 134, loss: 43.15424346923828\n",
      "epoch: 30,  batch step: 135, loss: 31.750545501708984\n",
      "epoch: 30,  batch step: 136, loss: 45.581077575683594\n",
      "epoch: 30,  batch step: 137, loss: 60.914608001708984\n",
      "epoch: 30,  batch step: 138, loss: 7.98148250579834\n",
      "epoch: 30,  batch step: 139, loss: 12.223342895507812\n",
      "epoch: 30,  batch step: 140, loss: 49.5234375\n",
      "epoch: 30,  batch step: 141, loss: 3.5735297203063965\n",
      "epoch: 30,  batch step: 142, loss: 16.718074798583984\n",
      "epoch: 30,  batch step: 143, loss: 15.641471862792969\n",
      "epoch: 30,  batch step: 144, loss: 67.73194885253906\n",
      "epoch: 30,  batch step: 145, loss: 23.36833953857422\n",
      "epoch: 30,  batch step: 146, loss: 7.708768367767334\n",
      "epoch: 30,  batch step: 147, loss: 68.65843200683594\n",
      "epoch: 30,  batch step: 148, loss: 37.375274658203125\n",
      "epoch: 30,  batch step: 149, loss: 118.30914306640625\n",
      "epoch: 30,  batch step: 150, loss: 4.537235260009766\n",
      "epoch: 30,  batch step: 151, loss: 71.61099243164062\n",
      "epoch: 30,  batch step: 152, loss: 7.53328275680542\n",
      "epoch: 30,  batch step: 153, loss: 4.425891399383545\n",
      "epoch: 30,  batch step: 154, loss: 9.63956069946289\n",
      "epoch: 30,  batch step: 155, loss: 66.14140319824219\n",
      "epoch: 30,  batch step: 156, loss: 32.789241790771484\n",
      "epoch: 30,  batch step: 157, loss: 28.483190536499023\n",
      "epoch: 30,  batch step: 158, loss: 91.2275390625\n",
      "epoch: 30,  batch step: 159, loss: 19.722688674926758\n",
      "epoch: 30,  batch step: 160, loss: 20.01637840270996\n",
      "epoch: 30,  batch step: 161, loss: 11.156111717224121\n",
      "epoch: 30,  batch step: 162, loss: 41.46696472167969\n",
      "epoch: 30,  batch step: 163, loss: 5.894889831542969\n",
      "epoch: 30,  batch step: 164, loss: 61.55915451049805\n",
      "epoch: 30,  batch step: 165, loss: 255.4315185546875\n",
      "epoch: 30,  batch step: 166, loss: 65.62084197998047\n",
      "epoch: 30,  batch step: 167, loss: 14.324186325073242\n",
      "epoch: 30,  batch step: 168, loss: 14.581049919128418\n",
      "epoch: 30,  batch step: 169, loss: 40.758628845214844\n",
      "epoch: 30,  batch step: 170, loss: 61.44952392578125\n",
      "epoch: 30,  batch step: 171, loss: 5.683699607849121\n",
      "epoch: 30,  batch step: 172, loss: 35.1804313659668\n",
      "epoch: 30,  batch step: 173, loss: 17.890514373779297\n",
      "epoch: 30,  batch step: 174, loss: 91.86871337890625\n",
      "epoch: 30,  batch step: 175, loss: 11.0052490234375\n",
      "epoch: 30,  batch step: 176, loss: 195.39439392089844\n",
      "epoch: 30,  batch step: 177, loss: 26.488515853881836\n",
      "epoch: 30,  batch step: 178, loss: 14.214373588562012\n",
      "epoch: 30,  batch step: 179, loss: 183.20492553710938\n",
      "epoch: 30,  batch step: 180, loss: 136.86691284179688\n",
      "epoch: 30,  batch step: 181, loss: 53.553565979003906\n",
      "epoch: 30,  batch step: 182, loss: 50.06829833984375\n",
      "epoch: 30,  batch step: 183, loss: 74.36915588378906\n",
      "epoch: 30,  batch step: 184, loss: 9.392406463623047\n",
      "epoch: 30,  batch step: 185, loss: 63.1825065612793\n",
      "epoch: 30,  batch step: 186, loss: 51.8242301940918\n",
      "epoch: 30,  batch step: 187, loss: 7.30176305770874\n",
      "epoch: 30,  batch step: 188, loss: 27.21755027770996\n",
      "epoch: 30,  batch step: 189, loss: 8.706271171569824\n",
      "epoch: 30,  batch step: 190, loss: 11.982475280761719\n",
      "epoch: 30,  batch step: 191, loss: 185.26699829101562\n",
      "epoch: 30,  batch step: 192, loss: 10.906265258789062\n",
      "epoch: 30,  batch step: 193, loss: 13.557425498962402\n",
      "epoch: 30,  batch step: 194, loss: 279.5567932128906\n",
      "epoch: 30,  batch step: 195, loss: 100.70867919921875\n",
      "epoch: 30,  batch step: 196, loss: 12.354606628417969\n",
      "epoch: 30,  batch step: 197, loss: 83.0501937866211\n",
      "epoch: 30,  batch step: 198, loss: 20.269184112548828\n",
      "epoch: 30,  batch step: 199, loss: 30.412687301635742\n",
      "epoch: 30,  batch step: 200, loss: 200.97161865234375\n",
      "epoch: 30,  batch step: 201, loss: 30.80919647216797\n",
      "epoch: 30,  batch step: 202, loss: 66.48307800292969\n",
      "epoch: 30,  batch step: 203, loss: 8.270772933959961\n",
      "epoch: 30,  batch step: 204, loss: 21.149383544921875\n",
      "epoch: 30,  batch step: 205, loss: 9.54815673828125\n",
      "epoch: 30,  batch step: 206, loss: 36.17240905761719\n",
      "epoch: 30,  batch step: 207, loss: 22.718822479248047\n",
      "epoch: 30,  batch step: 208, loss: 48.17760467529297\n",
      "epoch: 30,  batch step: 209, loss: 127.81273651123047\n",
      "epoch: 30,  batch step: 210, loss: 19.823326110839844\n",
      "epoch: 30,  batch step: 211, loss: 13.36775016784668\n",
      "epoch: 30,  batch step: 212, loss: 224.62030029296875\n",
      "epoch: 30,  batch step: 213, loss: 51.505035400390625\n",
      "epoch: 30,  batch step: 214, loss: 345.3326110839844\n",
      "epoch: 30,  batch step: 215, loss: 13.263751029968262\n",
      "epoch: 30,  batch step: 216, loss: 8.607540130615234\n",
      "epoch: 30,  batch step: 217, loss: 5.479789733886719\n",
      "epoch: 30,  batch step: 218, loss: 86.94473266601562\n",
      "epoch: 30,  batch step: 219, loss: 9.264467239379883\n",
      "epoch: 30,  batch step: 220, loss: 20.551410675048828\n",
      "epoch: 30,  batch step: 221, loss: 69.23468017578125\n",
      "epoch: 30,  batch step: 222, loss: 29.42926597595215\n",
      "epoch: 30,  batch step: 223, loss: 40.427268981933594\n",
      "epoch: 30,  batch step: 224, loss: 11.355647087097168\n",
      "epoch: 30,  batch step: 225, loss: 23.64468002319336\n",
      "epoch: 30,  batch step: 226, loss: 11.004961967468262\n",
      "epoch: 30,  batch step: 227, loss: 6.001603126525879\n",
      "epoch: 30,  batch step: 228, loss: 100.63444519042969\n",
      "epoch: 30,  batch step: 229, loss: 43.37035369873047\n",
      "epoch: 30,  batch step: 230, loss: 66.07124328613281\n",
      "epoch: 30,  batch step: 231, loss: 50.184959411621094\n",
      "epoch: 30,  batch step: 232, loss: 9.053069114685059\n",
      "epoch: 30,  batch step: 233, loss: 12.388326644897461\n",
      "epoch: 30,  batch step: 234, loss: 70.62931060791016\n",
      "epoch: 30,  batch step: 235, loss: 11.41699504852295\n",
      "epoch: 30,  batch step: 236, loss: 3.5707015991210938\n",
      "epoch: 30,  batch step: 237, loss: 144.7456817626953\n",
      "epoch: 30,  batch step: 238, loss: 33.426631927490234\n",
      "epoch: 30,  batch step: 239, loss: 12.24338150024414\n",
      "epoch: 30,  batch step: 240, loss: 10.632139205932617\n",
      "epoch: 30,  batch step: 241, loss: 71.97325134277344\n",
      "epoch: 30,  batch step: 242, loss: 33.584678649902344\n",
      "epoch: 30,  batch step: 243, loss: 11.56341552734375\n",
      "epoch: 30,  batch step: 244, loss: 151.0997314453125\n",
      "epoch: 30,  batch step: 245, loss: 5.392849922180176\n",
      "epoch: 30,  batch step: 246, loss: 17.40869903564453\n",
      "epoch: 30,  batch step: 247, loss: 45.34523010253906\n",
      "epoch: 30,  batch step: 248, loss: 8.455611228942871\n",
      "epoch: 30,  batch step: 249, loss: 113.60338592529297\n",
      "epoch: 30,  batch step: 250, loss: 80.68032836914062\n",
      "epoch: 30,  batch step: 251, loss: 123.4600830078125\n",
      "validation error epoch  30:    tensor(68.9331, device='cuda:0')\n",
      "316\n",
      "epoch: 31,  batch step: 0, loss: 43.962440490722656\n",
      "epoch: 31,  batch step: 1, loss: 6.7213263511657715\n",
      "epoch: 31,  batch step: 2, loss: 11.401006698608398\n",
      "epoch: 31,  batch step: 3, loss: 38.507667541503906\n",
      "epoch: 31,  batch step: 4, loss: 137.79483032226562\n",
      "epoch: 31,  batch step: 5, loss: 39.285953521728516\n",
      "epoch: 31,  batch step: 6, loss: 24.287290573120117\n",
      "epoch: 31,  batch step: 7, loss: 86.81917572021484\n",
      "epoch: 31,  batch step: 8, loss: 9.746851921081543\n",
      "epoch: 31,  batch step: 9, loss: 21.1639461517334\n",
      "epoch: 31,  batch step: 10, loss: 13.969449996948242\n",
      "epoch: 31,  batch step: 11, loss: 15.586339950561523\n",
      "epoch: 31,  batch step: 12, loss: 24.287565231323242\n",
      "epoch: 31,  batch step: 13, loss: 39.41505432128906\n",
      "epoch: 31,  batch step: 14, loss: 38.65068054199219\n",
      "epoch: 31,  batch step: 15, loss: 22.341094970703125\n",
      "epoch: 31,  batch step: 16, loss: 50.0841064453125\n",
      "epoch: 31,  batch step: 17, loss: 16.94900131225586\n",
      "epoch: 31,  batch step: 18, loss: 7.483222484588623\n",
      "epoch: 31,  batch step: 19, loss: 145.55836486816406\n",
      "epoch: 31,  batch step: 20, loss: 129.04470825195312\n",
      "epoch: 31,  batch step: 21, loss: 4.73056173324585\n",
      "epoch: 31,  batch step: 22, loss: 49.127845764160156\n",
      "epoch: 31,  batch step: 23, loss: 108.4973373413086\n",
      "epoch: 31,  batch step: 24, loss: 14.126920700073242\n",
      "epoch: 31,  batch step: 25, loss: 178.61343383789062\n",
      "epoch: 31,  batch step: 26, loss: 32.424964904785156\n",
      "epoch: 31,  batch step: 27, loss: 39.71485137939453\n",
      "epoch: 31,  batch step: 28, loss: 10.971765518188477\n",
      "epoch: 31,  batch step: 29, loss: 52.918636322021484\n",
      "epoch: 31,  batch step: 30, loss: 104.12823486328125\n",
      "epoch: 31,  batch step: 31, loss: 54.400962829589844\n",
      "epoch: 31,  batch step: 32, loss: 10.458072662353516\n",
      "epoch: 31,  batch step: 33, loss: 9.478750228881836\n",
      "epoch: 31,  batch step: 34, loss: 97.45476531982422\n",
      "epoch: 31,  batch step: 35, loss: 43.80349349975586\n",
      "epoch: 31,  batch step: 36, loss: 10.336997032165527\n",
      "epoch: 31,  batch step: 37, loss: 39.40443420410156\n",
      "epoch: 31,  batch step: 38, loss: 30.959938049316406\n",
      "epoch: 31,  batch step: 39, loss: 25.874204635620117\n",
      "epoch: 31,  batch step: 40, loss: 18.038679122924805\n",
      "epoch: 31,  batch step: 41, loss: 49.78896713256836\n",
      "epoch: 31,  batch step: 42, loss: 36.52402114868164\n",
      "epoch: 31,  batch step: 43, loss: 6.4629316329956055\n",
      "epoch: 31,  batch step: 44, loss: 12.349013328552246\n",
      "epoch: 31,  batch step: 45, loss: 13.538993835449219\n",
      "epoch: 31,  batch step: 46, loss: 121.50313568115234\n",
      "epoch: 31,  batch step: 47, loss: 42.183345794677734\n",
      "epoch: 31,  batch step: 48, loss: 5.681878089904785\n",
      "epoch: 31,  batch step: 49, loss: 27.880245208740234\n",
      "epoch: 31,  batch step: 50, loss: 29.278120040893555\n",
      "epoch: 31,  batch step: 51, loss: 280.246826171875\n",
      "epoch: 31,  batch step: 52, loss: 81.6693344116211\n",
      "epoch: 31,  batch step: 53, loss: 11.934672355651855\n",
      "epoch: 31,  batch step: 54, loss: 48.28805160522461\n",
      "epoch: 31,  batch step: 55, loss: 7.4395976066589355\n",
      "epoch: 31,  batch step: 56, loss: 78.20915222167969\n",
      "epoch: 31,  batch step: 57, loss: 12.883296966552734\n",
      "epoch: 31,  batch step: 58, loss: 13.515838623046875\n",
      "epoch: 31,  batch step: 59, loss: 58.570701599121094\n",
      "epoch: 31,  batch step: 60, loss: 12.3631010055542\n",
      "epoch: 31,  batch step: 61, loss: 13.596622467041016\n",
      "epoch: 31,  batch step: 62, loss: 148.95907592773438\n",
      "epoch: 31,  batch step: 63, loss: 5.704789638519287\n",
      "epoch: 31,  batch step: 64, loss: 84.99372863769531\n",
      "epoch: 31,  batch step: 65, loss: 4.959438323974609\n",
      "epoch: 31,  batch step: 66, loss: 29.411991119384766\n",
      "epoch: 31,  batch step: 67, loss: 41.88344192504883\n",
      "epoch: 31,  batch step: 68, loss: 12.50617790222168\n",
      "epoch: 31,  batch step: 69, loss: 8.281339645385742\n",
      "epoch: 31,  batch step: 70, loss: 174.6908416748047\n",
      "epoch: 31,  batch step: 71, loss: 69.05461883544922\n",
      "epoch: 31,  batch step: 72, loss: 6.9932756423950195\n",
      "epoch: 31,  batch step: 73, loss: 15.972887992858887\n",
      "epoch: 31,  batch step: 74, loss: 99.06971740722656\n",
      "epoch: 31,  batch step: 75, loss: 32.946720123291016\n",
      "epoch: 31,  batch step: 76, loss: 18.28725242614746\n",
      "epoch: 31,  batch step: 77, loss: 92.66273498535156\n",
      "epoch: 31,  batch step: 78, loss: 24.032926559448242\n",
      "epoch: 31,  batch step: 79, loss: 5.587953090667725\n",
      "epoch: 31,  batch step: 80, loss: 72.72438049316406\n",
      "epoch: 31,  batch step: 81, loss: 5.391589164733887\n",
      "epoch: 31,  batch step: 82, loss: 202.8702850341797\n",
      "epoch: 31,  batch step: 83, loss: 51.165435791015625\n",
      "epoch: 31,  batch step: 84, loss: 46.73870849609375\n",
      "epoch: 31,  batch step: 85, loss: 62.16598129272461\n",
      "epoch: 31,  batch step: 86, loss: 24.58535385131836\n",
      "epoch: 31,  batch step: 87, loss: 53.540794372558594\n",
      "epoch: 31,  batch step: 88, loss: 11.874166488647461\n",
      "epoch: 31,  batch step: 89, loss: 17.161190032958984\n",
      "epoch: 31,  batch step: 90, loss: 108.79391479492188\n",
      "epoch: 31,  batch step: 91, loss: 46.50352096557617\n",
      "epoch: 31,  batch step: 92, loss: 25.70296287536621\n",
      "epoch: 31,  batch step: 93, loss: 63.55243682861328\n",
      "epoch: 31,  batch step: 94, loss: 19.382408142089844\n",
      "epoch: 31,  batch step: 95, loss: 12.453460693359375\n",
      "epoch: 31,  batch step: 96, loss: 18.696250915527344\n",
      "epoch: 31,  batch step: 97, loss: 45.37272262573242\n",
      "epoch: 31,  batch step: 98, loss: 19.89252471923828\n",
      "epoch: 31,  batch step: 99, loss: 7.355316162109375\n",
      "epoch: 31,  batch step: 100, loss: 14.462936401367188\n",
      "epoch: 31,  batch step: 101, loss: 122.4305419921875\n",
      "epoch: 31,  batch step: 102, loss: 55.38440704345703\n",
      "epoch: 31,  batch step: 103, loss: 5.981435775756836\n",
      "epoch: 31,  batch step: 104, loss: 113.49849700927734\n",
      "epoch: 31,  batch step: 105, loss: 7.951716423034668\n",
      "epoch: 31,  batch step: 106, loss: 36.38047790527344\n",
      "epoch: 31,  batch step: 107, loss: 25.75397300720215\n",
      "epoch: 31,  batch step: 108, loss: 13.66818904876709\n",
      "epoch: 31,  batch step: 109, loss: 57.16877746582031\n",
      "epoch: 31,  batch step: 110, loss: 64.94571685791016\n",
      "epoch: 31,  batch step: 111, loss: 9.113727569580078\n",
      "epoch: 31,  batch step: 112, loss: 9.687392234802246\n",
      "epoch: 31,  batch step: 113, loss: 67.44465637207031\n",
      "epoch: 31,  batch step: 114, loss: 8.817532539367676\n",
      "epoch: 31,  batch step: 115, loss: 46.77749252319336\n",
      "epoch: 31,  batch step: 116, loss: 35.80006408691406\n",
      "epoch: 31,  batch step: 117, loss: 23.333471298217773\n",
      "epoch: 31,  batch step: 118, loss: 6.127532005310059\n",
      "epoch: 31,  batch step: 119, loss: 91.51901245117188\n",
      "epoch: 31,  batch step: 120, loss: 54.71318817138672\n",
      "epoch: 31,  batch step: 121, loss: 6.464394569396973\n",
      "epoch: 31,  batch step: 122, loss: 9.680325508117676\n",
      "epoch: 31,  batch step: 123, loss: 17.286958694458008\n",
      "epoch: 31,  batch step: 124, loss: 42.612403869628906\n",
      "epoch: 31,  batch step: 125, loss: 18.67504119873047\n",
      "epoch: 31,  batch step: 126, loss: 21.599750518798828\n",
      "epoch: 31,  batch step: 127, loss: 12.044790267944336\n",
      "epoch: 31,  batch step: 128, loss: 27.13044548034668\n",
      "epoch: 31,  batch step: 129, loss: 12.146049499511719\n",
      "epoch: 31,  batch step: 130, loss: 10.477352142333984\n",
      "epoch: 31,  batch step: 131, loss: 9.682565689086914\n",
      "epoch: 31,  batch step: 132, loss: 11.632108688354492\n",
      "epoch: 31,  batch step: 133, loss: 31.415510177612305\n",
      "epoch: 31,  batch step: 134, loss: 6.444872856140137\n",
      "epoch: 31,  batch step: 135, loss: 10.289799690246582\n",
      "epoch: 31,  batch step: 136, loss: 11.733287811279297\n",
      "epoch: 31,  batch step: 137, loss: 66.61434936523438\n",
      "epoch: 31,  batch step: 138, loss: 7.799804210662842\n",
      "epoch: 31,  batch step: 139, loss: 11.994401931762695\n",
      "epoch: 31,  batch step: 140, loss: 67.67539978027344\n",
      "epoch: 31,  batch step: 141, loss: 44.011688232421875\n",
      "epoch: 31,  batch step: 142, loss: 150.32260131835938\n",
      "epoch: 31,  batch step: 143, loss: 44.49287414550781\n",
      "epoch: 31,  batch step: 144, loss: 33.94013214111328\n",
      "epoch: 31,  batch step: 145, loss: 7.225500106811523\n",
      "epoch: 31,  batch step: 146, loss: 5.853429794311523\n",
      "epoch: 31,  batch step: 147, loss: 51.553466796875\n",
      "epoch: 31,  batch step: 148, loss: 14.946070671081543\n",
      "epoch: 31,  batch step: 149, loss: 10.618127822875977\n",
      "epoch: 31,  batch step: 150, loss: 44.89654541015625\n",
      "epoch: 31,  batch step: 151, loss: 3.7831835746765137\n",
      "epoch: 31,  batch step: 152, loss: 58.223419189453125\n",
      "epoch: 31,  batch step: 153, loss: 11.300471305847168\n",
      "epoch: 31,  batch step: 154, loss: 10.764960289001465\n",
      "epoch: 31,  batch step: 155, loss: 15.84242057800293\n",
      "epoch: 31,  batch step: 156, loss: 4.561744689941406\n",
      "epoch: 31,  batch step: 157, loss: 4.803319931030273\n",
      "epoch: 31,  batch step: 158, loss: 67.1171875\n",
      "epoch: 31,  batch step: 159, loss: 5.50894832611084\n",
      "epoch: 31,  batch step: 160, loss: 44.79458236694336\n",
      "epoch: 31,  batch step: 161, loss: 37.699119567871094\n",
      "epoch: 31,  batch step: 162, loss: 39.74128723144531\n",
      "epoch: 31,  batch step: 163, loss: 109.66888427734375\n",
      "epoch: 31,  batch step: 164, loss: 17.866214752197266\n",
      "epoch: 31,  batch step: 165, loss: 88.80264282226562\n",
      "epoch: 31,  batch step: 166, loss: 62.7287712097168\n",
      "epoch: 31,  batch step: 167, loss: 40.164886474609375\n",
      "epoch: 31,  batch step: 168, loss: 7.748354911804199\n",
      "epoch: 31,  batch step: 169, loss: 50.284385681152344\n",
      "epoch: 31,  batch step: 170, loss: 52.186763763427734\n",
      "epoch: 31,  batch step: 171, loss: 6.373175621032715\n",
      "epoch: 31,  batch step: 172, loss: 26.062679290771484\n",
      "epoch: 31,  batch step: 173, loss: 6.52908992767334\n",
      "epoch: 31,  batch step: 174, loss: 71.35946655273438\n",
      "epoch: 31,  batch step: 175, loss: 9.60821533203125\n",
      "epoch: 31,  batch step: 176, loss: 4.247757911682129\n",
      "epoch: 31,  batch step: 177, loss: 9.466419219970703\n",
      "epoch: 31,  batch step: 178, loss: 13.305998802185059\n",
      "epoch: 31,  batch step: 179, loss: 69.19676208496094\n",
      "epoch: 31,  batch step: 180, loss: 13.76921272277832\n",
      "epoch: 31,  batch step: 181, loss: 4.402111530303955\n",
      "epoch: 31,  batch step: 182, loss: 70.90264129638672\n",
      "epoch: 31,  batch step: 183, loss: 42.06916046142578\n",
      "epoch: 31,  batch step: 184, loss: 21.800029754638672\n",
      "epoch: 31,  batch step: 185, loss: 57.35643768310547\n",
      "epoch: 31,  batch step: 186, loss: 33.45116424560547\n",
      "epoch: 31,  batch step: 187, loss: 121.94981384277344\n",
      "epoch: 31,  batch step: 188, loss: 7.040666103363037\n",
      "epoch: 31,  batch step: 189, loss: 127.6771240234375\n",
      "epoch: 31,  batch step: 190, loss: 9.387380599975586\n",
      "epoch: 31,  batch step: 191, loss: 48.38379669189453\n",
      "epoch: 31,  batch step: 192, loss: 101.94183349609375\n",
      "epoch: 31,  batch step: 193, loss: 13.5755033493042\n",
      "epoch: 31,  batch step: 194, loss: 75.99967956542969\n",
      "epoch: 31,  batch step: 195, loss: 23.065059661865234\n",
      "epoch: 31,  batch step: 196, loss: 31.36031150817871\n",
      "epoch: 31,  batch step: 197, loss: 13.58985710144043\n",
      "epoch: 31,  batch step: 198, loss: 82.13238525390625\n",
      "epoch: 31,  batch step: 199, loss: 59.969932556152344\n",
      "epoch: 31,  batch step: 200, loss: 68.78955841064453\n",
      "epoch: 31,  batch step: 201, loss: 36.241966247558594\n",
      "epoch: 31,  batch step: 202, loss: 84.27629852294922\n",
      "epoch: 31,  batch step: 203, loss: 14.945883750915527\n",
      "epoch: 31,  batch step: 204, loss: 7.984267711639404\n",
      "epoch: 31,  batch step: 205, loss: 61.099143981933594\n",
      "epoch: 31,  batch step: 206, loss: 46.607704162597656\n",
      "epoch: 31,  batch step: 207, loss: 17.90164566040039\n",
      "epoch: 31,  batch step: 208, loss: 36.7072868347168\n",
      "epoch: 31,  batch step: 209, loss: 176.21078491210938\n",
      "epoch: 31,  batch step: 210, loss: 10.66499137878418\n",
      "epoch: 31,  batch step: 211, loss: 5.115488052368164\n",
      "epoch: 31,  batch step: 212, loss: 30.384307861328125\n",
      "epoch: 31,  batch step: 213, loss: 13.627187728881836\n",
      "epoch: 31,  batch step: 214, loss: 12.292974472045898\n",
      "epoch: 31,  batch step: 215, loss: 11.268745422363281\n",
      "epoch: 31,  batch step: 216, loss: 47.13285446166992\n",
      "epoch: 31,  batch step: 217, loss: 41.404136657714844\n",
      "epoch: 31,  batch step: 218, loss: 37.50721740722656\n",
      "epoch: 31,  batch step: 219, loss: 9.382978439331055\n",
      "epoch: 31,  batch step: 220, loss: 8.390741348266602\n",
      "epoch: 31,  batch step: 221, loss: 15.190070152282715\n",
      "epoch: 31,  batch step: 222, loss: 12.414342880249023\n",
      "epoch: 31,  batch step: 223, loss: 31.596181869506836\n",
      "epoch: 31,  batch step: 224, loss: 8.078916549682617\n",
      "epoch: 31,  batch step: 225, loss: 10.121383666992188\n",
      "epoch: 31,  batch step: 226, loss: 6.846152305603027\n",
      "epoch: 31,  batch step: 227, loss: 4.287929534912109\n",
      "epoch: 31,  batch step: 228, loss: 19.19984245300293\n",
      "epoch: 31,  batch step: 229, loss: 5.688416957855225\n",
      "epoch: 31,  batch step: 230, loss: 9.639524459838867\n",
      "epoch: 31,  batch step: 231, loss: 10.131087303161621\n",
      "epoch: 31,  batch step: 232, loss: 16.280912399291992\n",
      "epoch: 31,  batch step: 233, loss: 35.65019607543945\n",
      "epoch: 31,  batch step: 234, loss: 18.772247314453125\n",
      "epoch: 31,  batch step: 235, loss: 17.765995025634766\n",
      "epoch: 31,  batch step: 236, loss: 69.46292114257812\n",
      "epoch: 31,  batch step: 237, loss: 9.284358978271484\n",
      "epoch: 31,  batch step: 238, loss: 18.592618942260742\n",
      "epoch: 31,  batch step: 239, loss: 31.432540893554688\n",
      "epoch: 31,  batch step: 240, loss: 16.877094268798828\n",
      "epoch: 31,  batch step: 241, loss: 13.837242126464844\n",
      "epoch: 31,  batch step: 242, loss: 6.20590877532959\n",
      "epoch: 31,  batch step: 243, loss: 8.867140769958496\n",
      "epoch: 31,  batch step: 244, loss: 6.178037643432617\n",
      "epoch: 31,  batch step: 245, loss: 16.414249420166016\n",
      "epoch: 31,  batch step: 246, loss: 72.67854309082031\n",
      "epoch: 31,  batch step: 247, loss: 9.076065063476562\n",
      "epoch: 31,  batch step: 248, loss: 87.06417846679688\n",
      "epoch: 31,  batch step: 249, loss: 51.50751495361328\n",
      "epoch: 31,  batch step: 250, loss: 81.5434799194336\n",
      "epoch: 31,  batch step: 251, loss: 209.8376922607422\n",
      "validation error epoch  31:    tensor(65.1761, device='cuda:0')\n",
      "316\n",
      "epoch: 32,  batch step: 0, loss: 95.17525482177734\n",
      "epoch: 32,  batch step: 1, loss: 59.05222702026367\n",
      "epoch: 32,  batch step: 2, loss: 23.21208953857422\n",
      "epoch: 32,  batch step: 3, loss: 27.47646713256836\n",
      "epoch: 32,  batch step: 4, loss: 63.285064697265625\n",
      "epoch: 32,  batch step: 5, loss: 5.34680700302124\n",
      "epoch: 32,  batch step: 6, loss: 8.62253189086914\n",
      "epoch: 32,  batch step: 7, loss: 16.24073600769043\n",
      "epoch: 32,  batch step: 8, loss: 21.79410171508789\n",
      "epoch: 32,  batch step: 9, loss: 8.890437126159668\n",
      "epoch: 32,  batch step: 10, loss: 32.435482025146484\n",
      "epoch: 32,  batch step: 11, loss: 12.793869018554688\n",
      "epoch: 32,  batch step: 12, loss: 26.154211044311523\n",
      "epoch: 32,  batch step: 13, loss: 4.947941780090332\n",
      "epoch: 32,  batch step: 14, loss: 6.593764305114746\n",
      "epoch: 32,  batch step: 15, loss: 59.93754196166992\n",
      "epoch: 32,  batch step: 16, loss: 70.26089477539062\n",
      "epoch: 32,  batch step: 17, loss: 14.084502220153809\n",
      "epoch: 32,  batch step: 18, loss: 31.558128356933594\n",
      "epoch: 32,  batch step: 19, loss: 45.53744125366211\n",
      "epoch: 32,  batch step: 20, loss: 30.379627227783203\n",
      "epoch: 32,  batch step: 21, loss: 90.8031005859375\n",
      "epoch: 32,  batch step: 22, loss: 159.4921112060547\n",
      "epoch: 32,  batch step: 23, loss: 45.019775390625\n",
      "epoch: 32,  batch step: 24, loss: 14.096061706542969\n",
      "epoch: 32,  batch step: 25, loss: 11.83393383026123\n",
      "epoch: 32,  batch step: 26, loss: 35.35544967651367\n",
      "epoch: 32,  batch step: 27, loss: 44.23634338378906\n",
      "epoch: 32,  batch step: 28, loss: 20.09375762939453\n",
      "epoch: 32,  batch step: 29, loss: 38.55622863769531\n",
      "epoch: 32,  batch step: 30, loss: 76.87374114990234\n",
      "epoch: 32,  batch step: 31, loss: 8.655325889587402\n",
      "epoch: 32,  batch step: 32, loss: 12.92723274230957\n",
      "epoch: 32,  batch step: 33, loss: 34.489898681640625\n",
      "epoch: 32,  batch step: 34, loss: 33.72677230834961\n",
      "epoch: 32,  batch step: 35, loss: 15.897436141967773\n",
      "epoch: 32,  batch step: 36, loss: 7.419799327850342\n",
      "epoch: 32,  batch step: 37, loss: 16.585712432861328\n",
      "epoch: 32,  batch step: 38, loss: 17.767257690429688\n",
      "epoch: 32,  batch step: 39, loss: 33.243194580078125\n",
      "epoch: 32,  batch step: 40, loss: 33.041378021240234\n",
      "epoch: 32,  batch step: 41, loss: 79.39222717285156\n",
      "epoch: 32,  batch step: 42, loss: 95.03326416015625\n",
      "epoch: 32,  batch step: 43, loss: 9.119661331176758\n",
      "epoch: 32,  batch step: 44, loss: 26.23876953125\n",
      "epoch: 32,  batch step: 45, loss: 55.6212272644043\n",
      "epoch: 32,  batch step: 46, loss: 13.223529815673828\n",
      "epoch: 32,  batch step: 47, loss: 4.714402675628662\n",
      "epoch: 32,  batch step: 48, loss: 51.02639389038086\n",
      "epoch: 32,  batch step: 49, loss: 13.959861755371094\n",
      "epoch: 32,  batch step: 50, loss: 5.827622413635254\n",
      "epoch: 32,  batch step: 51, loss: 9.358285903930664\n",
      "epoch: 32,  batch step: 52, loss: 30.574321746826172\n",
      "epoch: 32,  batch step: 53, loss: 12.364667892456055\n",
      "epoch: 32,  batch step: 54, loss: 91.40272521972656\n",
      "epoch: 32,  batch step: 55, loss: 50.11676025390625\n",
      "epoch: 32,  batch step: 56, loss: 35.23701477050781\n",
      "epoch: 32,  batch step: 57, loss: 55.592987060546875\n",
      "epoch: 32,  batch step: 58, loss: 5.402259826660156\n",
      "epoch: 32,  batch step: 59, loss: 70.39998626708984\n",
      "epoch: 32,  batch step: 60, loss: 19.90615463256836\n",
      "epoch: 32,  batch step: 61, loss: 75.61505889892578\n",
      "epoch: 32,  batch step: 62, loss: 17.56355094909668\n",
      "epoch: 32,  batch step: 63, loss: 24.905912399291992\n",
      "epoch: 32,  batch step: 64, loss: 8.486197471618652\n",
      "epoch: 32,  batch step: 65, loss: 5.35914945602417\n",
      "epoch: 32,  batch step: 66, loss: 37.49371337890625\n",
      "epoch: 32,  batch step: 67, loss: 14.621793746948242\n",
      "epoch: 32,  batch step: 68, loss: 8.188867568969727\n",
      "epoch: 32,  batch step: 69, loss: 6.119365215301514\n",
      "epoch: 32,  batch step: 70, loss: 13.526115417480469\n",
      "epoch: 32,  batch step: 71, loss: 55.549774169921875\n",
      "epoch: 32,  batch step: 72, loss: 10.508712768554688\n",
      "epoch: 32,  batch step: 73, loss: 47.89405822753906\n",
      "epoch: 32,  batch step: 74, loss: 8.102212905883789\n",
      "epoch: 32,  batch step: 75, loss: 89.25701141357422\n",
      "epoch: 32,  batch step: 76, loss: 94.04146575927734\n",
      "epoch: 32,  batch step: 77, loss: 7.709218978881836\n",
      "epoch: 32,  batch step: 78, loss: 20.935394287109375\n",
      "epoch: 32,  batch step: 79, loss: 33.937984466552734\n",
      "epoch: 32,  batch step: 80, loss: 8.421913146972656\n",
      "epoch: 32,  batch step: 81, loss: 28.506635665893555\n",
      "epoch: 32,  batch step: 82, loss: 10.449800491333008\n",
      "epoch: 32,  batch step: 83, loss: 5.728280067443848\n",
      "epoch: 32,  batch step: 84, loss: 10.972394943237305\n",
      "epoch: 32,  batch step: 85, loss: 44.86834716796875\n",
      "epoch: 32,  batch step: 86, loss: 74.7590560913086\n",
      "epoch: 32,  batch step: 87, loss: 26.332128524780273\n",
      "epoch: 32,  batch step: 88, loss: 25.077682495117188\n",
      "epoch: 32,  batch step: 89, loss: 7.806512832641602\n",
      "epoch: 32,  batch step: 90, loss: 55.438072204589844\n",
      "epoch: 32,  batch step: 91, loss: 29.151195526123047\n",
      "epoch: 32,  batch step: 92, loss: 7.7043256759643555\n",
      "epoch: 32,  batch step: 93, loss: 95.64328002929688\n",
      "epoch: 32,  batch step: 94, loss: 5.841766357421875\n",
      "epoch: 32,  batch step: 95, loss: 6.25922966003418\n",
      "epoch: 32,  batch step: 96, loss: 15.330146789550781\n",
      "epoch: 32,  batch step: 97, loss: 56.77100372314453\n",
      "epoch: 32,  batch step: 98, loss: 45.186519622802734\n",
      "epoch: 32,  batch step: 99, loss: 73.235107421875\n",
      "epoch: 32,  batch step: 100, loss: 12.850549697875977\n",
      "epoch: 32,  batch step: 101, loss: 23.792308807373047\n",
      "epoch: 32,  batch step: 102, loss: 147.31898498535156\n",
      "epoch: 32,  batch step: 103, loss: 13.431596755981445\n",
      "epoch: 32,  batch step: 104, loss: 30.761932373046875\n",
      "epoch: 32,  batch step: 105, loss: 47.99950408935547\n",
      "epoch: 32,  batch step: 106, loss: 42.76647186279297\n",
      "epoch: 32,  batch step: 107, loss: 17.07063865661621\n",
      "epoch: 32,  batch step: 108, loss: 74.65100860595703\n",
      "epoch: 32,  batch step: 109, loss: 9.774236679077148\n",
      "epoch: 32,  batch step: 110, loss: 15.927907943725586\n",
      "epoch: 32,  batch step: 111, loss: 6.428133964538574\n",
      "epoch: 32,  batch step: 112, loss: 23.17884635925293\n",
      "epoch: 32,  batch step: 113, loss: 92.63027954101562\n",
      "epoch: 32,  batch step: 114, loss: 5.906716823577881\n",
      "epoch: 32,  batch step: 115, loss: 21.087270736694336\n",
      "epoch: 32,  batch step: 116, loss: 14.906482696533203\n",
      "epoch: 32,  batch step: 117, loss: 31.63556671142578\n",
      "epoch: 32,  batch step: 118, loss: 7.5042619705200195\n",
      "epoch: 32,  batch step: 119, loss: 23.247196197509766\n",
      "epoch: 32,  batch step: 120, loss: 147.00953674316406\n",
      "epoch: 32,  batch step: 121, loss: 8.642501831054688\n",
      "epoch: 32,  batch step: 122, loss: 73.723876953125\n",
      "epoch: 32,  batch step: 123, loss: 81.36576080322266\n",
      "epoch: 32,  batch step: 124, loss: 20.687841415405273\n",
      "epoch: 32,  batch step: 125, loss: 23.40492057800293\n",
      "epoch: 32,  batch step: 126, loss: 46.741676330566406\n",
      "epoch: 32,  batch step: 127, loss: 14.029769897460938\n",
      "epoch: 32,  batch step: 128, loss: 8.301725387573242\n",
      "epoch: 32,  batch step: 129, loss: 5.84900426864624\n",
      "epoch: 32,  batch step: 130, loss: 9.153526306152344\n",
      "epoch: 32,  batch step: 131, loss: 24.54330062866211\n",
      "epoch: 32,  batch step: 132, loss: 44.96222686767578\n",
      "epoch: 32,  batch step: 133, loss: 4.4898762702941895\n",
      "epoch: 32,  batch step: 134, loss: 63.23918914794922\n",
      "epoch: 32,  batch step: 135, loss: 6.186770439147949\n",
      "epoch: 32,  batch step: 136, loss: 9.179506301879883\n",
      "epoch: 32,  batch step: 137, loss: 100.19264221191406\n",
      "epoch: 32,  batch step: 138, loss: 82.24362182617188\n",
      "epoch: 32,  batch step: 139, loss: 5.334013938903809\n",
      "epoch: 32,  batch step: 140, loss: 9.008337020874023\n",
      "epoch: 32,  batch step: 141, loss: 37.234130859375\n",
      "epoch: 32,  batch step: 142, loss: 159.26780700683594\n",
      "epoch: 32,  batch step: 143, loss: 75.69940185546875\n",
      "epoch: 32,  batch step: 144, loss: 4.867037773132324\n",
      "epoch: 32,  batch step: 145, loss: 9.355676651000977\n",
      "epoch: 32,  batch step: 146, loss: 19.928354263305664\n",
      "epoch: 32,  batch step: 147, loss: 16.277149200439453\n",
      "epoch: 32,  batch step: 148, loss: 75.87332916259766\n",
      "epoch: 32,  batch step: 149, loss: 15.713115692138672\n",
      "epoch: 32,  batch step: 150, loss: 7.677249908447266\n",
      "epoch: 32,  batch step: 151, loss: 6.81096076965332\n",
      "epoch: 32,  batch step: 152, loss: 9.061946868896484\n",
      "epoch: 32,  batch step: 153, loss: 22.365333557128906\n",
      "epoch: 32,  batch step: 154, loss: 43.76043701171875\n",
      "epoch: 32,  batch step: 155, loss: 28.064186096191406\n",
      "epoch: 32,  batch step: 156, loss: 5.117425918579102\n",
      "epoch: 32,  batch step: 157, loss: 100.54447937011719\n",
      "epoch: 32,  batch step: 158, loss: 105.77177429199219\n",
      "epoch: 32,  batch step: 159, loss: 29.760173797607422\n",
      "epoch: 32,  batch step: 160, loss: 9.377815246582031\n",
      "epoch: 32,  batch step: 161, loss: 49.83226776123047\n",
      "epoch: 32,  batch step: 162, loss: 10.458045959472656\n",
      "epoch: 32,  batch step: 163, loss: 6.440644264221191\n",
      "epoch: 32,  batch step: 164, loss: 5.834493637084961\n",
      "epoch: 32,  batch step: 165, loss: 6.891228675842285\n",
      "epoch: 32,  batch step: 166, loss: 9.058784484863281\n",
      "epoch: 32,  batch step: 167, loss: 7.212342739105225\n",
      "epoch: 32,  batch step: 168, loss: 54.62697982788086\n",
      "epoch: 32,  batch step: 169, loss: 12.572275161743164\n",
      "epoch: 32,  batch step: 170, loss: 30.178709030151367\n",
      "epoch: 32,  batch step: 171, loss: 10.67395305633545\n",
      "epoch: 32,  batch step: 172, loss: 12.449957847595215\n",
      "epoch: 32,  batch step: 173, loss: 60.185604095458984\n",
      "epoch: 32,  batch step: 174, loss: 8.794776916503906\n",
      "epoch: 32,  batch step: 175, loss: 4.378379821777344\n",
      "epoch: 32,  batch step: 176, loss: 7.629864692687988\n",
      "epoch: 32,  batch step: 177, loss: 26.016233444213867\n",
      "epoch: 32,  batch step: 178, loss: 55.17655944824219\n",
      "epoch: 32,  batch step: 179, loss: 12.485374450683594\n",
      "epoch: 32,  batch step: 180, loss: 7.818391799926758\n",
      "epoch: 32,  batch step: 181, loss: 23.314266204833984\n",
      "epoch: 32,  batch step: 182, loss: 14.181562423706055\n",
      "epoch: 32,  batch step: 183, loss: 108.33828735351562\n",
      "epoch: 32,  batch step: 184, loss: 4.935338020324707\n",
      "epoch: 32,  batch step: 185, loss: 11.595325469970703\n",
      "epoch: 32,  batch step: 186, loss: 66.496826171875\n",
      "epoch: 32,  batch step: 187, loss: 42.78547286987305\n",
      "epoch: 32,  batch step: 188, loss: 5.562522888183594\n",
      "epoch: 32,  batch step: 189, loss: 45.0915641784668\n",
      "epoch: 32,  batch step: 190, loss: 49.54293441772461\n",
      "epoch: 32,  batch step: 191, loss: 7.1722564697265625\n",
      "epoch: 32,  batch step: 192, loss: 78.4569320678711\n",
      "epoch: 32,  batch step: 193, loss: 11.883842468261719\n",
      "epoch: 32,  batch step: 194, loss: 6.321280479431152\n",
      "epoch: 32,  batch step: 195, loss: 48.21929931640625\n",
      "epoch: 32,  batch step: 196, loss: 8.15705680847168\n",
      "epoch: 32,  batch step: 197, loss: 11.1813383102417\n",
      "epoch: 32,  batch step: 198, loss: 7.391584396362305\n",
      "epoch: 32,  batch step: 199, loss: 6.17047119140625\n",
      "epoch: 32,  batch step: 200, loss: 26.797405242919922\n",
      "epoch: 32,  batch step: 201, loss: 69.91958618164062\n",
      "epoch: 32,  batch step: 202, loss: 34.6338996887207\n",
      "epoch: 32,  batch step: 203, loss: 28.980199813842773\n",
      "epoch: 32,  batch step: 204, loss: 8.291901588439941\n",
      "epoch: 32,  batch step: 205, loss: 3.8009097576141357\n",
      "epoch: 32,  batch step: 206, loss: 20.238208770751953\n",
      "epoch: 32,  batch step: 207, loss: 11.274996757507324\n",
      "epoch: 32,  batch step: 208, loss: 72.08087158203125\n",
      "epoch: 32,  batch step: 209, loss: 22.386083602905273\n",
      "epoch: 32,  batch step: 210, loss: 85.60301971435547\n",
      "epoch: 32,  batch step: 211, loss: 35.019920349121094\n",
      "epoch: 32,  batch step: 212, loss: 16.894495010375977\n",
      "epoch: 32,  batch step: 213, loss: 43.2104606628418\n",
      "epoch: 32,  batch step: 214, loss: 213.0400848388672\n",
      "epoch: 32,  batch step: 215, loss: 27.879796981811523\n",
      "epoch: 32,  batch step: 216, loss: 19.114818572998047\n",
      "epoch: 32,  batch step: 217, loss: 36.69324493408203\n",
      "epoch: 32,  batch step: 218, loss: 76.93495178222656\n",
      "epoch: 32,  batch step: 219, loss: 79.17060852050781\n",
      "epoch: 32,  batch step: 220, loss: 9.543055534362793\n",
      "epoch: 32,  batch step: 221, loss: 29.19398307800293\n",
      "epoch: 32,  batch step: 222, loss: 15.691839218139648\n",
      "epoch: 32,  batch step: 223, loss: 7.681471824645996\n",
      "epoch: 32,  batch step: 224, loss: 36.612586975097656\n",
      "epoch: 32,  batch step: 225, loss: 55.440162658691406\n",
      "epoch: 32,  batch step: 226, loss: 14.844791412353516\n",
      "epoch: 32,  batch step: 227, loss: 105.83773040771484\n",
      "epoch: 32,  batch step: 228, loss: 58.473236083984375\n",
      "epoch: 32,  batch step: 229, loss: 71.08941650390625\n",
      "epoch: 32,  batch step: 230, loss: 11.238561630249023\n",
      "epoch: 32,  batch step: 231, loss: 63.63793182373047\n",
      "epoch: 32,  batch step: 232, loss: 8.50754451751709\n",
      "epoch: 32,  batch step: 233, loss: 72.5458984375\n",
      "epoch: 32,  batch step: 234, loss: 9.635446548461914\n",
      "epoch: 32,  batch step: 235, loss: 31.900779724121094\n",
      "epoch: 32,  batch step: 236, loss: 5.466299057006836\n",
      "epoch: 32,  batch step: 237, loss: 55.34495162963867\n",
      "epoch: 32,  batch step: 238, loss: 4.0281524658203125\n",
      "epoch: 32,  batch step: 239, loss: 17.516996383666992\n",
      "epoch: 32,  batch step: 240, loss: 8.590460777282715\n",
      "epoch: 32,  batch step: 241, loss: 44.51084899902344\n",
      "epoch: 32,  batch step: 242, loss: 47.35938262939453\n",
      "epoch: 32,  batch step: 243, loss: 4.311251163482666\n",
      "epoch: 32,  batch step: 244, loss: 123.54582977294922\n",
      "epoch: 32,  batch step: 245, loss: 54.38972473144531\n",
      "epoch: 32,  batch step: 246, loss: 9.65085220336914\n",
      "epoch: 32,  batch step: 247, loss: 5.883974075317383\n",
      "epoch: 32,  batch step: 248, loss: 20.149930953979492\n",
      "epoch: 32,  batch step: 249, loss: 82.76647186279297\n",
      "epoch: 32,  batch step: 250, loss: 5.047188758850098\n",
      "epoch: 32,  batch step: 251, loss: 45.157127380371094\n",
      "validation error epoch  32:    tensor(69.7636, device='cuda:0')\n",
      "316\n",
      "epoch: 33,  batch step: 0, loss: 5.606368541717529\n",
      "epoch: 33,  batch step: 1, loss: 4.726585388183594\n",
      "epoch: 33,  batch step: 2, loss: 18.137191772460938\n",
      "epoch: 33,  batch step: 3, loss: 6.2167792320251465\n",
      "epoch: 33,  batch step: 4, loss: 10.68402099609375\n",
      "epoch: 33,  batch step: 5, loss: 8.650554656982422\n",
      "epoch: 33,  batch step: 6, loss: 5.143618583679199\n",
      "epoch: 33,  batch step: 7, loss: 54.92315673828125\n",
      "epoch: 33,  batch step: 8, loss: 8.336063385009766\n",
      "epoch: 33,  batch step: 9, loss: 61.77369689941406\n",
      "epoch: 33,  batch step: 10, loss: 30.494705200195312\n",
      "epoch: 33,  batch step: 11, loss: 24.19917106628418\n",
      "epoch: 33,  batch step: 12, loss: 11.02579402923584\n",
      "epoch: 33,  batch step: 13, loss: 6.075644493103027\n",
      "epoch: 33,  batch step: 14, loss: 7.547469139099121\n",
      "epoch: 33,  batch step: 15, loss: 63.93418884277344\n",
      "epoch: 33,  batch step: 16, loss: 11.874537467956543\n",
      "epoch: 33,  batch step: 17, loss: 12.164295196533203\n",
      "epoch: 33,  batch step: 18, loss: 7.817142486572266\n",
      "epoch: 33,  batch step: 19, loss: 10.654894828796387\n",
      "epoch: 33,  batch step: 20, loss: 6.754992961883545\n",
      "epoch: 33,  batch step: 21, loss: 32.17053985595703\n",
      "epoch: 33,  batch step: 22, loss: 50.55485153198242\n",
      "epoch: 33,  batch step: 23, loss: 21.87698745727539\n",
      "epoch: 33,  batch step: 24, loss: 50.168148040771484\n",
      "epoch: 33,  batch step: 25, loss: 5.075094223022461\n",
      "epoch: 33,  batch step: 26, loss: 9.844367027282715\n",
      "epoch: 33,  batch step: 27, loss: 108.26488494873047\n",
      "epoch: 33,  batch step: 28, loss: 64.35108947753906\n",
      "epoch: 33,  batch step: 29, loss: 6.3244147300720215\n",
      "epoch: 33,  batch step: 30, loss: 7.970798492431641\n",
      "epoch: 33,  batch step: 31, loss: 21.60431480407715\n",
      "epoch: 33,  batch step: 32, loss: 7.682301998138428\n",
      "epoch: 33,  batch step: 33, loss: 3.854306697845459\n",
      "epoch: 33,  batch step: 34, loss: 7.130349636077881\n",
      "epoch: 33,  batch step: 35, loss: 7.532121181488037\n",
      "epoch: 33,  batch step: 36, loss: 8.108821868896484\n",
      "epoch: 33,  batch step: 37, loss: 6.818758964538574\n",
      "epoch: 33,  batch step: 38, loss: 5.542367458343506\n",
      "epoch: 33,  batch step: 39, loss: 17.21221923828125\n",
      "epoch: 33,  batch step: 40, loss: 51.28181457519531\n",
      "epoch: 33,  batch step: 41, loss: 5.378396511077881\n",
      "epoch: 33,  batch step: 42, loss: 61.3850212097168\n",
      "epoch: 33,  batch step: 43, loss: 21.622756958007812\n",
      "epoch: 33,  batch step: 44, loss: 36.898460388183594\n",
      "epoch: 33,  batch step: 45, loss: 8.662712097167969\n",
      "epoch: 33,  batch step: 46, loss: 72.48936462402344\n",
      "epoch: 33,  batch step: 47, loss: 14.951925277709961\n",
      "epoch: 33,  batch step: 48, loss: 22.84676742553711\n",
      "epoch: 33,  batch step: 49, loss: 39.605125427246094\n",
      "epoch: 33,  batch step: 50, loss: 10.878260612487793\n",
      "epoch: 33,  batch step: 51, loss: 41.69668197631836\n",
      "epoch: 33,  batch step: 52, loss: 42.131614685058594\n",
      "epoch: 33,  batch step: 53, loss: 35.49055480957031\n",
      "epoch: 33,  batch step: 54, loss: 9.577584266662598\n",
      "epoch: 33,  batch step: 55, loss: 14.203722953796387\n",
      "epoch: 33,  batch step: 56, loss: 5.849525451660156\n",
      "epoch: 33,  batch step: 57, loss: 35.73249816894531\n",
      "epoch: 33,  batch step: 58, loss: 6.366628646850586\n",
      "epoch: 33,  batch step: 59, loss: 4.661742210388184\n",
      "epoch: 33,  batch step: 60, loss: 86.54133605957031\n",
      "epoch: 33,  batch step: 61, loss: 10.740462303161621\n",
      "epoch: 33,  batch step: 62, loss: 116.01524353027344\n",
      "epoch: 33,  batch step: 63, loss: 6.633883953094482\n",
      "epoch: 33,  batch step: 64, loss: 73.07743835449219\n",
      "epoch: 33,  batch step: 65, loss: 34.2384147644043\n",
      "epoch: 33,  batch step: 66, loss: 9.368502616882324\n",
      "epoch: 33,  batch step: 67, loss: 19.494773864746094\n",
      "epoch: 33,  batch step: 68, loss: 10.95105266571045\n",
      "epoch: 33,  batch step: 69, loss: 47.027565002441406\n",
      "epoch: 33,  batch step: 70, loss: 41.47335433959961\n",
      "epoch: 33,  batch step: 71, loss: 159.0436553955078\n",
      "epoch: 33,  batch step: 72, loss: 6.3565778732299805\n",
      "epoch: 33,  batch step: 73, loss: 6.852729797363281\n",
      "epoch: 33,  batch step: 74, loss: 37.32737731933594\n",
      "epoch: 33,  batch step: 75, loss: 6.368350028991699\n",
      "epoch: 33,  batch step: 76, loss: 21.649093627929688\n",
      "epoch: 33,  batch step: 77, loss: 63.93330383300781\n",
      "epoch: 33,  batch step: 78, loss: 8.770425796508789\n",
      "epoch: 33,  batch step: 79, loss: 49.083675384521484\n",
      "epoch: 33,  batch step: 80, loss: 11.245794296264648\n",
      "epoch: 33,  batch step: 81, loss: 116.77987670898438\n",
      "epoch: 33,  batch step: 82, loss: 5.561236381530762\n",
      "epoch: 33,  batch step: 83, loss: 29.874839782714844\n",
      "epoch: 33,  batch step: 84, loss: 20.73117446899414\n",
      "epoch: 33,  batch step: 85, loss: 14.917646408081055\n",
      "epoch: 33,  batch step: 86, loss: 100.14811706542969\n",
      "epoch: 33,  batch step: 87, loss: 5.688690185546875\n",
      "epoch: 33,  batch step: 88, loss: 5.160330772399902\n",
      "epoch: 33,  batch step: 89, loss: 28.641942977905273\n",
      "epoch: 33,  batch step: 90, loss: 21.552169799804688\n",
      "epoch: 33,  batch step: 91, loss: 110.83049011230469\n",
      "epoch: 33,  batch step: 92, loss: 15.829801559448242\n",
      "epoch: 33,  batch step: 93, loss: 6.774569511413574\n",
      "epoch: 33,  batch step: 94, loss: 31.303726196289062\n",
      "epoch: 33,  batch step: 95, loss: 53.34800720214844\n",
      "epoch: 33,  batch step: 96, loss: 39.000789642333984\n",
      "epoch: 33,  batch step: 97, loss: 90.28858947753906\n",
      "epoch: 33,  batch step: 98, loss: 15.303445816040039\n",
      "epoch: 33,  batch step: 99, loss: 5.1615891456604\n",
      "epoch: 33,  batch step: 100, loss: 29.095218658447266\n",
      "epoch: 33,  batch step: 101, loss: 18.699087142944336\n",
      "epoch: 33,  batch step: 102, loss: 11.526763916015625\n",
      "epoch: 33,  batch step: 103, loss: 21.10513687133789\n",
      "epoch: 33,  batch step: 104, loss: 35.63505172729492\n",
      "epoch: 33,  batch step: 105, loss: 6.268799781799316\n",
      "epoch: 33,  batch step: 106, loss: 20.293546676635742\n",
      "epoch: 33,  batch step: 107, loss: 7.438455104827881\n",
      "epoch: 33,  batch step: 108, loss: 27.85650634765625\n",
      "epoch: 33,  batch step: 109, loss: 75.11022186279297\n",
      "epoch: 33,  batch step: 110, loss: 60.77570343017578\n",
      "epoch: 33,  batch step: 111, loss: 87.54051208496094\n",
      "epoch: 33,  batch step: 112, loss: 39.31748580932617\n",
      "epoch: 33,  batch step: 113, loss: 14.38142204284668\n",
      "epoch: 33,  batch step: 114, loss: 5.76987361907959\n",
      "epoch: 33,  batch step: 115, loss: 8.009397506713867\n",
      "epoch: 33,  batch step: 116, loss: 4.510616302490234\n",
      "epoch: 33,  batch step: 117, loss: 72.02889251708984\n",
      "epoch: 33,  batch step: 118, loss: 6.089201927185059\n",
      "epoch: 33,  batch step: 119, loss: 26.837970733642578\n",
      "epoch: 33,  batch step: 120, loss: 10.037796020507812\n",
      "epoch: 33,  batch step: 121, loss: 4.57454252243042\n",
      "epoch: 33,  batch step: 122, loss: 18.873952865600586\n",
      "epoch: 33,  batch step: 123, loss: 12.121511459350586\n",
      "epoch: 33,  batch step: 124, loss: 13.8392972946167\n",
      "epoch: 33,  batch step: 125, loss: 146.83914184570312\n",
      "epoch: 33,  batch step: 126, loss: 37.983642578125\n",
      "epoch: 33,  batch step: 127, loss: 56.29423141479492\n",
      "epoch: 33,  batch step: 128, loss: 20.144437789916992\n",
      "epoch: 33,  batch step: 129, loss: 46.24059295654297\n",
      "epoch: 33,  batch step: 130, loss: 10.582683563232422\n",
      "epoch: 33,  batch step: 131, loss: 5.519076347351074\n",
      "epoch: 33,  batch step: 132, loss: 13.280489921569824\n",
      "epoch: 33,  batch step: 133, loss: 12.916926383972168\n",
      "epoch: 33,  batch step: 134, loss: 16.13144302368164\n",
      "epoch: 33,  batch step: 135, loss: 55.04417419433594\n",
      "epoch: 33,  batch step: 136, loss: 11.129057884216309\n",
      "epoch: 33,  batch step: 137, loss: 44.841590881347656\n",
      "epoch: 33,  batch step: 138, loss: 11.302679061889648\n",
      "epoch: 33,  batch step: 139, loss: 278.7425537109375\n",
      "epoch: 33,  batch step: 140, loss: 17.029338836669922\n",
      "epoch: 33,  batch step: 141, loss: 151.49526977539062\n",
      "epoch: 33,  batch step: 142, loss: 5.045098304748535\n",
      "epoch: 33,  batch step: 143, loss: 10.244189262390137\n",
      "epoch: 33,  batch step: 144, loss: 5.561239242553711\n",
      "epoch: 33,  batch step: 145, loss: 38.48943328857422\n",
      "epoch: 33,  batch step: 146, loss: 9.07504653930664\n",
      "epoch: 33,  batch step: 147, loss: 51.41034698486328\n",
      "epoch: 33,  batch step: 148, loss: 21.85772705078125\n",
      "epoch: 33,  batch step: 149, loss: 13.588478088378906\n",
      "epoch: 33,  batch step: 150, loss: 18.698001861572266\n",
      "epoch: 33,  batch step: 151, loss: 9.102813720703125\n",
      "epoch: 33,  batch step: 152, loss: 21.182846069335938\n",
      "epoch: 33,  batch step: 153, loss: 77.32606506347656\n",
      "epoch: 33,  batch step: 154, loss: 74.20491027832031\n",
      "epoch: 33,  batch step: 155, loss: 26.160001754760742\n",
      "epoch: 33,  batch step: 156, loss: 25.122535705566406\n",
      "epoch: 33,  batch step: 157, loss: 8.015422821044922\n",
      "epoch: 33,  batch step: 158, loss: 49.32320785522461\n",
      "epoch: 33,  batch step: 159, loss: 8.744140625\n",
      "epoch: 33,  batch step: 160, loss: 118.39695739746094\n",
      "epoch: 33,  batch step: 161, loss: 10.408048629760742\n",
      "epoch: 33,  batch step: 162, loss: 8.694586753845215\n",
      "epoch: 33,  batch step: 163, loss: 28.405120849609375\n",
      "epoch: 33,  batch step: 164, loss: 6.483129024505615\n",
      "epoch: 33,  batch step: 165, loss: 30.65615463256836\n",
      "epoch: 33,  batch step: 166, loss: 61.097930908203125\n",
      "epoch: 33,  batch step: 167, loss: 6.621150970458984\n",
      "epoch: 33,  batch step: 168, loss: 46.10826873779297\n",
      "epoch: 33,  batch step: 169, loss: 10.143298149108887\n",
      "epoch: 33,  batch step: 170, loss: 10.83350944519043\n",
      "epoch: 33,  batch step: 171, loss: 13.495811462402344\n",
      "epoch: 33,  batch step: 172, loss: 4.895553112030029\n",
      "epoch: 33,  batch step: 173, loss: 107.6458969116211\n",
      "epoch: 33,  batch step: 174, loss: 24.660062789916992\n",
      "epoch: 33,  batch step: 175, loss: 46.70922088623047\n",
      "epoch: 33,  batch step: 176, loss: 53.526084899902344\n",
      "epoch: 33,  batch step: 177, loss: 10.353282928466797\n",
      "epoch: 33,  batch step: 178, loss: 29.39340591430664\n",
      "epoch: 33,  batch step: 179, loss: 12.257288932800293\n",
      "epoch: 33,  batch step: 180, loss: 9.527199745178223\n",
      "epoch: 33,  batch step: 181, loss: 4.724999904632568\n",
      "epoch: 33,  batch step: 182, loss: 4.674367427825928\n",
      "epoch: 33,  batch step: 183, loss: 96.94755554199219\n",
      "epoch: 33,  batch step: 184, loss: 8.98813247680664\n",
      "epoch: 33,  batch step: 185, loss: 7.313159942626953\n",
      "epoch: 33,  batch step: 186, loss: 10.44091796875\n",
      "epoch: 33,  batch step: 187, loss: 11.419646263122559\n",
      "epoch: 33,  batch step: 188, loss: 77.06585693359375\n",
      "epoch: 33,  batch step: 189, loss: 43.68439865112305\n",
      "epoch: 33,  batch step: 190, loss: 9.045209884643555\n",
      "epoch: 33,  batch step: 191, loss: 8.032812118530273\n",
      "epoch: 33,  batch step: 192, loss: 8.22714900970459\n",
      "epoch: 33,  batch step: 193, loss: 130.97097778320312\n",
      "epoch: 33,  batch step: 194, loss: 129.86685180664062\n",
      "epoch: 33,  batch step: 195, loss: 68.75524139404297\n",
      "epoch: 33,  batch step: 196, loss: 15.43115234375\n",
      "epoch: 33,  batch step: 197, loss: 76.52949523925781\n",
      "epoch: 33,  batch step: 198, loss: 56.71049118041992\n",
      "epoch: 33,  batch step: 199, loss: 50.59400177001953\n",
      "epoch: 33,  batch step: 200, loss: 6.612318992614746\n",
      "epoch: 33,  batch step: 201, loss: 31.125370025634766\n",
      "epoch: 33,  batch step: 202, loss: 42.45989227294922\n",
      "epoch: 33,  batch step: 203, loss: 54.97146224975586\n",
      "epoch: 33,  batch step: 204, loss: 6.677023887634277\n",
      "epoch: 33,  batch step: 205, loss: 15.260652542114258\n",
      "epoch: 33,  batch step: 206, loss: 5.0639238357543945\n",
      "epoch: 33,  batch step: 207, loss: 5.049056053161621\n",
      "epoch: 33,  batch step: 208, loss: 6.674310207366943\n",
      "epoch: 33,  batch step: 209, loss: 5.979950904846191\n",
      "epoch: 33,  batch step: 210, loss: 6.784602642059326\n",
      "epoch: 33,  batch step: 211, loss: 52.44103240966797\n",
      "epoch: 33,  batch step: 212, loss: 6.005556106567383\n",
      "epoch: 33,  batch step: 213, loss: 10.721261978149414\n",
      "epoch: 33,  batch step: 214, loss: 4.560528755187988\n",
      "epoch: 33,  batch step: 215, loss: 73.29754638671875\n",
      "epoch: 33,  batch step: 216, loss: 69.81919860839844\n",
      "epoch: 33,  batch step: 217, loss: 7.128007888793945\n",
      "epoch: 33,  batch step: 218, loss: 23.68355369567871\n",
      "epoch: 33,  batch step: 219, loss: 74.50873565673828\n",
      "epoch: 33,  batch step: 220, loss: 53.862770080566406\n",
      "epoch: 33,  batch step: 221, loss: 62.6893310546875\n",
      "epoch: 33,  batch step: 222, loss: 43.74064254760742\n",
      "epoch: 33,  batch step: 223, loss: 72.00809478759766\n",
      "epoch: 33,  batch step: 224, loss: 79.01795959472656\n",
      "epoch: 33,  batch step: 225, loss: 75.6594467163086\n",
      "epoch: 33,  batch step: 226, loss: 19.190290451049805\n",
      "epoch: 33,  batch step: 227, loss: 47.06443405151367\n",
      "epoch: 33,  batch step: 228, loss: 17.724132537841797\n",
      "epoch: 33,  batch step: 229, loss: 59.59721374511719\n",
      "epoch: 33,  batch step: 230, loss: 21.87084197998047\n",
      "epoch: 33,  batch step: 231, loss: 78.62989807128906\n",
      "epoch: 33,  batch step: 232, loss: 39.650333404541016\n",
      "epoch: 33,  batch step: 233, loss: 99.26815795898438\n",
      "epoch: 33,  batch step: 234, loss: 6.480952262878418\n",
      "epoch: 33,  batch step: 235, loss: 57.44297790527344\n",
      "epoch: 33,  batch step: 236, loss: 40.5789794921875\n",
      "epoch: 33,  batch step: 237, loss: 8.566717147827148\n",
      "epoch: 33,  batch step: 238, loss: 57.22312545776367\n",
      "epoch: 33,  batch step: 239, loss: 121.36528015136719\n",
      "epoch: 33,  batch step: 240, loss: 73.6204605102539\n",
      "epoch: 33,  batch step: 241, loss: 36.92573547363281\n",
      "epoch: 33,  batch step: 242, loss: 20.028350830078125\n",
      "epoch: 33,  batch step: 243, loss: 40.9433708190918\n",
      "epoch: 33,  batch step: 244, loss: 14.774354934692383\n",
      "epoch: 33,  batch step: 245, loss: 6.337277889251709\n",
      "epoch: 33,  batch step: 246, loss: 13.606412887573242\n",
      "epoch: 33,  batch step: 247, loss: 75.57746887207031\n",
      "epoch: 33,  batch step: 248, loss: 43.169219970703125\n",
      "epoch: 33,  batch step: 249, loss: 4.463347911834717\n",
      "epoch: 33,  batch step: 250, loss: 240.19247436523438\n",
      "epoch: 33,  batch step: 251, loss: 29.01006507873535\n",
      "validation error epoch  33:    tensor(72.1137, device='cuda:0')\n",
      "316\n",
      "epoch: 34,  batch step: 0, loss: 30.68082046508789\n",
      "epoch: 34,  batch step: 1, loss: 11.595752716064453\n",
      "epoch: 34,  batch step: 2, loss: 3.9232802391052246\n",
      "epoch: 34,  batch step: 3, loss: 28.23474884033203\n",
      "epoch: 34,  batch step: 4, loss: 174.41026306152344\n",
      "epoch: 34,  batch step: 5, loss: 12.291301727294922\n",
      "epoch: 34,  batch step: 6, loss: 9.999815940856934\n",
      "epoch: 34,  batch step: 7, loss: 13.75757884979248\n",
      "epoch: 34,  batch step: 8, loss: 126.24789428710938\n",
      "epoch: 34,  batch step: 9, loss: 75.74935913085938\n",
      "epoch: 34,  batch step: 10, loss: 43.80437469482422\n",
      "epoch: 34,  batch step: 11, loss: 31.121387481689453\n",
      "epoch: 34,  batch step: 12, loss: 5.141178131103516\n",
      "epoch: 34,  batch step: 13, loss: 8.141032218933105\n",
      "epoch: 34,  batch step: 14, loss: 16.60013771057129\n",
      "epoch: 34,  batch step: 15, loss: 24.448583602905273\n",
      "epoch: 34,  batch step: 16, loss: 118.36680603027344\n",
      "epoch: 34,  batch step: 17, loss: 6.824153900146484\n",
      "epoch: 34,  batch step: 18, loss: 15.54211711883545\n",
      "epoch: 34,  batch step: 19, loss: 38.11351013183594\n",
      "epoch: 34,  batch step: 20, loss: 11.293039321899414\n",
      "epoch: 34,  batch step: 21, loss: 21.21269989013672\n",
      "epoch: 34,  batch step: 22, loss: 100.69346618652344\n",
      "epoch: 34,  batch step: 23, loss: 13.891932487487793\n",
      "epoch: 34,  batch step: 24, loss: 17.080842971801758\n",
      "epoch: 34,  batch step: 25, loss: 14.05756950378418\n",
      "epoch: 34,  batch step: 26, loss: 13.982344627380371\n",
      "epoch: 34,  batch step: 27, loss: 57.095191955566406\n",
      "epoch: 34,  batch step: 28, loss: 49.99784469604492\n",
      "epoch: 34,  batch step: 29, loss: 95.74578857421875\n",
      "epoch: 34,  batch step: 30, loss: 7.305488586425781\n",
      "epoch: 34,  batch step: 31, loss: 20.26145362854004\n",
      "epoch: 34,  batch step: 32, loss: 17.772319793701172\n",
      "epoch: 34,  batch step: 33, loss: 46.84093475341797\n",
      "epoch: 34,  batch step: 34, loss: 10.561704635620117\n",
      "epoch: 34,  batch step: 35, loss: 6.779314041137695\n",
      "epoch: 34,  batch step: 36, loss: 13.955316543579102\n",
      "epoch: 34,  batch step: 37, loss: 39.404075622558594\n",
      "epoch: 34,  batch step: 38, loss: 72.4604263305664\n",
      "epoch: 34,  batch step: 39, loss: 56.46954345703125\n",
      "epoch: 34,  batch step: 40, loss: 13.254695892333984\n",
      "epoch: 34,  batch step: 41, loss: 5.852010250091553\n",
      "epoch: 34,  batch step: 42, loss: 2.7523818016052246\n",
      "epoch: 34,  batch step: 43, loss: 124.85111999511719\n",
      "epoch: 34,  batch step: 44, loss: 31.271141052246094\n",
      "epoch: 34,  batch step: 45, loss: 16.490886688232422\n",
      "epoch: 34,  batch step: 46, loss: 54.850887298583984\n",
      "epoch: 34,  batch step: 47, loss: 5.436014175415039\n",
      "epoch: 34,  batch step: 48, loss: 21.207035064697266\n",
      "epoch: 34,  batch step: 49, loss: 52.61476135253906\n",
      "epoch: 34,  batch step: 50, loss: 5.207265377044678\n",
      "epoch: 34,  batch step: 51, loss: 56.33543014526367\n",
      "epoch: 34,  batch step: 52, loss: 11.608195304870605\n",
      "epoch: 34,  batch step: 53, loss: 44.60105514526367\n",
      "epoch: 34,  batch step: 54, loss: 58.96403121948242\n",
      "epoch: 34,  batch step: 55, loss: 7.539971828460693\n",
      "epoch: 34,  batch step: 56, loss: 104.16349792480469\n",
      "epoch: 34,  batch step: 57, loss: 15.714086532592773\n",
      "epoch: 34,  batch step: 58, loss: 32.513099670410156\n",
      "epoch: 34,  batch step: 59, loss: 35.514923095703125\n",
      "epoch: 34,  batch step: 60, loss: 37.795143127441406\n",
      "epoch: 34,  batch step: 61, loss: 63.732784271240234\n",
      "epoch: 34,  batch step: 62, loss: 19.000911712646484\n",
      "epoch: 34,  batch step: 63, loss: 30.552562713623047\n",
      "epoch: 34,  batch step: 64, loss: 41.82025146484375\n",
      "epoch: 34,  batch step: 65, loss: 67.66287231445312\n",
      "epoch: 34,  batch step: 66, loss: 4.156731128692627\n",
      "epoch: 34,  batch step: 67, loss: 7.348943710327148\n",
      "epoch: 34,  batch step: 68, loss: 70.58271026611328\n",
      "epoch: 34,  batch step: 69, loss: 11.809806823730469\n",
      "epoch: 34,  batch step: 70, loss: 6.371311187744141\n",
      "epoch: 34,  batch step: 71, loss: 29.797840118408203\n",
      "epoch: 34,  batch step: 72, loss: 16.243183135986328\n",
      "epoch: 34,  batch step: 73, loss: 12.989456176757812\n",
      "epoch: 34,  batch step: 74, loss: 10.45119857788086\n",
      "epoch: 34,  batch step: 75, loss: 32.55972671508789\n",
      "epoch: 34,  batch step: 76, loss: 8.306260108947754\n",
      "epoch: 34,  batch step: 77, loss: 83.44164276123047\n",
      "epoch: 34,  batch step: 78, loss: 66.8955307006836\n",
      "epoch: 34,  batch step: 79, loss: 191.80361938476562\n",
      "epoch: 34,  batch step: 80, loss: 97.36919403076172\n",
      "epoch: 34,  batch step: 81, loss: 10.712236404418945\n",
      "epoch: 34,  batch step: 82, loss: 7.2916975021362305\n",
      "epoch: 34,  batch step: 83, loss: 69.65129852294922\n",
      "epoch: 34,  batch step: 84, loss: 42.809906005859375\n",
      "epoch: 34,  batch step: 85, loss: 14.786761283874512\n",
      "epoch: 34,  batch step: 86, loss: 10.412530899047852\n",
      "epoch: 34,  batch step: 87, loss: 10.2325439453125\n",
      "epoch: 34,  batch step: 88, loss: 19.000158309936523\n",
      "epoch: 34,  batch step: 89, loss: 6.560522079467773\n",
      "epoch: 34,  batch step: 90, loss: 66.35198211669922\n",
      "epoch: 34,  batch step: 91, loss: 53.79644775390625\n",
      "epoch: 34,  batch step: 92, loss: 8.616436958312988\n",
      "epoch: 34,  batch step: 93, loss: 71.03305053710938\n",
      "epoch: 34,  batch step: 94, loss: 25.592914581298828\n",
      "epoch: 34,  batch step: 95, loss: 65.3094711303711\n",
      "epoch: 34,  batch step: 96, loss: 9.151188850402832\n",
      "epoch: 34,  batch step: 97, loss: 26.511539459228516\n",
      "epoch: 34,  batch step: 98, loss: 78.7502670288086\n",
      "epoch: 34,  batch step: 99, loss: 64.99205780029297\n",
      "epoch: 34,  batch step: 100, loss: 18.432289123535156\n",
      "epoch: 34,  batch step: 101, loss: 30.423194885253906\n",
      "epoch: 34,  batch step: 102, loss: 8.070034980773926\n",
      "epoch: 34,  batch step: 103, loss: 20.68130111694336\n",
      "epoch: 34,  batch step: 104, loss: 39.290245056152344\n",
      "epoch: 34,  batch step: 105, loss: 6.003771781921387\n",
      "epoch: 34,  batch step: 106, loss: 44.39487838745117\n",
      "epoch: 34,  batch step: 107, loss: 27.08289909362793\n",
      "epoch: 34,  batch step: 108, loss: 27.0177001953125\n",
      "epoch: 34,  batch step: 109, loss: 19.638187408447266\n",
      "epoch: 34,  batch step: 110, loss: 5.843356132507324\n",
      "epoch: 34,  batch step: 111, loss: 5.413743019104004\n",
      "epoch: 34,  batch step: 112, loss: 8.067899703979492\n",
      "epoch: 34,  batch step: 113, loss: 9.812718391418457\n",
      "epoch: 34,  batch step: 114, loss: 30.46930694580078\n",
      "epoch: 34,  batch step: 115, loss: 34.6849479675293\n",
      "epoch: 34,  batch step: 116, loss: 164.51455688476562\n",
      "epoch: 34,  batch step: 117, loss: 53.58794403076172\n",
      "epoch: 34,  batch step: 118, loss: 6.602963447570801\n",
      "epoch: 34,  batch step: 119, loss: 47.324066162109375\n",
      "epoch: 34,  batch step: 120, loss: 10.367960929870605\n",
      "epoch: 34,  batch step: 121, loss: 21.42274284362793\n",
      "epoch: 34,  batch step: 122, loss: 5.142656326293945\n",
      "epoch: 34,  batch step: 123, loss: 17.97372055053711\n",
      "epoch: 34,  batch step: 124, loss: 14.544416427612305\n",
      "epoch: 34,  batch step: 125, loss: 15.013883590698242\n",
      "epoch: 34,  batch step: 126, loss: 47.719482421875\n",
      "epoch: 34,  batch step: 127, loss: 61.87635040283203\n",
      "epoch: 34,  batch step: 128, loss: 40.27131652832031\n",
      "epoch: 34,  batch step: 129, loss: 41.91837692260742\n",
      "epoch: 34,  batch step: 130, loss: 7.577144622802734\n",
      "epoch: 34,  batch step: 131, loss: 5.934735298156738\n",
      "epoch: 34,  batch step: 132, loss: 6.318839073181152\n",
      "epoch: 34,  batch step: 133, loss: 60.60796356201172\n",
      "epoch: 34,  batch step: 134, loss: 10.148295402526855\n",
      "epoch: 34,  batch step: 135, loss: 8.072208404541016\n",
      "epoch: 34,  batch step: 136, loss: 9.995434761047363\n",
      "epoch: 34,  batch step: 137, loss: 4.521358489990234\n",
      "epoch: 34,  batch step: 138, loss: 4.801095485687256\n",
      "epoch: 34,  batch step: 139, loss: 51.76548767089844\n",
      "epoch: 34,  batch step: 140, loss: 6.321403503417969\n",
      "epoch: 34,  batch step: 141, loss: 8.974313735961914\n",
      "epoch: 34,  batch step: 142, loss: 48.14698791503906\n",
      "epoch: 34,  batch step: 143, loss: 28.37063217163086\n",
      "epoch: 34,  batch step: 144, loss: 33.415855407714844\n",
      "epoch: 34,  batch step: 145, loss: 17.514442443847656\n",
      "epoch: 34,  batch step: 146, loss: 35.77763366699219\n",
      "epoch: 34,  batch step: 147, loss: 92.7004623413086\n",
      "epoch: 34,  batch step: 148, loss: 14.765212059020996\n",
      "epoch: 34,  batch step: 149, loss: 101.67638397216797\n",
      "epoch: 34,  batch step: 150, loss: 35.53446960449219\n",
      "epoch: 34,  batch step: 151, loss: 5.29036808013916\n",
      "epoch: 34,  batch step: 152, loss: 6.19948673248291\n",
      "epoch: 34,  batch step: 153, loss: 27.401012420654297\n",
      "epoch: 34,  batch step: 154, loss: 35.659427642822266\n",
      "epoch: 34,  batch step: 155, loss: 17.57433319091797\n",
      "epoch: 34,  batch step: 156, loss: 42.823909759521484\n",
      "epoch: 34,  batch step: 157, loss: 11.335115432739258\n",
      "epoch: 34,  batch step: 158, loss: 13.678647994995117\n",
      "epoch: 34,  batch step: 159, loss: 5.929767608642578\n",
      "epoch: 34,  batch step: 160, loss: 5.581864833831787\n",
      "epoch: 34,  batch step: 161, loss: 86.33665466308594\n",
      "epoch: 34,  batch step: 162, loss: 5.111719131469727\n",
      "epoch: 34,  batch step: 163, loss: 8.344303131103516\n",
      "epoch: 34,  batch step: 164, loss: 41.89582061767578\n",
      "epoch: 34,  batch step: 165, loss: 24.808788299560547\n",
      "epoch: 34,  batch step: 166, loss: 6.252580165863037\n",
      "epoch: 34,  batch step: 167, loss: 7.828861236572266\n",
      "epoch: 34,  batch step: 168, loss: 3.8612618446350098\n",
      "epoch: 34,  batch step: 169, loss: 9.084772109985352\n",
      "epoch: 34,  batch step: 170, loss: 7.390438079833984\n",
      "epoch: 34,  batch step: 171, loss: 24.325775146484375\n",
      "epoch: 34,  batch step: 172, loss: 13.944498062133789\n",
      "epoch: 34,  batch step: 173, loss: 50.10436248779297\n",
      "epoch: 34,  batch step: 174, loss: 8.23980712890625\n",
      "epoch: 34,  batch step: 175, loss: 10.265975952148438\n",
      "epoch: 34,  batch step: 176, loss: 102.45089721679688\n",
      "epoch: 34,  batch step: 177, loss: 63.469932556152344\n",
      "epoch: 34,  batch step: 178, loss: 26.95555877685547\n",
      "epoch: 34,  batch step: 179, loss: 4.872921466827393\n",
      "epoch: 34,  batch step: 180, loss: 25.152381896972656\n",
      "epoch: 34,  batch step: 181, loss: 39.7352294921875\n",
      "epoch: 34,  batch step: 182, loss: 15.681116104125977\n",
      "epoch: 34,  batch step: 183, loss: 11.308058738708496\n",
      "epoch: 34,  batch step: 184, loss: 6.0342512130737305\n",
      "epoch: 34,  batch step: 185, loss: 57.61096954345703\n",
      "epoch: 34,  batch step: 186, loss: 103.98429870605469\n",
      "epoch: 34,  batch step: 187, loss: 13.057196617126465\n",
      "epoch: 34,  batch step: 188, loss: 84.51641845703125\n",
      "epoch: 34,  batch step: 189, loss: 5.271920204162598\n",
      "epoch: 34,  batch step: 190, loss: 41.778953552246094\n",
      "epoch: 34,  batch step: 191, loss: 2.9333388805389404\n",
      "epoch: 34,  batch step: 192, loss: 27.516197204589844\n",
      "epoch: 34,  batch step: 193, loss: 7.9320831298828125\n",
      "epoch: 34,  batch step: 194, loss: 70.42713165283203\n",
      "epoch: 34,  batch step: 195, loss: 15.111631393432617\n",
      "epoch: 34,  batch step: 196, loss: 34.263404846191406\n",
      "epoch: 34,  batch step: 197, loss: 68.1290283203125\n",
      "epoch: 34,  batch step: 198, loss: 12.736720085144043\n",
      "epoch: 34,  batch step: 199, loss: 5.647220134735107\n",
      "epoch: 34,  batch step: 200, loss: 21.684057235717773\n",
      "epoch: 34,  batch step: 201, loss: 13.717432975769043\n",
      "epoch: 34,  batch step: 202, loss: 4.093481063842773\n",
      "epoch: 34,  batch step: 203, loss: 58.51598358154297\n",
      "epoch: 34,  batch step: 204, loss: 3.339489698410034\n",
      "epoch: 34,  batch step: 205, loss: 5.62222957611084\n",
      "epoch: 34,  batch step: 206, loss: 4.81156063079834\n",
      "epoch: 34,  batch step: 207, loss: 54.98704147338867\n",
      "epoch: 34,  batch step: 208, loss: 151.58193969726562\n",
      "epoch: 34,  batch step: 209, loss: 52.1031608581543\n",
      "epoch: 34,  batch step: 210, loss: 67.68136596679688\n",
      "epoch: 34,  batch step: 211, loss: 10.80237102508545\n",
      "epoch: 34,  batch step: 212, loss: 17.387123107910156\n",
      "epoch: 34,  batch step: 213, loss: 5.578070640563965\n",
      "epoch: 34,  batch step: 214, loss: 5.487888336181641\n",
      "epoch: 34,  batch step: 215, loss: 45.16338348388672\n",
      "epoch: 34,  batch step: 216, loss: 9.47244644165039\n",
      "epoch: 34,  batch step: 217, loss: 41.31415557861328\n",
      "epoch: 34,  batch step: 218, loss: 45.9219970703125\n",
      "epoch: 34,  batch step: 219, loss: 54.320125579833984\n",
      "epoch: 34,  batch step: 220, loss: 7.1282196044921875\n",
      "epoch: 34,  batch step: 221, loss: 11.007844924926758\n",
      "epoch: 34,  batch step: 222, loss: 53.43291473388672\n",
      "epoch: 34,  batch step: 223, loss: 39.51990509033203\n",
      "epoch: 34,  batch step: 224, loss: 44.7403564453125\n",
      "epoch: 34,  batch step: 225, loss: 46.05199432373047\n",
      "epoch: 34,  batch step: 226, loss: 13.824400901794434\n",
      "epoch: 34,  batch step: 227, loss: 82.94975280761719\n",
      "epoch: 34,  batch step: 228, loss: 28.141347885131836\n",
      "epoch: 34,  batch step: 229, loss: 10.644268035888672\n",
      "epoch: 34,  batch step: 230, loss: 31.605886459350586\n",
      "epoch: 34,  batch step: 231, loss: 7.672527313232422\n",
      "epoch: 34,  batch step: 232, loss: 26.803447723388672\n",
      "epoch: 34,  batch step: 233, loss: 4.274907112121582\n",
      "epoch: 34,  batch step: 234, loss: 4.868871688842773\n",
      "epoch: 34,  batch step: 235, loss: 28.3794002532959\n",
      "epoch: 34,  batch step: 236, loss: 7.118952751159668\n",
      "epoch: 34,  batch step: 237, loss: 7.566654205322266\n",
      "epoch: 34,  batch step: 238, loss: 8.982355117797852\n",
      "epoch: 34,  batch step: 239, loss: 23.652395248413086\n",
      "epoch: 34,  batch step: 240, loss: 6.840717315673828\n",
      "epoch: 34,  batch step: 241, loss: 26.908658981323242\n",
      "epoch: 34,  batch step: 242, loss: 40.99175262451172\n",
      "epoch: 34,  batch step: 243, loss: 39.406951904296875\n",
      "epoch: 34,  batch step: 244, loss: 77.17597961425781\n",
      "epoch: 34,  batch step: 245, loss: 67.46422576904297\n",
      "epoch: 34,  batch step: 246, loss: 11.549238204956055\n",
      "epoch: 34,  batch step: 247, loss: 5.997220039367676\n",
      "epoch: 34,  batch step: 248, loss: 22.00765037536621\n",
      "epoch: 34,  batch step: 249, loss: 6.664660453796387\n",
      "epoch: 34,  batch step: 250, loss: 21.859424591064453\n",
      "epoch: 34,  batch step: 251, loss: 30.050559997558594\n",
      "validation error epoch  34:    tensor(70.3210, device='cuda:0')\n",
      "316\n",
      "epoch: 35,  batch step: 0, loss: 9.785011291503906\n",
      "epoch: 35,  batch step: 1, loss: 198.50503540039062\n",
      "epoch: 35,  batch step: 2, loss: 101.8530502319336\n",
      "epoch: 35,  batch step: 3, loss: 44.14558410644531\n",
      "epoch: 35,  batch step: 4, loss: 8.713257789611816\n",
      "epoch: 35,  batch step: 5, loss: 58.88828659057617\n",
      "epoch: 35,  batch step: 6, loss: 3.911724805831909\n",
      "epoch: 35,  batch step: 7, loss: 95.10722351074219\n",
      "epoch: 35,  batch step: 8, loss: 9.585174560546875\n",
      "epoch: 35,  batch step: 9, loss: 95.6761703491211\n",
      "epoch: 35,  batch step: 10, loss: 16.171039581298828\n",
      "epoch: 35,  batch step: 11, loss: 10.661334037780762\n",
      "epoch: 35,  batch step: 12, loss: 49.702850341796875\n",
      "epoch: 35,  batch step: 13, loss: 20.59095001220703\n",
      "epoch: 35,  batch step: 14, loss: 14.920072555541992\n",
      "epoch: 35,  batch step: 15, loss: 45.92366027832031\n",
      "epoch: 35,  batch step: 16, loss: 111.44743347167969\n",
      "epoch: 35,  batch step: 17, loss: 6.2274274826049805\n",
      "epoch: 35,  batch step: 18, loss: 43.60948944091797\n",
      "epoch: 35,  batch step: 19, loss: 138.15487670898438\n",
      "epoch: 35,  batch step: 20, loss: 7.275583267211914\n",
      "epoch: 35,  batch step: 21, loss: 39.06851577758789\n",
      "epoch: 35,  batch step: 22, loss: 49.93227767944336\n",
      "epoch: 35,  batch step: 23, loss: 48.0947265625\n",
      "epoch: 35,  batch step: 24, loss: 27.256017684936523\n",
      "epoch: 35,  batch step: 25, loss: 15.062301635742188\n",
      "epoch: 35,  batch step: 26, loss: 6.582854747772217\n",
      "epoch: 35,  batch step: 27, loss: 7.890561580657959\n",
      "epoch: 35,  batch step: 28, loss: 7.621678829193115\n",
      "epoch: 35,  batch step: 29, loss: 7.501594543457031\n",
      "epoch: 35,  batch step: 30, loss: 23.930572509765625\n",
      "epoch: 35,  batch step: 31, loss: 10.803354263305664\n",
      "epoch: 35,  batch step: 32, loss: 5.577548980712891\n",
      "epoch: 35,  batch step: 33, loss: 9.03686237335205\n",
      "epoch: 35,  batch step: 34, loss: 12.156494140625\n",
      "epoch: 35,  batch step: 35, loss: 40.699066162109375\n",
      "epoch: 35,  batch step: 36, loss: 33.0256233215332\n",
      "epoch: 35,  batch step: 37, loss: 4.979852676391602\n",
      "epoch: 35,  batch step: 38, loss: 47.23588562011719\n",
      "epoch: 35,  batch step: 39, loss: 90.71782684326172\n",
      "epoch: 35,  batch step: 40, loss: 14.550159454345703\n",
      "epoch: 35,  batch step: 41, loss: 6.0021209716796875\n",
      "epoch: 35,  batch step: 42, loss: 6.8802809715271\n",
      "epoch: 35,  batch step: 43, loss: 33.31599426269531\n",
      "epoch: 35,  batch step: 44, loss: 15.07999324798584\n",
      "epoch: 35,  batch step: 45, loss: 53.73161315917969\n",
      "epoch: 35,  batch step: 46, loss: 7.242855072021484\n",
      "epoch: 35,  batch step: 47, loss: 9.69728946685791\n",
      "epoch: 35,  batch step: 48, loss: 72.51254272460938\n",
      "epoch: 35,  batch step: 49, loss: 50.46941375732422\n",
      "epoch: 35,  batch step: 50, loss: 19.409292221069336\n",
      "epoch: 35,  batch step: 51, loss: 36.38233184814453\n",
      "epoch: 35,  batch step: 52, loss: 7.810137748718262\n",
      "epoch: 35,  batch step: 53, loss: 31.873779296875\n",
      "epoch: 35,  batch step: 54, loss: 76.89439392089844\n",
      "epoch: 35,  batch step: 55, loss: 3.628511905670166\n",
      "epoch: 35,  batch step: 56, loss: 11.012786865234375\n",
      "epoch: 35,  batch step: 57, loss: 5.629944801330566\n",
      "epoch: 35,  batch step: 58, loss: 48.38610076904297\n",
      "epoch: 35,  batch step: 59, loss: 10.941064834594727\n",
      "epoch: 35,  batch step: 60, loss: 6.375131607055664\n",
      "epoch: 35,  batch step: 61, loss: 8.31808090209961\n",
      "epoch: 35,  batch step: 62, loss: 31.8220272064209\n",
      "epoch: 35,  batch step: 63, loss: 8.900301933288574\n",
      "epoch: 35,  batch step: 64, loss: 5.861424446105957\n",
      "epoch: 35,  batch step: 65, loss: 22.800199508666992\n",
      "epoch: 35,  batch step: 66, loss: 27.228803634643555\n",
      "epoch: 35,  batch step: 67, loss: 6.098586082458496\n",
      "epoch: 35,  batch step: 68, loss: 42.85103988647461\n",
      "epoch: 35,  batch step: 69, loss: 7.955898284912109\n",
      "epoch: 35,  batch step: 70, loss: 46.64565658569336\n",
      "epoch: 35,  batch step: 71, loss: 7.392592430114746\n",
      "epoch: 35,  batch step: 72, loss: 6.680309295654297\n",
      "epoch: 35,  batch step: 73, loss: 10.575535774230957\n",
      "epoch: 35,  batch step: 74, loss: 19.19168472290039\n",
      "epoch: 35,  batch step: 75, loss: 51.50408172607422\n",
      "epoch: 35,  batch step: 76, loss: 27.596237182617188\n",
      "epoch: 35,  batch step: 77, loss: 74.46784973144531\n",
      "epoch: 35,  batch step: 78, loss: 7.23310661315918\n",
      "epoch: 35,  batch step: 79, loss: 5.353265285491943\n",
      "epoch: 35,  batch step: 80, loss: 4.427350044250488\n",
      "epoch: 35,  batch step: 81, loss: 3.716595411300659\n",
      "epoch: 35,  batch step: 82, loss: 63.98644256591797\n",
      "epoch: 35,  batch step: 83, loss: 4.301118850708008\n",
      "epoch: 35,  batch step: 84, loss: 87.00849914550781\n",
      "epoch: 35,  batch step: 85, loss: 5.607391357421875\n",
      "epoch: 35,  batch step: 86, loss: 8.926642417907715\n",
      "epoch: 35,  batch step: 87, loss: 34.566253662109375\n",
      "epoch: 35,  batch step: 88, loss: 19.581544876098633\n",
      "epoch: 35,  batch step: 89, loss: 41.13484191894531\n",
      "epoch: 35,  batch step: 90, loss: 70.0070571899414\n",
      "epoch: 35,  batch step: 91, loss: 28.33571434020996\n",
      "epoch: 35,  batch step: 92, loss: 29.27907943725586\n",
      "epoch: 35,  batch step: 93, loss: 28.867259979248047\n",
      "epoch: 35,  batch step: 94, loss: 9.589719772338867\n",
      "epoch: 35,  batch step: 95, loss: 5.368096828460693\n",
      "epoch: 35,  batch step: 96, loss: 28.69070053100586\n",
      "epoch: 35,  batch step: 97, loss: 62.700191497802734\n",
      "epoch: 35,  batch step: 98, loss: 12.604205131530762\n",
      "epoch: 35,  batch step: 99, loss: 60.91844177246094\n",
      "epoch: 35,  batch step: 100, loss: 16.1812801361084\n",
      "epoch: 35,  batch step: 101, loss: 22.464218139648438\n",
      "epoch: 35,  batch step: 102, loss: 5.013298988342285\n",
      "epoch: 35,  batch step: 103, loss: 6.812480926513672\n",
      "epoch: 35,  batch step: 104, loss: 93.29307556152344\n",
      "epoch: 35,  batch step: 105, loss: 164.8328094482422\n",
      "epoch: 35,  batch step: 106, loss: 10.245574951171875\n",
      "epoch: 35,  batch step: 107, loss: 6.722959518432617\n",
      "epoch: 35,  batch step: 108, loss: 48.90218734741211\n",
      "epoch: 35,  batch step: 109, loss: 7.708400726318359\n",
      "epoch: 35,  batch step: 110, loss: 75.83805084228516\n",
      "epoch: 35,  batch step: 111, loss: 67.77278900146484\n",
      "epoch: 35,  batch step: 112, loss: 30.506668090820312\n",
      "epoch: 35,  batch step: 113, loss: 59.1883659362793\n",
      "epoch: 35,  batch step: 114, loss: 34.3658332824707\n",
      "epoch: 35,  batch step: 115, loss: 7.820103645324707\n",
      "epoch: 35,  batch step: 116, loss: 28.490631103515625\n",
      "epoch: 35,  batch step: 117, loss: 9.808830261230469\n",
      "epoch: 35,  batch step: 118, loss: 5.062551021575928\n",
      "epoch: 35,  batch step: 119, loss: 9.702129364013672\n",
      "epoch: 35,  batch step: 120, loss: 84.67720031738281\n",
      "epoch: 35,  batch step: 121, loss: 10.937265396118164\n",
      "epoch: 35,  batch step: 122, loss: 11.747259140014648\n",
      "epoch: 35,  batch step: 123, loss: 28.069412231445312\n",
      "epoch: 35,  batch step: 124, loss: 64.18101501464844\n",
      "epoch: 35,  batch step: 125, loss: 55.30775451660156\n",
      "epoch: 35,  batch step: 126, loss: 58.352291107177734\n",
      "epoch: 35,  batch step: 127, loss: 9.105339050292969\n",
      "epoch: 35,  batch step: 128, loss: 16.363811492919922\n",
      "epoch: 35,  batch step: 129, loss: 8.812562942504883\n",
      "epoch: 35,  batch step: 130, loss: 33.819496154785156\n",
      "epoch: 35,  batch step: 131, loss: 13.632522583007812\n",
      "epoch: 35,  batch step: 132, loss: 18.4542293548584\n",
      "epoch: 35,  batch step: 133, loss: 19.7220458984375\n",
      "epoch: 35,  batch step: 134, loss: 14.065092086791992\n",
      "epoch: 35,  batch step: 135, loss: 7.388600826263428\n",
      "epoch: 35,  batch step: 136, loss: 55.583770751953125\n",
      "epoch: 35,  batch step: 137, loss: 71.45347595214844\n",
      "epoch: 35,  batch step: 138, loss: 21.154098510742188\n",
      "epoch: 35,  batch step: 139, loss: 13.957884788513184\n",
      "epoch: 35,  batch step: 140, loss: 53.958396911621094\n",
      "epoch: 35,  batch step: 141, loss: 5.1121673583984375\n",
      "epoch: 35,  batch step: 142, loss: 7.576024532318115\n",
      "epoch: 35,  batch step: 143, loss: 106.59978485107422\n",
      "epoch: 35,  batch step: 144, loss: 58.75721740722656\n",
      "epoch: 35,  batch step: 145, loss: 46.490028381347656\n",
      "epoch: 35,  batch step: 146, loss: 45.923519134521484\n",
      "epoch: 35,  batch step: 147, loss: 13.94645881652832\n",
      "epoch: 35,  batch step: 148, loss: 62.56098556518555\n",
      "epoch: 35,  batch step: 149, loss: 64.88247680664062\n",
      "epoch: 35,  batch step: 150, loss: 70.40351867675781\n",
      "epoch: 35,  batch step: 151, loss: 6.9410810470581055\n",
      "epoch: 35,  batch step: 152, loss: 4.299285888671875\n",
      "epoch: 35,  batch step: 153, loss: 9.986875534057617\n",
      "epoch: 35,  batch step: 154, loss: 7.858600616455078\n",
      "epoch: 35,  batch step: 155, loss: 34.530479431152344\n",
      "epoch: 35,  batch step: 156, loss: 9.25769329071045\n",
      "epoch: 35,  batch step: 157, loss: 3.333235740661621\n",
      "epoch: 35,  batch step: 158, loss: 4.578233242034912\n",
      "epoch: 35,  batch step: 159, loss: 36.62986755371094\n",
      "epoch: 35,  batch step: 160, loss: 32.46440124511719\n",
      "epoch: 35,  batch step: 161, loss: 5.483606338500977\n",
      "epoch: 35,  batch step: 162, loss: 57.020751953125\n",
      "epoch: 35,  batch step: 163, loss: 8.418827056884766\n",
      "epoch: 35,  batch step: 164, loss: 9.349149703979492\n",
      "epoch: 35,  batch step: 165, loss: 7.339763164520264\n",
      "epoch: 35,  batch step: 166, loss: 4.692805290222168\n",
      "epoch: 35,  batch step: 167, loss: 33.81764221191406\n",
      "epoch: 35,  batch step: 168, loss: 47.821441650390625\n",
      "epoch: 35,  batch step: 169, loss: 5.877101421356201\n",
      "epoch: 35,  batch step: 170, loss: 47.80819320678711\n",
      "epoch: 35,  batch step: 171, loss: 12.436574935913086\n",
      "epoch: 35,  batch step: 172, loss: 51.81254196166992\n",
      "epoch: 35,  batch step: 173, loss: 7.5276689529418945\n",
      "epoch: 35,  batch step: 174, loss: 18.301387786865234\n",
      "epoch: 35,  batch step: 175, loss: 46.35005187988281\n",
      "epoch: 35,  batch step: 176, loss: 35.45269012451172\n",
      "epoch: 35,  batch step: 177, loss: 10.338860511779785\n",
      "epoch: 35,  batch step: 178, loss: 51.45530319213867\n",
      "epoch: 35,  batch step: 179, loss: 11.1376371383667\n",
      "epoch: 35,  batch step: 180, loss: 4.196684837341309\n",
      "epoch: 35,  batch step: 181, loss: 22.61774444580078\n",
      "epoch: 35,  batch step: 182, loss: 13.256431579589844\n",
      "epoch: 35,  batch step: 183, loss: 6.222898483276367\n",
      "epoch: 35,  batch step: 184, loss: 92.68853759765625\n",
      "epoch: 35,  batch step: 185, loss: 7.415626049041748\n",
      "epoch: 35,  batch step: 186, loss: 4.717167377471924\n",
      "epoch: 35,  batch step: 187, loss: 7.521868705749512\n",
      "epoch: 35,  batch step: 188, loss: 24.401927947998047\n",
      "epoch: 35,  batch step: 189, loss: 55.15203857421875\n",
      "epoch: 35,  batch step: 190, loss: 52.624507904052734\n",
      "epoch: 35,  batch step: 191, loss: 40.45030975341797\n",
      "epoch: 35,  batch step: 192, loss: 48.35345458984375\n",
      "epoch: 35,  batch step: 193, loss: 50.40913391113281\n",
      "epoch: 35,  batch step: 194, loss: 25.58279037475586\n",
      "epoch: 35,  batch step: 195, loss: 10.710001945495605\n",
      "epoch: 35,  batch step: 196, loss: 6.223293304443359\n",
      "epoch: 35,  batch step: 197, loss: 5.182822227478027\n",
      "epoch: 35,  batch step: 198, loss: 42.64565658569336\n",
      "epoch: 35,  batch step: 199, loss: 10.376808166503906\n",
      "epoch: 35,  batch step: 200, loss: 4.993894577026367\n",
      "epoch: 35,  batch step: 201, loss: 48.88605880737305\n",
      "epoch: 35,  batch step: 202, loss: 4.107019424438477\n",
      "epoch: 35,  batch step: 203, loss: 6.413191795349121\n",
      "epoch: 35,  batch step: 204, loss: 4.301854133605957\n",
      "epoch: 35,  batch step: 205, loss: 84.63629150390625\n",
      "epoch: 35,  batch step: 206, loss: 6.0492048263549805\n",
      "epoch: 35,  batch step: 207, loss: 170.11911010742188\n",
      "epoch: 35,  batch step: 208, loss: 29.864635467529297\n",
      "epoch: 35,  batch step: 209, loss: 79.94397735595703\n",
      "epoch: 35,  batch step: 210, loss: 6.180152893066406\n",
      "epoch: 35,  batch step: 211, loss: 54.574363708496094\n",
      "epoch: 35,  batch step: 212, loss: 39.74980163574219\n",
      "epoch: 35,  batch step: 213, loss: 31.65420913696289\n",
      "epoch: 35,  batch step: 214, loss: 11.4945707321167\n",
      "epoch: 35,  batch step: 215, loss: 47.1739501953125\n",
      "epoch: 35,  batch step: 216, loss: 49.25629425048828\n",
      "epoch: 35,  batch step: 217, loss: 10.536391258239746\n",
      "epoch: 35,  batch step: 218, loss: 5.947650909423828\n",
      "epoch: 35,  batch step: 219, loss: 29.944061279296875\n",
      "epoch: 35,  batch step: 220, loss: 54.85790252685547\n",
      "epoch: 35,  batch step: 221, loss: 18.70159149169922\n",
      "epoch: 35,  batch step: 222, loss: 11.552596092224121\n",
      "epoch: 35,  batch step: 223, loss: 4.229886531829834\n",
      "epoch: 35,  batch step: 224, loss: 13.252528190612793\n",
      "epoch: 35,  batch step: 225, loss: 28.30569076538086\n",
      "epoch: 35,  batch step: 226, loss: 11.128520011901855\n",
      "epoch: 35,  batch step: 227, loss: 5.90411901473999\n",
      "epoch: 35,  batch step: 228, loss: 8.791690826416016\n",
      "epoch: 35,  batch step: 229, loss: 43.58451843261719\n",
      "epoch: 35,  batch step: 230, loss: 64.84432983398438\n",
      "epoch: 35,  batch step: 231, loss: 170.08810424804688\n",
      "epoch: 35,  batch step: 232, loss: 7.190704345703125\n",
      "epoch: 35,  batch step: 233, loss: 59.543540954589844\n",
      "epoch: 35,  batch step: 234, loss: 38.25590515136719\n",
      "epoch: 35,  batch step: 235, loss: 82.58491516113281\n",
      "epoch: 35,  batch step: 236, loss: 140.14703369140625\n",
      "epoch: 35,  batch step: 237, loss: 36.119956970214844\n",
      "epoch: 35,  batch step: 238, loss: 71.50419616699219\n",
      "epoch: 35,  batch step: 239, loss: 14.105566024780273\n",
      "epoch: 35,  batch step: 240, loss: 48.33808135986328\n",
      "epoch: 35,  batch step: 241, loss: 56.88654708862305\n",
      "epoch: 35,  batch step: 242, loss: 61.01499938964844\n",
      "epoch: 35,  batch step: 243, loss: 6.88815450668335\n",
      "epoch: 35,  batch step: 244, loss: 19.417530059814453\n",
      "epoch: 35,  batch step: 245, loss: 6.221351146697998\n",
      "epoch: 35,  batch step: 246, loss: 15.75844955444336\n",
      "epoch: 35,  batch step: 247, loss: 9.806039810180664\n",
      "epoch: 35,  batch step: 248, loss: 8.758752822875977\n",
      "epoch: 35,  batch step: 249, loss: 6.530881881713867\n",
      "epoch: 35,  batch step: 250, loss: 3.6327388286590576\n",
      "epoch: 35,  batch step: 251, loss: 53.01696014404297\n",
      "validation error epoch  35:    tensor(66.3582, device='cuda:0')\n",
      "316\n",
      "epoch: 36,  batch step: 0, loss: 7.550753593444824\n",
      "epoch: 36,  batch step: 1, loss: 47.749107360839844\n",
      "epoch: 36,  batch step: 2, loss: 9.667198181152344\n",
      "epoch: 36,  batch step: 3, loss: 126.6490478515625\n",
      "epoch: 36,  batch step: 4, loss: 16.420339584350586\n",
      "epoch: 36,  batch step: 5, loss: 14.528651237487793\n",
      "epoch: 36,  batch step: 6, loss: 33.263511657714844\n",
      "epoch: 36,  batch step: 7, loss: 14.686789512634277\n",
      "epoch: 36,  batch step: 8, loss: 7.848875999450684\n",
      "epoch: 36,  batch step: 9, loss: 85.88772583007812\n",
      "epoch: 36,  batch step: 10, loss: 12.947418212890625\n",
      "epoch: 36,  batch step: 11, loss: 5.524936199188232\n",
      "epoch: 36,  batch step: 12, loss: 27.165212631225586\n",
      "epoch: 36,  batch step: 13, loss: 6.046759605407715\n",
      "epoch: 36,  batch step: 14, loss: 72.89874267578125\n",
      "epoch: 36,  batch step: 15, loss: 85.55278015136719\n",
      "epoch: 36,  batch step: 16, loss: 4.848959922790527\n",
      "epoch: 36,  batch step: 17, loss: 7.6886749267578125\n",
      "epoch: 36,  batch step: 18, loss: 193.7051239013672\n",
      "epoch: 36,  batch step: 19, loss: 48.449554443359375\n",
      "epoch: 36,  batch step: 20, loss: 20.940372467041016\n",
      "epoch: 36,  batch step: 21, loss: 33.33367919921875\n",
      "epoch: 36,  batch step: 22, loss: 35.2527961730957\n",
      "epoch: 36,  batch step: 23, loss: 5.591783046722412\n",
      "epoch: 36,  batch step: 24, loss: 6.1404218673706055\n",
      "epoch: 36,  batch step: 25, loss: 34.033119201660156\n",
      "epoch: 36,  batch step: 26, loss: 8.590033531188965\n",
      "epoch: 36,  batch step: 27, loss: 21.279664993286133\n",
      "epoch: 36,  batch step: 28, loss: 94.46672058105469\n",
      "epoch: 36,  batch step: 29, loss: 9.89285659790039\n",
      "epoch: 36,  batch step: 30, loss: 29.751544952392578\n",
      "epoch: 36,  batch step: 31, loss: 84.26347351074219\n",
      "epoch: 36,  batch step: 32, loss: 10.2924222946167\n",
      "epoch: 36,  batch step: 33, loss: 24.585721969604492\n",
      "epoch: 36,  batch step: 34, loss: 42.095191955566406\n",
      "epoch: 36,  batch step: 35, loss: 20.416183471679688\n",
      "epoch: 36,  batch step: 36, loss: 5.79477071762085\n",
      "epoch: 36,  batch step: 37, loss: 12.920519828796387\n",
      "epoch: 36,  batch step: 38, loss: 30.13117027282715\n",
      "epoch: 36,  batch step: 39, loss: 44.179649353027344\n",
      "epoch: 36,  batch step: 40, loss: 11.174537658691406\n",
      "epoch: 36,  batch step: 41, loss: 22.681106567382812\n",
      "epoch: 36,  batch step: 42, loss: 24.089282989501953\n",
      "epoch: 36,  batch step: 43, loss: 6.056962966918945\n",
      "epoch: 36,  batch step: 44, loss: 7.269399166107178\n",
      "epoch: 36,  batch step: 45, loss: 8.29109001159668\n",
      "epoch: 36,  batch step: 46, loss: 66.25196838378906\n",
      "epoch: 36,  batch step: 47, loss: 7.206264019012451\n",
      "epoch: 36,  batch step: 48, loss: 3.827261447906494\n",
      "epoch: 36,  batch step: 49, loss: 136.92221069335938\n",
      "epoch: 36,  batch step: 50, loss: 6.0798234939575195\n",
      "epoch: 36,  batch step: 51, loss: 58.5963134765625\n",
      "epoch: 36,  batch step: 52, loss: 26.424583435058594\n",
      "epoch: 36,  batch step: 53, loss: 53.594810485839844\n",
      "epoch: 36,  batch step: 54, loss: 5.773576259613037\n",
      "epoch: 36,  batch step: 55, loss: 73.1427230834961\n",
      "epoch: 36,  batch step: 56, loss: 26.224594116210938\n",
      "epoch: 36,  batch step: 57, loss: 40.04281997680664\n",
      "epoch: 36,  batch step: 58, loss: 40.53287887573242\n",
      "epoch: 36,  batch step: 59, loss: 14.991918563842773\n",
      "epoch: 36,  batch step: 60, loss: 7.219876289367676\n",
      "epoch: 36,  batch step: 61, loss: 4.56270694732666\n",
      "epoch: 36,  batch step: 62, loss: 9.991973876953125\n",
      "epoch: 36,  batch step: 63, loss: 57.832210540771484\n",
      "epoch: 36,  batch step: 64, loss: 9.099555969238281\n",
      "epoch: 36,  batch step: 65, loss: 6.166083335876465\n",
      "epoch: 36,  batch step: 66, loss: 5.077248573303223\n",
      "epoch: 36,  batch step: 67, loss: 6.036437511444092\n",
      "epoch: 36,  batch step: 68, loss: 4.253517150878906\n",
      "epoch: 36,  batch step: 69, loss: 10.752218246459961\n",
      "epoch: 36,  batch step: 70, loss: 105.60374450683594\n",
      "epoch: 36,  batch step: 71, loss: 71.23988342285156\n",
      "epoch: 36,  batch step: 72, loss: 3.970811605453491\n",
      "epoch: 36,  batch step: 73, loss: 11.849712371826172\n",
      "epoch: 36,  batch step: 74, loss: 8.624137878417969\n",
      "epoch: 36,  batch step: 75, loss: 10.519256591796875\n",
      "epoch: 36,  batch step: 76, loss: 13.667967796325684\n",
      "epoch: 36,  batch step: 77, loss: 77.67906951904297\n",
      "epoch: 36,  batch step: 78, loss: 44.370601654052734\n",
      "epoch: 36,  batch step: 79, loss: 5.474198818206787\n",
      "epoch: 36,  batch step: 80, loss: 6.217107772827148\n",
      "epoch: 36,  batch step: 81, loss: 25.298412322998047\n",
      "epoch: 36,  batch step: 82, loss: 7.686103820800781\n",
      "epoch: 36,  batch step: 83, loss: 85.88003540039062\n",
      "epoch: 36,  batch step: 84, loss: 72.5120849609375\n",
      "epoch: 36,  batch step: 85, loss: 8.493936538696289\n",
      "epoch: 36,  batch step: 86, loss: 6.927288055419922\n",
      "epoch: 36,  batch step: 87, loss: 57.1318359375\n",
      "epoch: 36,  batch step: 88, loss: 22.84591293334961\n",
      "epoch: 36,  batch step: 89, loss: 52.59367370605469\n",
      "epoch: 36,  batch step: 90, loss: 31.027851104736328\n",
      "epoch: 36,  batch step: 91, loss: 10.752867698669434\n",
      "epoch: 36,  batch step: 92, loss: 14.193309783935547\n",
      "epoch: 36,  batch step: 93, loss: 15.120128631591797\n",
      "epoch: 36,  batch step: 94, loss: 11.766294479370117\n",
      "epoch: 36,  batch step: 95, loss: 4.315935134887695\n",
      "epoch: 36,  batch step: 96, loss: 52.46665954589844\n",
      "epoch: 36,  batch step: 97, loss: 46.79166793823242\n",
      "epoch: 36,  batch step: 98, loss: 14.726191520690918\n",
      "epoch: 36,  batch step: 99, loss: 20.012176513671875\n",
      "epoch: 36,  batch step: 100, loss: 26.29970932006836\n",
      "epoch: 36,  batch step: 101, loss: 36.904205322265625\n",
      "epoch: 36,  batch step: 102, loss: 6.072164058685303\n",
      "epoch: 36,  batch step: 103, loss: 13.192296981811523\n",
      "epoch: 36,  batch step: 104, loss: 14.593643188476562\n",
      "epoch: 36,  batch step: 105, loss: 43.23202133178711\n",
      "epoch: 36,  batch step: 106, loss: 15.070980072021484\n",
      "epoch: 36,  batch step: 107, loss: 136.35812377929688\n",
      "epoch: 36,  batch step: 108, loss: 62.487152099609375\n",
      "epoch: 36,  batch step: 109, loss: 23.995880126953125\n",
      "epoch: 36,  batch step: 110, loss: 6.6919097900390625\n",
      "epoch: 36,  batch step: 111, loss: 60.681793212890625\n",
      "epoch: 36,  batch step: 112, loss: 19.898326873779297\n",
      "epoch: 36,  batch step: 113, loss: 13.724386215209961\n",
      "epoch: 36,  batch step: 114, loss: 88.81523895263672\n",
      "epoch: 36,  batch step: 115, loss: 187.77218627929688\n",
      "epoch: 36,  batch step: 116, loss: 13.293685913085938\n",
      "epoch: 36,  batch step: 117, loss: 17.158493041992188\n",
      "epoch: 36,  batch step: 118, loss: 8.038152694702148\n",
      "epoch: 36,  batch step: 119, loss: 141.68148803710938\n",
      "epoch: 36,  batch step: 120, loss: 6.804555416107178\n",
      "epoch: 36,  batch step: 121, loss: 9.512641906738281\n",
      "epoch: 36,  batch step: 122, loss: 79.88690185546875\n",
      "epoch: 36,  batch step: 123, loss: 110.38438415527344\n",
      "epoch: 36,  batch step: 124, loss: 47.42015075683594\n",
      "epoch: 36,  batch step: 125, loss: 28.397369384765625\n",
      "epoch: 36,  batch step: 126, loss: 7.629372596740723\n",
      "epoch: 36,  batch step: 127, loss: 42.06156921386719\n",
      "epoch: 36,  batch step: 128, loss: 6.993028163909912\n",
      "epoch: 36,  batch step: 129, loss: 8.132328987121582\n",
      "epoch: 36,  batch step: 130, loss: 13.294913291931152\n",
      "epoch: 36,  batch step: 131, loss: 7.158267021179199\n",
      "epoch: 36,  batch step: 132, loss: 40.016143798828125\n",
      "epoch: 36,  batch step: 133, loss: 9.307456970214844\n",
      "epoch: 36,  batch step: 134, loss: 59.46551513671875\n",
      "epoch: 36,  batch step: 135, loss: 30.701313018798828\n",
      "epoch: 36,  batch step: 136, loss: 6.3279314041137695\n",
      "epoch: 36,  batch step: 137, loss: 9.9439058303833\n",
      "epoch: 36,  batch step: 138, loss: 34.655235290527344\n",
      "epoch: 36,  batch step: 139, loss: 39.640625\n",
      "epoch: 36,  batch step: 140, loss: 29.648025512695312\n",
      "epoch: 36,  batch step: 141, loss: 164.68167114257812\n",
      "epoch: 36,  batch step: 142, loss: 93.55482482910156\n",
      "epoch: 36,  batch step: 143, loss: 22.04446029663086\n",
      "epoch: 36,  batch step: 144, loss: 13.630386352539062\n",
      "epoch: 36,  batch step: 145, loss: 23.291675567626953\n",
      "epoch: 36,  batch step: 146, loss: 28.296260833740234\n",
      "epoch: 36,  batch step: 147, loss: 58.702964782714844\n",
      "epoch: 36,  batch step: 148, loss: 47.93608856201172\n",
      "epoch: 36,  batch step: 149, loss: 12.6848783493042\n",
      "epoch: 36,  batch step: 150, loss: 7.639803886413574\n",
      "epoch: 36,  batch step: 151, loss: 63.70649337768555\n",
      "epoch: 36,  batch step: 152, loss: 95.97649383544922\n",
      "epoch: 36,  batch step: 153, loss: 52.62406539916992\n",
      "epoch: 36,  batch step: 154, loss: 45.12882995605469\n",
      "epoch: 36,  batch step: 155, loss: 8.869621276855469\n",
      "epoch: 36,  batch step: 156, loss: 7.871471405029297\n",
      "epoch: 36,  batch step: 157, loss: 8.671436309814453\n",
      "epoch: 36,  batch step: 158, loss: 11.909904479980469\n",
      "epoch: 36,  batch step: 159, loss: 99.47628784179688\n",
      "epoch: 36,  batch step: 160, loss: 8.777904510498047\n",
      "epoch: 36,  batch step: 161, loss: 17.474220275878906\n",
      "epoch: 36,  batch step: 162, loss: 41.58831024169922\n",
      "epoch: 36,  batch step: 163, loss: 8.141641616821289\n",
      "epoch: 36,  batch step: 164, loss: 6.27215576171875\n",
      "epoch: 36,  batch step: 165, loss: 47.14268493652344\n",
      "epoch: 36,  batch step: 166, loss: 125.50352478027344\n",
      "epoch: 36,  batch step: 167, loss: 92.02104949951172\n",
      "epoch: 36,  batch step: 168, loss: 15.33768367767334\n",
      "epoch: 36,  batch step: 169, loss: 7.1845703125\n",
      "epoch: 36,  batch step: 170, loss: 17.538166046142578\n",
      "epoch: 36,  batch step: 171, loss: 21.09970474243164\n",
      "epoch: 36,  batch step: 172, loss: 24.702383041381836\n",
      "epoch: 36,  batch step: 173, loss: 33.7236328125\n",
      "epoch: 36,  batch step: 174, loss: 5.479213714599609\n",
      "epoch: 36,  batch step: 175, loss: 20.194942474365234\n",
      "epoch: 36,  batch step: 176, loss: 12.817789077758789\n",
      "epoch: 36,  batch step: 177, loss: 7.182956695556641\n",
      "epoch: 36,  batch step: 178, loss: 5.130314826965332\n",
      "epoch: 36,  batch step: 179, loss: 4.915506362915039\n",
      "epoch: 36,  batch step: 180, loss: 10.609248161315918\n",
      "epoch: 36,  batch step: 181, loss: 47.4672966003418\n",
      "epoch: 36,  batch step: 182, loss: 38.64960479736328\n",
      "epoch: 36,  batch step: 183, loss: 5.8104376792907715\n",
      "epoch: 36,  batch step: 184, loss: 5.738497734069824\n",
      "epoch: 36,  batch step: 185, loss: 27.213592529296875\n",
      "epoch: 36,  batch step: 186, loss: 28.023122787475586\n",
      "epoch: 36,  batch step: 187, loss: 6.01651668548584\n",
      "epoch: 36,  batch step: 188, loss: 8.03719425201416\n",
      "epoch: 36,  batch step: 189, loss: 40.91632843017578\n",
      "epoch: 36,  batch step: 190, loss: 4.829268932342529\n",
      "epoch: 36,  batch step: 191, loss: 93.33692932128906\n",
      "epoch: 36,  batch step: 192, loss: 28.09308433532715\n",
      "epoch: 36,  batch step: 193, loss: 99.6534652709961\n",
      "epoch: 36,  batch step: 194, loss: 26.066190719604492\n",
      "epoch: 36,  batch step: 195, loss: 21.935035705566406\n",
      "epoch: 36,  batch step: 196, loss: 16.72922134399414\n",
      "epoch: 36,  batch step: 197, loss: 57.3300895690918\n",
      "epoch: 36,  batch step: 198, loss: 3.7720541954040527\n",
      "epoch: 36,  batch step: 199, loss: 8.789189338684082\n",
      "epoch: 36,  batch step: 200, loss: 96.3137435913086\n",
      "epoch: 36,  batch step: 201, loss: 29.411048889160156\n",
      "epoch: 36,  batch step: 202, loss: 7.735973358154297\n",
      "epoch: 36,  batch step: 203, loss: 58.43499755859375\n",
      "epoch: 36,  batch step: 204, loss: 26.232791900634766\n",
      "epoch: 36,  batch step: 205, loss: 6.532495021820068\n",
      "epoch: 36,  batch step: 206, loss: 4.785912990570068\n",
      "epoch: 36,  batch step: 207, loss: 110.51776123046875\n",
      "epoch: 36,  batch step: 208, loss: 73.7009506225586\n",
      "epoch: 36,  batch step: 209, loss: 8.577423095703125\n",
      "epoch: 36,  batch step: 210, loss: 39.074485778808594\n",
      "epoch: 36,  batch step: 211, loss: 10.681781768798828\n",
      "epoch: 36,  batch step: 212, loss: 22.139171600341797\n",
      "epoch: 36,  batch step: 213, loss: 81.31068420410156\n",
      "epoch: 36,  batch step: 214, loss: 6.511395454406738\n",
      "epoch: 36,  batch step: 215, loss: 8.555014610290527\n",
      "epoch: 36,  batch step: 216, loss: 30.0268611907959\n",
      "epoch: 36,  batch step: 217, loss: 13.31743049621582\n",
      "epoch: 36,  batch step: 218, loss: 7.124555587768555\n",
      "epoch: 36,  batch step: 219, loss: 7.601663112640381\n",
      "epoch: 36,  batch step: 220, loss: 58.18407440185547\n",
      "epoch: 36,  batch step: 221, loss: 13.390840530395508\n",
      "epoch: 36,  batch step: 222, loss: 13.17859172821045\n",
      "epoch: 36,  batch step: 223, loss: 19.097618103027344\n",
      "epoch: 36,  batch step: 224, loss: 6.326231956481934\n",
      "epoch: 36,  batch step: 225, loss: 78.31658935546875\n",
      "epoch: 36,  batch step: 226, loss: 5.403587341308594\n",
      "epoch: 36,  batch step: 227, loss: 33.58158493041992\n",
      "epoch: 36,  batch step: 228, loss: 36.901206970214844\n",
      "epoch: 36,  batch step: 229, loss: 16.096847534179688\n",
      "epoch: 36,  batch step: 230, loss: 195.73422241210938\n",
      "epoch: 36,  batch step: 231, loss: 2.815331220626831\n",
      "epoch: 36,  batch step: 232, loss: 8.003756523132324\n",
      "epoch: 36,  batch step: 233, loss: 20.11435317993164\n",
      "epoch: 36,  batch step: 234, loss: 75.86212158203125\n",
      "epoch: 36,  batch step: 235, loss: 10.62696647644043\n",
      "epoch: 36,  batch step: 236, loss: 76.39399719238281\n",
      "epoch: 36,  batch step: 237, loss: 32.487388610839844\n",
      "epoch: 36,  batch step: 238, loss: 5.005613803863525\n",
      "epoch: 36,  batch step: 239, loss: 20.450239181518555\n",
      "epoch: 36,  batch step: 240, loss: 24.653532028198242\n",
      "epoch: 36,  batch step: 241, loss: 67.89894104003906\n",
      "epoch: 36,  batch step: 242, loss: 9.334498405456543\n",
      "epoch: 36,  batch step: 243, loss: 4.947263717651367\n",
      "epoch: 36,  batch step: 244, loss: 89.51266479492188\n",
      "epoch: 36,  batch step: 245, loss: 127.01699829101562\n",
      "epoch: 36,  batch step: 246, loss: 29.71917724609375\n",
      "epoch: 36,  batch step: 247, loss: 6.456481456756592\n",
      "epoch: 36,  batch step: 248, loss: 22.143306732177734\n",
      "epoch: 36,  batch step: 249, loss: 27.872554779052734\n",
      "epoch: 36,  batch step: 250, loss: 7.133755683898926\n",
      "epoch: 36,  batch step: 251, loss: 436.8090515136719\n",
      "validation error epoch  36:    tensor(82.1285, device='cuda:0')\n",
      "316\n",
      "epoch: 37,  batch step: 0, loss: 94.57794189453125\n",
      "epoch: 37,  batch step: 1, loss: 13.282252311706543\n",
      "epoch: 37,  batch step: 2, loss: 57.92150115966797\n",
      "epoch: 37,  batch step: 3, loss: 26.76456069946289\n",
      "epoch: 37,  batch step: 4, loss: 13.502909660339355\n",
      "epoch: 37,  batch step: 5, loss: 81.94452667236328\n",
      "epoch: 37,  batch step: 6, loss: 7.172379970550537\n",
      "epoch: 37,  batch step: 7, loss: 73.41532897949219\n",
      "epoch: 37,  batch step: 8, loss: 14.885150909423828\n",
      "epoch: 37,  batch step: 9, loss: 40.77559280395508\n",
      "epoch: 37,  batch step: 10, loss: 87.17278289794922\n",
      "epoch: 37,  batch step: 11, loss: 45.903343200683594\n",
      "epoch: 37,  batch step: 12, loss: 14.303770065307617\n",
      "epoch: 37,  batch step: 13, loss: 6.033946514129639\n",
      "epoch: 37,  batch step: 14, loss: 26.956275939941406\n",
      "epoch: 37,  batch step: 15, loss: 103.05146789550781\n",
      "epoch: 37,  batch step: 16, loss: 28.243520736694336\n",
      "epoch: 37,  batch step: 17, loss: 30.13765525817871\n",
      "epoch: 37,  batch step: 18, loss: 14.047174453735352\n",
      "epoch: 37,  batch step: 19, loss: 12.01511001586914\n",
      "epoch: 37,  batch step: 20, loss: 8.665523529052734\n",
      "epoch: 37,  batch step: 21, loss: 49.30062484741211\n",
      "epoch: 37,  batch step: 22, loss: 4.212940692901611\n",
      "epoch: 37,  batch step: 23, loss: 18.8199520111084\n",
      "epoch: 37,  batch step: 24, loss: 24.994098663330078\n",
      "epoch: 37,  batch step: 25, loss: 13.30106258392334\n",
      "epoch: 37,  batch step: 26, loss: 17.34496307373047\n",
      "epoch: 37,  batch step: 27, loss: 15.991771697998047\n",
      "epoch: 37,  batch step: 28, loss: 9.340751647949219\n",
      "epoch: 37,  batch step: 29, loss: 49.09691619873047\n",
      "epoch: 37,  batch step: 30, loss: 12.222472190856934\n",
      "epoch: 37,  batch step: 31, loss: 8.030583381652832\n",
      "epoch: 37,  batch step: 32, loss: 9.488726615905762\n",
      "epoch: 37,  batch step: 33, loss: 15.427505493164062\n",
      "epoch: 37,  batch step: 34, loss: 104.13211059570312\n",
      "epoch: 37,  batch step: 35, loss: 40.829132080078125\n",
      "epoch: 37,  batch step: 36, loss: 82.39940643310547\n",
      "epoch: 37,  batch step: 37, loss: 7.582633018493652\n",
      "epoch: 37,  batch step: 38, loss: 8.064111709594727\n",
      "epoch: 37,  batch step: 39, loss: 42.42281723022461\n",
      "epoch: 37,  batch step: 40, loss: 74.09947967529297\n",
      "epoch: 37,  batch step: 41, loss: 13.786567687988281\n",
      "epoch: 37,  batch step: 42, loss: 12.660014152526855\n",
      "epoch: 37,  batch step: 43, loss: 91.03021240234375\n",
      "epoch: 37,  batch step: 44, loss: 9.476759910583496\n",
      "epoch: 37,  batch step: 45, loss: 44.01911163330078\n",
      "epoch: 37,  batch step: 46, loss: 27.432249069213867\n",
      "epoch: 37,  batch step: 47, loss: 6.884219169616699\n",
      "epoch: 37,  batch step: 48, loss: 5.394700050354004\n",
      "epoch: 37,  batch step: 49, loss: 100.911376953125\n",
      "epoch: 37,  batch step: 50, loss: 8.605319023132324\n",
      "epoch: 37,  batch step: 51, loss: 70.07307434082031\n",
      "epoch: 37,  batch step: 52, loss: 8.471651077270508\n",
      "epoch: 37,  batch step: 53, loss: 39.44098663330078\n",
      "epoch: 37,  batch step: 54, loss: 4.538455009460449\n",
      "epoch: 37,  batch step: 55, loss: 58.51085662841797\n",
      "epoch: 37,  batch step: 56, loss: 8.289201736450195\n",
      "epoch: 37,  batch step: 57, loss: 8.98703670501709\n",
      "epoch: 37,  batch step: 58, loss: 7.332944393157959\n",
      "epoch: 37,  batch step: 59, loss: 21.682964324951172\n",
      "epoch: 37,  batch step: 60, loss: 14.539756774902344\n",
      "epoch: 37,  batch step: 61, loss: 12.455465316772461\n",
      "epoch: 37,  batch step: 62, loss: 17.094608306884766\n",
      "epoch: 37,  batch step: 63, loss: 27.636831283569336\n",
      "epoch: 37,  batch step: 64, loss: 6.644632339477539\n",
      "epoch: 37,  batch step: 65, loss: 40.081886291503906\n",
      "epoch: 37,  batch step: 66, loss: 49.601356506347656\n",
      "epoch: 37,  batch step: 67, loss: 72.97552490234375\n",
      "epoch: 37,  batch step: 68, loss: 77.46123504638672\n",
      "epoch: 37,  batch step: 69, loss: 40.146034240722656\n",
      "epoch: 37,  batch step: 70, loss: 10.370296478271484\n",
      "epoch: 37,  batch step: 71, loss: 49.557342529296875\n",
      "epoch: 37,  batch step: 72, loss: 76.13516235351562\n",
      "epoch: 37,  batch step: 73, loss: 38.040016174316406\n",
      "epoch: 37,  batch step: 74, loss: 5.958676338195801\n",
      "epoch: 37,  batch step: 75, loss: 33.106529235839844\n",
      "epoch: 37,  batch step: 76, loss: 8.310213088989258\n",
      "epoch: 37,  batch step: 77, loss: 21.000988006591797\n",
      "epoch: 37,  batch step: 78, loss: 8.99498176574707\n",
      "epoch: 37,  batch step: 79, loss: 5.823451519012451\n",
      "epoch: 37,  batch step: 80, loss: 4.960867881774902\n",
      "epoch: 37,  batch step: 81, loss: 27.81696128845215\n",
      "epoch: 37,  batch step: 82, loss: 4.042476177215576\n",
      "epoch: 37,  batch step: 83, loss: 4.483523368835449\n",
      "epoch: 37,  batch step: 84, loss: 48.41578674316406\n",
      "epoch: 37,  batch step: 85, loss: 6.696471691131592\n",
      "epoch: 37,  batch step: 86, loss: 67.48131561279297\n",
      "epoch: 37,  batch step: 87, loss: 5.022754669189453\n",
      "epoch: 37,  batch step: 88, loss: 12.151021957397461\n",
      "epoch: 37,  batch step: 89, loss: 6.202591896057129\n",
      "epoch: 37,  batch step: 90, loss: 37.83073043823242\n",
      "epoch: 37,  batch step: 91, loss: 91.88134765625\n",
      "epoch: 37,  batch step: 92, loss: 8.356975555419922\n",
      "epoch: 37,  batch step: 93, loss: 52.36940002441406\n",
      "epoch: 37,  batch step: 94, loss: 4.190231800079346\n",
      "epoch: 37,  batch step: 95, loss: 54.326271057128906\n",
      "epoch: 37,  batch step: 96, loss: 52.52040100097656\n",
      "epoch: 37,  batch step: 97, loss: 5.511660575866699\n",
      "epoch: 37,  batch step: 98, loss: 8.231237411499023\n",
      "epoch: 37,  batch step: 99, loss: 22.192874908447266\n",
      "epoch: 37,  batch step: 100, loss: 45.9106559753418\n",
      "epoch: 37,  batch step: 101, loss: 9.87291145324707\n",
      "epoch: 37,  batch step: 102, loss: 5.7666544914245605\n",
      "epoch: 37,  batch step: 103, loss: 40.517677307128906\n",
      "epoch: 37,  batch step: 104, loss: 52.70738983154297\n",
      "epoch: 37,  batch step: 105, loss: 26.190269470214844\n",
      "epoch: 37,  batch step: 106, loss: 37.23888397216797\n",
      "epoch: 37,  batch step: 107, loss: 28.859405517578125\n",
      "epoch: 37,  batch step: 108, loss: 88.8058090209961\n",
      "epoch: 37,  batch step: 109, loss: 6.645806789398193\n",
      "epoch: 37,  batch step: 110, loss: 186.0757293701172\n",
      "epoch: 37,  batch step: 111, loss: 9.155682563781738\n",
      "epoch: 37,  batch step: 112, loss: 50.5948600769043\n",
      "epoch: 37,  batch step: 113, loss: 4.91412353515625\n",
      "epoch: 37,  batch step: 114, loss: 114.53338623046875\n",
      "epoch: 37,  batch step: 115, loss: 6.50456428527832\n",
      "epoch: 37,  batch step: 116, loss: 6.871980667114258\n",
      "epoch: 37,  batch step: 117, loss: 51.684532165527344\n",
      "epoch: 37,  batch step: 118, loss: 9.562095642089844\n",
      "epoch: 37,  batch step: 119, loss: 13.402857780456543\n",
      "epoch: 37,  batch step: 120, loss: 88.53840637207031\n",
      "epoch: 37,  batch step: 121, loss: 4.260887145996094\n",
      "epoch: 37,  batch step: 122, loss: 56.37272644042969\n",
      "epoch: 37,  batch step: 123, loss: 5.86590576171875\n",
      "epoch: 37,  batch step: 124, loss: 34.96043014526367\n",
      "epoch: 37,  batch step: 125, loss: 22.985197067260742\n",
      "epoch: 37,  batch step: 126, loss: 5.016060829162598\n",
      "epoch: 37,  batch step: 127, loss: 5.841022491455078\n",
      "epoch: 37,  batch step: 128, loss: 5.118655681610107\n",
      "epoch: 37,  batch step: 129, loss: 48.461273193359375\n",
      "epoch: 37,  batch step: 130, loss: 3.798393964767456\n",
      "epoch: 37,  batch step: 131, loss: 32.92763137817383\n",
      "epoch: 37,  batch step: 132, loss: 6.728000164031982\n",
      "epoch: 37,  batch step: 133, loss: 72.2828369140625\n",
      "epoch: 37,  batch step: 134, loss: 9.030699729919434\n",
      "epoch: 37,  batch step: 135, loss: 69.13075256347656\n",
      "epoch: 37,  batch step: 136, loss: 131.1131134033203\n",
      "epoch: 37,  batch step: 137, loss: 5.24100399017334\n",
      "epoch: 37,  batch step: 138, loss: 39.76498031616211\n",
      "epoch: 37,  batch step: 139, loss: 18.789079666137695\n",
      "epoch: 37,  batch step: 140, loss: 8.366985321044922\n",
      "epoch: 37,  batch step: 141, loss: 8.749323844909668\n",
      "epoch: 37,  batch step: 142, loss: 6.16845703125\n",
      "epoch: 37,  batch step: 143, loss: 5.36586856842041\n",
      "epoch: 37,  batch step: 144, loss: 19.278545379638672\n",
      "epoch: 37,  batch step: 145, loss: 4.900372505187988\n",
      "epoch: 37,  batch step: 146, loss: 41.21154022216797\n",
      "epoch: 37,  batch step: 147, loss: 62.183448791503906\n",
      "epoch: 37,  batch step: 148, loss: 31.84992790222168\n",
      "epoch: 37,  batch step: 149, loss: 6.590539455413818\n",
      "epoch: 37,  batch step: 150, loss: 8.842902183532715\n",
      "epoch: 37,  batch step: 151, loss: 40.70243835449219\n",
      "epoch: 37,  batch step: 152, loss: 10.770648002624512\n",
      "epoch: 37,  batch step: 153, loss: 5.378901958465576\n",
      "epoch: 37,  batch step: 154, loss: 56.72731018066406\n",
      "epoch: 37,  batch step: 155, loss: 30.737613677978516\n",
      "epoch: 37,  batch step: 156, loss: 6.371070384979248\n",
      "epoch: 37,  batch step: 157, loss: 38.03575134277344\n",
      "epoch: 37,  batch step: 158, loss: 9.843563079833984\n",
      "epoch: 37,  batch step: 159, loss: 40.940555572509766\n",
      "epoch: 37,  batch step: 160, loss: 13.012228965759277\n",
      "epoch: 37,  batch step: 161, loss: 30.49297332763672\n",
      "epoch: 37,  batch step: 162, loss: 83.27238464355469\n",
      "epoch: 37,  batch step: 163, loss: 12.192628860473633\n",
      "epoch: 37,  batch step: 164, loss: 8.641321182250977\n",
      "epoch: 37,  batch step: 165, loss: 5.8068389892578125\n",
      "epoch: 37,  batch step: 166, loss: 10.958671569824219\n",
      "epoch: 37,  batch step: 167, loss: 21.741592407226562\n",
      "epoch: 37,  batch step: 168, loss: 45.372005462646484\n",
      "epoch: 37,  batch step: 169, loss: 9.086801528930664\n",
      "epoch: 37,  batch step: 170, loss: 9.836170196533203\n",
      "epoch: 37,  batch step: 171, loss: 29.92845916748047\n",
      "epoch: 37,  batch step: 172, loss: 37.203041076660156\n",
      "epoch: 37,  batch step: 173, loss: 47.726219177246094\n",
      "epoch: 37,  batch step: 174, loss: 9.786415100097656\n",
      "epoch: 37,  batch step: 175, loss: 66.76295471191406\n",
      "epoch: 37,  batch step: 176, loss: 6.111682891845703\n",
      "epoch: 37,  batch step: 177, loss: 10.11493968963623\n",
      "epoch: 37,  batch step: 178, loss: 17.671993255615234\n",
      "epoch: 37,  batch step: 179, loss: 4.461367130279541\n",
      "epoch: 37,  batch step: 180, loss: 15.00879192352295\n",
      "epoch: 37,  batch step: 181, loss: 6.158313274383545\n",
      "epoch: 37,  batch step: 182, loss: 65.30718231201172\n",
      "epoch: 37,  batch step: 183, loss: 5.015008926391602\n",
      "epoch: 37,  batch step: 184, loss: 5.609523773193359\n",
      "epoch: 37,  batch step: 185, loss: 6.9382758140563965\n",
      "epoch: 37,  batch step: 186, loss: 6.486992835998535\n",
      "epoch: 37,  batch step: 187, loss: 14.943143844604492\n",
      "epoch: 37,  batch step: 188, loss: 32.507080078125\n",
      "epoch: 37,  batch step: 189, loss: 56.755531311035156\n",
      "epoch: 37,  batch step: 190, loss: 43.14487838745117\n",
      "epoch: 37,  batch step: 191, loss: 6.110075950622559\n",
      "epoch: 37,  batch step: 192, loss: 42.821502685546875\n",
      "epoch: 37,  batch step: 193, loss: 29.515663146972656\n",
      "epoch: 37,  batch step: 194, loss: 69.73951721191406\n",
      "epoch: 37,  batch step: 195, loss: 62.96466064453125\n",
      "epoch: 37,  batch step: 196, loss: 7.189429759979248\n",
      "epoch: 37,  batch step: 197, loss: 32.74896240234375\n",
      "epoch: 37,  batch step: 198, loss: 11.302888870239258\n",
      "epoch: 37,  batch step: 199, loss: 28.319263458251953\n",
      "epoch: 37,  batch step: 200, loss: 33.213417053222656\n",
      "epoch: 37,  batch step: 201, loss: 10.503969192504883\n",
      "epoch: 37,  batch step: 202, loss: 43.589576721191406\n",
      "epoch: 37,  batch step: 203, loss: 5.097064018249512\n",
      "epoch: 37,  batch step: 204, loss: 16.769954681396484\n",
      "epoch: 37,  batch step: 205, loss: 6.59253454208374\n",
      "epoch: 37,  batch step: 206, loss: 91.14820861816406\n",
      "epoch: 37,  batch step: 207, loss: 11.57901668548584\n",
      "epoch: 37,  batch step: 208, loss: 4.935529708862305\n",
      "epoch: 37,  batch step: 209, loss: 7.16353702545166\n",
      "epoch: 37,  batch step: 210, loss: 6.713135719299316\n",
      "epoch: 37,  batch step: 211, loss: 10.914972305297852\n",
      "epoch: 37,  batch step: 212, loss: 23.692123413085938\n",
      "epoch: 37,  batch step: 213, loss: 5.817095756530762\n",
      "epoch: 37,  batch step: 214, loss: 66.17727661132812\n",
      "epoch: 37,  batch step: 215, loss: 3.1116158962249756\n",
      "epoch: 37,  batch step: 216, loss: 23.448333740234375\n",
      "epoch: 37,  batch step: 217, loss: 57.866798400878906\n",
      "epoch: 37,  batch step: 218, loss: 99.09709167480469\n",
      "epoch: 37,  batch step: 219, loss: 28.822463989257812\n",
      "epoch: 37,  batch step: 220, loss: 5.4854841232299805\n",
      "epoch: 37,  batch step: 221, loss: 94.09115600585938\n",
      "epoch: 37,  batch step: 222, loss: 47.010311126708984\n",
      "epoch: 37,  batch step: 223, loss: 32.139076232910156\n",
      "epoch: 37,  batch step: 224, loss: 37.494239807128906\n",
      "epoch: 37,  batch step: 225, loss: 41.72414016723633\n",
      "epoch: 37,  batch step: 226, loss: 11.029180526733398\n",
      "epoch: 37,  batch step: 227, loss: 54.800086975097656\n",
      "epoch: 37,  batch step: 228, loss: 3.3379600048065186\n",
      "epoch: 37,  batch step: 229, loss: 50.58026123046875\n",
      "epoch: 37,  batch step: 230, loss: 19.840911865234375\n",
      "epoch: 37,  batch step: 231, loss: 45.71563720703125\n",
      "epoch: 37,  batch step: 232, loss: 7.4217529296875\n",
      "epoch: 37,  batch step: 233, loss: 5.171995162963867\n",
      "epoch: 37,  batch step: 234, loss: 54.561859130859375\n",
      "epoch: 37,  batch step: 235, loss: 16.39491844177246\n",
      "epoch: 37,  batch step: 236, loss: 23.549345016479492\n",
      "epoch: 37,  batch step: 237, loss: 8.50014877319336\n",
      "epoch: 37,  batch step: 238, loss: 25.85164451599121\n",
      "epoch: 37,  batch step: 239, loss: 10.391622543334961\n",
      "epoch: 37,  batch step: 240, loss: 10.65870475769043\n",
      "epoch: 37,  batch step: 241, loss: 7.488740921020508\n",
      "epoch: 37,  batch step: 242, loss: 7.0037841796875\n",
      "epoch: 37,  batch step: 243, loss: 63.09116744995117\n",
      "epoch: 37,  batch step: 244, loss: 35.62767791748047\n",
      "epoch: 37,  batch step: 245, loss: 26.836116790771484\n",
      "epoch: 37,  batch step: 246, loss: 4.951512813568115\n",
      "epoch: 37,  batch step: 247, loss: 164.52304077148438\n",
      "epoch: 37,  batch step: 248, loss: 22.858627319335938\n",
      "epoch: 37,  batch step: 249, loss: 32.47686767578125\n",
      "epoch: 37,  batch step: 250, loss: 50.61446762084961\n",
      "epoch: 37,  batch step: 251, loss: 1466.19384765625\n",
      "validation error epoch  37:    tensor(66.9520, device='cuda:0')\n",
      "316\n",
      "epoch: 38,  batch step: 0, loss: 40.533138275146484\n",
      "epoch: 38,  batch step: 1, loss: 10.730987548828125\n",
      "epoch: 38,  batch step: 2, loss: 16.17430305480957\n",
      "epoch: 38,  batch step: 3, loss: 52.644775390625\n",
      "epoch: 38,  batch step: 4, loss: 85.83987426757812\n",
      "epoch: 38,  batch step: 5, loss: 32.51508712768555\n",
      "epoch: 38,  batch step: 6, loss: 43.04212188720703\n",
      "epoch: 38,  batch step: 7, loss: 30.137069702148438\n",
      "epoch: 38,  batch step: 8, loss: 92.21285247802734\n",
      "epoch: 38,  batch step: 9, loss: 23.452083587646484\n",
      "epoch: 38,  batch step: 10, loss: 98.3149642944336\n",
      "epoch: 38,  batch step: 11, loss: 64.27226257324219\n",
      "epoch: 38,  batch step: 12, loss: 33.85845184326172\n",
      "epoch: 38,  batch step: 13, loss: 20.340152740478516\n",
      "epoch: 38,  batch step: 14, loss: 18.172840118408203\n",
      "epoch: 38,  batch step: 15, loss: 14.494712829589844\n",
      "epoch: 38,  batch step: 16, loss: 52.70696258544922\n",
      "epoch: 38,  batch step: 17, loss: 14.042892456054688\n",
      "epoch: 38,  batch step: 18, loss: 19.88465690612793\n",
      "epoch: 38,  batch step: 19, loss: 10.685550689697266\n",
      "epoch: 38,  batch step: 20, loss: 38.31709289550781\n",
      "epoch: 38,  batch step: 21, loss: 19.676992416381836\n",
      "epoch: 38,  batch step: 22, loss: 15.181048393249512\n",
      "epoch: 38,  batch step: 23, loss: 14.359637260437012\n",
      "epoch: 38,  batch step: 24, loss: 35.030113220214844\n",
      "epoch: 38,  batch step: 25, loss: 55.41329574584961\n",
      "epoch: 38,  batch step: 26, loss: 9.998363494873047\n",
      "epoch: 38,  batch step: 27, loss: 70.1629638671875\n",
      "epoch: 38,  batch step: 28, loss: 13.992172241210938\n",
      "epoch: 38,  batch step: 29, loss: 9.501885414123535\n",
      "epoch: 38,  batch step: 30, loss: 131.1462860107422\n",
      "epoch: 38,  batch step: 31, loss: 14.62369155883789\n",
      "epoch: 38,  batch step: 32, loss: 124.89095306396484\n",
      "epoch: 38,  batch step: 33, loss: 60.65157699584961\n",
      "epoch: 38,  batch step: 34, loss: 9.988578796386719\n",
      "epoch: 38,  batch step: 35, loss: 50.21961975097656\n",
      "epoch: 38,  batch step: 36, loss: 6.354853630065918\n",
      "epoch: 38,  batch step: 37, loss: 39.81939697265625\n",
      "epoch: 38,  batch step: 38, loss: 70.41770935058594\n",
      "epoch: 38,  batch step: 39, loss: 36.51927947998047\n",
      "epoch: 38,  batch step: 40, loss: 70.36924743652344\n",
      "epoch: 38,  batch step: 41, loss: 22.524127960205078\n",
      "epoch: 38,  batch step: 42, loss: 35.781837463378906\n",
      "epoch: 38,  batch step: 43, loss: 6.855554103851318\n",
      "epoch: 38,  batch step: 44, loss: 144.5800018310547\n",
      "epoch: 38,  batch step: 45, loss: 14.315206527709961\n",
      "epoch: 38,  batch step: 46, loss: 172.50717163085938\n",
      "epoch: 38,  batch step: 47, loss: 37.12014389038086\n",
      "epoch: 38,  batch step: 48, loss: 16.821697235107422\n",
      "epoch: 38,  batch step: 49, loss: 54.120521545410156\n",
      "epoch: 38,  batch step: 50, loss: 168.76312255859375\n",
      "epoch: 38,  batch step: 51, loss: 32.32963180541992\n",
      "epoch: 38,  batch step: 52, loss: 117.98728942871094\n",
      "epoch: 38,  batch step: 53, loss: 7.698164939880371\n",
      "epoch: 38,  batch step: 54, loss: 88.89573669433594\n",
      "epoch: 38,  batch step: 55, loss: 11.886306762695312\n",
      "epoch: 38,  batch step: 56, loss: 7.603882789611816\n",
      "epoch: 38,  batch step: 57, loss: 9.692520141601562\n",
      "epoch: 38,  batch step: 58, loss: 35.624473571777344\n",
      "epoch: 38,  batch step: 59, loss: 64.53512573242188\n",
      "epoch: 38,  batch step: 60, loss: 4.768607139587402\n",
      "epoch: 38,  batch step: 61, loss: 48.932395935058594\n",
      "epoch: 38,  batch step: 62, loss: 47.5734977722168\n",
      "epoch: 38,  batch step: 63, loss: 54.78242111206055\n",
      "epoch: 38,  batch step: 64, loss: 8.76667594909668\n",
      "epoch: 38,  batch step: 65, loss: 41.502357482910156\n",
      "epoch: 38,  batch step: 66, loss: 10.405376434326172\n",
      "epoch: 38,  batch step: 67, loss: 34.14637756347656\n",
      "epoch: 38,  batch step: 68, loss: 12.23639965057373\n",
      "epoch: 38,  batch step: 69, loss: 4.488835334777832\n",
      "epoch: 38,  batch step: 70, loss: 48.151611328125\n",
      "epoch: 38,  batch step: 71, loss: 34.5318603515625\n",
      "epoch: 38,  batch step: 72, loss: 39.358604431152344\n",
      "epoch: 38,  batch step: 73, loss: 58.3250732421875\n",
      "epoch: 38,  batch step: 74, loss: 60.21959686279297\n",
      "epoch: 38,  batch step: 75, loss: 16.070390701293945\n",
      "epoch: 38,  batch step: 76, loss: 4.862116813659668\n",
      "epoch: 38,  batch step: 77, loss: 9.258726119995117\n",
      "epoch: 38,  batch step: 78, loss: 5.009824275970459\n",
      "epoch: 38,  batch step: 79, loss: 56.905357360839844\n",
      "epoch: 38,  batch step: 80, loss: 10.87452507019043\n",
      "epoch: 38,  batch step: 81, loss: 6.040142059326172\n",
      "epoch: 38,  batch step: 82, loss: 128.5563201904297\n",
      "epoch: 38,  batch step: 83, loss: 22.678327560424805\n",
      "epoch: 38,  batch step: 84, loss: 58.02417755126953\n",
      "epoch: 38,  batch step: 85, loss: 32.179779052734375\n",
      "epoch: 38,  batch step: 86, loss: 34.251373291015625\n",
      "epoch: 38,  batch step: 87, loss: 55.82337188720703\n",
      "epoch: 38,  batch step: 88, loss: 9.030685424804688\n",
      "epoch: 38,  batch step: 89, loss: 10.117355346679688\n",
      "epoch: 38,  batch step: 90, loss: 5.536290645599365\n",
      "epoch: 38,  batch step: 91, loss: 48.68040466308594\n",
      "epoch: 38,  batch step: 92, loss: 29.135055541992188\n",
      "epoch: 38,  batch step: 93, loss: 67.01709747314453\n",
      "epoch: 38,  batch step: 94, loss: 19.77411651611328\n",
      "epoch: 38,  batch step: 95, loss: 134.41680908203125\n",
      "epoch: 38,  batch step: 96, loss: 32.11576843261719\n",
      "epoch: 38,  batch step: 97, loss: 6.644301414489746\n",
      "epoch: 38,  batch step: 98, loss: 53.13086700439453\n",
      "epoch: 38,  batch step: 99, loss: 45.11063766479492\n",
      "epoch: 38,  batch step: 100, loss: 8.533832550048828\n",
      "epoch: 38,  batch step: 101, loss: 75.17753601074219\n",
      "epoch: 38,  batch step: 102, loss: 73.60990905761719\n",
      "epoch: 38,  batch step: 103, loss: 12.831296920776367\n",
      "epoch: 38,  batch step: 104, loss: 8.235529899597168\n",
      "epoch: 38,  batch step: 105, loss: 13.964906692504883\n",
      "epoch: 38,  batch step: 106, loss: 45.470436096191406\n",
      "epoch: 38,  batch step: 107, loss: 30.49409294128418\n",
      "epoch: 38,  batch step: 108, loss: 10.319467544555664\n",
      "epoch: 38,  batch step: 109, loss: 18.347976684570312\n",
      "epoch: 38,  batch step: 110, loss: 10.15822982788086\n",
      "epoch: 38,  batch step: 111, loss: 17.554834365844727\n",
      "epoch: 38,  batch step: 112, loss: 32.14823913574219\n",
      "epoch: 38,  batch step: 113, loss: 15.736921310424805\n",
      "epoch: 38,  batch step: 114, loss: 5.653151512145996\n",
      "epoch: 38,  batch step: 115, loss: 13.479789733886719\n",
      "epoch: 38,  batch step: 116, loss: 7.114253520965576\n",
      "epoch: 38,  batch step: 117, loss: 3.8969485759735107\n",
      "epoch: 38,  batch step: 118, loss: 58.79087829589844\n",
      "epoch: 38,  batch step: 119, loss: 9.0570068359375\n",
      "epoch: 38,  batch step: 120, loss: 214.8966064453125\n",
      "epoch: 38,  batch step: 121, loss: 22.353282928466797\n",
      "epoch: 38,  batch step: 122, loss: 8.8704252243042\n",
      "epoch: 38,  batch step: 123, loss: 56.29352569580078\n",
      "epoch: 38,  batch step: 124, loss: 8.075623512268066\n",
      "epoch: 38,  batch step: 125, loss: 7.84842586517334\n",
      "epoch: 38,  batch step: 126, loss: 11.426989555358887\n",
      "epoch: 38,  batch step: 127, loss: 71.31867218017578\n",
      "epoch: 38,  batch step: 128, loss: 17.5551815032959\n",
      "epoch: 38,  batch step: 129, loss: 22.02865219116211\n",
      "epoch: 38,  batch step: 130, loss: 34.0207633972168\n",
      "epoch: 38,  batch step: 131, loss: 6.122300148010254\n",
      "epoch: 38,  batch step: 132, loss: 28.509124755859375\n",
      "epoch: 38,  batch step: 133, loss: 10.403051376342773\n",
      "epoch: 38,  batch step: 134, loss: 8.381847381591797\n",
      "epoch: 38,  batch step: 135, loss: 30.575664520263672\n",
      "epoch: 38,  batch step: 136, loss: 10.46334457397461\n",
      "epoch: 38,  batch step: 137, loss: 12.778066635131836\n",
      "epoch: 38,  batch step: 138, loss: 8.144747734069824\n",
      "epoch: 38,  batch step: 139, loss: 31.780380249023438\n",
      "epoch: 38,  batch step: 140, loss: 9.631753921508789\n",
      "epoch: 38,  batch step: 141, loss: 46.40519332885742\n",
      "epoch: 38,  batch step: 142, loss: 8.073272705078125\n",
      "epoch: 38,  batch step: 143, loss: 20.373891830444336\n",
      "epoch: 38,  batch step: 144, loss: 49.886993408203125\n",
      "epoch: 38,  batch step: 145, loss: 22.869915008544922\n",
      "epoch: 38,  batch step: 146, loss: 33.75006103515625\n",
      "epoch: 38,  batch step: 147, loss: 8.641925811767578\n",
      "epoch: 38,  batch step: 148, loss: 42.29920196533203\n",
      "epoch: 38,  batch step: 149, loss: 13.67879867553711\n",
      "epoch: 38,  batch step: 150, loss: 29.064870834350586\n",
      "epoch: 38,  batch step: 151, loss: 4.920995712280273\n",
      "epoch: 38,  batch step: 152, loss: 179.7200164794922\n",
      "epoch: 38,  batch step: 153, loss: 6.883096218109131\n",
      "epoch: 38,  batch step: 154, loss: 6.7428717613220215\n",
      "epoch: 38,  batch step: 155, loss: 78.738525390625\n",
      "epoch: 38,  batch step: 156, loss: 15.220325469970703\n",
      "epoch: 38,  batch step: 157, loss: 3.065951347351074\n",
      "epoch: 38,  batch step: 158, loss: 12.263335227966309\n",
      "epoch: 38,  batch step: 159, loss: 8.8218412399292\n",
      "epoch: 38,  batch step: 160, loss: 5.343328475952148\n",
      "epoch: 38,  batch step: 161, loss: 5.588006496429443\n",
      "epoch: 38,  batch step: 162, loss: 15.707071304321289\n",
      "epoch: 38,  batch step: 163, loss: 24.50507164001465\n",
      "epoch: 38,  batch step: 164, loss: 9.449006080627441\n",
      "epoch: 38,  batch step: 165, loss: 8.599864959716797\n",
      "epoch: 38,  batch step: 166, loss: 9.107938766479492\n",
      "epoch: 38,  batch step: 167, loss: 51.12189483642578\n",
      "epoch: 38,  batch step: 168, loss: 128.6352081298828\n",
      "epoch: 38,  batch step: 169, loss: 35.31959915161133\n",
      "epoch: 38,  batch step: 170, loss: 8.78882122039795\n",
      "epoch: 38,  batch step: 171, loss: 38.09519577026367\n",
      "epoch: 38,  batch step: 172, loss: 64.92210388183594\n",
      "epoch: 38,  batch step: 173, loss: 8.425729751586914\n",
      "epoch: 38,  batch step: 174, loss: 17.869945526123047\n",
      "epoch: 38,  batch step: 175, loss: 10.624053001403809\n",
      "epoch: 38,  batch step: 176, loss: 32.164676666259766\n",
      "epoch: 38,  batch step: 177, loss: 6.120297908782959\n",
      "epoch: 38,  batch step: 178, loss: 3.3027801513671875\n",
      "epoch: 38,  batch step: 179, loss: 18.702131271362305\n",
      "epoch: 38,  batch step: 180, loss: 7.724210739135742\n",
      "epoch: 38,  batch step: 181, loss: 92.38168334960938\n",
      "epoch: 38,  batch step: 182, loss: 72.13528442382812\n",
      "epoch: 38,  batch step: 183, loss: 47.48221969604492\n",
      "epoch: 38,  batch step: 184, loss: 8.107261657714844\n",
      "epoch: 38,  batch step: 185, loss: 15.2720365524292\n",
      "epoch: 38,  batch step: 186, loss: 13.34779167175293\n",
      "epoch: 38,  batch step: 187, loss: 41.90240478515625\n",
      "epoch: 38,  batch step: 188, loss: 35.73554229736328\n",
      "epoch: 38,  batch step: 189, loss: 85.2260971069336\n",
      "epoch: 38,  batch step: 190, loss: 8.628183364868164\n",
      "epoch: 38,  batch step: 191, loss: 26.468381881713867\n",
      "epoch: 38,  batch step: 192, loss: 8.774576187133789\n",
      "epoch: 38,  batch step: 193, loss: 7.661688804626465\n",
      "epoch: 38,  batch step: 194, loss: 96.9536361694336\n",
      "epoch: 38,  batch step: 195, loss: 7.137907981872559\n",
      "epoch: 38,  batch step: 196, loss: 65.71173095703125\n",
      "epoch: 38,  batch step: 197, loss: 9.777955055236816\n",
      "epoch: 38,  batch step: 198, loss: 20.50021743774414\n",
      "epoch: 38,  batch step: 199, loss: 5.484305381774902\n",
      "epoch: 38,  batch step: 200, loss: 6.2274274826049805\n",
      "epoch: 38,  batch step: 201, loss: 6.7115983963012695\n",
      "epoch: 38,  batch step: 202, loss: 89.37509155273438\n",
      "epoch: 38,  batch step: 203, loss: 91.0678939819336\n",
      "epoch: 38,  batch step: 204, loss: 34.458946228027344\n",
      "epoch: 38,  batch step: 205, loss: 43.42780685424805\n",
      "epoch: 38,  batch step: 206, loss: 3.53985595703125\n",
      "epoch: 38,  batch step: 207, loss: 40.19715881347656\n",
      "epoch: 38,  batch step: 208, loss: 56.59519958496094\n",
      "epoch: 38,  batch step: 209, loss: 31.488134384155273\n",
      "epoch: 38,  batch step: 210, loss: 4.561806678771973\n",
      "epoch: 38,  batch step: 211, loss: 6.272502899169922\n",
      "epoch: 38,  batch step: 212, loss: 7.450645446777344\n",
      "epoch: 38,  batch step: 213, loss: 43.37826156616211\n",
      "epoch: 38,  batch step: 214, loss: 13.807581901550293\n",
      "epoch: 38,  batch step: 215, loss: 57.72222900390625\n",
      "epoch: 38,  batch step: 216, loss: 59.37097930908203\n",
      "epoch: 38,  batch step: 217, loss: 15.027032852172852\n",
      "epoch: 38,  batch step: 218, loss: 104.64689636230469\n",
      "epoch: 38,  batch step: 219, loss: 8.749025344848633\n",
      "epoch: 38,  batch step: 220, loss: 8.239676475524902\n",
      "epoch: 38,  batch step: 221, loss: 52.774200439453125\n",
      "epoch: 38,  batch step: 222, loss: 7.5403852462768555\n",
      "epoch: 38,  batch step: 223, loss: 6.249382019042969\n",
      "epoch: 38,  batch step: 224, loss: 17.158552169799805\n",
      "epoch: 38,  batch step: 225, loss: 6.573022365570068\n",
      "epoch: 38,  batch step: 226, loss: 70.76824188232422\n",
      "epoch: 38,  batch step: 227, loss: 11.120058059692383\n",
      "epoch: 38,  batch step: 228, loss: 9.501789093017578\n",
      "epoch: 38,  batch step: 229, loss: 26.693683624267578\n",
      "epoch: 38,  batch step: 230, loss: 107.00300598144531\n",
      "epoch: 38,  batch step: 231, loss: 4.565188884735107\n",
      "epoch: 38,  batch step: 232, loss: 13.672292709350586\n",
      "epoch: 38,  batch step: 233, loss: 44.708213806152344\n",
      "epoch: 38,  batch step: 234, loss: 20.545547485351562\n",
      "epoch: 38,  batch step: 235, loss: 47.067840576171875\n",
      "epoch: 38,  batch step: 236, loss: 4.8808913230896\n",
      "epoch: 38,  batch step: 237, loss: 35.95439147949219\n",
      "epoch: 38,  batch step: 238, loss: 8.301275253295898\n",
      "epoch: 38,  batch step: 239, loss: 13.605140686035156\n",
      "epoch: 38,  batch step: 240, loss: 93.31320190429688\n",
      "epoch: 38,  batch step: 241, loss: 6.395532131195068\n",
      "epoch: 38,  batch step: 242, loss: 8.06196403503418\n",
      "epoch: 38,  batch step: 243, loss: 4.964583873748779\n",
      "epoch: 38,  batch step: 244, loss: 95.17687225341797\n",
      "epoch: 38,  batch step: 245, loss: 5.387484550476074\n",
      "epoch: 38,  batch step: 246, loss: 5.7955451011657715\n",
      "epoch: 38,  batch step: 247, loss: 289.5806884765625\n",
      "epoch: 38,  batch step: 248, loss: 4.987720012664795\n",
      "epoch: 38,  batch step: 249, loss: 40.33179473876953\n",
      "epoch: 38,  batch step: 250, loss: 93.73088836669922\n",
      "epoch: 38,  batch step: 251, loss: 41.31383514404297\n",
      "validation error epoch  38:    tensor(87.1357, device='cuda:0')\n",
      "316\n",
      "epoch: 39,  batch step: 0, loss: 10.950202941894531\n",
      "epoch: 39,  batch step: 1, loss: 50.68146896362305\n",
      "epoch: 39,  batch step: 2, loss: 49.45888900756836\n",
      "epoch: 39,  batch step: 3, loss: 86.08100891113281\n",
      "epoch: 39,  batch step: 4, loss: 48.79026412963867\n",
      "epoch: 39,  batch step: 5, loss: 5.31480598449707\n",
      "epoch: 39,  batch step: 6, loss: 19.221881866455078\n",
      "epoch: 39,  batch step: 7, loss: 12.413268089294434\n",
      "epoch: 39,  batch step: 8, loss: 10.970088005065918\n",
      "epoch: 39,  batch step: 9, loss: 6.941904067993164\n",
      "epoch: 39,  batch step: 10, loss: 37.382442474365234\n",
      "epoch: 39,  batch step: 11, loss: 46.94053268432617\n",
      "epoch: 39,  batch step: 12, loss: 51.81681442260742\n",
      "epoch: 39,  batch step: 13, loss: 6.045383453369141\n",
      "epoch: 39,  batch step: 14, loss: 129.51177978515625\n",
      "epoch: 39,  batch step: 15, loss: 6.131181716918945\n",
      "epoch: 39,  batch step: 16, loss: 4.11939811706543\n",
      "epoch: 39,  batch step: 17, loss: 35.6981201171875\n",
      "epoch: 39,  batch step: 18, loss: 80.17729187011719\n",
      "epoch: 39,  batch step: 19, loss: 12.039371490478516\n",
      "epoch: 39,  batch step: 20, loss: 28.750043869018555\n",
      "epoch: 39,  batch step: 21, loss: 4.403478145599365\n",
      "epoch: 39,  batch step: 22, loss: 57.82355880737305\n",
      "epoch: 39,  batch step: 23, loss: 6.054032802581787\n",
      "epoch: 39,  batch step: 24, loss: 27.01718521118164\n",
      "epoch: 39,  batch step: 25, loss: 10.531021118164062\n",
      "epoch: 39,  batch step: 26, loss: 10.538902282714844\n",
      "epoch: 39,  batch step: 27, loss: 85.32913970947266\n",
      "epoch: 39,  batch step: 28, loss: 5.6695685386657715\n",
      "epoch: 39,  batch step: 29, loss: 56.45214080810547\n",
      "epoch: 39,  batch step: 30, loss: 31.751110076904297\n",
      "epoch: 39,  batch step: 31, loss: 20.25286293029785\n",
      "epoch: 39,  batch step: 32, loss: 7.050504207611084\n",
      "epoch: 39,  batch step: 33, loss: 124.615234375\n",
      "epoch: 39,  batch step: 34, loss: 6.906915664672852\n",
      "epoch: 39,  batch step: 35, loss: 9.17697811126709\n",
      "epoch: 39,  batch step: 36, loss: 42.84685516357422\n",
      "epoch: 39,  batch step: 37, loss: 7.873159408569336\n",
      "epoch: 39,  batch step: 38, loss: 20.946277618408203\n",
      "epoch: 39,  batch step: 39, loss: 63.997047424316406\n",
      "epoch: 39,  batch step: 40, loss: 23.590301513671875\n",
      "epoch: 39,  batch step: 41, loss: 27.215368270874023\n",
      "epoch: 39,  batch step: 42, loss: 6.849732875823975\n",
      "epoch: 39,  batch step: 43, loss: 52.91130828857422\n",
      "epoch: 39,  batch step: 44, loss: 75.22313690185547\n",
      "epoch: 39,  batch step: 45, loss: 5.8369574546813965\n",
      "epoch: 39,  batch step: 46, loss: 3.968925952911377\n",
      "epoch: 39,  batch step: 47, loss: 9.799245834350586\n",
      "epoch: 39,  batch step: 48, loss: 71.43628692626953\n",
      "epoch: 39,  batch step: 49, loss: 10.955986022949219\n",
      "epoch: 39,  batch step: 50, loss: 47.10468673706055\n",
      "epoch: 39,  batch step: 51, loss: 4.9165143966674805\n",
      "epoch: 39,  batch step: 52, loss: 7.688117504119873\n",
      "epoch: 39,  batch step: 53, loss: 5.0474371910095215\n",
      "epoch: 39,  batch step: 54, loss: 69.6386947631836\n",
      "epoch: 39,  batch step: 55, loss: 7.514866828918457\n",
      "epoch: 39,  batch step: 56, loss: 20.072771072387695\n",
      "epoch: 39,  batch step: 57, loss: 18.917463302612305\n",
      "epoch: 39,  batch step: 58, loss: 85.57096862792969\n",
      "epoch: 39,  batch step: 59, loss: 67.35004425048828\n",
      "epoch: 39,  batch step: 60, loss: 6.363929748535156\n",
      "epoch: 39,  batch step: 61, loss: 7.757692337036133\n",
      "epoch: 39,  batch step: 62, loss: 107.12443542480469\n",
      "epoch: 39,  batch step: 63, loss: 28.29690170288086\n",
      "epoch: 39,  batch step: 64, loss: 13.654536247253418\n",
      "epoch: 39,  batch step: 65, loss: 31.064231872558594\n",
      "epoch: 39,  batch step: 66, loss: 77.86634826660156\n",
      "epoch: 39,  batch step: 67, loss: 8.121224403381348\n",
      "epoch: 39,  batch step: 68, loss: 12.263168334960938\n",
      "epoch: 39,  batch step: 69, loss: 33.28185272216797\n",
      "epoch: 39,  batch step: 70, loss: 28.96190643310547\n",
      "epoch: 39,  batch step: 71, loss: 3.4137351512908936\n",
      "epoch: 39,  batch step: 72, loss: 19.15156364440918\n",
      "epoch: 39,  batch step: 73, loss: 11.521650314331055\n",
      "epoch: 39,  batch step: 74, loss: 41.26532745361328\n",
      "epoch: 39,  batch step: 75, loss: 4.047580242156982\n",
      "epoch: 39,  batch step: 76, loss: 16.99030876159668\n",
      "epoch: 39,  batch step: 77, loss: 12.999701499938965\n",
      "epoch: 39,  batch step: 78, loss: 14.889976501464844\n",
      "epoch: 39,  batch step: 79, loss: 21.18768310546875\n",
      "epoch: 39,  batch step: 80, loss: 6.949344635009766\n",
      "epoch: 39,  batch step: 81, loss: 8.553560256958008\n",
      "epoch: 39,  batch step: 82, loss: 92.23946380615234\n",
      "epoch: 39,  batch step: 83, loss: 28.027538299560547\n",
      "epoch: 39,  batch step: 84, loss: 68.62496185302734\n",
      "epoch: 39,  batch step: 85, loss: 35.72407531738281\n",
      "epoch: 39,  batch step: 86, loss: 40.32084655761719\n",
      "epoch: 39,  batch step: 87, loss: 73.50918579101562\n",
      "epoch: 39,  batch step: 88, loss: 46.39335632324219\n",
      "epoch: 39,  batch step: 89, loss: 133.51768493652344\n",
      "epoch: 39,  batch step: 90, loss: 15.940412521362305\n",
      "epoch: 39,  batch step: 91, loss: 49.103023529052734\n",
      "epoch: 39,  batch step: 92, loss: 8.97464370727539\n",
      "epoch: 39,  batch step: 93, loss: 98.0112533569336\n",
      "epoch: 39,  batch step: 94, loss: 28.496381759643555\n",
      "epoch: 39,  batch step: 95, loss: 32.60231399536133\n",
      "epoch: 39,  batch step: 96, loss: 9.131854057312012\n",
      "epoch: 39,  batch step: 97, loss: 5.74588680267334\n",
      "epoch: 39,  batch step: 98, loss: 13.769380569458008\n",
      "epoch: 39,  batch step: 99, loss: 12.028341293334961\n",
      "epoch: 39,  batch step: 100, loss: 32.41472625732422\n",
      "epoch: 39,  batch step: 101, loss: 37.15308380126953\n",
      "epoch: 39,  batch step: 102, loss: 12.415770530700684\n",
      "epoch: 39,  batch step: 103, loss: 18.647459030151367\n",
      "epoch: 39,  batch step: 104, loss: 10.932453155517578\n",
      "epoch: 39,  batch step: 105, loss: 5.878637313842773\n",
      "epoch: 39,  batch step: 106, loss: 12.472739219665527\n",
      "epoch: 39,  batch step: 107, loss: 108.02227783203125\n",
      "epoch: 39,  batch step: 108, loss: 72.30863952636719\n",
      "epoch: 39,  batch step: 109, loss: 15.444053649902344\n",
      "epoch: 39,  batch step: 110, loss: 5.355403423309326\n",
      "epoch: 39,  batch step: 111, loss: 22.360923767089844\n",
      "epoch: 39,  batch step: 112, loss: 8.997906684875488\n",
      "epoch: 39,  batch step: 113, loss: 61.16169738769531\n",
      "epoch: 39,  batch step: 114, loss: 6.33295202255249\n",
      "epoch: 39,  batch step: 115, loss: 132.4902801513672\n",
      "epoch: 39,  batch step: 116, loss: 46.94683837890625\n",
      "epoch: 39,  batch step: 117, loss: 10.43423080444336\n",
      "epoch: 39,  batch step: 118, loss: 7.029996395111084\n",
      "epoch: 39,  batch step: 119, loss: 6.940783500671387\n",
      "epoch: 39,  batch step: 120, loss: 10.46518611907959\n",
      "epoch: 39,  batch step: 121, loss: 14.196605682373047\n",
      "epoch: 39,  batch step: 122, loss: 14.895233154296875\n",
      "epoch: 39,  batch step: 123, loss: 12.622424125671387\n",
      "epoch: 39,  batch step: 124, loss: 54.923789978027344\n",
      "epoch: 39,  batch step: 125, loss: 24.105220794677734\n",
      "epoch: 39,  batch step: 126, loss: 44.68412780761719\n",
      "epoch: 39,  batch step: 127, loss: 104.95333862304688\n",
      "epoch: 39,  batch step: 128, loss: 5.030402183532715\n",
      "epoch: 39,  batch step: 129, loss: 72.27970123291016\n",
      "epoch: 39,  batch step: 130, loss: 41.7938232421875\n",
      "epoch: 39,  batch step: 131, loss: 5.9047651290893555\n",
      "epoch: 39,  batch step: 132, loss: 31.54462432861328\n",
      "epoch: 39,  batch step: 133, loss: 16.663009643554688\n",
      "epoch: 39,  batch step: 134, loss: 5.591047286987305\n",
      "epoch: 39,  batch step: 135, loss: 33.54866409301758\n",
      "epoch: 39,  batch step: 136, loss: 8.19693660736084\n",
      "epoch: 39,  batch step: 137, loss: 12.588297843933105\n",
      "epoch: 39,  batch step: 138, loss: 38.46660614013672\n",
      "epoch: 39,  batch step: 139, loss: 38.221351623535156\n",
      "epoch: 39,  batch step: 140, loss: 16.861677169799805\n",
      "epoch: 39,  batch step: 141, loss: 3.543393611907959\n",
      "epoch: 39,  batch step: 142, loss: 5.438982963562012\n",
      "epoch: 39,  batch step: 143, loss: 31.889556884765625\n",
      "epoch: 39,  batch step: 144, loss: 10.192278861999512\n",
      "epoch: 39,  batch step: 145, loss: 20.600833892822266\n",
      "epoch: 39,  batch step: 146, loss: 25.780242919921875\n",
      "epoch: 39,  batch step: 147, loss: 94.56810760498047\n",
      "epoch: 39,  batch step: 148, loss: 4.614638328552246\n",
      "epoch: 39,  batch step: 149, loss: 49.49144744873047\n",
      "epoch: 39,  batch step: 150, loss: 9.537130355834961\n",
      "epoch: 39,  batch step: 151, loss: 51.33344650268555\n",
      "epoch: 39,  batch step: 152, loss: 4.490949630737305\n",
      "epoch: 39,  batch step: 153, loss: 7.355426788330078\n",
      "epoch: 39,  batch step: 154, loss: 100.6316146850586\n",
      "epoch: 39,  batch step: 155, loss: 31.297080993652344\n",
      "epoch: 39,  batch step: 156, loss: 8.04578685760498\n",
      "epoch: 39,  batch step: 157, loss: 10.61684513092041\n",
      "epoch: 39,  batch step: 158, loss: 11.383814811706543\n",
      "epoch: 39,  batch step: 159, loss: 19.85965347290039\n",
      "epoch: 39,  batch step: 160, loss: 5.720023155212402\n",
      "epoch: 39,  batch step: 161, loss: 27.319900512695312\n",
      "epoch: 39,  batch step: 162, loss: 29.461746215820312\n",
      "epoch: 39,  batch step: 163, loss: 22.803403854370117\n",
      "epoch: 39,  batch step: 164, loss: 130.6064910888672\n",
      "epoch: 39,  batch step: 165, loss: 4.17580509185791\n",
      "epoch: 39,  batch step: 166, loss: 55.974693298339844\n",
      "epoch: 39,  batch step: 167, loss: 6.295407295227051\n",
      "epoch: 39,  batch step: 168, loss: 27.803762435913086\n",
      "epoch: 39,  batch step: 169, loss: 87.9923324584961\n",
      "epoch: 39,  batch step: 170, loss: 4.366095066070557\n",
      "epoch: 39,  batch step: 171, loss: 3.930023193359375\n",
      "epoch: 39,  batch step: 172, loss: 15.2144193649292\n",
      "epoch: 39,  batch step: 173, loss: 6.32310676574707\n",
      "epoch: 39,  batch step: 174, loss: 10.772562980651855\n",
      "epoch: 39,  batch step: 175, loss: 6.253263473510742\n",
      "epoch: 39,  batch step: 176, loss: 53.497154235839844\n",
      "epoch: 39,  batch step: 177, loss: 32.30129623413086\n",
      "epoch: 39,  batch step: 178, loss: 5.478858470916748\n",
      "epoch: 39,  batch step: 179, loss: 13.59522819519043\n",
      "epoch: 39,  batch step: 180, loss: 5.275110244750977\n",
      "epoch: 39,  batch step: 181, loss: 48.58192443847656\n",
      "epoch: 39,  batch step: 182, loss: 61.405906677246094\n",
      "epoch: 39,  batch step: 183, loss: 88.86619567871094\n",
      "epoch: 39,  batch step: 184, loss: 9.100236892700195\n",
      "epoch: 39,  batch step: 185, loss: 85.01551818847656\n",
      "epoch: 39,  batch step: 186, loss: 6.155811309814453\n",
      "epoch: 39,  batch step: 187, loss: 28.537734985351562\n",
      "epoch: 39,  batch step: 188, loss: 71.23511505126953\n",
      "epoch: 39,  batch step: 189, loss: 5.865279197692871\n",
      "epoch: 39,  batch step: 190, loss: 57.005859375\n",
      "epoch: 39,  batch step: 191, loss: 3.1472606658935547\n",
      "epoch: 39,  batch step: 192, loss: 7.897227764129639\n",
      "epoch: 39,  batch step: 193, loss: 248.02484130859375\n",
      "epoch: 39,  batch step: 194, loss: 6.129817008972168\n",
      "epoch: 39,  batch step: 195, loss: 64.32600402832031\n",
      "epoch: 39,  batch step: 196, loss: 41.06861114501953\n",
      "epoch: 39,  batch step: 197, loss: 27.138076782226562\n",
      "epoch: 39,  batch step: 198, loss: 36.73196792602539\n",
      "epoch: 39,  batch step: 199, loss: 46.98918914794922\n",
      "epoch: 39,  batch step: 200, loss: 29.634090423583984\n",
      "epoch: 39,  batch step: 201, loss: 53.05044174194336\n",
      "epoch: 39,  batch step: 202, loss: 21.289493560791016\n",
      "epoch: 39,  batch step: 203, loss: 36.48535919189453\n",
      "epoch: 39,  batch step: 204, loss: 47.872596740722656\n",
      "epoch: 39,  batch step: 205, loss: 6.990984916687012\n",
      "epoch: 39,  batch step: 206, loss: 63.60871124267578\n",
      "epoch: 39,  batch step: 207, loss: 19.400381088256836\n",
      "epoch: 39,  batch step: 208, loss: 66.54376220703125\n",
      "epoch: 39,  batch step: 209, loss: 59.27897262573242\n",
      "epoch: 39,  batch step: 210, loss: 6.492201805114746\n",
      "epoch: 39,  batch step: 211, loss: 5.295097351074219\n",
      "epoch: 39,  batch step: 212, loss: 75.95986938476562\n",
      "epoch: 39,  batch step: 213, loss: 6.447770595550537\n",
      "epoch: 39,  batch step: 214, loss: 11.901293754577637\n",
      "epoch: 39,  batch step: 215, loss: 34.0657844543457\n",
      "epoch: 39,  batch step: 216, loss: 11.823569297790527\n",
      "epoch: 39,  batch step: 217, loss: 35.1192626953125\n",
      "epoch: 39,  batch step: 218, loss: 8.266326904296875\n",
      "epoch: 39,  batch step: 219, loss: 12.131546020507812\n",
      "epoch: 39,  batch step: 220, loss: 30.672340393066406\n",
      "epoch: 39,  batch step: 221, loss: 59.71788787841797\n",
      "epoch: 39,  batch step: 222, loss: 6.294149398803711\n",
      "epoch: 39,  batch step: 223, loss: 8.242961883544922\n",
      "epoch: 39,  batch step: 224, loss: 82.0105209350586\n",
      "epoch: 39,  batch step: 225, loss: 10.791762351989746\n",
      "epoch: 39,  batch step: 226, loss: 18.445892333984375\n",
      "epoch: 39,  batch step: 227, loss: 40.070289611816406\n",
      "epoch: 39,  batch step: 228, loss: 5.160901069641113\n",
      "epoch: 39,  batch step: 229, loss: 160.26470947265625\n",
      "epoch: 39,  batch step: 230, loss: 4.3157477378845215\n",
      "epoch: 39,  batch step: 231, loss: 43.478515625\n",
      "epoch: 39,  batch step: 232, loss: 5.328426361083984\n",
      "epoch: 39,  batch step: 233, loss: 4.374082088470459\n",
      "epoch: 39,  batch step: 234, loss: 4.681830406188965\n",
      "epoch: 39,  batch step: 235, loss: 7.863185882568359\n",
      "epoch: 39,  batch step: 236, loss: 6.929099082946777\n",
      "epoch: 39,  batch step: 237, loss: 6.118399620056152\n",
      "epoch: 39,  batch step: 238, loss: 30.396343231201172\n",
      "epoch: 39,  batch step: 239, loss: 6.574978828430176\n",
      "epoch: 39,  batch step: 240, loss: 102.11195373535156\n",
      "epoch: 39,  batch step: 241, loss: 20.305723190307617\n",
      "epoch: 39,  batch step: 242, loss: 55.79100799560547\n",
      "epoch: 39,  batch step: 243, loss: 22.636398315429688\n",
      "epoch: 39,  batch step: 244, loss: 37.77740478515625\n",
      "epoch: 39,  batch step: 245, loss: 6.322251319885254\n",
      "epoch: 39,  batch step: 246, loss: 9.59830093383789\n",
      "epoch: 39,  batch step: 247, loss: 136.52952575683594\n",
      "epoch: 39,  batch step: 248, loss: 17.97747230529785\n",
      "epoch: 39,  batch step: 249, loss: 13.620954513549805\n",
      "epoch: 39,  batch step: 250, loss: 38.78733825683594\n",
      "epoch: 39,  batch step: 251, loss: 17.42132568359375\n",
      "finished saving checkpoints\n",
      "validation error epoch  39:    tensor(70.3634, device='cuda:0')\n",
      "316\n",
      "epoch: 40,  batch step: 0, loss: 29.547016143798828\n",
      "epoch: 40,  batch step: 1, loss: 4.527654647827148\n",
      "epoch: 40,  batch step: 2, loss: 54.657005310058594\n",
      "epoch: 40,  batch step: 3, loss: 3.593465805053711\n",
      "epoch: 40,  batch step: 4, loss: 4.340064525604248\n",
      "epoch: 40,  batch step: 5, loss: 24.770370483398438\n",
      "epoch: 40,  batch step: 6, loss: 18.877281188964844\n",
      "epoch: 40,  batch step: 7, loss: 46.81013488769531\n",
      "epoch: 40,  batch step: 8, loss: 3.7230725288391113\n",
      "epoch: 40,  batch step: 9, loss: 115.36714172363281\n",
      "epoch: 40,  batch step: 10, loss: 9.462233543395996\n",
      "epoch: 40,  batch step: 11, loss: 53.82468795776367\n",
      "epoch: 40,  batch step: 12, loss: 6.21552848815918\n",
      "epoch: 40,  batch step: 13, loss: 20.16843605041504\n",
      "epoch: 40,  batch step: 14, loss: 6.110860824584961\n",
      "epoch: 40,  batch step: 15, loss: 5.055066108703613\n",
      "epoch: 40,  batch step: 16, loss: 12.263057708740234\n",
      "epoch: 40,  batch step: 17, loss: 9.790384292602539\n",
      "epoch: 40,  batch step: 18, loss: 5.856764793395996\n",
      "epoch: 40,  batch step: 19, loss: 26.434621810913086\n",
      "epoch: 40,  batch step: 20, loss: 33.412899017333984\n",
      "epoch: 40,  batch step: 21, loss: 77.71434020996094\n",
      "epoch: 40,  batch step: 22, loss: 8.722131729125977\n",
      "epoch: 40,  batch step: 23, loss: 5.5823211669921875\n",
      "epoch: 40,  batch step: 24, loss: 36.70464324951172\n",
      "epoch: 40,  batch step: 25, loss: 5.942811489105225\n",
      "epoch: 40,  batch step: 26, loss: 91.79064178466797\n",
      "epoch: 40,  batch step: 27, loss: 3.806446075439453\n",
      "epoch: 40,  batch step: 28, loss: 21.916275024414062\n",
      "epoch: 40,  batch step: 29, loss: 68.26763916015625\n",
      "epoch: 40,  batch step: 30, loss: 6.864192008972168\n",
      "epoch: 40,  batch step: 31, loss: 35.715126037597656\n",
      "epoch: 40,  batch step: 32, loss: 3.294400215148926\n",
      "epoch: 40,  batch step: 33, loss: 61.079551696777344\n",
      "epoch: 40,  batch step: 34, loss: 16.079681396484375\n",
      "epoch: 40,  batch step: 35, loss: 33.07655715942383\n",
      "epoch: 40,  batch step: 36, loss: 60.454742431640625\n",
      "epoch: 40,  batch step: 37, loss: 5.019113540649414\n",
      "epoch: 40,  batch step: 38, loss: 7.671544551849365\n",
      "epoch: 40,  batch step: 39, loss: 28.963300704956055\n",
      "epoch: 40,  batch step: 40, loss: 4.2356791496276855\n",
      "epoch: 40,  batch step: 41, loss: 35.297969818115234\n",
      "epoch: 40,  batch step: 42, loss: 5.081716537475586\n",
      "epoch: 40,  batch step: 43, loss: 14.649269104003906\n",
      "epoch: 40,  batch step: 44, loss: 6.143333435058594\n",
      "epoch: 40,  batch step: 45, loss: 19.979164123535156\n",
      "epoch: 40,  batch step: 46, loss: 50.24009704589844\n",
      "epoch: 40,  batch step: 47, loss: 7.845703125\n",
      "epoch: 40,  batch step: 48, loss: 19.934833526611328\n",
      "epoch: 40,  batch step: 49, loss: 29.068557739257812\n",
      "epoch: 40,  batch step: 50, loss: 5.729975700378418\n",
      "epoch: 40,  batch step: 51, loss: 5.512144088745117\n",
      "epoch: 40,  batch step: 52, loss: 50.01687240600586\n",
      "epoch: 40,  batch step: 53, loss: 3.7654130458831787\n",
      "epoch: 40,  batch step: 54, loss: 10.557024002075195\n",
      "epoch: 40,  batch step: 55, loss: 3.0519447326660156\n",
      "epoch: 40,  batch step: 56, loss: 12.241413116455078\n",
      "epoch: 40,  batch step: 57, loss: 17.562824249267578\n",
      "epoch: 40,  batch step: 58, loss: 108.065673828125\n",
      "epoch: 40,  batch step: 59, loss: 25.597660064697266\n",
      "epoch: 40,  batch step: 60, loss: 58.6224365234375\n",
      "epoch: 40,  batch step: 61, loss: 19.359516143798828\n",
      "epoch: 40,  batch step: 62, loss: 9.843765258789062\n",
      "epoch: 40,  batch step: 63, loss: 108.202880859375\n",
      "epoch: 40,  batch step: 64, loss: 66.04984283447266\n",
      "epoch: 40,  batch step: 65, loss: 4.520733833312988\n",
      "epoch: 40,  batch step: 66, loss: 70.19619750976562\n",
      "epoch: 40,  batch step: 67, loss: 9.175840377807617\n",
      "epoch: 40,  batch step: 68, loss: 9.011651992797852\n",
      "epoch: 40,  batch step: 69, loss: 4.357259750366211\n",
      "epoch: 40,  batch step: 70, loss: 50.940879821777344\n",
      "epoch: 40,  batch step: 71, loss: 93.91293334960938\n",
      "epoch: 40,  batch step: 72, loss: 6.1820292472839355\n",
      "epoch: 40,  batch step: 73, loss: 100.85832977294922\n",
      "epoch: 40,  batch step: 74, loss: 6.4242072105407715\n",
      "epoch: 40,  batch step: 75, loss: 8.277755737304688\n",
      "epoch: 40,  batch step: 76, loss: 103.46734619140625\n",
      "epoch: 40,  batch step: 77, loss: 13.166532516479492\n",
      "epoch: 40,  batch step: 78, loss: 52.41780090332031\n",
      "epoch: 40,  batch step: 79, loss: 5.245815277099609\n",
      "epoch: 40,  batch step: 80, loss: 5.166407585144043\n",
      "epoch: 40,  batch step: 81, loss: 52.820587158203125\n",
      "epoch: 40,  batch step: 82, loss: 18.036962509155273\n",
      "epoch: 40,  batch step: 83, loss: 39.01737594604492\n",
      "epoch: 40,  batch step: 84, loss: 12.242609024047852\n",
      "epoch: 40,  batch step: 85, loss: 4.073558807373047\n",
      "epoch: 40,  batch step: 86, loss: 14.381953239440918\n",
      "epoch: 40,  batch step: 87, loss: 35.45508575439453\n",
      "epoch: 40,  batch step: 88, loss: 44.03501892089844\n",
      "epoch: 40,  batch step: 89, loss: 9.069478988647461\n",
      "epoch: 40,  batch step: 90, loss: 45.60095977783203\n",
      "epoch: 40,  batch step: 91, loss: 10.671595573425293\n",
      "epoch: 40,  batch step: 92, loss: 35.20222473144531\n",
      "epoch: 40,  batch step: 93, loss: 8.11185359954834\n",
      "epoch: 40,  batch step: 94, loss: 43.249717712402344\n",
      "epoch: 40,  batch step: 95, loss: 17.077653884887695\n",
      "epoch: 40,  batch step: 96, loss: 5.860319137573242\n",
      "epoch: 40,  batch step: 97, loss: 27.853343963623047\n",
      "epoch: 40,  batch step: 98, loss: 4.0973896980285645\n",
      "epoch: 40,  batch step: 99, loss: 5.289111137390137\n",
      "epoch: 40,  batch step: 100, loss: 8.749702453613281\n",
      "epoch: 40,  batch step: 101, loss: 64.90677642822266\n",
      "epoch: 40,  batch step: 102, loss: 22.95924186706543\n",
      "epoch: 40,  batch step: 103, loss: 5.975215435028076\n",
      "epoch: 40,  batch step: 104, loss: 8.325454711914062\n",
      "epoch: 40,  batch step: 105, loss: 4.8031392097473145\n",
      "epoch: 40,  batch step: 106, loss: 3.9525651931762695\n",
      "epoch: 40,  batch step: 107, loss: 55.16027069091797\n",
      "epoch: 40,  batch step: 108, loss: 34.83891296386719\n",
      "epoch: 40,  batch step: 109, loss: 5.033507347106934\n",
      "epoch: 40,  batch step: 110, loss: 154.63560485839844\n",
      "epoch: 40,  batch step: 111, loss: 13.34942626953125\n",
      "epoch: 40,  batch step: 112, loss: 8.581867218017578\n",
      "epoch: 40,  batch step: 113, loss: 6.6484808921813965\n",
      "epoch: 40,  batch step: 114, loss: 10.067686080932617\n",
      "epoch: 40,  batch step: 115, loss: 18.79899024963379\n",
      "epoch: 40,  batch step: 116, loss: 16.897071838378906\n",
      "epoch: 40,  batch step: 117, loss: 5.5529937744140625\n",
      "epoch: 40,  batch step: 118, loss: 4.654977321624756\n",
      "epoch: 40,  batch step: 119, loss: 7.435147285461426\n",
      "epoch: 40,  batch step: 120, loss: 4.777414798736572\n",
      "epoch: 40,  batch step: 121, loss: 50.759368896484375\n",
      "epoch: 40,  batch step: 122, loss: 7.937227249145508\n",
      "epoch: 40,  batch step: 123, loss: 52.759422302246094\n",
      "epoch: 40,  batch step: 124, loss: 71.77408599853516\n",
      "epoch: 40,  batch step: 125, loss: 14.4406156539917\n",
      "epoch: 40,  batch step: 126, loss: 97.81352996826172\n",
      "epoch: 40,  batch step: 127, loss: 8.736136436462402\n",
      "epoch: 40,  batch step: 128, loss: 25.512004852294922\n",
      "epoch: 40,  batch step: 129, loss: 9.085270881652832\n",
      "epoch: 40,  batch step: 130, loss: 5.761725425720215\n",
      "epoch: 40,  batch step: 131, loss: 18.208904266357422\n",
      "epoch: 40,  batch step: 132, loss: 62.440696716308594\n",
      "epoch: 40,  batch step: 133, loss: 88.45364379882812\n",
      "epoch: 40,  batch step: 134, loss: 34.954978942871094\n",
      "epoch: 40,  batch step: 135, loss: 4.526515007019043\n",
      "epoch: 40,  batch step: 136, loss: 71.62773132324219\n",
      "epoch: 40,  batch step: 137, loss: 7.333024978637695\n",
      "epoch: 40,  batch step: 138, loss: 67.81051635742188\n",
      "epoch: 40,  batch step: 139, loss: 32.56629943847656\n",
      "epoch: 40,  batch step: 140, loss: 57.827674865722656\n",
      "epoch: 40,  batch step: 141, loss: 5.493531227111816\n",
      "epoch: 40,  batch step: 142, loss: 35.05488586425781\n",
      "epoch: 40,  batch step: 143, loss: 42.49524688720703\n",
      "epoch: 40,  batch step: 144, loss: 4.6335554122924805\n",
      "epoch: 40,  batch step: 145, loss: 45.159942626953125\n",
      "epoch: 40,  batch step: 146, loss: 4.044495582580566\n",
      "epoch: 40,  batch step: 147, loss: 49.44093322753906\n",
      "epoch: 40,  batch step: 148, loss: 5.729524612426758\n",
      "epoch: 40,  batch step: 149, loss: 7.759622097015381\n",
      "epoch: 40,  batch step: 150, loss: 54.19757843017578\n",
      "epoch: 40,  batch step: 151, loss: 39.288475036621094\n",
      "epoch: 40,  batch step: 152, loss: 14.17627239227295\n",
      "epoch: 40,  batch step: 153, loss: 6.229835033416748\n",
      "epoch: 40,  batch step: 154, loss: 120.68083953857422\n",
      "epoch: 40,  batch step: 155, loss: 5.593910217285156\n",
      "epoch: 40,  batch step: 156, loss: 18.250457763671875\n",
      "epoch: 40,  batch step: 157, loss: 13.1308012008667\n",
      "epoch: 40,  batch step: 158, loss: 6.600631237030029\n",
      "epoch: 40,  batch step: 159, loss: 6.313990592956543\n",
      "epoch: 40,  batch step: 160, loss: 82.43917846679688\n",
      "epoch: 40,  batch step: 161, loss: 69.94187927246094\n",
      "epoch: 40,  batch step: 162, loss: 28.584707260131836\n",
      "epoch: 40,  batch step: 163, loss: 46.50260925292969\n",
      "epoch: 40,  batch step: 164, loss: 4.758848190307617\n",
      "epoch: 40,  batch step: 165, loss: 39.823699951171875\n",
      "epoch: 40,  batch step: 166, loss: 42.92112350463867\n",
      "epoch: 40,  batch step: 167, loss: 10.583492279052734\n",
      "epoch: 40,  batch step: 168, loss: 8.824588775634766\n",
      "epoch: 40,  batch step: 169, loss: 81.521240234375\n",
      "epoch: 40,  batch step: 170, loss: 6.250710487365723\n",
      "epoch: 40,  batch step: 171, loss: 8.197652816772461\n",
      "epoch: 40,  batch step: 172, loss: 85.04096984863281\n",
      "epoch: 40,  batch step: 173, loss: 39.396095275878906\n",
      "epoch: 40,  batch step: 174, loss: 7.5779924392700195\n",
      "epoch: 40,  batch step: 175, loss: 45.404762268066406\n",
      "epoch: 40,  batch step: 176, loss: 7.158030986785889\n",
      "epoch: 40,  batch step: 177, loss: 50.96543502807617\n",
      "epoch: 40,  batch step: 178, loss: 5.459112167358398\n",
      "epoch: 40,  batch step: 179, loss: 5.245473861694336\n",
      "epoch: 40,  batch step: 180, loss: 29.56795883178711\n",
      "epoch: 40,  batch step: 181, loss: 21.49994468688965\n",
      "epoch: 40,  batch step: 182, loss: 75.76399230957031\n",
      "epoch: 40,  batch step: 183, loss: 8.884811401367188\n",
      "epoch: 40,  batch step: 184, loss: 66.87843322753906\n",
      "epoch: 40,  batch step: 185, loss: 39.858036041259766\n",
      "epoch: 40,  batch step: 186, loss: 9.039786338806152\n",
      "epoch: 40,  batch step: 187, loss: 59.372161865234375\n",
      "epoch: 40,  batch step: 188, loss: 36.883750915527344\n",
      "epoch: 40,  batch step: 189, loss: 3.101827621459961\n",
      "epoch: 40,  batch step: 190, loss: 43.94255065917969\n",
      "epoch: 40,  batch step: 191, loss: 4.377375602722168\n",
      "epoch: 40,  batch step: 192, loss: 7.302918434143066\n",
      "epoch: 40,  batch step: 193, loss: 6.468550682067871\n",
      "epoch: 40,  batch step: 194, loss: 16.40553092956543\n",
      "epoch: 40,  batch step: 195, loss: 102.37703704833984\n",
      "epoch: 40,  batch step: 196, loss: 3.9802043437957764\n",
      "epoch: 40,  batch step: 197, loss: 10.18342399597168\n",
      "epoch: 40,  batch step: 198, loss: 89.58364868164062\n",
      "epoch: 40,  batch step: 199, loss: 8.807474136352539\n",
      "epoch: 40,  batch step: 200, loss: 14.99213695526123\n",
      "epoch: 40,  batch step: 201, loss: 13.493856430053711\n",
      "epoch: 40,  batch step: 202, loss: 39.885704040527344\n",
      "epoch: 40,  batch step: 203, loss: 39.38334655761719\n",
      "epoch: 40,  batch step: 204, loss: 6.343287944793701\n",
      "epoch: 40,  batch step: 205, loss: 6.747852325439453\n",
      "epoch: 40,  batch step: 206, loss: 126.10050964355469\n",
      "epoch: 40,  batch step: 207, loss: 46.134578704833984\n",
      "epoch: 40,  batch step: 208, loss: 33.41648864746094\n",
      "epoch: 40,  batch step: 209, loss: 14.994192123413086\n",
      "epoch: 40,  batch step: 210, loss: 8.360319137573242\n",
      "epoch: 40,  batch step: 211, loss: 12.450721740722656\n",
      "epoch: 40,  batch step: 212, loss: 41.02777099609375\n",
      "epoch: 40,  batch step: 213, loss: 32.58686065673828\n",
      "epoch: 40,  batch step: 214, loss: 8.273783683776855\n",
      "epoch: 40,  batch step: 215, loss: 7.998441219329834\n",
      "epoch: 40,  batch step: 216, loss: 8.775156021118164\n",
      "epoch: 40,  batch step: 217, loss: 5.968483924865723\n",
      "epoch: 40,  batch step: 218, loss: 49.86654281616211\n",
      "epoch: 40,  batch step: 219, loss: 51.05882263183594\n",
      "epoch: 40,  batch step: 220, loss: 5.85982608795166\n",
      "epoch: 40,  batch step: 221, loss: 5.322299957275391\n",
      "epoch: 40,  batch step: 222, loss: 6.516740322113037\n",
      "epoch: 40,  batch step: 223, loss: 60.15311813354492\n",
      "epoch: 40,  batch step: 224, loss: 14.807245254516602\n",
      "epoch: 40,  batch step: 225, loss: 7.490198135375977\n",
      "epoch: 40,  batch step: 226, loss: 46.448299407958984\n",
      "epoch: 40,  batch step: 227, loss: 133.63565063476562\n",
      "epoch: 40,  batch step: 228, loss: 4.556683540344238\n",
      "epoch: 40,  batch step: 229, loss: 5.012610912322998\n",
      "epoch: 40,  batch step: 230, loss: 25.15187644958496\n",
      "epoch: 40,  batch step: 231, loss: 81.29739379882812\n",
      "epoch: 40,  batch step: 232, loss: 29.759750366210938\n",
      "epoch: 40,  batch step: 233, loss: 6.743207931518555\n",
      "epoch: 40,  batch step: 234, loss: 8.48007583618164\n",
      "epoch: 40,  batch step: 235, loss: 16.420408248901367\n",
      "epoch: 40,  batch step: 236, loss: 16.735607147216797\n",
      "epoch: 40,  batch step: 237, loss: 103.43893432617188\n",
      "epoch: 40,  batch step: 238, loss: 58.7169303894043\n",
      "epoch: 40,  batch step: 239, loss: 5.730140686035156\n",
      "epoch: 40,  batch step: 240, loss: 87.86650085449219\n",
      "epoch: 40,  batch step: 241, loss: 3.3779351711273193\n",
      "epoch: 40,  batch step: 242, loss: 58.465118408203125\n",
      "epoch: 40,  batch step: 243, loss: 11.168693542480469\n",
      "epoch: 40,  batch step: 244, loss: 31.739574432373047\n",
      "epoch: 40,  batch step: 245, loss: 5.657898902893066\n",
      "epoch: 40,  batch step: 246, loss: 8.875568389892578\n",
      "epoch: 40,  batch step: 247, loss: 6.019000053405762\n",
      "epoch: 40,  batch step: 248, loss: 42.8551025390625\n",
      "epoch: 40,  batch step: 249, loss: 41.61399841308594\n",
      "epoch: 40,  batch step: 250, loss: 11.765726089477539\n",
      "epoch: 40,  batch step: 251, loss: 63.57772445678711\n",
      "validation error epoch  40:    tensor(67.8702, device='cuda:0')\n",
      "316\n",
      "epoch: 41,  batch step: 0, loss: 16.420673370361328\n",
      "epoch: 41,  batch step: 1, loss: 6.37470006942749\n",
      "epoch: 41,  batch step: 2, loss: 18.127717971801758\n",
      "epoch: 41,  batch step: 3, loss: 6.033949851989746\n",
      "epoch: 41,  batch step: 4, loss: 25.82027816772461\n",
      "epoch: 41,  batch step: 5, loss: 34.755210876464844\n",
      "epoch: 41,  batch step: 6, loss: 4.458207130432129\n",
      "epoch: 41,  batch step: 7, loss: 9.460320472717285\n",
      "epoch: 41,  batch step: 8, loss: 88.31375122070312\n",
      "epoch: 41,  batch step: 9, loss: 5.404660224914551\n",
      "epoch: 41,  batch step: 10, loss: 6.259184837341309\n",
      "epoch: 41,  batch step: 11, loss: 5.999054908752441\n",
      "epoch: 41,  batch step: 12, loss: 19.087976455688477\n",
      "epoch: 41,  batch step: 13, loss: 9.750711441040039\n",
      "epoch: 41,  batch step: 14, loss: 63.502838134765625\n",
      "epoch: 41,  batch step: 15, loss: 11.367927551269531\n",
      "epoch: 41,  batch step: 16, loss: 4.675055503845215\n",
      "epoch: 41,  batch step: 17, loss: 34.766170501708984\n",
      "epoch: 41,  batch step: 18, loss: 39.84023666381836\n",
      "epoch: 41,  batch step: 19, loss: 5.92496395111084\n",
      "epoch: 41,  batch step: 20, loss: 12.089034080505371\n",
      "epoch: 41,  batch step: 21, loss: 8.890853881835938\n",
      "epoch: 41,  batch step: 22, loss: 4.68915319442749\n",
      "epoch: 41,  batch step: 23, loss: 15.547008514404297\n",
      "epoch: 41,  batch step: 24, loss: 121.53939056396484\n",
      "epoch: 41,  batch step: 25, loss: 6.5906877517700195\n",
      "epoch: 41,  batch step: 26, loss: 85.94429016113281\n",
      "epoch: 41,  batch step: 27, loss: 16.633708953857422\n",
      "epoch: 41,  batch step: 28, loss: 42.136138916015625\n",
      "epoch: 41,  batch step: 29, loss: 53.71672821044922\n",
      "epoch: 41,  batch step: 30, loss: 20.584152221679688\n",
      "epoch: 41,  batch step: 31, loss: 39.9651985168457\n",
      "epoch: 41,  batch step: 32, loss: 19.92108726501465\n",
      "epoch: 41,  batch step: 33, loss: 5.169325828552246\n",
      "epoch: 41,  batch step: 34, loss: 5.674413204193115\n",
      "epoch: 41,  batch step: 35, loss: 3.338646650314331\n",
      "epoch: 41,  batch step: 36, loss: 5.092066764831543\n",
      "epoch: 41,  batch step: 37, loss: 68.58712005615234\n",
      "epoch: 41,  batch step: 38, loss: 173.24960327148438\n",
      "epoch: 41,  batch step: 39, loss: 20.086219787597656\n",
      "epoch: 41,  batch step: 40, loss: 33.44749069213867\n",
      "epoch: 41,  batch step: 41, loss: 8.561029434204102\n",
      "epoch: 41,  batch step: 42, loss: 38.75365447998047\n",
      "epoch: 41,  batch step: 43, loss: 110.73158264160156\n",
      "epoch: 41,  batch step: 44, loss: 31.347747802734375\n",
      "epoch: 41,  batch step: 45, loss: 85.70285034179688\n",
      "epoch: 41,  batch step: 46, loss: 32.79171371459961\n",
      "epoch: 41,  batch step: 47, loss: 39.359798431396484\n",
      "epoch: 41,  batch step: 48, loss: 31.47054672241211\n",
      "epoch: 41,  batch step: 49, loss: 6.496140956878662\n",
      "epoch: 41,  batch step: 50, loss: 42.382774353027344\n",
      "epoch: 41,  batch step: 51, loss: 11.885589599609375\n",
      "epoch: 41,  batch step: 52, loss: 9.120323181152344\n",
      "epoch: 41,  batch step: 53, loss: 6.64668083190918\n",
      "epoch: 41,  batch step: 54, loss: 10.359436988830566\n",
      "epoch: 41,  batch step: 55, loss: 65.37223815917969\n",
      "epoch: 41,  batch step: 56, loss: 4.887155532836914\n",
      "epoch: 41,  batch step: 57, loss: 32.41822814941406\n",
      "epoch: 41,  batch step: 58, loss: 81.48921203613281\n",
      "epoch: 41,  batch step: 59, loss: 64.61322784423828\n",
      "epoch: 41,  batch step: 60, loss: 13.253721237182617\n",
      "epoch: 41,  batch step: 61, loss: 8.492883682250977\n",
      "epoch: 41,  batch step: 62, loss: 18.470787048339844\n",
      "epoch: 41,  batch step: 63, loss: 6.285591125488281\n",
      "epoch: 41,  batch step: 64, loss: 22.824247360229492\n",
      "epoch: 41,  batch step: 65, loss: 9.083832740783691\n",
      "epoch: 41,  batch step: 66, loss: 5.723372936248779\n",
      "epoch: 41,  batch step: 67, loss: 8.044248580932617\n",
      "epoch: 41,  batch step: 68, loss: 8.662148475646973\n",
      "epoch: 41,  batch step: 69, loss: 13.39985466003418\n",
      "epoch: 41,  batch step: 70, loss: 4.774046897888184\n",
      "epoch: 41,  batch step: 71, loss: 5.781620025634766\n",
      "epoch: 41,  batch step: 72, loss: 79.56280517578125\n",
      "epoch: 41,  batch step: 73, loss: 17.989810943603516\n",
      "epoch: 41,  batch step: 74, loss: 14.321542739868164\n",
      "epoch: 41,  batch step: 75, loss: 23.64502716064453\n",
      "epoch: 41,  batch step: 76, loss: 11.2180814743042\n",
      "epoch: 41,  batch step: 77, loss: 33.29175567626953\n",
      "epoch: 41,  batch step: 78, loss: 54.323760986328125\n",
      "epoch: 41,  batch step: 79, loss: 50.519447326660156\n",
      "epoch: 41,  batch step: 80, loss: 12.325197219848633\n",
      "epoch: 41,  batch step: 81, loss: 4.5514421463012695\n",
      "epoch: 41,  batch step: 82, loss: 4.651128768920898\n",
      "epoch: 41,  batch step: 83, loss: 53.51112365722656\n",
      "epoch: 41,  batch step: 84, loss: 5.048555850982666\n",
      "epoch: 41,  batch step: 85, loss: 6.6518659591674805\n",
      "epoch: 41,  batch step: 86, loss: 3.7809863090515137\n",
      "epoch: 41,  batch step: 87, loss: 4.700927257537842\n",
      "epoch: 41,  batch step: 88, loss: 6.351706504821777\n",
      "epoch: 41,  batch step: 89, loss: 7.392482757568359\n",
      "epoch: 41,  batch step: 90, loss: 51.65753173828125\n",
      "epoch: 41,  batch step: 91, loss: 58.34833908081055\n",
      "epoch: 41,  batch step: 92, loss: 22.085790634155273\n",
      "epoch: 41,  batch step: 93, loss: 38.68166732788086\n",
      "epoch: 41,  batch step: 94, loss: 108.68016815185547\n",
      "epoch: 41,  batch step: 95, loss: 15.449243545532227\n",
      "epoch: 41,  batch step: 96, loss: 74.21599578857422\n",
      "epoch: 41,  batch step: 97, loss: 3.5423038005828857\n",
      "epoch: 41,  batch step: 98, loss: 6.041642665863037\n",
      "epoch: 41,  batch step: 99, loss: 7.447961330413818\n",
      "epoch: 41,  batch step: 100, loss: 3.0741183757781982\n",
      "epoch: 41,  batch step: 101, loss: 9.373404502868652\n",
      "epoch: 41,  batch step: 102, loss: 6.072993278503418\n",
      "epoch: 41,  batch step: 103, loss: 10.573667526245117\n",
      "epoch: 41,  batch step: 104, loss: 4.480696678161621\n",
      "epoch: 41,  batch step: 105, loss: 8.174808502197266\n",
      "epoch: 41,  batch step: 106, loss: 45.91138458251953\n",
      "epoch: 41,  batch step: 107, loss: 34.265464782714844\n",
      "epoch: 41,  batch step: 108, loss: 8.860368728637695\n",
      "epoch: 41,  batch step: 109, loss: 8.611319541931152\n",
      "epoch: 41,  batch step: 110, loss: 31.759502410888672\n",
      "epoch: 41,  batch step: 111, loss: 50.199371337890625\n",
      "epoch: 41,  batch step: 112, loss: 174.06439208984375\n",
      "epoch: 41,  batch step: 113, loss: 35.583656311035156\n",
      "epoch: 41,  batch step: 114, loss: 17.327117919921875\n",
      "epoch: 41,  batch step: 115, loss: 20.833904266357422\n",
      "epoch: 41,  batch step: 116, loss: 7.94583797454834\n",
      "epoch: 41,  batch step: 117, loss: 13.410079002380371\n",
      "epoch: 41,  batch step: 118, loss: 47.761016845703125\n",
      "epoch: 41,  batch step: 119, loss: 5.020375728607178\n",
      "epoch: 41,  batch step: 120, loss: 50.002498626708984\n",
      "epoch: 41,  batch step: 121, loss: 21.343833923339844\n",
      "epoch: 41,  batch step: 122, loss: 6.348378658294678\n",
      "epoch: 41,  batch step: 123, loss: 20.874656677246094\n",
      "epoch: 41,  batch step: 124, loss: 18.457477569580078\n",
      "epoch: 41,  batch step: 125, loss: 27.306257247924805\n",
      "epoch: 41,  batch step: 126, loss: 6.953089714050293\n",
      "epoch: 41,  batch step: 127, loss: 6.812352657318115\n",
      "epoch: 41,  batch step: 128, loss: 51.350521087646484\n",
      "epoch: 41,  batch step: 129, loss: 3.7271535396575928\n",
      "epoch: 41,  batch step: 130, loss: 25.491010665893555\n",
      "epoch: 41,  batch step: 131, loss: 47.581756591796875\n",
      "epoch: 41,  batch step: 132, loss: 31.92850112915039\n",
      "epoch: 41,  batch step: 133, loss: 4.056545257568359\n",
      "epoch: 41,  batch step: 134, loss: 29.505935668945312\n",
      "epoch: 41,  batch step: 135, loss: 6.006354808807373\n",
      "epoch: 41,  batch step: 136, loss: 8.02332878112793\n",
      "epoch: 41,  batch step: 137, loss: 4.11060905456543\n",
      "epoch: 41,  batch step: 138, loss: 29.374832153320312\n",
      "epoch: 41,  batch step: 139, loss: 24.501781463623047\n",
      "epoch: 41,  batch step: 140, loss: 5.079102993011475\n",
      "epoch: 41,  batch step: 141, loss: 56.90037536621094\n",
      "epoch: 41,  batch step: 142, loss: 20.133094787597656\n",
      "epoch: 41,  batch step: 143, loss: 4.098377227783203\n",
      "epoch: 41,  batch step: 144, loss: 7.295434474945068\n",
      "epoch: 41,  batch step: 145, loss: 13.893572807312012\n",
      "epoch: 41,  batch step: 146, loss: 9.347959518432617\n",
      "epoch: 41,  batch step: 147, loss: 28.2075252532959\n",
      "epoch: 41,  batch step: 148, loss: 3.3457822799682617\n",
      "epoch: 41,  batch step: 149, loss: 55.48942184448242\n",
      "epoch: 41,  batch step: 150, loss: 8.835575103759766\n",
      "epoch: 41,  batch step: 151, loss: 60.61109924316406\n",
      "epoch: 41,  batch step: 152, loss: 24.44046401977539\n",
      "epoch: 41,  batch step: 153, loss: 11.192964553833008\n",
      "epoch: 41,  batch step: 154, loss: 9.777910232543945\n",
      "epoch: 41,  batch step: 155, loss: 14.784591674804688\n",
      "epoch: 41,  batch step: 156, loss: 8.473230361938477\n",
      "epoch: 41,  batch step: 157, loss: 28.44502830505371\n",
      "epoch: 41,  batch step: 158, loss: 4.2825727462768555\n",
      "epoch: 41,  batch step: 159, loss: 19.98886489868164\n",
      "epoch: 41,  batch step: 160, loss: 11.290279388427734\n",
      "epoch: 41,  batch step: 161, loss: 18.628406524658203\n",
      "epoch: 41,  batch step: 162, loss: 6.38330602645874\n",
      "epoch: 41,  batch step: 163, loss: 21.371837615966797\n",
      "epoch: 41,  batch step: 164, loss: 29.897682189941406\n",
      "epoch: 41,  batch step: 165, loss: 3.942277669906616\n",
      "epoch: 41,  batch step: 166, loss: 84.72649383544922\n",
      "epoch: 41,  batch step: 167, loss: 5.686336517333984\n",
      "epoch: 41,  batch step: 168, loss: 75.70484924316406\n",
      "epoch: 41,  batch step: 169, loss: 9.955690383911133\n",
      "epoch: 41,  batch step: 170, loss: 7.231842994689941\n",
      "epoch: 41,  batch step: 171, loss: 52.9482536315918\n",
      "epoch: 41,  batch step: 172, loss: 42.854835510253906\n",
      "epoch: 41,  batch step: 173, loss: 5.679888725280762\n",
      "epoch: 41,  batch step: 174, loss: 55.049400329589844\n",
      "epoch: 41,  batch step: 175, loss: 52.71000289916992\n",
      "epoch: 41,  batch step: 176, loss: 9.841261863708496\n",
      "epoch: 41,  batch step: 177, loss: 26.560400009155273\n",
      "epoch: 41,  batch step: 178, loss: 35.56901550292969\n",
      "epoch: 41,  batch step: 179, loss: 3.6081244945526123\n",
      "epoch: 41,  batch step: 180, loss: 42.63836669921875\n",
      "epoch: 41,  batch step: 181, loss: 6.289020538330078\n",
      "epoch: 41,  batch step: 182, loss: 26.582199096679688\n",
      "epoch: 41,  batch step: 183, loss: 45.415306091308594\n",
      "epoch: 41,  batch step: 184, loss: 5.219634532928467\n",
      "epoch: 41,  batch step: 185, loss: 7.178725242614746\n",
      "epoch: 41,  batch step: 186, loss: 65.47537994384766\n",
      "epoch: 41,  batch step: 187, loss: 80.58544921875\n",
      "epoch: 41,  batch step: 188, loss: 67.46337890625\n",
      "epoch: 41,  batch step: 189, loss: 8.981322288513184\n",
      "epoch: 41,  batch step: 190, loss: 31.206260681152344\n",
      "epoch: 41,  batch step: 191, loss: 31.706350326538086\n",
      "epoch: 41,  batch step: 192, loss: 8.332685470581055\n",
      "epoch: 41,  batch step: 193, loss: 39.73351287841797\n",
      "epoch: 41,  batch step: 194, loss: 68.58905029296875\n",
      "epoch: 41,  batch step: 195, loss: 39.01905059814453\n",
      "epoch: 41,  batch step: 196, loss: 36.980567932128906\n",
      "epoch: 41,  batch step: 197, loss: 18.529888153076172\n",
      "epoch: 41,  batch step: 198, loss: 23.886213302612305\n",
      "epoch: 41,  batch step: 199, loss: 36.66259002685547\n",
      "epoch: 41,  batch step: 200, loss: 33.41996765136719\n",
      "epoch: 41,  batch step: 201, loss: 28.66895294189453\n",
      "epoch: 41,  batch step: 202, loss: 24.552967071533203\n",
      "epoch: 41,  batch step: 203, loss: 45.86238098144531\n",
      "epoch: 41,  batch step: 204, loss: 44.20226287841797\n",
      "epoch: 41,  batch step: 205, loss: 34.420310974121094\n",
      "epoch: 41,  batch step: 206, loss: 12.736936569213867\n",
      "epoch: 41,  batch step: 207, loss: 3.9147634506225586\n",
      "epoch: 41,  batch step: 208, loss: 36.93364715576172\n",
      "epoch: 41,  batch step: 209, loss: 12.088005065917969\n",
      "epoch: 41,  batch step: 210, loss: 3.156825542449951\n",
      "epoch: 41,  batch step: 211, loss: 3.8333890438079834\n",
      "epoch: 41,  batch step: 212, loss: 7.8099284172058105\n",
      "epoch: 41,  batch step: 213, loss: 35.38692855834961\n",
      "epoch: 41,  batch step: 214, loss: 53.848228454589844\n",
      "epoch: 41,  batch step: 215, loss: 31.11053466796875\n",
      "epoch: 41,  batch step: 216, loss: 3.405031681060791\n",
      "epoch: 41,  batch step: 217, loss: 2.9384543895721436\n",
      "epoch: 41,  batch step: 218, loss: 3.124936580657959\n",
      "epoch: 41,  batch step: 219, loss: 5.3805084228515625\n",
      "epoch: 41,  batch step: 220, loss: 6.364495277404785\n",
      "epoch: 41,  batch step: 221, loss: 53.596397399902344\n",
      "epoch: 41,  batch step: 222, loss: 8.085935592651367\n",
      "epoch: 41,  batch step: 223, loss: 64.2496566772461\n",
      "epoch: 41,  batch step: 224, loss: 36.98363494873047\n",
      "epoch: 41,  batch step: 225, loss: 44.303218841552734\n",
      "epoch: 41,  batch step: 226, loss: 32.43297576904297\n",
      "epoch: 41,  batch step: 227, loss: 16.209638595581055\n",
      "epoch: 41,  batch step: 228, loss: 23.841270446777344\n",
      "epoch: 41,  batch step: 229, loss: 3.162508010864258\n",
      "epoch: 41,  batch step: 230, loss: 6.4533820152282715\n",
      "epoch: 41,  batch step: 231, loss: 16.785037994384766\n",
      "epoch: 41,  batch step: 232, loss: 57.60735321044922\n",
      "epoch: 41,  batch step: 233, loss: 2.9304656982421875\n",
      "epoch: 41,  batch step: 234, loss: 11.366914749145508\n",
      "epoch: 41,  batch step: 235, loss: 5.200212478637695\n",
      "epoch: 41,  batch step: 236, loss: 6.446725845336914\n",
      "epoch: 41,  batch step: 237, loss: 16.709228515625\n",
      "epoch: 41,  batch step: 238, loss: 31.71297264099121\n",
      "epoch: 41,  batch step: 239, loss: 71.2347412109375\n",
      "epoch: 41,  batch step: 240, loss: 77.50469970703125\n",
      "epoch: 41,  batch step: 241, loss: 23.329124450683594\n",
      "epoch: 41,  batch step: 242, loss: 36.634002685546875\n",
      "epoch: 41,  batch step: 243, loss: 12.1940336227417\n",
      "epoch: 41,  batch step: 244, loss: 24.774654388427734\n",
      "epoch: 41,  batch step: 245, loss: 116.20408630371094\n",
      "epoch: 41,  batch step: 246, loss: 5.611008644104004\n",
      "epoch: 41,  batch step: 247, loss: 16.573896408081055\n",
      "epoch: 41,  batch step: 248, loss: 23.57379150390625\n",
      "epoch: 41,  batch step: 249, loss: 37.375\n",
      "epoch: 41,  batch step: 250, loss: 5.180437088012695\n",
      "epoch: 41,  batch step: 251, loss: 145.90664672851562\n",
      "validation error epoch  41:    tensor(68.2244, device='cuda:0')\n",
      "316\n",
      "epoch: 42,  batch step: 0, loss: 57.62934494018555\n",
      "epoch: 42,  batch step: 1, loss: 5.984477996826172\n",
      "epoch: 42,  batch step: 2, loss: 6.522424697875977\n",
      "epoch: 42,  batch step: 3, loss: 8.025823593139648\n",
      "epoch: 42,  batch step: 4, loss: 5.028296947479248\n",
      "epoch: 42,  batch step: 5, loss: 65.81192016601562\n",
      "epoch: 42,  batch step: 6, loss: 67.33697509765625\n",
      "epoch: 42,  batch step: 7, loss: 9.310295104980469\n",
      "epoch: 42,  batch step: 8, loss: 22.486907958984375\n",
      "epoch: 42,  batch step: 9, loss: 10.157682418823242\n",
      "epoch: 42,  batch step: 10, loss: 12.186452865600586\n",
      "epoch: 42,  batch step: 11, loss: 7.514227867126465\n",
      "epoch: 42,  batch step: 12, loss: 92.75386047363281\n",
      "epoch: 42,  batch step: 13, loss: 42.12732696533203\n",
      "epoch: 42,  batch step: 14, loss: 6.549361228942871\n",
      "epoch: 42,  batch step: 15, loss: 30.35309410095215\n",
      "epoch: 42,  batch step: 16, loss: 77.51705169677734\n",
      "epoch: 42,  batch step: 17, loss: 9.592059135437012\n",
      "epoch: 42,  batch step: 18, loss: 29.938560485839844\n",
      "epoch: 42,  batch step: 19, loss: 7.429827690124512\n",
      "epoch: 42,  batch step: 20, loss: 51.36787414550781\n",
      "epoch: 42,  batch step: 21, loss: 12.66450309753418\n",
      "epoch: 42,  batch step: 22, loss: 75.28630065917969\n",
      "epoch: 42,  batch step: 23, loss: 31.23745346069336\n",
      "epoch: 42,  batch step: 24, loss: 42.1077766418457\n",
      "epoch: 42,  batch step: 25, loss: 39.56255340576172\n",
      "epoch: 42,  batch step: 26, loss: 46.99884033203125\n",
      "epoch: 42,  batch step: 27, loss: 23.193965911865234\n",
      "epoch: 42,  batch step: 28, loss: 7.836054801940918\n",
      "epoch: 42,  batch step: 29, loss: 15.268917083740234\n",
      "epoch: 42,  batch step: 30, loss: 32.32086181640625\n",
      "epoch: 42,  batch step: 31, loss: 25.321170806884766\n",
      "epoch: 42,  batch step: 32, loss: 7.598365306854248\n",
      "epoch: 42,  batch step: 33, loss: 5.129015922546387\n",
      "epoch: 42,  batch step: 34, loss: 24.645488739013672\n",
      "epoch: 42,  batch step: 35, loss: 33.60546112060547\n",
      "epoch: 42,  batch step: 36, loss: 51.15090560913086\n",
      "epoch: 42,  batch step: 37, loss: 25.747617721557617\n",
      "epoch: 42,  batch step: 38, loss: 26.311351776123047\n",
      "epoch: 42,  batch step: 39, loss: 56.96117401123047\n",
      "epoch: 42,  batch step: 40, loss: 31.150829315185547\n",
      "epoch: 42,  batch step: 41, loss: 219.47366333007812\n",
      "epoch: 42,  batch step: 42, loss: 34.53511428833008\n",
      "epoch: 42,  batch step: 43, loss: 11.804447174072266\n",
      "epoch: 42,  batch step: 44, loss: 36.294944763183594\n",
      "epoch: 42,  batch step: 45, loss: 81.4393310546875\n",
      "epoch: 42,  batch step: 46, loss: 5.991752624511719\n",
      "epoch: 42,  batch step: 47, loss: 12.58932113647461\n",
      "epoch: 42,  batch step: 48, loss: 30.51494598388672\n",
      "epoch: 42,  batch step: 49, loss: 29.672714233398438\n",
      "epoch: 42,  batch step: 50, loss: 49.08729553222656\n",
      "epoch: 42,  batch step: 51, loss: 34.38462829589844\n",
      "epoch: 42,  batch step: 52, loss: 10.574894905090332\n",
      "epoch: 42,  batch step: 53, loss: 3.797851085662842\n",
      "epoch: 42,  batch step: 54, loss: 9.045650482177734\n",
      "epoch: 42,  batch step: 55, loss: 57.69615936279297\n",
      "epoch: 42,  batch step: 56, loss: 8.984164237976074\n",
      "epoch: 42,  batch step: 57, loss: 7.341976165771484\n",
      "epoch: 42,  batch step: 58, loss: 33.42074966430664\n",
      "epoch: 42,  batch step: 59, loss: 5.168505668640137\n",
      "epoch: 42,  batch step: 60, loss: 6.4386305809021\n",
      "epoch: 42,  batch step: 61, loss: 132.76141357421875\n",
      "epoch: 42,  batch step: 62, loss: 6.693387985229492\n",
      "epoch: 42,  batch step: 63, loss: 35.13483428955078\n",
      "epoch: 42,  batch step: 64, loss: 6.24200963973999\n",
      "epoch: 42,  batch step: 65, loss: 5.476739883422852\n",
      "epoch: 42,  batch step: 66, loss: 5.430383205413818\n",
      "epoch: 42,  batch step: 67, loss: 4.010148048400879\n",
      "epoch: 42,  batch step: 68, loss: 63.566627502441406\n",
      "epoch: 42,  batch step: 69, loss: 56.644752502441406\n",
      "epoch: 42,  batch step: 70, loss: 60.065528869628906\n",
      "epoch: 42,  batch step: 71, loss: 69.49774169921875\n",
      "epoch: 42,  batch step: 72, loss: 2.7510554790496826\n",
      "epoch: 42,  batch step: 73, loss: 4.148375511169434\n",
      "epoch: 42,  batch step: 74, loss: 30.53302764892578\n",
      "epoch: 42,  batch step: 75, loss: 37.31755828857422\n",
      "epoch: 42,  batch step: 76, loss: 46.42100524902344\n",
      "epoch: 42,  batch step: 77, loss: 5.876705169677734\n",
      "epoch: 42,  batch step: 78, loss: 9.10628890991211\n",
      "epoch: 42,  batch step: 79, loss: 3.880277633666992\n",
      "epoch: 42,  batch step: 80, loss: 32.7515983581543\n",
      "epoch: 42,  batch step: 81, loss: 34.59425354003906\n",
      "epoch: 42,  batch step: 82, loss: 31.706615447998047\n",
      "epoch: 42,  batch step: 83, loss: 23.79438018798828\n",
      "epoch: 42,  batch step: 84, loss: 34.40416717529297\n",
      "epoch: 42,  batch step: 85, loss: 27.19825553894043\n",
      "epoch: 42,  batch step: 86, loss: 7.743995189666748\n",
      "epoch: 42,  batch step: 87, loss: 31.606731414794922\n",
      "epoch: 42,  batch step: 88, loss: 5.213832855224609\n",
      "epoch: 42,  batch step: 89, loss: 27.423648834228516\n",
      "epoch: 42,  batch step: 90, loss: 30.80115509033203\n",
      "epoch: 42,  batch step: 91, loss: 50.27178955078125\n",
      "epoch: 42,  batch step: 92, loss: 4.502191543579102\n",
      "epoch: 42,  batch step: 93, loss: 16.028417587280273\n",
      "epoch: 42,  batch step: 94, loss: 7.405486106872559\n",
      "epoch: 42,  batch step: 95, loss: 22.069095611572266\n",
      "epoch: 42,  batch step: 96, loss: 4.467385768890381\n",
      "epoch: 42,  batch step: 97, loss: 12.846506118774414\n",
      "epoch: 42,  batch step: 98, loss: 47.55625534057617\n",
      "epoch: 42,  batch step: 99, loss: 11.085779190063477\n",
      "epoch: 42,  batch step: 100, loss: 13.770515441894531\n",
      "epoch: 42,  batch step: 101, loss: 31.64712142944336\n",
      "epoch: 42,  batch step: 102, loss: 2.9360511302948\n",
      "epoch: 42,  batch step: 103, loss: 56.37632369995117\n",
      "epoch: 42,  batch step: 104, loss: 37.90133285522461\n",
      "epoch: 42,  batch step: 105, loss: 87.224609375\n",
      "epoch: 42,  batch step: 106, loss: 4.837481498718262\n",
      "epoch: 42,  batch step: 107, loss: 37.43510437011719\n",
      "epoch: 42,  batch step: 108, loss: 11.556428909301758\n",
      "epoch: 42,  batch step: 109, loss: 43.66299057006836\n",
      "epoch: 42,  batch step: 110, loss: 15.567436218261719\n",
      "epoch: 42,  batch step: 111, loss: 4.173084259033203\n",
      "epoch: 42,  batch step: 112, loss: 40.152069091796875\n",
      "epoch: 42,  batch step: 113, loss: 39.5954475402832\n",
      "epoch: 42,  batch step: 114, loss: 17.03899383544922\n",
      "epoch: 42,  batch step: 115, loss: 34.881629943847656\n",
      "epoch: 42,  batch step: 116, loss: 8.515323638916016\n",
      "epoch: 42,  batch step: 117, loss: 9.673080444335938\n",
      "epoch: 42,  batch step: 118, loss: 5.91256856918335\n",
      "epoch: 42,  batch step: 119, loss: 135.49192810058594\n",
      "epoch: 42,  batch step: 120, loss: 68.49886322021484\n",
      "epoch: 42,  batch step: 121, loss: 30.73543357849121\n",
      "epoch: 42,  batch step: 122, loss: 5.778730392456055\n",
      "epoch: 42,  batch step: 123, loss: 42.626976013183594\n",
      "epoch: 42,  batch step: 124, loss: 45.89439392089844\n",
      "epoch: 42,  batch step: 125, loss: 5.4280242919921875\n",
      "epoch: 42,  batch step: 126, loss: 9.576587677001953\n",
      "epoch: 42,  batch step: 127, loss: 52.648712158203125\n",
      "epoch: 42,  batch step: 128, loss: 11.676919937133789\n",
      "epoch: 42,  batch step: 129, loss: 11.962133407592773\n",
      "epoch: 42,  batch step: 130, loss: 6.530402183532715\n",
      "epoch: 42,  batch step: 131, loss: 13.19049072265625\n",
      "epoch: 42,  batch step: 132, loss: 6.791616439819336\n",
      "epoch: 42,  batch step: 133, loss: 10.91651725769043\n",
      "epoch: 42,  batch step: 134, loss: 6.3340163230896\n",
      "epoch: 42,  batch step: 135, loss: 18.013912200927734\n",
      "epoch: 42,  batch step: 136, loss: 4.602578639984131\n",
      "epoch: 42,  batch step: 137, loss: 4.590588569641113\n",
      "epoch: 42,  batch step: 138, loss: 5.039316654205322\n",
      "epoch: 42,  batch step: 139, loss: 14.279292106628418\n",
      "epoch: 42,  batch step: 140, loss: 96.9654541015625\n",
      "epoch: 42,  batch step: 141, loss: 8.948031425476074\n",
      "epoch: 42,  batch step: 142, loss: 18.776390075683594\n",
      "epoch: 42,  batch step: 143, loss: 18.63728904724121\n",
      "epoch: 42,  batch step: 144, loss: 6.106135845184326\n",
      "epoch: 42,  batch step: 145, loss: 18.079608917236328\n",
      "epoch: 42,  batch step: 146, loss: 7.5967230796813965\n",
      "epoch: 42,  batch step: 147, loss: 75.24518585205078\n",
      "epoch: 42,  batch step: 148, loss: 5.217789649963379\n",
      "epoch: 42,  batch step: 149, loss: 10.003320693969727\n",
      "epoch: 42,  batch step: 150, loss: 3.571288585662842\n",
      "epoch: 42,  batch step: 151, loss: 4.86323356628418\n",
      "epoch: 42,  batch step: 152, loss: 16.21561622619629\n",
      "epoch: 42,  batch step: 153, loss: 3.111060857772827\n",
      "epoch: 42,  batch step: 154, loss: 55.13618469238281\n",
      "epoch: 42,  batch step: 155, loss: 4.722815036773682\n",
      "epoch: 42,  batch step: 156, loss: 23.434226989746094\n",
      "epoch: 42,  batch step: 157, loss: 8.51229190826416\n",
      "epoch: 42,  batch step: 158, loss: 75.27163696289062\n",
      "epoch: 42,  batch step: 159, loss: 48.789222717285156\n",
      "epoch: 42,  batch step: 160, loss: 4.800046920776367\n",
      "epoch: 42,  batch step: 161, loss: 6.623551845550537\n",
      "epoch: 42,  batch step: 162, loss: 30.04865074157715\n",
      "epoch: 42,  batch step: 163, loss: 4.5814924240112305\n",
      "epoch: 42,  batch step: 164, loss: 5.412163734436035\n",
      "epoch: 42,  batch step: 165, loss: 5.25023078918457\n",
      "epoch: 42,  batch step: 166, loss: 7.798570156097412\n",
      "epoch: 42,  batch step: 167, loss: 6.692224025726318\n",
      "epoch: 42,  batch step: 168, loss: 68.84622192382812\n",
      "epoch: 42,  batch step: 169, loss: 3.251626491546631\n",
      "epoch: 42,  batch step: 170, loss: 8.586511611938477\n",
      "epoch: 42,  batch step: 171, loss: 20.458194732666016\n",
      "epoch: 42,  batch step: 172, loss: 25.8692684173584\n",
      "epoch: 42,  batch step: 173, loss: 36.455543518066406\n",
      "epoch: 42,  batch step: 174, loss: 107.3491439819336\n",
      "epoch: 42,  batch step: 175, loss: 3.5041298866271973\n",
      "epoch: 42,  batch step: 176, loss: 7.078895092010498\n",
      "epoch: 42,  batch step: 177, loss: 22.981191635131836\n",
      "epoch: 42,  batch step: 178, loss: 4.01948356628418\n",
      "epoch: 42,  batch step: 179, loss: 12.016923904418945\n",
      "epoch: 42,  batch step: 180, loss: 5.537936210632324\n",
      "epoch: 42,  batch step: 181, loss: 8.907618522644043\n",
      "epoch: 42,  batch step: 182, loss: 30.299598693847656\n",
      "epoch: 42,  batch step: 183, loss: 7.7573676109313965\n",
      "epoch: 42,  batch step: 184, loss: 19.45839500427246\n",
      "epoch: 42,  batch step: 185, loss: 19.991043090820312\n",
      "epoch: 42,  batch step: 186, loss: 5.792057037353516\n",
      "epoch: 42,  batch step: 187, loss: 39.599891662597656\n",
      "epoch: 42,  batch step: 188, loss: 8.227972984313965\n",
      "epoch: 42,  batch step: 189, loss: 4.660490036010742\n",
      "epoch: 42,  batch step: 190, loss: 62.95344543457031\n",
      "epoch: 42,  batch step: 191, loss: 3.1892895698547363\n",
      "epoch: 42,  batch step: 192, loss: 3.973158597946167\n",
      "epoch: 42,  batch step: 193, loss: 5.451888561248779\n",
      "epoch: 42,  batch step: 194, loss: 73.60861206054688\n",
      "epoch: 42,  batch step: 195, loss: 5.807113170623779\n",
      "epoch: 42,  batch step: 196, loss: 23.615013122558594\n",
      "epoch: 42,  batch step: 197, loss: 11.439181327819824\n",
      "epoch: 42,  batch step: 198, loss: 11.671018600463867\n",
      "epoch: 42,  batch step: 199, loss: 37.44304275512695\n",
      "epoch: 42,  batch step: 200, loss: 3.9952549934387207\n",
      "epoch: 42,  batch step: 201, loss: 4.745898246765137\n",
      "epoch: 42,  batch step: 202, loss: 7.704665660858154\n",
      "epoch: 42,  batch step: 203, loss: 3.8118910789489746\n",
      "epoch: 42,  batch step: 204, loss: 24.820697784423828\n",
      "epoch: 42,  batch step: 205, loss: 5.596616268157959\n",
      "epoch: 42,  batch step: 206, loss: 33.19142150878906\n",
      "epoch: 42,  batch step: 207, loss: 6.4292216300964355\n",
      "epoch: 42,  batch step: 208, loss: 4.482301235198975\n",
      "epoch: 42,  batch step: 209, loss: 3.6490626335144043\n",
      "epoch: 42,  batch step: 210, loss: 74.45211791992188\n",
      "epoch: 42,  batch step: 211, loss: 4.499817371368408\n",
      "epoch: 42,  batch step: 212, loss: 2.9006400108337402\n",
      "epoch: 42,  batch step: 213, loss: 39.922515869140625\n",
      "epoch: 42,  batch step: 214, loss: 32.22901153564453\n",
      "epoch: 42,  batch step: 215, loss: 17.97433853149414\n",
      "epoch: 42,  batch step: 216, loss: 14.419577598571777\n",
      "epoch: 42,  batch step: 217, loss: 3.9551875591278076\n",
      "epoch: 42,  batch step: 218, loss: 99.57608032226562\n",
      "epoch: 42,  batch step: 219, loss: 6.263343811035156\n",
      "epoch: 42,  batch step: 220, loss: 44.26713562011719\n",
      "epoch: 42,  batch step: 221, loss: 4.880580425262451\n",
      "epoch: 42,  batch step: 222, loss: 37.72016143798828\n",
      "epoch: 42,  batch step: 223, loss: 7.131402969360352\n",
      "epoch: 42,  batch step: 224, loss: 5.092324733734131\n",
      "epoch: 42,  batch step: 225, loss: 3.3263444900512695\n",
      "epoch: 42,  batch step: 226, loss: 33.486385345458984\n",
      "epoch: 42,  batch step: 227, loss: 6.177253246307373\n",
      "epoch: 42,  batch step: 228, loss: 25.153858184814453\n",
      "epoch: 42,  batch step: 229, loss: 42.66395568847656\n",
      "epoch: 42,  batch step: 230, loss: 10.576766014099121\n",
      "epoch: 42,  batch step: 231, loss: 31.452350616455078\n",
      "epoch: 42,  batch step: 232, loss: 31.342967987060547\n",
      "epoch: 42,  batch step: 233, loss: 14.75348949432373\n",
      "epoch: 42,  batch step: 234, loss: 4.97971248626709\n",
      "epoch: 42,  batch step: 235, loss: 6.453258514404297\n",
      "epoch: 42,  batch step: 236, loss: 5.321925163269043\n",
      "epoch: 42,  batch step: 237, loss: 44.38794708251953\n",
      "epoch: 42,  batch step: 238, loss: 5.0678791999816895\n",
      "epoch: 42,  batch step: 239, loss: 25.975688934326172\n",
      "epoch: 42,  batch step: 240, loss: 4.443446159362793\n",
      "epoch: 42,  batch step: 241, loss: 5.094378471374512\n",
      "epoch: 42,  batch step: 242, loss: 7.506999969482422\n",
      "epoch: 42,  batch step: 243, loss: 11.745200157165527\n",
      "epoch: 42,  batch step: 244, loss: 11.337023735046387\n",
      "epoch: 42,  batch step: 245, loss: 24.161001205444336\n",
      "epoch: 42,  batch step: 246, loss: 36.33897018432617\n",
      "epoch: 42,  batch step: 247, loss: 5.052416801452637\n",
      "epoch: 42,  batch step: 248, loss: 5.0636749267578125\n",
      "epoch: 42,  batch step: 249, loss: 30.806785583496094\n",
      "epoch: 42,  batch step: 250, loss: 11.363941192626953\n",
      "epoch: 42,  batch step: 251, loss: 100.14360046386719\n",
      "validation error epoch  42:    tensor(65.5476, device='cuda:0')\n",
      "316\n",
      "epoch: 43,  batch step: 0, loss: 3.718080520629883\n",
      "epoch: 43,  batch step: 1, loss: 21.124914169311523\n",
      "epoch: 43,  batch step: 2, loss: 24.734922409057617\n",
      "epoch: 43,  batch step: 3, loss: 15.78149700164795\n",
      "epoch: 43,  batch step: 4, loss: 3.1514620780944824\n",
      "epoch: 43,  batch step: 5, loss: 3.355849266052246\n",
      "epoch: 43,  batch step: 6, loss: 12.979454040527344\n",
      "epoch: 43,  batch step: 7, loss: 15.439939498901367\n",
      "epoch: 43,  batch step: 8, loss: 51.38706588745117\n",
      "epoch: 43,  batch step: 9, loss: 105.806640625\n",
      "epoch: 43,  batch step: 10, loss: 24.946758270263672\n",
      "epoch: 43,  batch step: 11, loss: 47.752037048339844\n",
      "epoch: 43,  batch step: 12, loss: 43.001258850097656\n",
      "epoch: 43,  batch step: 13, loss: 8.089630126953125\n",
      "epoch: 43,  batch step: 14, loss: 4.208571434020996\n",
      "epoch: 43,  batch step: 15, loss: 12.142091751098633\n",
      "epoch: 43,  batch step: 16, loss: 26.147762298583984\n",
      "epoch: 43,  batch step: 17, loss: 9.756258010864258\n",
      "epoch: 43,  batch step: 18, loss: 3.910238265991211\n",
      "epoch: 43,  batch step: 19, loss: 21.065641403198242\n",
      "epoch: 43,  batch step: 20, loss: 5.70319128036499\n",
      "epoch: 43,  batch step: 21, loss: 103.0073471069336\n",
      "epoch: 43,  batch step: 22, loss: 38.65902328491211\n",
      "epoch: 43,  batch step: 23, loss: 17.399831771850586\n",
      "epoch: 43,  batch step: 24, loss: 18.608192443847656\n",
      "epoch: 43,  batch step: 25, loss: 6.818734645843506\n",
      "epoch: 43,  batch step: 26, loss: 5.750821590423584\n",
      "epoch: 43,  batch step: 27, loss: 6.470820426940918\n",
      "epoch: 43,  batch step: 28, loss: 109.01365661621094\n",
      "epoch: 43,  batch step: 29, loss: 17.278133392333984\n",
      "epoch: 43,  batch step: 30, loss: 84.08860778808594\n",
      "epoch: 43,  batch step: 31, loss: 8.71290397644043\n",
      "epoch: 43,  batch step: 32, loss: 6.58853816986084\n",
      "epoch: 43,  batch step: 33, loss: 116.54216766357422\n",
      "epoch: 43,  batch step: 34, loss: 3.7347259521484375\n",
      "epoch: 43,  batch step: 35, loss: 9.77354621887207\n",
      "epoch: 43,  batch step: 36, loss: 3.6673483848571777\n",
      "epoch: 43,  batch step: 37, loss: 8.43039608001709\n",
      "epoch: 43,  batch step: 38, loss: 8.633277893066406\n",
      "epoch: 43,  batch step: 39, loss: 35.904869079589844\n",
      "epoch: 43,  batch step: 40, loss: 107.8440170288086\n",
      "epoch: 43,  batch step: 41, loss: 56.182945251464844\n",
      "epoch: 43,  batch step: 42, loss: 21.37164878845215\n",
      "epoch: 43,  batch step: 43, loss: 6.408710479736328\n",
      "epoch: 43,  batch step: 44, loss: 30.477689743041992\n",
      "epoch: 43,  batch step: 45, loss: 7.5584211349487305\n",
      "epoch: 43,  batch step: 46, loss: 8.978736877441406\n",
      "epoch: 43,  batch step: 47, loss: 39.621734619140625\n",
      "epoch: 43,  batch step: 48, loss: 6.746367454528809\n",
      "epoch: 43,  batch step: 49, loss: 4.100089073181152\n",
      "epoch: 43,  batch step: 50, loss: 52.51682662963867\n",
      "epoch: 43,  batch step: 51, loss: 20.22948455810547\n",
      "epoch: 43,  batch step: 52, loss: 3.2713637351989746\n",
      "epoch: 43,  batch step: 53, loss: 69.60612487792969\n",
      "epoch: 43,  batch step: 54, loss: 8.00726318359375\n",
      "epoch: 43,  batch step: 55, loss: 7.600376129150391\n",
      "epoch: 43,  batch step: 56, loss: 32.106571197509766\n",
      "epoch: 43,  batch step: 57, loss: 11.151851654052734\n",
      "epoch: 43,  batch step: 58, loss: 58.4018669128418\n",
      "epoch: 43,  batch step: 59, loss: 6.055152893066406\n",
      "epoch: 43,  batch step: 60, loss: 60.00975799560547\n",
      "epoch: 43,  batch step: 61, loss: 9.97876262664795\n",
      "epoch: 43,  batch step: 62, loss: 17.21581268310547\n",
      "epoch: 43,  batch step: 63, loss: 4.5311994552612305\n",
      "epoch: 43,  batch step: 64, loss: 43.99908447265625\n",
      "epoch: 43,  batch step: 65, loss: 5.636254787445068\n",
      "epoch: 43,  batch step: 66, loss: 39.680030822753906\n",
      "epoch: 43,  batch step: 67, loss: 39.1010856628418\n",
      "epoch: 43,  batch step: 68, loss: 37.208683013916016\n",
      "epoch: 43,  batch step: 69, loss: 37.853843688964844\n",
      "epoch: 43,  batch step: 70, loss: 41.46159362792969\n",
      "epoch: 43,  batch step: 71, loss: 17.459728240966797\n",
      "epoch: 43,  batch step: 72, loss: 7.3436079025268555\n",
      "epoch: 43,  batch step: 73, loss: 55.23142623901367\n",
      "epoch: 43,  batch step: 74, loss: 34.524375915527344\n",
      "epoch: 43,  batch step: 75, loss: 6.271273612976074\n",
      "epoch: 43,  batch step: 76, loss: 9.370445251464844\n",
      "epoch: 43,  batch step: 77, loss: 51.66657638549805\n",
      "epoch: 43,  batch step: 78, loss: 6.5070624351501465\n",
      "epoch: 43,  batch step: 79, loss: 39.526668548583984\n",
      "epoch: 43,  batch step: 80, loss: 12.415420532226562\n",
      "epoch: 43,  batch step: 81, loss: 67.42438507080078\n",
      "epoch: 43,  batch step: 82, loss: 4.561587333679199\n",
      "epoch: 43,  batch step: 83, loss: 3.906564712524414\n",
      "epoch: 43,  batch step: 84, loss: 45.87806701660156\n",
      "epoch: 43,  batch step: 85, loss: 67.98158264160156\n",
      "epoch: 43,  batch step: 86, loss: 10.744466781616211\n",
      "epoch: 43,  batch step: 87, loss: 43.986961364746094\n",
      "epoch: 43,  batch step: 88, loss: 10.540168762207031\n",
      "epoch: 43,  batch step: 89, loss: 4.383635520935059\n",
      "epoch: 43,  batch step: 90, loss: 4.285952568054199\n",
      "epoch: 43,  batch step: 91, loss: 31.633834838867188\n",
      "epoch: 43,  batch step: 92, loss: 19.197221755981445\n",
      "epoch: 43,  batch step: 93, loss: 15.154500961303711\n",
      "epoch: 43,  batch step: 94, loss: 30.238842010498047\n",
      "epoch: 43,  batch step: 95, loss: 3.679089307785034\n",
      "epoch: 43,  batch step: 96, loss: 105.91130828857422\n",
      "epoch: 43,  batch step: 97, loss: 30.588668823242188\n",
      "epoch: 43,  batch step: 98, loss: 35.25634765625\n",
      "epoch: 43,  batch step: 99, loss: 6.599837303161621\n",
      "epoch: 43,  batch step: 100, loss: 4.2016730308532715\n",
      "epoch: 43,  batch step: 101, loss: 36.019981384277344\n",
      "epoch: 43,  batch step: 102, loss: 12.304308891296387\n",
      "epoch: 43,  batch step: 103, loss: 13.708806991577148\n",
      "epoch: 43,  batch step: 104, loss: 8.951289176940918\n",
      "epoch: 43,  batch step: 105, loss: 5.411561012268066\n",
      "epoch: 43,  batch step: 106, loss: 19.72332763671875\n",
      "epoch: 43,  batch step: 107, loss: 63.170989990234375\n",
      "epoch: 43,  batch step: 108, loss: 77.46673583984375\n",
      "epoch: 43,  batch step: 109, loss: 65.70166015625\n",
      "epoch: 43,  batch step: 110, loss: 21.43958854675293\n",
      "epoch: 43,  batch step: 111, loss: 3.8470592498779297\n",
      "epoch: 43,  batch step: 112, loss: 5.737614631652832\n",
      "epoch: 43,  batch step: 113, loss: 71.20455932617188\n",
      "epoch: 43,  batch step: 114, loss: 26.280187606811523\n",
      "epoch: 43,  batch step: 115, loss: 5.135044574737549\n",
      "epoch: 43,  batch step: 116, loss: 17.77867889404297\n",
      "epoch: 43,  batch step: 117, loss: 3.996690273284912\n",
      "epoch: 43,  batch step: 118, loss: 8.425861358642578\n",
      "epoch: 43,  batch step: 119, loss: 4.7343950271606445\n",
      "epoch: 43,  batch step: 120, loss: 43.918060302734375\n",
      "epoch: 43,  batch step: 121, loss: 101.22029113769531\n",
      "epoch: 43,  batch step: 122, loss: 73.26332092285156\n",
      "epoch: 43,  batch step: 123, loss: 4.231285095214844\n",
      "epoch: 43,  batch step: 124, loss: 52.94222640991211\n",
      "epoch: 43,  batch step: 125, loss: 5.771022796630859\n",
      "epoch: 43,  batch step: 126, loss: 3.8580665588378906\n",
      "epoch: 43,  batch step: 127, loss: 6.749716758728027\n",
      "epoch: 43,  batch step: 128, loss: 58.283668518066406\n",
      "epoch: 43,  batch step: 129, loss: 35.7111930847168\n",
      "epoch: 43,  batch step: 130, loss: 5.023852825164795\n",
      "epoch: 43,  batch step: 131, loss: 8.427846908569336\n",
      "epoch: 43,  batch step: 132, loss: 4.462919235229492\n",
      "epoch: 43,  batch step: 133, loss: 11.570272445678711\n",
      "epoch: 43,  batch step: 134, loss: 4.518214225769043\n",
      "epoch: 43,  batch step: 135, loss: 41.17841720581055\n",
      "epoch: 43,  batch step: 136, loss: 3.859973430633545\n",
      "epoch: 43,  batch step: 137, loss: 7.816592216491699\n",
      "epoch: 43,  batch step: 138, loss: 5.952414512634277\n",
      "epoch: 43,  batch step: 139, loss: 35.39720916748047\n",
      "epoch: 43,  batch step: 140, loss: 53.92141342163086\n",
      "epoch: 43,  batch step: 141, loss: 3.647599935531616\n",
      "epoch: 43,  batch step: 142, loss: 8.24941635131836\n",
      "epoch: 43,  batch step: 143, loss: 9.060468673706055\n",
      "epoch: 43,  batch step: 144, loss: 9.556726455688477\n",
      "epoch: 43,  batch step: 145, loss: 5.268527984619141\n",
      "epoch: 43,  batch step: 146, loss: 137.17205810546875\n",
      "epoch: 43,  batch step: 147, loss: 7.618618011474609\n",
      "epoch: 43,  batch step: 148, loss: 39.05773162841797\n",
      "epoch: 43,  batch step: 149, loss: 57.138572692871094\n",
      "epoch: 43,  batch step: 150, loss: 5.310390472412109\n",
      "epoch: 43,  batch step: 151, loss: 18.095458984375\n",
      "epoch: 43,  batch step: 152, loss: 8.54379653930664\n",
      "epoch: 43,  batch step: 153, loss: 5.575624465942383\n",
      "epoch: 43,  batch step: 154, loss: 17.312503814697266\n",
      "epoch: 43,  batch step: 155, loss: 6.114945888519287\n",
      "epoch: 43,  batch step: 156, loss: 21.474681854248047\n",
      "epoch: 43,  batch step: 157, loss: 40.66226577758789\n",
      "epoch: 43,  batch step: 158, loss: 78.06602478027344\n",
      "epoch: 43,  batch step: 159, loss: 29.41035270690918\n",
      "epoch: 43,  batch step: 160, loss: 27.771087646484375\n",
      "epoch: 43,  batch step: 161, loss: 32.67550277709961\n",
      "epoch: 43,  batch step: 162, loss: 8.691507339477539\n",
      "epoch: 43,  batch step: 163, loss: 77.20616149902344\n",
      "epoch: 43,  batch step: 164, loss: 46.441650390625\n",
      "epoch: 43,  batch step: 165, loss: 33.98112487792969\n",
      "epoch: 43,  batch step: 166, loss: 8.499829292297363\n",
      "epoch: 43,  batch step: 167, loss: 3.9301400184631348\n",
      "epoch: 43,  batch step: 168, loss: 25.92891502380371\n",
      "epoch: 43,  batch step: 169, loss: 205.42898559570312\n",
      "epoch: 43,  batch step: 170, loss: 96.14884948730469\n",
      "epoch: 43,  batch step: 171, loss: 43.6800537109375\n",
      "epoch: 43,  batch step: 172, loss: 36.673885345458984\n",
      "epoch: 43,  batch step: 173, loss: 7.1258978843688965\n",
      "epoch: 43,  batch step: 174, loss: 40.22190856933594\n",
      "epoch: 43,  batch step: 175, loss: 64.52218627929688\n",
      "epoch: 43,  batch step: 176, loss: 27.479690551757812\n",
      "epoch: 43,  batch step: 177, loss: 26.902517318725586\n",
      "epoch: 43,  batch step: 178, loss: 107.37228393554688\n",
      "epoch: 43,  batch step: 179, loss: 6.807248115539551\n",
      "epoch: 43,  batch step: 180, loss: 36.54120635986328\n",
      "epoch: 43,  batch step: 181, loss: 180.8848114013672\n",
      "epoch: 43,  batch step: 182, loss: 7.243180751800537\n",
      "epoch: 43,  batch step: 183, loss: 51.40392303466797\n",
      "epoch: 43,  batch step: 184, loss: 7.465466499328613\n",
      "epoch: 43,  batch step: 185, loss: 58.90299606323242\n",
      "epoch: 43,  batch step: 186, loss: 21.6495361328125\n",
      "epoch: 43,  batch step: 187, loss: 11.042247772216797\n",
      "epoch: 43,  batch step: 188, loss: 13.355528831481934\n",
      "epoch: 43,  batch step: 189, loss: 104.80977630615234\n",
      "epoch: 43,  batch step: 190, loss: 6.948052406311035\n",
      "epoch: 43,  batch step: 191, loss: 11.086024284362793\n",
      "epoch: 43,  batch step: 192, loss: 69.01993560791016\n",
      "epoch: 43,  batch step: 193, loss: 48.250267028808594\n",
      "epoch: 43,  batch step: 194, loss: 4.783511161804199\n",
      "epoch: 43,  batch step: 195, loss: 68.9574966430664\n",
      "epoch: 43,  batch step: 196, loss: 93.57460021972656\n",
      "epoch: 43,  batch step: 197, loss: 5.001738548278809\n",
      "epoch: 43,  batch step: 198, loss: 6.044972896575928\n",
      "epoch: 43,  batch step: 199, loss: 9.072173118591309\n",
      "epoch: 43,  batch step: 200, loss: 14.204009056091309\n",
      "epoch: 43,  batch step: 201, loss: 82.94686889648438\n",
      "epoch: 43,  batch step: 202, loss: 20.182811737060547\n",
      "epoch: 43,  batch step: 203, loss: 15.662864685058594\n",
      "epoch: 43,  batch step: 204, loss: 174.4373016357422\n",
      "epoch: 43,  batch step: 205, loss: 11.67786979675293\n",
      "epoch: 43,  batch step: 206, loss: 39.87912368774414\n",
      "epoch: 43,  batch step: 207, loss: 5.291251182556152\n",
      "epoch: 43,  batch step: 208, loss: 21.057308197021484\n",
      "epoch: 43,  batch step: 209, loss: 8.264754295349121\n",
      "epoch: 43,  batch step: 210, loss: 20.01523208618164\n",
      "epoch: 43,  batch step: 211, loss: 31.183753967285156\n",
      "epoch: 43,  batch step: 212, loss: 30.262741088867188\n",
      "epoch: 43,  batch step: 213, loss: 7.536898612976074\n",
      "epoch: 43,  batch step: 214, loss: 8.871352195739746\n",
      "epoch: 43,  batch step: 215, loss: 3.8371634483337402\n",
      "epoch: 43,  batch step: 216, loss: 8.393299102783203\n",
      "epoch: 43,  batch step: 217, loss: 5.8086347579956055\n",
      "epoch: 43,  batch step: 218, loss: 14.538957595825195\n",
      "epoch: 43,  batch step: 219, loss: 42.34221649169922\n",
      "epoch: 43,  batch step: 220, loss: 102.1279525756836\n",
      "epoch: 43,  batch step: 221, loss: 9.287364959716797\n",
      "epoch: 43,  batch step: 222, loss: 32.57286834716797\n",
      "epoch: 43,  batch step: 223, loss: 10.780380249023438\n",
      "epoch: 43,  batch step: 224, loss: 9.839189529418945\n",
      "epoch: 43,  batch step: 225, loss: 143.69921875\n",
      "epoch: 43,  batch step: 226, loss: 211.93817138671875\n",
      "epoch: 43,  batch step: 227, loss: 95.16228485107422\n",
      "epoch: 43,  batch step: 228, loss: 118.93719482421875\n",
      "epoch: 43,  batch step: 229, loss: 5.006121635437012\n",
      "epoch: 43,  batch step: 230, loss: 36.23181915283203\n",
      "epoch: 43,  batch step: 231, loss: 11.582576751708984\n",
      "epoch: 43,  batch step: 232, loss: 20.71883773803711\n",
      "epoch: 43,  batch step: 233, loss: 38.31440734863281\n",
      "epoch: 43,  batch step: 234, loss: 15.941200256347656\n",
      "epoch: 43,  batch step: 235, loss: 29.359777450561523\n",
      "epoch: 43,  batch step: 236, loss: 14.459989547729492\n",
      "epoch: 43,  batch step: 237, loss: 32.56222152709961\n",
      "epoch: 43,  batch step: 238, loss: 59.96303939819336\n",
      "epoch: 43,  batch step: 239, loss: 59.095542907714844\n",
      "epoch: 43,  batch step: 240, loss: 71.14412689208984\n",
      "epoch: 43,  batch step: 241, loss: 5.706908226013184\n",
      "epoch: 43,  batch step: 242, loss: 46.40062713623047\n",
      "epoch: 43,  batch step: 243, loss: 36.13119125366211\n",
      "epoch: 43,  batch step: 244, loss: 8.45917797088623\n",
      "epoch: 43,  batch step: 245, loss: 149.4075164794922\n",
      "epoch: 43,  batch step: 246, loss: 3.880643367767334\n",
      "epoch: 43,  batch step: 247, loss: 9.619582176208496\n",
      "epoch: 43,  batch step: 248, loss: 6.626091957092285\n",
      "epoch: 43,  batch step: 249, loss: 34.11744689941406\n",
      "epoch: 43,  batch step: 250, loss: 14.188175201416016\n",
      "epoch: 43,  batch step: 251, loss: 8.880895614624023\n",
      "validation error epoch  43:    tensor(78.1892, device='cuda:0')\n",
      "316\n",
      "epoch: 44,  batch step: 0, loss: 62.540985107421875\n",
      "epoch: 44,  batch step: 1, loss: 7.566916465759277\n",
      "epoch: 44,  batch step: 2, loss: 10.199766159057617\n",
      "epoch: 44,  batch step: 3, loss: 7.14738655090332\n",
      "epoch: 44,  batch step: 4, loss: 20.857162475585938\n",
      "epoch: 44,  batch step: 5, loss: 33.1199836730957\n",
      "epoch: 44,  batch step: 6, loss: 20.48671531677246\n",
      "epoch: 44,  batch step: 7, loss: 52.30610275268555\n",
      "epoch: 44,  batch step: 8, loss: 12.646631240844727\n",
      "epoch: 44,  batch step: 9, loss: 9.497267723083496\n",
      "epoch: 44,  batch step: 10, loss: 14.093955993652344\n",
      "epoch: 44,  batch step: 11, loss: 6.320275783538818\n",
      "epoch: 44,  batch step: 12, loss: 5.885990142822266\n",
      "epoch: 44,  batch step: 13, loss: 51.30561447143555\n",
      "epoch: 44,  batch step: 14, loss: 7.6184916496276855\n",
      "epoch: 44,  batch step: 15, loss: 52.542320251464844\n",
      "epoch: 44,  batch step: 16, loss: 7.162769317626953\n",
      "epoch: 44,  batch step: 17, loss: 79.63316345214844\n",
      "epoch: 44,  batch step: 18, loss: 9.653976440429688\n",
      "epoch: 44,  batch step: 19, loss: 46.26807403564453\n",
      "epoch: 44,  batch step: 20, loss: 9.609672546386719\n",
      "epoch: 44,  batch step: 21, loss: 38.74715805053711\n",
      "epoch: 44,  batch step: 22, loss: 35.22306442260742\n",
      "epoch: 44,  batch step: 23, loss: 8.860560417175293\n",
      "epoch: 44,  batch step: 24, loss: 6.423909664154053\n",
      "epoch: 44,  batch step: 25, loss: 55.10188293457031\n",
      "epoch: 44,  batch step: 26, loss: 8.36741828918457\n",
      "epoch: 44,  batch step: 27, loss: 117.68067932128906\n",
      "epoch: 44,  batch step: 28, loss: 159.8067169189453\n",
      "epoch: 44,  batch step: 29, loss: 7.972263336181641\n",
      "epoch: 44,  batch step: 30, loss: 5.995091915130615\n",
      "epoch: 44,  batch step: 31, loss: 77.07568359375\n",
      "epoch: 44,  batch step: 32, loss: 43.5552978515625\n",
      "epoch: 44,  batch step: 33, loss: 51.12139129638672\n",
      "epoch: 44,  batch step: 34, loss: 5.390440940856934\n",
      "epoch: 44,  batch step: 35, loss: 3.6483640670776367\n",
      "epoch: 44,  batch step: 36, loss: 30.375722885131836\n",
      "epoch: 44,  batch step: 37, loss: 104.27898406982422\n",
      "epoch: 44,  batch step: 38, loss: 29.010574340820312\n",
      "epoch: 44,  batch step: 39, loss: 26.797683715820312\n",
      "epoch: 44,  batch step: 40, loss: 53.295570373535156\n",
      "epoch: 44,  batch step: 41, loss: 4.288804054260254\n",
      "epoch: 44,  batch step: 42, loss: 20.509014129638672\n",
      "epoch: 44,  batch step: 43, loss: 36.74113845825195\n",
      "epoch: 44,  batch step: 44, loss: 18.378677368164062\n",
      "epoch: 44,  batch step: 45, loss: 4.648240089416504\n",
      "epoch: 44,  batch step: 46, loss: 34.37946319580078\n",
      "epoch: 44,  batch step: 47, loss: 5.726657867431641\n",
      "epoch: 44,  batch step: 48, loss: 23.577194213867188\n",
      "epoch: 44,  batch step: 49, loss: 3.464844226837158\n",
      "epoch: 44,  batch step: 50, loss: 4.227584362030029\n",
      "epoch: 44,  batch step: 51, loss: 2.965540885925293\n",
      "epoch: 44,  batch step: 52, loss: 3.32798433303833\n",
      "epoch: 44,  batch step: 53, loss: 5.91217565536499\n",
      "epoch: 44,  batch step: 54, loss: 83.11122131347656\n",
      "epoch: 44,  batch step: 55, loss: 75.92584228515625\n",
      "epoch: 44,  batch step: 56, loss: 16.50824737548828\n",
      "epoch: 44,  batch step: 57, loss: 30.73446273803711\n",
      "epoch: 44,  batch step: 58, loss: 9.832929611206055\n",
      "epoch: 44,  batch step: 59, loss: 7.332688808441162\n",
      "epoch: 44,  batch step: 60, loss: 5.555390357971191\n",
      "epoch: 44,  batch step: 61, loss: 95.99615478515625\n",
      "epoch: 44,  batch step: 62, loss: 6.378057479858398\n",
      "epoch: 44,  batch step: 63, loss: 4.208133697509766\n",
      "epoch: 44,  batch step: 64, loss: 133.9732666015625\n",
      "epoch: 44,  batch step: 65, loss: 9.1661376953125\n",
      "epoch: 44,  batch step: 66, loss: 10.11721420288086\n",
      "epoch: 44,  batch step: 67, loss: 11.533824920654297\n",
      "epoch: 44,  batch step: 68, loss: 7.075389862060547\n",
      "epoch: 44,  batch step: 69, loss: 97.13816833496094\n",
      "epoch: 44,  batch step: 70, loss: 6.941516876220703\n",
      "epoch: 44,  batch step: 71, loss: 10.900312423706055\n",
      "epoch: 44,  batch step: 72, loss: 7.231437683105469\n",
      "epoch: 44,  batch step: 73, loss: 99.6991195678711\n",
      "epoch: 44,  batch step: 74, loss: 63.84765625\n",
      "epoch: 44,  batch step: 75, loss: 17.65505599975586\n",
      "epoch: 44,  batch step: 76, loss: 4.196252346038818\n",
      "epoch: 44,  batch step: 77, loss: 25.0672607421875\n",
      "epoch: 44,  batch step: 78, loss: 23.63614273071289\n",
      "epoch: 44,  batch step: 79, loss: 57.08812713623047\n",
      "epoch: 44,  batch step: 80, loss: 74.29676818847656\n",
      "epoch: 44,  batch step: 81, loss: 52.00453186035156\n",
      "epoch: 44,  batch step: 82, loss: 3.927018642425537\n",
      "epoch: 44,  batch step: 83, loss: 6.887986183166504\n",
      "epoch: 44,  batch step: 84, loss: 9.023345947265625\n",
      "epoch: 44,  batch step: 85, loss: 3.2261693477630615\n",
      "epoch: 44,  batch step: 86, loss: 36.77667236328125\n",
      "epoch: 44,  batch step: 87, loss: 9.911041259765625\n",
      "epoch: 44,  batch step: 88, loss: 35.401763916015625\n",
      "epoch: 44,  batch step: 89, loss: 10.334815979003906\n",
      "epoch: 44,  batch step: 90, loss: 17.827816009521484\n",
      "epoch: 44,  batch step: 91, loss: 39.7158088684082\n",
      "epoch: 44,  batch step: 92, loss: 28.769439697265625\n",
      "epoch: 44,  batch step: 93, loss: 25.30023193359375\n",
      "epoch: 44,  batch step: 94, loss: 43.840328216552734\n",
      "epoch: 44,  batch step: 95, loss: 10.923563003540039\n",
      "epoch: 44,  batch step: 96, loss: 87.75608825683594\n",
      "epoch: 44,  batch step: 97, loss: 52.22698211669922\n",
      "epoch: 44,  batch step: 98, loss: 11.126853942871094\n",
      "epoch: 44,  batch step: 99, loss: 7.5111212730407715\n",
      "epoch: 44,  batch step: 100, loss: 7.569122314453125\n",
      "epoch: 44,  batch step: 101, loss: 20.844913482666016\n",
      "epoch: 44,  batch step: 102, loss: 25.94818687438965\n",
      "epoch: 44,  batch step: 103, loss: 6.673802852630615\n",
      "epoch: 44,  batch step: 104, loss: 11.665698051452637\n",
      "epoch: 44,  batch step: 105, loss: 34.48359298706055\n",
      "epoch: 44,  batch step: 106, loss: 5.083847522735596\n",
      "epoch: 44,  batch step: 107, loss: 5.9632463455200195\n",
      "epoch: 44,  batch step: 108, loss: 4.880299091339111\n",
      "epoch: 44,  batch step: 109, loss: 26.068115234375\n",
      "epoch: 44,  batch step: 110, loss: 9.209930419921875\n",
      "epoch: 44,  batch step: 111, loss: 6.264829158782959\n",
      "epoch: 44,  batch step: 112, loss: 176.952392578125\n",
      "epoch: 44,  batch step: 113, loss: 5.310141086578369\n",
      "epoch: 44,  batch step: 114, loss: 3.676424980163574\n",
      "epoch: 44,  batch step: 115, loss: 90.44471740722656\n",
      "epoch: 44,  batch step: 116, loss: 10.770927429199219\n",
      "epoch: 44,  batch step: 117, loss: 8.93073558807373\n",
      "epoch: 44,  batch step: 118, loss: 3.0075831413269043\n",
      "epoch: 44,  batch step: 119, loss: 38.257179260253906\n",
      "epoch: 44,  batch step: 120, loss: 28.923934936523438\n",
      "epoch: 44,  batch step: 121, loss: 48.617855072021484\n",
      "epoch: 44,  batch step: 122, loss: 18.38585662841797\n",
      "epoch: 44,  batch step: 123, loss: 30.784381866455078\n",
      "epoch: 44,  batch step: 124, loss: 39.88555145263672\n",
      "epoch: 44,  batch step: 125, loss: 37.236183166503906\n",
      "epoch: 44,  batch step: 126, loss: 5.095311164855957\n",
      "epoch: 44,  batch step: 127, loss: 28.974483489990234\n",
      "epoch: 44,  batch step: 128, loss: 7.355220794677734\n",
      "epoch: 44,  batch step: 129, loss: 11.18795394897461\n",
      "epoch: 44,  batch step: 130, loss: 5.344962120056152\n",
      "epoch: 44,  batch step: 131, loss: 125.27104949951172\n",
      "epoch: 44,  batch step: 132, loss: 16.119009017944336\n",
      "epoch: 44,  batch step: 133, loss: 5.675647258758545\n",
      "epoch: 44,  batch step: 134, loss: 6.112876892089844\n",
      "epoch: 44,  batch step: 135, loss: 55.341644287109375\n",
      "epoch: 44,  batch step: 136, loss: 9.57553482055664\n",
      "epoch: 44,  batch step: 137, loss: 3.726940155029297\n",
      "epoch: 44,  batch step: 138, loss: 168.20266723632812\n",
      "epoch: 44,  batch step: 139, loss: 9.104328155517578\n",
      "epoch: 44,  batch step: 140, loss: 17.458187103271484\n",
      "epoch: 44,  batch step: 141, loss: 29.523771286010742\n",
      "epoch: 44,  batch step: 142, loss: 105.67668914794922\n",
      "epoch: 44,  batch step: 143, loss: 6.423366546630859\n",
      "epoch: 44,  batch step: 144, loss: 5.587616920471191\n",
      "epoch: 44,  batch step: 145, loss: 7.35870361328125\n",
      "epoch: 44,  batch step: 146, loss: 40.70842742919922\n",
      "epoch: 44,  batch step: 147, loss: 111.61207580566406\n",
      "epoch: 44,  batch step: 148, loss: 7.795701503753662\n",
      "epoch: 44,  batch step: 149, loss: 54.29652404785156\n",
      "epoch: 44,  batch step: 150, loss: 22.30855941772461\n",
      "epoch: 44,  batch step: 151, loss: 29.964191436767578\n",
      "epoch: 44,  batch step: 152, loss: 52.02934265136719\n",
      "epoch: 44,  batch step: 153, loss: 20.356021881103516\n",
      "epoch: 44,  batch step: 154, loss: 72.64041900634766\n",
      "epoch: 44,  batch step: 155, loss: 30.13955307006836\n",
      "epoch: 44,  batch step: 156, loss: 11.496916770935059\n",
      "epoch: 44,  batch step: 157, loss: 13.55024528503418\n",
      "epoch: 44,  batch step: 158, loss: 52.16124725341797\n",
      "epoch: 44,  batch step: 159, loss: 11.248034477233887\n",
      "epoch: 44,  batch step: 160, loss: 7.856449127197266\n",
      "epoch: 44,  batch step: 161, loss: 43.47672653198242\n",
      "epoch: 44,  batch step: 162, loss: 7.242522239685059\n",
      "epoch: 44,  batch step: 163, loss: 4.492730140686035\n",
      "epoch: 44,  batch step: 164, loss: 20.6607666015625\n",
      "epoch: 44,  batch step: 165, loss: 36.52192687988281\n",
      "epoch: 44,  batch step: 166, loss: 8.265775680541992\n",
      "epoch: 44,  batch step: 167, loss: 45.825164794921875\n",
      "epoch: 44,  batch step: 168, loss: 4.166866302490234\n",
      "epoch: 44,  batch step: 169, loss: 18.288002014160156\n",
      "epoch: 44,  batch step: 170, loss: 39.742454528808594\n",
      "epoch: 44,  batch step: 171, loss: 21.823360443115234\n",
      "epoch: 44,  batch step: 172, loss: 59.74317169189453\n",
      "epoch: 44,  batch step: 173, loss: 218.64927673339844\n",
      "epoch: 44,  batch step: 174, loss: 5.571074485778809\n",
      "epoch: 44,  batch step: 175, loss: 44.899810791015625\n",
      "epoch: 44,  batch step: 176, loss: 9.273818969726562\n",
      "epoch: 44,  batch step: 177, loss: 16.11740493774414\n",
      "epoch: 44,  batch step: 178, loss: 36.727474212646484\n",
      "epoch: 44,  batch step: 179, loss: 8.219145774841309\n",
      "epoch: 44,  batch step: 180, loss: 14.182361602783203\n",
      "epoch: 44,  batch step: 181, loss: 29.697086334228516\n",
      "epoch: 44,  batch step: 182, loss: 6.189752101898193\n",
      "epoch: 44,  batch step: 183, loss: 11.92573070526123\n",
      "epoch: 44,  batch step: 184, loss: 32.19402313232422\n",
      "epoch: 44,  batch step: 185, loss: 9.56100082397461\n",
      "epoch: 44,  batch step: 186, loss: 101.11009216308594\n",
      "epoch: 44,  batch step: 187, loss: 8.5281400680542\n",
      "epoch: 44,  batch step: 188, loss: 138.23318481445312\n",
      "epoch: 44,  batch step: 189, loss: 47.53876876831055\n",
      "epoch: 44,  batch step: 190, loss: 69.61703491210938\n",
      "epoch: 44,  batch step: 191, loss: 5.474828720092773\n",
      "epoch: 44,  batch step: 192, loss: 26.34284782409668\n",
      "epoch: 44,  batch step: 193, loss: 7.202878952026367\n",
      "epoch: 44,  batch step: 194, loss: 15.963653564453125\n",
      "epoch: 44,  batch step: 195, loss: 25.4746150970459\n",
      "epoch: 44,  batch step: 196, loss: 57.7769775390625\n",
      "epoch: 44,  batch step: 197, loss: 125.27215576171875\n",
      "epoch: 44,  batch step: 198, loss: 64.56229400634766\n",
      "epoch: 44,  batch step: 199, loss: 53.953704833984375\n",
      "epoch: 44,  batch step: 200, loss: 46.34636688232422\n",
      "epoch: 44,  batch step: 201, loss: 26.919448852539062\n",
      "epoch: 44,  batch step: 202, loss: 32.12699890136719\n",
      "epoch: 44,  batch step: 203, loss: 55.231788635253906\n",
      "epoch: 44,  batch step: 204, loss: 6.54355525970459\n",
      "epoch: 44,  batch step: 205, loss: 6.838587760925293\n",
      "epoch: 44,  batch step: 206, loss: 10.252893447875977\n",
      "epoch: 44,  batch step: 207, loss: 8.894859313964844\n",
      "epoch: 44,  batch step: 208, loss: 9.380921363830566\n",
      "epoch: 44,  batch step: 209, loss: 7.119739055633545\n",
      "epoch: 44,  batch step: 210, loss: 47.56135559082031\n",
      "epoch: 44,  batch step: 211, loss: 7.419154167175293\n",
      "epoch: 44,  batch step: 212, loss: 4.90547513961792\n",
      "epoch: 44,  batch step: 213, loss: 26.551259994506836\n",
      "epoch: 44,  batch step: 214, loss: 15.165942192077637\n",
      "epoch: 44,  batch step: 215, loss: 4.326305389404297\n",
      "epoch: 44,  batch step: 216, loss: 3.2471225261688232\n",
      "epoch: 44,  batch step: 217, loss: 18.057044982910156\n",
      "epoch: 44,  batch step: 218, loss: 51.844329833984375\n",
      "epoch: 44,  batch step: 219, loss: 42.88483810424805\n",
      "epoch: 44,  batch step: 220, loss: 30.0852108001709\n",
      "epoch: 44,  batch step: 221, loss: 5.169785976409912\n",
      "epoch: 44,  batch step: 222, loss: 26.075885772705078\n",
      "epoch: 44,  batch step: 223, loss: 5.978738307952881\n",
      "epoch: 44,  batch step: 224, loss: 68.22103881835938\n",
      "epoch: 44,  batch step: 225, loss: 5.961322784423828\n",
      "epoch: 44,  batch step: 226, loss: 4.586040019989014\n",
      "epoch: 44,  batch step: 227, loss: 29.483325958251953\n",
      "epoch: 44,  batch step: 228, loss: 5.747189998626709\n",
      "epoch: 44,  batch step: 229, loss: 5.968830108642578\n",
      "epoch: 44,  batch step: 230, loss: 45.39309310913086\n",
      "epoch: 44,  batch step: 231, loss: 24.770748138427734\n",
      "epoch: 44,  batch step: 232, loss: 24.181076049804688\n",
      "epoch: 44,  batch step: 233, loss: 50.22520446777344\n",
      "epoch: 44,  batch step: 234, loss: 8.513936996459961\n",
      "epoch: 44,  batch step: 235, loss: 23.481679916381836\n",
      "epoch: 44,  batch step: 236, loss: 8.633345603942871\n",
      "epoch: 44,  batch step: 237, loss: 5.9748663902282715\n",
      "epoch: 44,  batch step: 238, loss: 7.375231742858887\n",
      "epoch: 44,  batch step: 239, loss: 5.912249565124512\n",
      "epoch: 44,  batch step: 240, loss: 61.31782150268555\n",
      "epoch: 44,  batch step: 241, loss: 28.752904891967773\n",
      "epoch: 44,  batch step: 242, loss: 3.526228666305542\n",
      "epoch: 44,  batch step: 243, loss: 2.9524216651916504\n",
      "epoch: 44,  batch step: 244, loss: 3.8210291862487793\n",
      "epoch: 44,  batch step: 245, loss: 96.14851379394531\n",
      "epoch: 44,  batch step: 246, loss: 74.35176086425781\n",
      "epoch: 44,  batch step: 247, loss: 7.64348030090332\n",
      "epoch: 44,  batch step: 248, loss: 4.333929061889648\n",
      "epoch: 44,  batch step: 249, loss: 5.505731582641602\n",
      "epoch: 44,  batch step: 250, loss: 23.64037322998047\n",
      "epoch: 44,  batch step: 251, loss: 76.47685241699219\n",
      "validation error epoch  44:    tensor(71.0261, device='cuda:0')\n",
      "316\n",
      "epoch: 45,  batch step: 0, loss: 89.26774597167969\n",
      "epoch: 45,  batch step: 1, loss: 8.252188682556152\n",
      "epoch: 45,  batch step: 2, loss: 48.609169006347656\n",
      "epoch: 45,  batch step: 3, loss: 6.374972343444824\n",
      "epoch: 45,  batch step: 4, loss: 41.179046630859375\n",
      "epoch: 45,  batch step: 5, loss: 35.92369842529297\n",
      "epoch: 45,  batch step: 6, loss: 57.17896270751953\n",
      "epoch: 45,  batch step: 7, loss: 102.35533142089844\n",
      "epoch: 45,  batch step: 8, loss: 11.841854095458984\n",
      "epoch: 45,  batch step: 9, loss: 12.90265941619873\n",
      "epoch: 45,  batch step: 10, loss: 8.098285675048828\n",
      "epoch: 45,  batch step: 11, loss: 4.696873664855957\n",
      "epoch: 45,  batch step: 12, loss: 26.980697631835938\n",
      "epoch: 45,  batch step: 13, loss: 12.271139144897461\n",
      "epoch: 45,  batch step: 14, loss: 47.90188217163086\n",
      "epoch: 45,  batch step: 15, loss: 85.67879486083984\n",
      "epoch: 45,  batch step: 16, loss: 3.177929401397705\n",
      "epoch: 45,  batch step: 17, loss: 19.09680938720703\n",
      "epoch: 45,  batch step: 18, loss: 3.567700147628784\n",
      "epoch: 45,  batch step: 19, loss: 51.91205596923828\n",
      "epoch: 45,  batch step: 20, loss: 26.368772506713867\n",
      "epoch: 45,  batch step: 21, loss: 5.204223155975342\n",
      "epoch: 45,  batch step: 22, loss: 41.75935363769531\n",
      "epoch: 45,  batch step: 23, loss: 6.447197914123535\n",
      "epoch: 45,  batch step: 24, loss: 21.929981231689453\n",
      "epoch: 45,  batch step: 25, loss: 6.545039176940918\n",
      "epoch: 45,  batch step: 26, loss: 3.2904410362243652\n",
      "epoch: 45,  batch step: 27, loss: 46.87165069580078\n",
      "epoch: 45,  batch step: 28, loss: 12.77503776550293\n",
      "epoch: 45,  batch step: 29, loss: 9.2806396484375\n",
      "epoch: 45,  batch step: 30, loss: 70.65512084960938\n",
      "epoch: 45,  batch step: 31, loss: 6.962672233581543\n",
      "epoch: 45,  batch step: 32, loss: 5.6034345626831055\n",
      "epoch: 45,  batch step: 33, loss: 38.09048080444336\n",
      "epoch: 45,  batch step: 34, loss: 48.65052795410156\n",
      "epoch: 45,  batch step: 35, loss: 27.546451568603516\n",
      "epoch: 45,  batch step: 36, loss: 44.0339469909668\n",
      "epoch: 45,  batch step: 37, loss: 21.408681869506836\n",
      "epoch: 45,  batch step: 38, loss: 12.587507247924805\n",
      "epoch: 45,  batch step: 39, loss: 31.167539596557617\n",
      "epoch: 45,  batch step: 40, loss: 37.27316665649414\n",
      "epoch: 45,  batch step: 41, loss: 60.45599365234375\n",
      "epoch: 45,  batch step: 42, loss: 7.2509894371032715\n",
      "epoch: 45,  batch step: 43, loss: 5.2209367752075195\n",
      "epoch: 45,  batch step: 44, loss: 11.514968872070312\n",
      "epoch: 45,  batch step: 45, loss: 30.400527954101562\n",
      "epoch: 45,  batch step: 46, loss: 19.147300720214844\n",
      "epoch: 45,  batch step: 47, loss: 41.76079559326172\n",
      "epoch: 45,  batch step: 48, loss: 48.04069519042969\n",
      "epoch: 45,  batch step: 49, loss: 9.364984512329102\n",
      "epoch: 45,  batch step: 50, loss: 21.14634895324707\n",
      "epoch: 45,  batch step: 51, loss: 55.279762268066406\n",
      "epoch: 45,  batch step: 52, loss: 44.858665466308594\n",
      "epoch: 45,  batch step: 53, loss: 6.391709327697754\n",
      "epoch: 45,  batch step: 54, loss: 4.683637619018555\n",
      "epoch: 45,  batch step: 55, loss: 11.319788932800293\n",
      "epoch: 45,  batch step: 56, loss: 34.98174285888672\n",
      "epoch: 45,  batch step: 57, loss: 5.115415573120117\n",
      "epoch: 45,  batch step: 58, loss: 42.3452033996582\n",
      "epoch: 45,  batch step: 59, loss: 66.2343978881836\n",
      "epoch: 45,  batch step: 60, loss: 9.53022289276123\n",
      "epoch: 45,  batch step: 61, loss: 35.981990814208984\n",
      "epoch: 45,  batch step: 62, loss: 4.078277111053467\n",
      "epoch: 45,  batch step: 63, loss: 4.4237823486328125\n",
      "epoch: 45,  batch step: 64, loss: 58.27082824707031\n",
      "epoch: 45,  batch step: 65, loss: 9.293490409851074\n",
      "epoch: 45,  batch step: 66, loss: 21.678220748901367\n",
      "epoch: 45,  batch step: 67, loss: 33.76654052734375\n",
      "epoch: 45,  batch step: 68, loss: 4.246427059173584\n",
      "epoch: 45,  batch step: 69, loss: 3.334374189376831\n",
      "epoch: 45,  batch step: 70, loss: 5.225327968597412\n",
      "epoch: 45,  batch step: 71, loss: 13.624183654785156\n",
      "epoch: 45,  batch step: 72, loss: 3.6324777603149414\n",
      "epoch: 45,  batch step: 73, loss: 6.0220184326171875\n",
      "epoch: 45,  batch step: 74, loss: 3.5732944011688232\n",
      "epoch: 45,  batch step: 75, loss: 8.898423194885254\n",
      "epoch: 45,  batch step: 76, loss: 6.636033535003662\n",
      "epoch: 45,  batch step: 77, loss: 4.834503173828125\n",
      "epoch: 45,  batch step: 78, loss: 54.160518646240234\n",
      "epoch: 45,  batch step: 79, loss: 14.442401885986328\n",
      "epoch: 45,  batch step: 80, loss: 17.966005325317383\n",
      "epoch: 45,  batch step: 81, loss: 7.115804195404053\n",
      "epoch: 45,  batch step: 82, loss: 14.069037437438965\n",
      "epoch: 45,  batch step: 83, loss: 16.15013885498047\n",
      "epoch: 45,  batch step: 84, loss: 3.2940738201141357\n",
      "epoch: 45,  batch step: 85, loss: 4.706796646118164\n",
      "epoch: 45,  batch step: 86, loss: 26.750791549682617\n",
      "epoch: 45,  batch step: 87, loss: 15.492938041687012\n",
      "epoch: 45,  batch step: 88, loss: 42.50701904296875\n",
      "epoch: 45,  batch step: 89, loss: 52.55902099609375\n",
      "epoch: 45,  batch step: 90, loss: 5.584468841552734\n",
      "epoch: 45,  batch step: 91, loss: 17.850299835205078\n",
      "epoch: 45,  batch step: 92, loss: 4.068476676940918\n",
      "epoch: 45,  batch step: 93, loss: 3.663886070251465\n",
      "epoch: 45,  batch step: 94, loss: 3.3507184982299805\n",
      "epoch: 45,  batch step: 95, loss: 23.996801376342773\n",
      "epoch: 45,  batch step: 96, loss: 38.0489501953125\n",
      "epoch: 45,  batch step: 97, loss: 3.7200515270233154\n",
      "epoch: 45,  batch step: 98, loss: 20.001686096191406\n",
      "epoch: 45,  batch step: 99, loss: 4.013764381408691\n",
      "epoch: 45,  batch step: 100, loss: 4.6409010887146\n",
      "epoch: 45,  batch step: 101, loss: 15.84471321105957\n",
      "epoch: 45,  batch step: 102, loss: 7.722289085388184\n",
      "epoch: 45,  batch step: 103, loss: 16.56132698059082\n",
      "epoch: 45,  batch step: 104, loss: 12.346418380737305\n",
      "epoch: 45,  batch step: 105, loss: 3.0794379711151123\n",
      "epoch: 45,  batch step: 106, loss: 5.255488395690918\n",
      "epoch: 45,  batch step: 107, loss: 3.53010892868042\n",
      "epoch: 45,  batch step: 108, loss: 4.050144195556641\n",
      "epoch: 45,  batch step: 109, loss: 6.2750067710876465\n",
      "epoch: 45,  batch step: 110, loss: 132.322021484375\n",
      "epoch: 45,  batch step: 111, loss: 17.00259017944336\n",
      "epoch: 45,  batch step: 112, loss: 65.52188873291016\n",
      "epoch: 45,  batch step: 113, loss: 9.852930068969727\n",
      "epoch: 45,  batch step: 114, loss: 25.658384323120117\n",
      "epoch: 45,  batch step: 115, loss: 4.649273872375488\n",
      "epoch: 45,  batch step: 116, loss: 11.629664421081543\n",
      "epoch: 45,  batch step: 117, loss: 19.41429328918457\n",
      "epoch: 45,  batch step: 118, loss: 21.126453399658203\n",
      "epoch: 45,  batch step: 119, loss: 3.3189189434051514\n",
      "epoch: 45,  batch step: 120, loss: 23.067466735839844\n",
      "epoch: 45,  batch step: 121, loss: 14.100364685058594\n",
      "epoch: 45,  batch step: 122, loss: 144.42893981933594\n",
      "epoch: 45,  batch step: 123, loss: 13.676498413085938\n",
      "epoch: 45,  batch step: 124, loss: 27.213977813720703\n",
      "epoch: 45,  batch step: 125, loss: 8.22371768951416\n",
      "epoch: 45,  batch step: 126, loss: 4.200758934020996\n",
      "epoch: 45,  batch step: 127, loss: 3.789724349975586\n",
      "epoch: 45,  batch step: 128, loss: 4.615275859832764\n",
      "epoch: 45,  batch step: 129, loss: 23.597213745117188\n",
      "epoch: 45,  batch step: 130, loss: 13.421229362487793\n",
      "epoch: 45,  batch step: 131, loss: 36.03395462036133\n",
      "epoch: 45,  batch step: 132, loss: 50.62006378173828\n",
      "epoch: 45,  batch step: 133, loss: 11.822677612304688\n",
      "epoch: 45,  batch step: 134, loss: 27.39435386657715\n",
      "epoch: 45,  batch step: 135, loss: 15.584086418151855\n",
      "epoch: 45,  batch step: 136, loss: 8.087484359741211\n",
      "epoch: 45,  batch step: 137, loss: 25.919361114501953\n",
      "epoch: 45,  batch step: 138, loss: 41.681373596191406\n",
      "epoch: 45,  batch step: 139, loss: 8.777165412902832\n",
      "epoch: 45,  batch step: 140, loss: 39.76396179199219\n",
      "epoch: 45,  batch step: 141, loss: 70.5604248046875\n",
      "epoch: 45,  batch step: 142, loss: 3.381415367126465\n",
      "epoch: 45,  batch step: 143, loss: 4.018474578857422\n",
      "epoch: 45,  batch step: 144, loss: 16.345169067382812\n",
      "epoch: 45,  batch step: 145, loss: 4.390572547912598\n",
      "epoch: 45,  batch step: 146, loss: 24.219593048095703\n",
      "epoch: 45,  batch step: 147, loss: 22.261127471923828\n",
      "epoch: 45,  batch step: 148, loss: 7.816535472869873\n",
      "epoch: 45,  batch step: 149, loss: 8.677589416503906\n",
      "epoch: 45,  batch step: 150, loss: 5.363299369812012\n",
      "epoch: 45,  batch step: 151, loss: 4.244675636291504\n",
      "epoch: 45,  batch step: 152, loss: 19.559707641601562\n",
      "epoch: 45,  batch step: 153, loss: 62.17481994628906\n",
      "epoch: 45,  batch step: 154, loss: 11.498213768005371\n",
      "epoch: 45,  batch step: 155, loss: 64.80451202392578\n",
      "epoch: 45,  batch step: 156, loss: 2.8521409034729004\n",
      "epoch: 45,  batch step: 157, loss: 8.974245071411133\n",
      "epoch: 45,  batch step: 158, loss: 29.22933578491211\n",
      "epoch: 45,  batch step: 159, loss: 69.64385223388672\n",
      "epoch: 45,  batch step: 160, loss: 6.331840515136719\n",
      "epoch: 45,  batch step: 161, loss: 11.200155258178711\n",
      "epoch: 45,  batch step: 162, loss: 28.650686264038086\n",
      "epoch: 45,  batch step: 163, loss: 4.642705917358398\n",
      "epoch: 45,  batch step: 164, loss: 10.618124008178711\n",
      "epoch: 45,  batch step: 165, loss: 5.591655254364014\n",
      "epoch: 45,  batch step: 166, loss: 16.509403228759766\n",
      "epoch: 45,  batch step: 167, loss: 3.677480697631836\n",
      "epoch: 45,  batch step: 168, loss: 5.869345664978027\n",
      "epoch: 45,  batch step: 169, loss: 9.981680870056152\n",
      "epoch: 45,  batch step: 170, loss: 6.774240016937256\n",
      "epoch: 45,  batch step: 171, loss: 19.31468963623047\n",
      "epoch: 45,  batch step: 172, loss: 82.38156127929688\n",
      "epoch: 45,  batch step: 173, loss: 8.219204902648926\n",
      "epoch: 45,  batch step: 174, loss: 34.74036407470703\n",
      "epoch: 45,  batch step: 175, loss: 4.276944637298584\n",
      "epoch: 45,  batch step: 176, loss: 4.858081817626953\n",
      "epoch: 45,  batch step: 177, loss: 30.72185516357422\n",
      "epoch: 45,  batch step: 178, loss: 41.35040283203125\n",
      "epoch: 45,  batch step: 179, loss: 4.5348663330078125\n",
      "epoch: 45,  batch step: 180, loss: 36.22835922241211\n",
      "epoch: 45,  batch step: 181, loss: 102.94136810302734\n",
      "epoch: 45,  batch step: 182, loss: 48.31920623779297\n",
      "epoch: 45,  batch step: 183, loss: 2.9868717193603516\n",
      "epoch: 45,  batch step: 184, loss: 24.040739059448242\n",
      "epoch: 45,  batch step: 185, loss: 6.951323509216309\n",
      "epoch: 45,  batch step: 186, loss: 4.238463401794434\n",
      "epoch: 45,  batch step: 187, loss: 3.7282333374023438\n",
      "epoch: 45,  batch step: 188, loss: 59.47545623779297\n",
      "epoch: 45,  batch step: 189, loss: 66.3917007446289\n",
      "epoch: 45,  batch step: 190, loss: 10.752235412597656\n",
      "epoch: 45,  batch step: 191, loss: 15.940722465515137\n",
      "epoch: 45,  batch step: 192, loss: 39.97857666015625\n",
      "epoch: 45,  batch step: 193, loss: 5.626574516296387\n",
      "epoch: 45,  batch step: 194, loss: 58.88103485107422\n",
      "epoch: 45,  batch step: 195, loss: 59.753700256347656\n",
      "epoch: 45,  batch step: 196, loss: 5.600183010101318\n",
      "epoch: 45,  batch step: 197, loss: 25.61573028564453\n",
      "epoch: 45,  batch step: 198, loss: 34.473052978515625\n",
      "epoch: 45,  batch step: 199, loss: 2.2748219966888428\n",
      "epoch: 45,  batch step: 200, loss: 20.140764236450195\n",
      "epoch: 45,  batch step: 201, loss: 44.71227264404297\n",
      "epoch: 45,  batch step: 202, loss: 56.91059875488281\n",
      "epoch: 45,  batch step: 203, loss: 3.6151492595672607\n",
      "epoch: 45,  batch step: 204, loss: 9.157766342163086\n",
      "epoch: 45,  batch step: 205, loss: 3.014148235321045\n",
      "epoch: 45,  batch step: 206, loss: 7.382607936859131\n",
      "epoch: 45,  batch step: 207, loss: 30.970779418945312\n",
      "epoch: 45,  batch step: 208, loss: 7.199686050415039\n",
      "epoch: 45,  batch step: 209, loss: 40.2311897277832\n",
      "epoch: 45,  batch step: 210, loss: 16.74018096923828\n",
      "epoch: 45,  batch step: 211, loss: 5.138358116149902\n",
      "epoch: 45,  batch step: 212, loss: 29.321453094482422\n",
      "epoch: 45,  batch step: 213, loss: 4.448528289794922\n",
      "epoch: 45,  batch step: 214, loss: 4.984157562255859\n",
      "epoch: 45,  batch step: 215, loss: 4.105957984924316\n",
      "epoch: 45,  batch step: 216, loss: 31.62136459350586\n",
      "epoch: 45,  batch step: 217, loss: 36.8779182434082\n",
      "epoch: 45,  batch step: 218, loss: 11.85711669921875\n",
      "epoch: 45,  batch step: 219, loss: 14.814437866210938\n",
      "epoch: 45,  batch step: 220, loss: 54.42070770263672\n",
      "epoch: 45,  batch step: 221, loss: 4.571170330047607\n",
      "epoch: 45,  batch step: 222, loss: 2.861955165863037\n",
      "epoch: 45,  batch step: 223, loss: 24.804868698120117\n",
      "epoch: 45,  batch step: 224, loss: 5.073647499084473\n",
      "epoch: 45,  batch step: 225, loss: 3.4276139736175537\n",
      "epoch: 45,  batch step: 226, loss: 5.663835525512695\n",
      "epoch: 45,  batch step: 227, loss: 22.65814971923828\n",
      "epoch: 45,  batch step: 228, loss: 7.515809535980225\n",
      "epoch: 45,  batch step: 229, loss: 26.893781661987305\n",
      "epoch: 45,  batch step: 230, loss: 69.61911010742188\n",
      "epoch: 45,  batch step: 231, loss: 4.4997453689575195\n",
      "epoch: 45,  batch step: 232, loss: 23.124786376953125\n",
      "epoch: 45,  batch step: 233, loss: 12.315145492553711\n",
      "epoch: 45,  batch step: 234, loss: 10.358748435974121\n",
      "epoch: 45,  batch step: 235, loss: 36.883975982666016\n",
      "epoch: 45,  batch step: 236, loss: 33.11951446533203\n",
      "epoch: 45,  batch step: 237, loss: 4.369716167449951\n",
      "epoch: 45,  batch step: 238, loss: 9.309982299804688\n",
      "epoch: 45,  batch step: 239, loss: 7.605186462402344\n",
      "epoch: 45,  batch step: 240, loss: 21.448917388916016\n",
      "epoch: 45,  batch step: 241, loss: 3.530621290206909\n",
      "epoch: 45,  batch step: 242, loss: 5.398695945739746\n",
      "epoch: 45,  batch step: 243, loss: 13.900580406188965\n",
      "epoch: 45,  batch step: 244, loss: 44.186126708984375\n",
      "epoch: 45,  batch step: 245, loss: 42.50322341918945\n",
      "epoch: 45,  batch step: 246, loss: 10.344900131225586\n",
      "epoch: 45,  batch step: 247, loss: 71.55399322509766\n",
      "epoch: 45,  batch step: 248, loss: 26.771474838256836\n",
      "epoch: 45,  batch step: 249, loss: 65.34436798095703\n",
      "epoch: 45,  batch step: 250, loss: 16.336139678955078\n",
      "epoch: 45,  batch step: 251, loss: 20.44413185119629\n",
      "validation error epoch  45:    tensor(73.3818, device='cuda:0')\n",
      "316\n",
      "epoch: 46,  batch step: 0, loss: 21.865333557128906\n",
      "epoch: 46,  batch step: 1, loss: 77.3198471069336\n",
      "epoch: 46,  batch step: 2, loss: 4.803043842315674\n",
      "epoch: 46,  batch step: 3, loss: 24.26717758178711\n",
      "epoch: 46,  batch step: 4, loss: 8.42477798461914\n",
      "epoch: 46,  batch step: 5, loss: 40.2361946105957\n",
      "epoch: 46,  batch step: 6, loss: 103.29823303222656\n",
      "epoch: 46,  batch step: 7, loss: 2.8157901763916016\n",
      "epoch: 46,  batch step: 8, loss: 23.389760971069336\n",
      "epoch: 46,  batch step: 9, loss: 30.24702262878418\n",
      "epoch: 46,  batch step: 10, loss: 7.4245500564575195\n",
      "epoch: 46,  batch step: 11, loss: 8.707740783691406\n",
      "epoch: 46,  batch step: 12, loss: 9.993313789367676\n",
      "epoch: 46,  batch step: 13, loss: 40.7046012878418\n",
      "epoch: 46,  batch step: 14, loss: 5.025503158569336\n",
      "epoch: 46,  batch step: 15, loss: 4.6172285079956055\n",
      "epoch: 46,  batch step: 16, loss: 69.42130279541016\n",
      "epoch: 46,  batch step: 17, loss: 3.9488513469696045\n",
      "epoch: 46,  batch step: 18, loss: 3.2805182933807373\n",
      "epoch: 46,  batch step: 19, loss: 8.087258338928223\n",
      "epoch: 46,  batch step: 20, loss: 5.398590087890625\n",
      "epoch: 46,  batch step: 21, loss: 149.31893920898438\n",
      "epoch: 46,  batch step: 22, loss: 123.73847961425781\n",
      "epoch: 46,  batch step: 23, loss: 33.56371307373047\n",
      "epoch: 46,  batch step: 24, loss: 9.912836074829102\n",
      "epoch: 46,  batch step: 25, loss: 114.26033782958984\n",
      "epoch: 46,  batch step: 26, loss: 4.419212341308594\n",
      "epoch: 46,  batch step: 27, loss: 15.648504257202148\n",
      "epoch: 46,  batch step: 28, loss: 34.24428176879883\n",
      "epoch: 46,  batch step: 29, loss: 26.090042114257812\n",
      "epoch: 46,  batch step: 30, loss: 24.930374145507812\n",
      "epoch: 46,  batch step: 31, loss: 7.648909568786621\n",
      "epoch: 46,  batch step: 32, loss: 38.18952941894531\n",
      "epoch: 46,  batch step: 33, loss: 53.22142791748047\n",
      "epoch: 46,  batch step: 34, loss: 68.9054183959961\n",
      "epoch: 46,  batch step: 35, loss: 6.023221969604492\n",
      "epoch: 46,  batch step: 36, loss: 6.35669469833374\n",
      "epoch: 46,  batch step: 37, loss: 46.321163177490234\n",
      "epoch: 46,  batch step: 38, loss: 61.99041748046875\n",
      "epoch: 46,  batch step: 39, loss: 36.83392333984375\n",
      "epoch: 46,  batch step: 40, loss: 20.77727699279785\n",
      "epoch: 46,  batch step: 41, loss: 2.878303050994873\n",
      "epoch: 46,  batch step: 42, loss: 50.61909103393555\n",
      "epoch: 46,  batch step: 43, loss: 17.308944702148438\n",
      "epoch: 46,  batch step: 44, loss: 52.58372497558594\n",
      "epoch: 46,  batch step: 45, loss: 2.7931602001190186\n",
      "epoch: 46,  batch step: 46, loss: 22.137401580810547\n",
      "epoch: 46,  batch step: 47, loss: 18.149946212768555\n",
      "epoch: 46,  batch step: 48, loss: 18.3919734954834\n",
      "epoch: 46,  batch step: 49, loss: 6.438604354858398\n",
      "epoch: 46,  batch step: 50, loss: 95.76296997070312\n",
      "epoch: 46,  batch step: 51, loss: 6.598092555999756\n",
      "epoch: 46,  batch step: 52, loss: 4.261765003204346\n",
      "epoch: 46,  batch step: 53, loss: 17.38890838623047\n",
      "epoch: 46,  batch step: 54, loss: 25.026569366455078\n",
      "epoch: 46,  batch step: 55, loss: 3.8991408348083496\n",
      "epoch: 46,  batch step: 56, loss: 97.62522888183594\n",
      "epoch: 46,  batch step: 57, loss: 22.18958854675293\n",
      "epoch: 46,  batch step: 58, loss: 4.088130950927734\n",
      "epoch: 46,  batch step: 59, loss: 58.80132293701172\n",
      "epoch: 46,  batch step: 60, loss: 36.30408477783203\n",
      "epoch: 46,  batch step: 61, loss: 42.348182678222656\n",
      "epoch: 46,  batch step: 62, loss: 54.10010528564453\n",
      "epoch: 46,  batch step: 63, loss: 4.519762992858887\n",
      "epoch: 46,  batch step: 64, loss: 30.51486587524414\n",
      "epoch: 46,  batch step: 65, loss: 27.621761322021484\n",
      "epoch: 46,  batch step: 66, loss: 22.425701141357422\n",
      "epoch: 46,  batch step: 67, loss: 13.740118980407715\n",
      "epoch: 46,  batch step: 68, loss: 6.498557090759277\n",
      "epoch: 46,  batch step: 69, loss: 5.11987829208374\n",
      "epoch: 46,  batch step: 70, loss: 4.702945709228516\n",
      "epoch: 46,  batch step: 71, loss: 50.34919357299805\n",
      "epoch: 46,  batch step: 72, loss: 20.929462432861328\n",
      "epoch: 46,  batch step: 73, loss: 4.658454418182373\n",
      "epoch: 46,  batch step: 74, loss: 5.359347343444824\n",
      "epoch: 46,  batch step: 75, loss: 34.28823471069336\n",
      "epoch: 46,  batch step: 76, loss: 18.203948974609375\n",
      "epoch: 46,  batch step: 77, loss: 40.181400299072266\n",
      "epoch: 46,  batch step: 78, loss: 30.044891357421875\n",
      "epoch: 46,  batch step: 79, loss: 34.45170593261719\n",
      "epoch: 46,  batch step: 80, loss: 4.512167453765869\n",
      "epoch: 46,  batch step: 81, loss: 6.96580696105957\n",
      "epoch: 46,  batch step: 82, loss: 50.30986022949219\n",
      "epoch: 46,  batch step: 83, loss: 4.648868083953857\n",
      "epoch: 46,  batch step: 84, loss: 29.119796752929688\n",
      "epoch: 46,  batch step: 85, loss: 31.79227066040039\n",
      "epoch: 46,  batch step: 86, loss: 13.635961532592773\n",
      "epoch: 46,  batch step: 87, loss: 27.631439208984375\n",
      "epoch: 46,  batch step: 88, loss: 10.459505081176758\n",
      "epoch: 46,  batch step: 89, loss: 4.272459030151367\n",
      "epoch: 46,  batch step: 90, loss: 3.9508140087127686\n",
      "epoch: 46,  batch step: 91, loss: 42.00383377075195\n",
      "epoch: 46,  batch step: 92, loss: 29.129390716552734\n",
      "epoch: 46,  batch step: 93, loss: 28.97313690185547\n",
      "epoch: 46,  batch step: 94, loss: 8.770641326904297\n",
      "epoch: 46,  batch step: 95, loss: 6.6687750816345215\n",
      "epoch: 46,  batch step: 96, loss: 32.285919189453125\n",
      "epoch: 46,  batch step: 97, loss: 68.41928100585938\n",
      "epoch: 46,  batch step: 98, loss: 67.36328125\n",
      "epoch: 46,  batch step: 99, loss: 29.97560691833496\n",
      "epoch: 46,  batch step: 100, loss: 4.750766277313232\n",
      "epoch: 46,  batch step: 101, loss: 43.64886474609375\n",
      "epoch: 46,  batch step: 102, loss: 42.977867126464844\n",
      "epoch: 46,  batch step: 103, loss: 6.609745025634766\n",
      "epoch: 46,  batch step: 104, loss: 6.26334285736084\n",
      "epoch: 46,  batch step: 105, loss: 3.432570219039917\n",
      "epoch: 46,  batch step: 106, loss: 15.1830472946167\n",
      "epoch: 46,  batch step: 107, loss: 4.732449531555176\n",
      "epoch: 46,  batch step: 108, loss: 48.83442306518555\n",
      "epoch: 46,  batch step: 109, loss: 9.934326171875\n",
      "epoch: 46,  batch step: 110, loss: 4.564465522766113\n",
      "epoch: 46,  batch step: 111, loss: 53.01456832885742\n",
      "epoch: 46,  batch step: 112, loss: 6.311046600341797\n",
      "epoch: 46,  batch step: 113, loss: 5.495696544647217\n",
      "epoch: 46,  batch step: 114, loss: 14.320241928100586\n",
      "epoch: 46,  batch step: 115, loss: 4.041022300720215\n",
      "epoch: 46,  batch step: 116, loss: 19.505550384521484\n",
      "epoch: 46,  batch step: 117, loss: 4.043392181396484\n",
      "epoch: 46,  batch step: 118, loss: 5.709865570068359\n",
      "epoch: 46,  batch step: 119, loss: 47.051048278808594\n",
      "epoch: 46,  batch step: 120, loss: 10.77363395690918\n",
      "epoch: 46,  batch step: 121, loss: 71.63154602050781\n",
      "epoch: 46,  batch step: 122, loss: 39.6182975769043\n",
      "epoch: 46,  batch step: 123, loss: 64.31947326660156\n",
      "epoch: 46,  batch step: 124, loss: 23.56100082397461\n",
      "epoch: 46,  batch step: 125, loss: 55.78630828857422\n",
      "epoch: 46,  batch step: 126, loss: 2.903459310531616\n",
      "epoch: 46,  batch step: 127, loss: 17.327123641967773\n",
      "epoch: 46,  batch step: 128, loss: 5.629239559173584\n",
      "epoch: 46,  batch step: 129, loss: 20.803258895874023\n",
      "epoch: 46,  batch step: 130, loss: 4.454216957092285\n",
      "epoch: 46,  batch step: 131, loss: 3.027616262435913\n",
      "epoch: 46,  batch step: 132, loss: 63.75823974609375\n",
      "epoch: 46,  batch step: 133, loss: 4.053788185119629\n",
      "epoch: 46,  batch step: 134, loss: 58.81050109863281\n",
      "epoch: 46,  batch step: 135, loss: 36.46345520019531\n",
      "epoch: 46,  batch step: 136, loss: 39.15290832519531\n",
      "epoch: 46,  batch step: 137, loss: 4.541638374328613\n",
      "epoch: 46,  batch step: 138, loss: 9.503654479980469\n",
      "epoch: 46,  batch step: 139, loss: 4.144808769226074\n",
      "epoch: 46,  batch step: 140, loss: 31.21673583984375\n",
      "epoch: 46,  batch step: 141, loss: 27.129547119140625\n",
      "epoch: 46,  batch step: 142, loss: 53.68267059326172\n",
      "epoch: 46,  batch step: 143, loss: 17.64872932434082\n",
      "epoch: 46,  batch step: 144, loss: 5.620025157928467\n",
      "epoch: 46,  batch step: 145, loss: 5.305377006530762\n",
      "epoch: 46,  batch step: 146, loss: 4.024477958679199\n",
      "epoch: 46,  batch step: 147, loss: 13.815424919128418\n",
      "epoch: 46,  batch step: 148, loss: 22.536319732666016\n",
      "epoch: 46,  batch step: 149, loss: 17.57956314086914\n",
      "epoch: 46,  batch step: 150, loss: 8.6143798828125\n",
      "epoch: 46,  batch step: 151, loss: 3.5969595909118652\n",
      "epoch: 46,  batch step: 152, loss: 5.580770492553711\n",
      "epoch: 46,  batch step: 153, loss: 4.311538219451904\n",
      "epoch: 46,  batch step: 154, loss: 17.486896514892578\n",
      "epoch: 46,  batch step: 155, loss: 3.1360855102539062\n",
      "epoch: 46,  batch step: 156, loss: 43.71807861328125\n",
      "epoch: 46,  batch step: 157, loss: 3.4619359970092773\n",
      "epoch: 46,  batch step: 158, loss: 8.71353530883789\n",
      "epoch: 46,  batch step: 159, loss: 4.017313003540039\n",
      "epoch: 46,  batch step: 160, loss: 4.493048667907715\n",
      "epoch: 46,  batch step: 161, loss: 19.72378158569336\n",
      "epoch: 46,  batch step: 162, loss: 9.187666893005371\n",
      "epoch: 46,  batch step: 163, loss: 58.445770263671875\n",
      "epoch: 46,  batch step: 164, loss: 3.254152297973633\n",
      "epoch: 46,  batch step: 165, loss: 26.360191345214844\n",
      "epoch: 46,  batch step: 166, loss: 27.334129333496094\n",
      "epoch: 46,  batch step: 167, loss: 11.418127059936523\n",
      "epoch: 46,  batch step: 168, loss: 4.064819812774658\n",
      "epoch: 46,  batch step: 169, loss: 2.9446399211883545\n",
      "epoch: 46,  batch step: 170, loss: 3.3311266899108887\n",
      "epoch: 46,  batch step: 171, loss: 39.104774475097656\n",
      "epoch: 46,  batch step: 172, loss: 5.846233367919922\n",
      "epoch: 46,  batch step: 173, loss: 18.519977569580078\n",
      "epoch: 46,  batch step: 174, loss: 9.444730758666992\n",
      "epoch: 46,  batch step: 175, loss: 44.955928802490234\n",
      "epoch: 46,  batch step: 176, loss: 29.045345306396484\n",
      "epoch: 46,  batch step: 177, loss: 33.91936492919922\n",
      "epoch: 46,  batch step: 178, loss: 5.966191291809082\n",
      "epoch: 46,  batch step: 179, loss: 5.9099578857421875\n",
      "epoch: 46,  batch step: 180, loss: 4.4584503173828125\n",
      "epoch: 46,  batch step: 181, loss: 13.010207176208496\n",
      "epoch: 46,  batch step: 182, loss: 5.822854995727539\n",
      "epoch: 46,  batch step: 183, loss: 5.351699352264404\n",
      "epoch: 46,  batch step: 184, loss: 4.963958263397217\n",
      "epoch: 46,  batch step: 185, loss: 2.696066379547119\n",
      "epoch: 46,  batch step: 186, loss: 7.637205123901367\n",
      "epoch: 46,  batch step: 187, loss: 29.83043670654297\n",
      "epoch: 46,  batch step: 188, loss: 3.222184181213379\n",
      "epoch: 46,  batch step: 189, loss: 21.092491149902344\n",
      "epoch: 46,  batch step: 190, loss: 45.565765380859375\n",
      "epoch: 46,  batch step: 191, loss: 6.453066825866699\n",
      "epoch: 46,  batch step: 192, loss: 7.333975791931152\n",
      "epoch: 46,  batch step: 193, loss: 37.22865295410156\n",
      "epoch: 46,  batch step: 194, loss: 5.212774276733398\n",
      "epoch: 46,  batch step: 195, loss: 24.515155792236328\n",
      "epoch: 46,  batch step: 196, loss: 23.492902755737305\n",
      "epoch: 46,  batch step: 197, loss: 4.6058030128479\n",
      "epoch: 46,  batch step: 198, loss: 6.7740478515625\n",
      "epoch: 46,  batch step: 199, loss: 7.329282283782959\n",
      "epoch: 46,  batch step: 200, loss: 6.833956718444824\n",
      "epoch: 46,  batch step: 201, loss: 50.68297576904297\n",
      "epoch: 46,  batch step: 202, loss: 75.72002410888672\n",
      "epoch: 46,  batch step: 203, loss: 36.991790771484375\n",
      "epoch: 46,  batch step: 204, loss: 16.561214447021484\n",
      "epoch: 46,  batch step: 205, loss: 33.09916305541992\n",
      "epoch: 46,  batch step: 206, loss: 31.838016510009766\n",
      "epoch: 46,  batch step: 207, loss: 3.5587127208709717\n",
      "epoch: 46,  batch step: 208, loss: 42.4375\n",
      "epoch: 46,  batch step: 209, loss: 41.10558319091797\n",
      "epoch: 46,  batch step: 210, loss: 4.177472114562988\n",
      "epoch: 46,  batch step: 211, loss: 75.57801055908203\n",
      "epoch: 46,  batch step: 212, loss: 21.159835815429688\n",
      "epoch: 46,  batch step: 213, loss: 12.360830307006836\n",
      "epoch: 46,  batch step: 214, loss: 6.1578688621521\n",
      "epoch: 46,  batch step: 215, loss: 42.518497467041016\n",
      "epoch: 46,  batch step: 216, loss: 11.733007431030273\n",
      "epoch: 46,  batch step: 217, loss: 5.136382579803467\n",
      "epoch: 46,  batch step: 218, loss: 8.94412899017334\n",
      "epoch: 46,  batch step: 219, loss: 4.012778282165527\n",
      "epoch: 46,  batch step: 220, loss: 134.68038940429688\n",
      "epoch: 46,  batch step: 221, loss: 22.871379852294922\n",
      "epoch: 46,  batch step: 222, loss: 25.663419723510742\n",
      "epoch: 46,  batch step: 223, loss: 23.936237335205078\n",
      "epoch: 46,  batch step: 224, loss: 50.4975700378418\n",
      "epoch: 46,  batch step: 225, loss: 2.455852746963501\n",
      "epoch: 46,  batch step: 226, loss: 29.354759216308594\n",
      "epoch: 46,  batch step: 227, loss: 15.018905639648438\n",
      "epoch: 46,  batch step: 228, loss: 20.973583221435547\n",
      "epoch: 46,  batch step: 229, loss: 7.475719451904297\n",
      "epoch: 46,  batch step: 230, loss: 10.563238143920898\n",
      "epoch: 46,  batch step: 231, loss: 3.307685613632202\n",
      "epoch: 46,  batch step: 232, loss: 18.413419723510742\n",
      "epoch: 46,  batch step: 233, loss: 35.137454986572266\n",
      "epoch: 46,  batch step: 234, loss: 7.449693202972412\n",
      "epoch: 46,  batch step: 235, loss: 16.178512573242188\n",
      "epoch: 46,  batch step: 236, loss: 5.247169494628906\n",
      "epoch: 46,  batch step: 237, loss: 3.9036684036254883\n",
      "epoch: 46,  batch step: 238, loss: 25.038063049316406\n",
      "epoch: 46,  batch step: 239, loss: 25.948705673217773\n",
      "epoch: 46,  batch step: 240, loss: 31.361717224121094\n",
      "epoch: 46,  batch step: 241, loss: 3.735813856124878\n",
      "epoch: 46,  batch step: 242, loss: 19.172462463378906\n",
      "epoch: 46,  batch step: 243, loss: 5.204292297363281\n",
      "epoch: 46,  batch step: 244, loss: 3.595979690551758\n",
      "epoch: 46,  batch step: 245, loss: 7.644813537597656\n",
      "epoch: 46,  batch step: 246, loss: 55.009700775146484\n",
      "epoch: 46,  batch step: 247, loss: 3.420680522918701\n",
      "epoch: 46,  batch step: 248, loss: 45.34382247924805\n",
      "epoch: 46,  batch step: 249, loss: 2.7922725677490234\n",
      "epoch: 46,  batch step: 250, loss: 2.775799512863159\n",
      "epoch: 46,  batch step: 251, loss: 108.4512939453125\n",
      "validation error epoch  46:    tensor(69.9431, device='cuda:0')\n",
      "316\n",
      "epoch: 47,  batch step: 0, loss: 20.54067611694336\n",
      "epoch: 47,  batch step: 1, loss: 9.466127395629883\n",
      "epoch: 47,  batch step: 2, loss: 8.090010643005371\n",
      "epoch: 47,  batch step: 3, loss: 31.589771270751953\n",
      "epoch: 47,  batch step: 4, loss: 5.401734828948975\n",
      "epoch: 47,  batch step: 5, loss: 49.460235595703125\n",
      "epoch: 47,  batch step: 6, loss: 6.518191337585449\n",
      "epoch: 47,  batch step: 7, loss: 97.94453430175781\n",
      "epoch: 47,  batch step: 8, loss: 19.641372680664062\n",
      "epoch: 47,  batch step: 9, loss: 17.184141159057617\n",
      "epoch: 47,  batch step: 10, loss: 4.893001079559326\n",
      "epoch: 47,  batch step: 11, loss: 84.34391784667969\n",
      "epoch: 47,  batch step: 12, loss: 5.663088798522949\n",
      "epoch: 47,  batch step: 13, loss: 5.303617477416992\n",
      "epoch: 47,  batch step: 14, loss: 5.922769069671631\n",
      "epoch: 47,  batch step: 15, loss: 19.796466827392578\n",
      "epoch: 47,  batch step: 16, loss: 38.00222396850586\n",
      "epoch: 47,  batch step: 17, loss: 6.0110087394714355\n",
      "epoch: 47,  batch step: 18, loss: 7.806266784667969\n",
      "epoch: 47,  batch step: 19, loss: 6.00715970993042\n",
      "epoch: 47,  batch step: 20, loss: 6.169753074645996\n",
      "epoch: 47,  batch step: 21, loss: 30.537700653076172\n",
      "epoch: 47,  batch step: 22, loss: 4.6281962394714355\n",
      "epoch: 47,  batch step: 23, loss: 35.30188751220703\n",
      "epoch: 47,  batch step: 24, loss: 53.84075164794922\n",
      "epoch: 47,  batch step: 25, loss: 4.393322467803955\n",
      "epoch: 47,  batch step: 26, loss: 8.03707504272461\n",
      "epoch: 47,  batch step: 27, loss: 76.25984191894531\n",
      "epoch: 47,  batch step: 28, loss: 23.858840942382812\n",
      "epoch: 47,  batch step: 29, loss: 29.4696044921875\n",
      "epoch: 47,  batch step: 30, loss: 41.38524627685547\n",
      "epoch: 47,  batch step: 31, loss: 5.09080696105957\n",
      "epoch: 47,  batch step: 32, loss: 5.970130443572998\n",
      "epoch: 47,  batch step: 33, loss: 17.6199893951416\n",
      "epoch: 47,  batch step: 34, loss: 3.907644271850586\n",
      "epoch: 47,  batch step: 35, loss: 14.402566909790039\n",
      "epoch: 47,  batch step: 36, loss: 5.127755641937256\n",
      "epoch: 47,  batch step: 37, loss: 10.936834335327148\n",
      "epoch: 47,  batch step: 38, loss: 18.86349868774414\n",
      "epoch: 47,  batch step: 39, loss: 21.251829147338867\n",
      "epoch: 47,  batch step: 40, loss: 11.610430717468262\n",
      "epoch: 47,  batch step: 41, loss: 9.311395645141602\n",
      "epoch: 47,  batch step: 42, loss: 9.254263877868652\n",
      "epoch: 47,  batch step: 43, loss: 4.76949405670166\n",
      "epoch: 47,  batch step: 44, loss: 19.147991180419922\n",
      "epoch: 47,  batch step: 45, loss: 22.33004379272461\n",
      "epoch: 47,  batch step: 46, loss: 3.747795581817627\n",
      "epoch: 47,  batch step: 47, loss: 3.4242992401123047\n",
      "epoch: 47,  batch step: 48, loss: 22.951589584350586\n",
      "epoch: 47,  batch step: 49, loss: 34.57728576660156\n",
      "epoch: 47,  batch step: 50, loss: 40.48655700683594\n",
      "epoch: 47,  batch step: 51, loss: 5.4952497482299805\n",
      "epoch: 47,  batch step: 52, loss: 16.935251235961914\n",
      "epoch: 47,  batch step: 53, loss: 32.62702941894531\n",
      "epoch: 47,  batch step: 54, loss: 10.35512924194336\n",
      "epoch: 47,  batch step: 55, loss: 4.775010108947754\n",
      "epoch: 47,  batch step: 56, loss: 2.7151660919189453\n",
      "epoch: 47,  batch step: 57, loss: 46.48687744140625\n",
      "epoch: 47,  batch step: 58, loss: 17.44930648803711\n",
      "epoch: 47,  batch step: 59, loss: 4.9225687980651855\n",
      "epoch: 47,  batch step: 60, loss: 6.939260482788086\n",
      "epoch: 47,  batch step: 61, loss: 42.90979766845703\n",
      "epoch: 47,  batch step: 62, loss: 30.113365173339844\n",
      "epoch: 47,  batch step: 63, loss: 9.76132869720459\n",
      "epoch: 47,  batch step: 64, loss: 4.331723690032959\n",
      "epoch: 47,  batch step: 65, loss: 34.3073844909668\n",
      "epoch: 47,  batch step: 66, loss: 32.74868392944336\n",
      "epoch: 47,  batch step: 67, loss: 26.951282501220703\n",
      "epoch: 47,  batch step: 68, loss: 35.395835876464844\n",
      "epoch: 47,  batch step: 69, loss: 5.932136058807373\n",
      "epoch: 47,  batch step: 70, loss: 92.59829711914062\n",
      "epoch: 47,  batch step: 71, loss: 20.259445190429688\n",
      "epoch: 47,  batch step: 72, loss: 3.130889892578125\n",
      "epoch: 47,  batch step: 73, loss: 38.313961029052734\n",
      "epoch: 47,  batch step: 74, loss: 91.48260498046875\n",
      "epoch: 47,  batch step: 75, loss: 8.898584365844727\n",
      "epoch: 47,  batch step: 76, loss: 44.797271728515625\n",
      "epoch: 47,  batch step: 77, loss: 43.8255729675293\n",
      "epoch: 47,  batch step: 78, loss: 6.215826034545898\n",
      "epoch: 47,  batch step: 79, loss: 17.055580139160156\n",
      "epoch: 47,  batch step: 80, loss: 3.8331308364868164\n",
      "epoch: 47,  batch step: 81, loss: 53.96412658691406\n",
      "epoch: 47,  batch step: 82, loss: 5.999031066894531\n",
      "epoch: 47,  batch step: 83, loss: 4.252433776855469\n",
      "epoch: 47,  batch step: 84, loss: 2.2697885036468506\n",
      "epoch: 47,  batch step: 85, loss: 141.3449249267578\n",
      "epoch: 47,  batch step: 86, loss: 3.875887870788574\n",
      "epoch: 47,  batch step: 87, loss: 5.108087062835693\n",
      "epoch: 47,  batch step: 88, loss: 55.58677291870117\n",
      "epoch: 47,  batch step: 89, loss: 18.727432250976562\n",
      "epoch: 47,  batch step: 90, loss: 5.557389736175537\n",
      "epoch: 47,  batch step: 91, loss: 4.470186233520508\n",
      "epoch: 47,  batch step: 92, loss: 5.737712860107422\n",
      "epoch: 47,  batch step: 93, loss: 20.476964950561523\n",
      "epoch: 47,  batch step: 94, loss: 26.366147994995117\n",
      "epoch: 47,  batch step: 95, loss: 24.299015045166016\n",
      "epoch: 47,  batch step: 96, loss: 35.08430862426758\n",
      "epoch: 47,  batch step: 97, loss: 4.212426662445068\n",
      "epoch: 47,  batch step: 98, loss: 4.516877174377441\n",
      "epoch: 47,  batch step: 99, loss: 26.106304168701172\n",
      "epoch: 47,  batch step: 100, loss: 26.854557037353516\n",
      "epoch: 47,  batch step: 101, loss: 21.674785614013672\n",
      "epoch: 47,  batch step: 102, loss: 16.443336486816406\n",
      "epoch: 47,  batch step: 103, loss: 9.944254875183105\n",
      "epoch: 47,  batch step: 104, loss: 60.539241790771484\n",
      "epoch: 47,  batch step: 105, loss: 5.900420665740967\n",
      "epoch: 47,  batch step: 106, loss: 69.08818054199219\n",
      "epoch: 47,  batch step: 107, loss: 30.738800048828125\n",
      "epoch: 47,  batch step: 108, loss: 3.9583678245544434\n",
      "epoch: 47,  batch step: 109, loss: 3.4473795890808105\n",
      "epoch: 47,  batch step: 110, loss: 31.24758529663086\n",
      "epoch: 47,  batch step: 111, loss: 6.667134761810303\n",
      "epoch: 47,  batch step: 112, loss: 4.1357808113098145\n",
      "epoch: 47,  batch step: 113, loss: 5.049487113952637\n",
      "epoch: 47,  batch step: 114, loss: 4.202728271484375\n",
      "epoch: 47,  batch step: 115, loss: 23.77461814880371\n",
      "epoch: 47,  batch step: 116, loss: 24.03042221069336\n",
      "epoch: 47,  batch step: 117, loss: 53.958251953125\n",
      "epoch: 47,  batch step: 118, loss: 34.97248840332031\n",
      "epoch: 47,  batch step: 119, loss: 2.9613423347473145\n",
      "epoch: 47,  batch step: 120, loss: 4.222575664520264\n",
      "epoch: 47,  batch step: 121, loss: 72.82779693603516\n",
      "epoch: 47,  batch step: 122, loss: 5.548463344573975\n",
      "epoch: 47,  batch step: 123, loss: 5.465829372406006\n",
      "epoch: 47,  batch step: 124, loss: 3.1854255199432373\n",
      "epoch: 47,  batch step: 125, loss: 8.111434936523438\n",
      "epoch: 47,  batch step: 126, loss: 7.778552055358887\n",
      "epoch: 47,  batch step: 127, loss: 3.798046827316284\n",
      "epoch: 47,  batch step: 128, loss: 4.908490180969238\n",
      "epoch: 47,  batch step: 129, loss: 7.837558269500732\n",
      "epoch: 47,  batch step: 130, loss: 3.9287376403808594\n",
      "epoch: 47,  batch step: 131, loss: 5.593218803405762\n",
      "epoch: 47,  batch step: 132, loss: 20.570621490478516\n",
      "epoch: 47,  batch step: 133, loss: 3.973433494567871\n",
      "epoch: 47,  batch step: 134, loss: 7.119693756103516\n",
      "epoch: 47,  batch step: 135, loss: 5.46872615814209\n",
      "epoch: 47,  batch step: 136, loss: 61.081939697265625\n",
      "epoch: 47,  batch step: 137, loss: 10.563738822937012\n",
      "epoch: 47,  batch step: 138, loss: 92.20378112792969\n",
      "epoch: 47,  batch step: 139, loss: 53.55291748046875\n",
      "epoch: 47,  batch step: 140, loss: 5.330939292907715\n",
      "epoch: 47,  batch step: 141, loss: 20.32790756225586\n",
      "epoch: 47,  batch step: 142, loss: 32.22869110107422\n",
      "epoch: 47,  batch step: 143, loss: 53.393516540527344\n",
      "epoch: 47,  batch step: 144, loss: 35.05447006225586\n",
      "epoch: 47,  batch step: 145, loss: 5.494513988494873\n",
      "epoch: 47,  batch step: 146, loss: 5.631328582763672\n",
      "epoch: 47,  batch step: 147, loss: 4.23289680480957\n",
      "epoch: 47,  batch step: 148, loss: 51.44032287597656\n",
      "epoch: 47,  batch step: 149, loss: 9.589290618896484\n",
      "epoch: 47,  batch step: 150, loss: 108.39332580566406\n",
      "epoch: 47,  batch step: 151, loss: 4.837299346923828\n",
      "epoch: 47,  batch step: 152, loss: 53.95881271362305\n",
      "epoch: 47,  batch step: 153, loss: 4.19760799407959\n",
      "epoch: 47,  batch step: 154, loss: 47.2559814453125\n",
      "epoch: 47,  batch step: 155, loss: 5.674511909484863\n",
      "epoch: 47,  batch step: 156, loss: 6.802436828613281\n",
      "epoch: 47,  batch step: 157, loss: 10.010293960571289\n",
      "epoch: 47,  batch step: 158, loss: 6.350198745727539\n",
      "epoch: 47,  batch step: 159, loss: 35.000614166259766\n",
      "epoch: 47,  batch step: 160, loss: 25.728307723999023\n",
      "epoch: 47,  batch step: 161, loss: 30.305259704589844\n",
      "epoch: 47,  batch step: 162, loss: 6.096977233886719\n",
      "epoch: 47,  batch step: 163, loss: 9.761344909667969\n",
      "epoch: 47,  batch step: 164, loss: 3.127048969268799\n",
      "epoch: 47,  batch step: 165, loss: 17.632352828979492\n",
      "epoch: 47,  batch step: 166, loss: 3.394955635070801\n",
      "epoch: 47,  batch step: 167, loss: 47.404273986816406\n",
      "epoch: 47,  batch step: 168, loss: 2.320636749267578\n",
      "epoch: 47,  batch step: 169, loss: 4.360968112945557\n",
      "epoch: 47,  batch step: 170, loss: 28.597965240478516\n",
      "epoch: 47,  batch step: 171, loss: 6.296048164367676\n",
      "epoch: 47,  batch step: 172, loss: 4.2859296798706055\n",
      "epoch: 47,  batch step: 173, loss: 8.09962272644043\n",
      "epoch: 47,  batch step: 174, loss: 4.69837760925293\n",
      "epoch: 47,  batch step: 175, loss: 2.7189931869506836\n",
      "epoch: 47,  batch step: 176, loss: 10.260708808898926\n",
      "epoch: 47,  batch step: 177, loss: 24.436506271362305\n",
      "epoch: 47,  batch step: 178, loss: 3.645505666732788\n",
      "epoch: 47,  batch step: 179, loss: 3.9326083660125732\n",
      "epoch: 47,  batch step: 180, loss: 11.163420677185059\n",
      "epoch: 47,  batch step: 181, loss: 6.6610612869262695\n",
      "epoch: 47,  batch step: 182, loss: 49.42253875732422\n",
      "epoch: 47,  batch step: 183, loss: 62.69575119018555\n",
      "epoch: 47,  batch step: 184, loss: 18.007686614990234\n",
      "epoch: 47,  batch step: 185, loss: 6.127341270446777\n",
      "epoch: 47,  batch step: 186, loss: 47.29081344604492\n",
      "epoch: 47,  batch step: 187, loss: 22.554059982299805\n",
      "epoch: 47,  batch step: 188, loss: 57.48679733276367\n",
      "epoch: 47,  batch step: 189, loss: 5.472492218017578\n",
      "epoch: 47,  batch step: 190, loss: 16.506895065307617\n",
      "epoch: 47,  batch step: 191, loss: 41.92194366455078\n",
      "epoch: 47,  batch step: 192, loss: 10.15719223022461\n",
      "epoch: 47,  batch step: 193, loss: 17.234201431274414\n",
      "epoch: 47,  batch step: 194, loss: 24.942134857177734\n",
      "epoch: 47,  batch step: 195, loss: 17.390026092529297\n",
      "epoch: 47,  batch step: 196, loss: 11.252551078796387\n",
      "epoch: 47,  batch step: 197, loss: 9.970053672790527\n",
      "epoch: 47,  batch step: 198, loss: 6.1619696617126465\n",
      "epoch: 47,  batch step: 199, loss: 15.424957275390625\n",
      "epoch: 47,  batch step: 200, loss: 5.240426540374756\n",
      "epoch: 47,  batch step: 201, loss: 27.621320724487305\n",
      "epoch: 47,  batch step: 202, loss: 3.5565643310546875\n",
      "epoch: 47,  batch step: 203, loss: 4.33258056640625\n",
      "epoch: 47,  batch step: 204, loss: 17.034542083740234\n",
      "epoch: 47,  batch step: 205, loss: 63.56393051147461\n",
      "epoch: 47,  batch step: 206, loss: 69.98563385009766\n",
      "epoch: 47,  batch step: 207, loss: 30.856563568115234\n",
      "epoch: 47,  batch step: 208, loss: 2.69925594329834\n",
      "epoch: 47,  batch step: 209, loss: 70.79655456542969\n",
      "epoch: 47,  batch step: 210, loss: 5.4170122146606445\n",
      "epoch: 47,  batch step: 211, loss: 5.052131652832031\n",
      "epoch: 47,  batch step: 212, loss: 6.528390407562256\n",
      "epoch: 47,  batch step: 213, loss: 43.860145568847656\n",
      "epoch: 47,  batch step: 214, loss: 6.510242462158203\n",
      "epoch: 47,  batch step: 215, loss: 36.00640106201172\n",
      "epoch: 47,  batch step: 216, loss: 20.644948959350586\n",
      "epoch: 47,  batch step: 217, loss: 20.88279914855957\n",
      "epoch: 47,  batch step: 218, loss: 6.569211483001709\n",
      "epoch: 47,  batch step: 219, loss: 32.22798156738281\n",
      "epoch: 47,  batch step: 220, loss: 6.513611793518066\n",
      "epoch: 47,  batch step: 221, loss: 4.823470115661621\n",
      "epoch: 47,  batch step: 222, loss: 2.7998156547546387\n",
      "epoch: 47,  batch step: 223, loss: 2.970269203186035\n",
      "epoch: 47,  batch step: 224, loss: 70.02169799804688\n",
      "epoch: 47,  batch step: 225, loss: 12.332901000976562\n",
      "epoch: 47,  batch step: 226, loss: 5.091214656829834\n",
      "epoch: 47,  batch step: 227, loss: 16.65431785583496\n",
      "epoch: 47,  batch step: 228, loss: 51.560447692871094\n",
      "epoch: 47,  batch step: 229, loss: 33.66883850097656\n",
      "epoch: 47,  batch step: 230, loss: 5.530997276306152\n",
      "epoch: 47,  batch step: 231, loss: 54.45398712158203\n",
      "epoch: 47,  batch step: 232, loss: 43.23712158203125\n",
      "epoch: 47,  batch step: 233, loss: 3.8264870643615723\n",
      "epoch: 47,  batch step: 234, loss: 3.100095510482788\n",
      "epoch: 47,  batch step: 235, loss: 39.803977966308594\n",
      "epoch: 47,  batch step: 236, loss: 4.743908882141113\n",
      "epoch: 47,  batch step: 237, loss: 36.72478485107422\n",
      "epoch: 47,  batch step: 238, loss: 58.25358200073242\n",
      "epoch: 47,  batch step: 239, loss: 6.372955322265625\n",
      "epoch: 47,  batch step: 240, loss: 2.8895623683929443\n",
      "epoch: 47,  batch step: 241, loss: 4.780534267425537\n",
      "epoch: 47,  batch step: 242, loss: 29.60860824584961\n",
      "epoch: 47,  batch step: 243, loss: 12.238306045532227\n",
      "epoch: 47,  batch step: 244, loss: 64.79304504394531\n",
      "epoch: 47,  batch step: 245, loss: 3.92319393157959\n",
      "epoch: 47,  batch step: 246, loss: 28.17657470703125\n",
      "epoch: 47,  batch step: 247, loss: 19.822492599487305\n",
      "epoch: 47,  batch step: 248, loss: 3.627983808517456\n",
      "epoch: 47,  batch step: 249, loss: 28.73231315612793\n",
      "epoch: 47,  batch step: 250, loss: 3.647397756576538\n",
      "epoch: 47,  batch step: 251, loss: 52.49724197387695\n",
      "validation error epoch  47:    tensor(76.4600, device='cuda:0')\n",
      "316\n",
      "epoch: 48,  batch step: 0, loss: 30.464475631713867\n",
      "epoch: 48,  batch step: 1, loss: 19.448917388916016\n",
      "epoch: 48,  batch step: 2, loss: 7.315099239349365\n",
      "epoch: 48,  batch step: 3, loss: 6.126961708068848\n",
      "epoch: 48,  batch step: 4, loss: 72.10903930664062\n",
      "epoch: 48,  batch step: 5, loss: 4.132604598999023\n",
      "epoch: 48,  batch step: 6, loss: 26.65789794921875\n",
      "epoch: 48,  batch step: 7, loss: 32.10679626464844\n",
      "epoch: 48,  batch step: 8, loss: 48.550758361816406\n",
      "epoch: 48,  batch step: 9, loss: 5.027458190917969\n",
      "epoch: 48,  batch step: 10, loss: 50.66017150878906\n",
      "epoch: 48,  batch step: 11, loss: 17.47344207763672\n",
      "epoch: 48,  batch step: 12, loss: 8.20851993560791\n",
      "epoch: 48,  batch step: 13, loss: 3.66626238822937\n",
      "epoch: 48,  batch step: 14, loss: 5.950092315673828\n",
      "epoch: 48,  batch step: 15, loss: 25.72541046142578\n",
      "epoch: 48,  batch step: 16, loss: 19.24889373779297\n",
      "epoch: 48,  batch step: 17, loss: 6.365965843200684\n",
      "epoch: 48,  batch step: 18, loss: 18.6758975982666\n",
      "epoch: 48,  batch step: 19, loss: 8.497873306274414\n",
      "epoch: 48,  batch step: 20, loss: 17.65090560913086\n",
      "epoch: 48,  batch step: 21, loss: 3.5298399925231934\n",
      "epoch: 48,  batch step: 22, loss: 7.007894039154053\n",
      "epoch: 48,  batch step: 23, loss: 38.19197082519531\n",
      "epoch: 48,  batch step: 24, loss: 3.0934667587280273\n",
      "epoch: 48,  batch step: 25, loss: 28.449737548828125\n",
      "epoch: 48,  batch step: 26, loss: 3.524623394012451\n",
      "epoch: 48,  batch step: 27, loss: 3.856874465942383\n",
      "epoch: 48,  batch step: 28, loss: 23.527671813964844\n",
      "epoch: 48,  batch step: 29, loss: 153.4306640625\n",
      "epoch: 48,  batch step: 30, loss: 6.637147426605225\n",
      "epoch: 48,  batch step: 31, loss: 21.550304412841797\n",
      "epoch: 48,  batch step: 32, loss: 18.823806762695312\n",
      "epoch: 48,  batch step: 33, loss: 4.912228107452393\n",
      "epoch: 48,  batch step: 34, loss: 13.73149299621582\n",
      "epoch: 48,  batch step: 35, loss: 6.4148101806640625\n",
      "epoch: 48,  batch step: 36, loss: 18.09978675842285\n",
      "epoch: 48,  batch step: 37, loss: 36.487037658691406\n",
      "epoch: 48,  batch step: 38, loss: 102.71361541748047\n",
      "epoch: 48,  batch step: 39, loss: 20.545921325683594\n",
      "epoch: 48,  batch step: 40, loss: 5.115670204162598\n",
      "epoch: 48,  batch step: 41, loss: 8.233800888061523\n",
      "epoch: 48,  batch step: 42, loss: 6.191701412200928\n",
      "epoch: 48,  batch step: 43, loss: 6.57490873336792\n",
      "epoch: 48,  batch step: 44, loss: 89.5457763671875\n",
      "epoch: 48,  batch step: 45, loss: 18.856412887573242\n",
      "epoch: 48,  batch step: 46, loss: 8.237102508544922\n",
      "epoch: 48,  batch step: 47, loss: 95.2784423828125\n",
      "epoch: 48,  batch step: 48, loss: 24.19698715209961\n",
      "epoch: 48,  batch step: 49, loss: 105.87962341308594\n",
      "epoch: 48,  batch step: 50, loss: 27.545455932617188\n",
      "epoch: 48,  batch step: 51, loss: 15.517718315124512\n",
      "epoch: 48,  batch step: 52, loss: 4.430819511413574\n",
      "epoch: 48,  batch step: 53, loss: 4.142533779144287\n",
      "epoch: 48,  batch step: 54, loss: 10.049189567565918\n",
      "epoch: 48,  batch step: 55, loss: 6.321074962615967\n",
      "epoch: 48,  batch step: 56, loss: 32.30323028564453\n",
      "epoch: 48,  batch step: 57, loss: 5.642037868499756\n",
      "epoch: 48,  batch step: 58, loss: 51.5865592956543\n",
      "epoch: 48,  batch step: 59, loss: 131.42807006835938\n",
      "epoch: 48,  batch step: 60, loss: 36.323341369628906\n",
      "epoch: 48,  batch step: 61, loss: 39.8257942199707\n",
      "epoch: 48,  batch step: 62, loss: 10.35007095336914\n",
      "epoch: 48,  batch step: 63, loss: 20.253890991210938\n",
      "epoch: 48,  batch step: 64, loss: 46.681236267089844\n",
      "epoch: 48,  batch step: 65, loss: 8.047053337097168\n",
      "epoch: 48,  batch step: 66, loss: 22.90506362915039\n",
      "epoch: 48,  batch step: 67, loss: 6.747470378875732\n",
      "epoch: 48,  batch step: 68, loss: 23.72179412841797\n",
      "epoch: 48,  batch step: 69, loss: 29.677024841308594\n",
      "epoch: 48,  batch step: 70, loss: 6.269212245941162\n",
      "epoch: 48,  batch step: 71, loss: 37.60862350463867\n",
      "epoch: 48,  batch step: 72, loss: 76.89129638671875\n",
      "epoch: 48,  batch step: 73, loss: 22.927791595458984\n",
      "epoch: 48,  batch step: 74, loss: 29.058034896850586\n",
      "epoch: 48,  batch step: 75, loss: 17.646846771240234\n",
      "epoch: 48,  batch step: 76, loss: 4.954068183898926\n",
      "epoch: 48,  batch step: 77, loss: 13.604033470153809\n",
      "epoch: 48,  batch step: 78, loss: 8.088774681091309\n",
      "epoch: 48,  batch step: 79, loss: 4.278072357177734\n",
      "epoch: 48,  batch step: 80, loss: 45.80077362060547\n",
      "epoch: 48,  batch step: 81, loss: 22.555339813232422\n",
      "epoch: 48,  batch step: 82, loss: 22.898178100585938\n",
      "epoch: 48,  batch step: 83, loss: 12.01783275604248\n",
      "epoch: 48,  batch step: 84, loss: 6.462431907653809\n",
      "epoch: 48,  batch step: 85, loss: 8.18935489654541\n",
      "epoch: 48,  batch step: 86, loss: 10.417892456054688\n",
      "epoch: 48,  batch step: 87, loss: 20.98346710205078\n",
      "epoch: 48,  batch step: 88, loss: 9.493619918823242\n",
      "epoch: 48,  batch step: 89, loss: 29.397415161132812\n",
      "epoch: 48,  batch step: 90, loss: 3.6486778259277344\n",
      "epoch: 48,  batch step: 91, loss: 11.513805389404297\n",
      "epoch: 48,  batch step: 92, loss: 7.815080642700195\n",
      "epoch: 48,  batch step: 93, loss: 29.60940933227539\n",
      "epoch: 48,  batch step: 94, loss: 41.40855407714844\n",
      "epoch: 48,  batch step: 95, loss: 14.071266174316406\n",
      "epoch: 48,  batch step: 96, loss: 5.512104034423828\n",
      "epoch: 48,  batch step: 97, loss: 6.035977363586426\n",
      "epoch: 48,  batch step: 98, loss: 72.2296142578125\n",
      "epoch: 48,  batch step: 99, loss: 5.485744476318359\n",
      "epoch: 48,  batch step: 100, loss: 34.54521560668945\n",
      "epoch: 48,  batch step: 101, loss: 3.820946455001831\n",
      "epoch: 48,  batch step: 102, loss: 7.522146224975586\n",
      "epoch: 48,  batch step: 103, loss: 43.300254821777344\n",
      "epoch: 48,  batch step: 104, loss: 6.115328788757324\n",
      "epoch: 48,  batch step: 105, loss: 26.747955322265625\n",
      "epoch: 48,  batch step: 106, loss: 15.267488479614258\n",
      "epoch: 48,  batch step: 107, loss: 13.563461303710938\n",
      "epoch: 48,  batch step: 108, loss: 50.094032287597656\n",
      "epoch: 48,  batch step: 109, loss: 16.92760467529297\n",
      "epoch: 48,  batch step: 110, loss: 2.8176238536834717\n",
      "epoch: 48,  batch step: 111, loss: 4.850072860717773\n",
      "epoch: 48,  batch step: 112, loss: 7.661300182342529\n",
      "epoch: 48,  batch step: 113, loss: 32.08974075317383\n",
      "epoch: 48,  batch step: 114, loss: 158.68475341796875\n",
      "epoch: 48,  batch step: 115, loss: 85.91036224365234\n",
      "epoch: 48,  batch step: 116, loss: 34.63050842285156\n",
      "epoch: 48,  batch step: 117, loss: 4.890009880065918\n",
      "epoch: 48,  batch step: 118, loss: 36.030757904052734\n",
      "epoch: 48,  batch step: 119, loss: 4.048713684082031\n",
      "epoch: 48,  batch step: 120, loss: 27.266643524169922\n",
      "epoch: 48,  batch step: 121, loss: 27.16725730895996\n",
      "epoch: 48,  batch step: 122, loss: 6.828965663909912\n",
      "epoch: 48,  batch step: 123, loss: 9.93678092956543\n",
      "epoch: 48,  batch step: 124, loss: 4.693100452423096\n",
      "epoch: 48,  batch step: 125, loss: 50.438087463378906\n",
      "epoch: 48,  batch step: 126, loss: 87.43297576904297\n",
      "epoch: 48,  batch step: 127, loss: 11.261756896972656\n",
      "epoch: 48,  batch step: 128, loss: 6.381659984588623\n",
      "epoch: 48,  batch step: 129, loss: 6.614856719970703\n",
      "epoch: 48,  batch step: 130, loss: 4.059871196746826\n",
      "epoch: 48,  batch step: 131, loss: 40.41473388671875\n",
      "epoch: 48,  batch step: 132, loss: 30.97037124633789\n",
      "epoch: 48,  batch step: 133, loss: 4.906778335571289\n",
      "epoch: 48,  batch step: 134, loss: 63.1978874206543\n",
      "epoch: 48,  batch step: 135, loss: 5.447322845458984\n",
      "epoch: 48,  batch step: 136, loss: 5.367452144622803\n",
      "epoch: 48,  batch step: 137, loss: 79.09046936035156\n",
      "epoch: 48,  batch step: 138, loss: 7.937312126159668\n",
      "epoch: 48,  batch step: 139, loss: 32.22035217285156\n",
      "epoch: 48,  batch step: 140, loss: 3.4600658416748047\n",
      "epoch: 48,  batch step: 141, loss: 2.307281494140625\n",
      "epoch: 48,  batch step: 142, loss: 16.01697540283203\n",
      "epoch: 48,  batch step: 143, loss: 25.855213165283203\n",
      "epoch: 48,  batch step: 144, loss: 13.547691345214844\n",
      "epoch: 48,  batch step: 145, loss: 10.994028091430664\n",
      "epoch: 48,  batch step: 146, loss: 5.041416168212891\n",
      "epoch: 48,  batch step: 147, loss: 9.200752258300781\n",
      "epoch: 48,  batch step: 148, loss: 9.06884765625\n",
      "epoch: 48,  batch step: 149, loss: 33.27265930175781\n",
      "epoch: 48,  batch step: 150, loss: 4.1381731033325195\n",
      "epoch: 48,  batch step: 151, loss: 119.38758850097656\n",
      "epoch: 48,  batch step: 152, loss: 6.852914333343506\n",
      "epoch: 48,  batch step: 153, loss: 5.489293575286865\n",
      "epoch: 48,  batch step: 154, loss: 3.9246909618377686\n",
      "epoch: 48,  batch step: 155, loss: 43.15316390991211\n",
      "epoch: 48,  batch step: 156, loss: 28.613018035888672\n",
      "epoch: 48,  batch step: 157, loss: 37.3545036315918\n",
      "epoch: 48,  batch step: 158, loss: 3.336024045944214\n",
      "epoch: 48,  batch step: 159, loss: 30.795202255249023\n",
      "epoch: 48,  batch step: 160, loss: 3.6663999557495117\n",
      "epoch: 48,  batch step: 161, loss: 3.9406936168670654\n",
      "epoch: 48,  batch step: 162, loss: 26.870925903320312\n",
      "epoch: 48,  batch step: 163, loss: 9.94703483581543\n",
      "epoch: 48,  batch step: 164, loss: 7.894710540771484\n",
      "epoch: 48,  batch step: 165, loss: 25.11000633239746\n",
      "epoch: 48,  batch step: 166, loss: 19.026208877563477\n",
      "epoch: 48,  batch step: 167, loss: 6.661081314086914\n",
      "epoch: 48,  batch step: 168, loss: 3.0305240154266357\n",
      "epoch: 48,  batch step: 169, loss: 5.179224491119385\n",
      "epoch: 48,  batch step: 170, loss: 8.82139778137207\n",
      "epoch: 48,  batch step: 171, loss: 29.592138290405273\n",
      "epoch: 48,  batch step: 172, loss: 4.477731227874756\n",
      "epoch: 48,  batch step: 173, loss: 3.7450456619262695\n",
      "epoch: 48,  batch step: 174, loss: 5.188125133514404\n",
      "epoch: 48,  batch step: 175, loss: 80.22813415527344\n",
      "epoch: 48,  batch step: 176, loss: 5.601357460021973\n",
      "epoch: 48,  batch step: 177, loss: 4.26485013961792\n",
      "epoch: 48,  batch step: 178, loss: 5.8151044845581055\n",
      "epoch: 48,  batch step: 179, loss: 17.75379753112793\n",
      "epoch: 48,  batch step: 180, loss: 13.83529281616211\n",
      "epoch: 48,  batch step: 181, loss: 77.84442138671875\n",
      "epoch: 48,  batch step: 182, loss: 56.9121208190918\n",
      "epoch: 48,  batch step: 183, loss: 9.852569580078125\n",
      "epoch: 48,  batch step: 184, loss: 21.469493865966797\n",
      "epoch: 48,  batch step: 185, loss: 14.661243438720703\n",
      "epoch: 48,  batch step: 186, loss: 11.546416282653809\n",
      "epoch: 48,  batch step: 187, loss: 19.638690948486328\n",
      "epoch: 48,  batch step: 188, loss: 16.681541442871094\n",
      "epoch: 48,  batch step: 189, loss: 5.036938667297363\n",
      "epoch: 48,  batch step: 190, loss: 4.018096923828125\n",
      "epoch: 48,  batch step: 191, loss: 6.9725751876831055\n",
      "epoch: 48,  batch step: 192, loss: 12.30637264251709\n",
      "epoch: 48,  batch step: 193, loss: 34.185916900634766\n",
      "epoch: 48,  batch step: 194, loss: 2.9644381999969482\n",
      "epoch: 48,  batch step: 195, loss: 22.89020347595215\n",
      "epoch: 48,  batch step: 196, loss: 38.43459701538086\n",
      "epoch: 48,  batch step: 197, loss: 3.4473705291748047\n",
      "epoch: 48,  batch step: 198, loss: 44.29830551147461\n",
      "epoch: 48,  batch step: 199, loss: 6.341341972351074\n",
      "epoch: 48,  batch step: 200, loss: 8.222043991088867\n",
      "epoch: 48,  batch step: 201, loss: 16.749691009521484\n",
      "epoch: 48,  batch step: 202, loss: 17.74641227722168\n",
      "epoch: 48,  batch step: 203, loss: 3.696537733078003\n",
      "epoch: 48,  batch step: 204, loss: 51.436607360839844\n",
      "epoch: 48,  batch step: 205, loss: 19.515121459960938\n",
      "epoch: 48,  batch step: 206, loss: 4.891798496246338\n",
      "epoch: 48,  batch step: 207, loss: 4.603059768676758\n",
      "epoch: 48,  batch step: 208, loss: 24.831037521362305\n",
      "epoch: 48,  batch step: 209, loss: 17.159229278564453\n",
      "epoch: 48,  batch step: 210, loss: 4.429995059967041\n",
      "epoch: 48,  batch step: 211, loss: 7.523885726928711\n",
      "epoch: 48,  batch step: 212, loss: 74.05281829833984\n",
      "epoch: 48,  batch step: 213, loss: 26.217548370361328\n",
      "epoch: 48,  batch step: 214, loss: 58.80535888671875\n",
      "epoch: 48,  batch step: 215, loss: 4.837233543395996\n",
      "epoch: 48,  batch step: 216, loss: 11.733175277709961\n",
      "epoch: 48,  batch step: 217, loss: 36.55311584472656\n",
      "epoch: 48,  batch step: 218, loss: 10.174516677856445\n",
      "epoch: 48,  batch step: 219, loss: 21.125816345214844\n",
      "epoch: 48,  batch step: 220, loss: 9.566946983337402\n",
      "epoch: 48,  batch step: 221, loss: 23.041784286499023\n",
      "epoch: 48,  batch step: 222, loss: 23.932231903076172\n",
      "epoch: 48,  batch step: 223, loss: 2.9717776775360107\n",
      "epoch: 48,  batch step: 224, loss: 3.447350263595581\n",
      "epoch: 48,  batch step: 225, loss: 5.197690963745117\n",
      "epoch: 48,  batch step: 226, loss: 4.346208095550537\n",
      "epoch: 48,  batch step: 227, loss: 71.52674865722656\n",
      "epoch: 48,  batch step: 228, loss: 36.89167404174805\n",
      "epoch: 48,  batch step: 229, loss: 45.034324645996094\n",
      "epoch: 48,  batch step: 230, loss: 23.213294982910156\n",
      "epoch: 48,  batch step: 231, loss: 17.57515525817871\n",
      "epoch: 48,  batch step: 232, loss: 53.55389404296875\n",
      "epoch: 48,  batch step: 233, loss: 2.522264242172241\n",
      "epoch: 48,  batch step: 234, loss: 17.48672866821289\n",
      "epoch: 48,  batch step: 235, loss: 50.315895080566406\n",
      "epoch: 48,  batch step: 236, loss: 6.0532050132751465\n",
      "epoch: 48,  batch step: 237, loss: 2.86877703666687\n",
      "epoch: 48,  batch step: 238, loss: 44.597694396972656\n",
      "epoch: 48,  batch step: 239, loss: 6.866908073425293\n",
      "epoch: 48,  batch step: 240, loss: 7.13971471786499\n",
      "epoch: 48,  batch step: 241, loss: 2.4567770957946777\n",
      "epoch: 48,  batch step: 242, loss: 33.27687072753906\n",
      "epoch: 48,  batch step: 243, loss: 3.1435561180114746\n",
      "epoch: 48,  batch step: 244, loss: 3.2968015670776367\n",
      "epoch: 48,  batch step: 245, loss: 2.517143964767456\n",
      "epoch: 48,  batch step: 246, loss: 36.74646759033203\n",
      "epoch: 48,  batch step: 247, loss: 6.500461101531982\n",
      "epoch: 48,  batch step: 248, loss: 67.76200866699219\n",
      "epoch: 48,  batch step: 249, loss: 31.222679138183594\n",
      "epoch: 48,  batch step: 250, loss: 25.846221923828125\n",
      "epoch: 48,  batch step: 251, loss: 30.07659149169922\n",
      "validation error epoch  48:    tensor(66.8418, device='cuda:0')\n",
      "316\n",
      "epoch: 49,  batch step: 0, loss: 35.67649841308594\n",
      "epoch: 49,  batch step: 1, loss: 3.7746033668518066\n",
      "epoch: 49,  batch step: 2, loss: 42.623817443847656\n",
      "epoch: 49,  batch step: 3, loss: 10.463918685913086\n",
      "epoch: 49,  batch step: 4, loss: 15.353899955749512\n",
      "epoch: 49,  batch step: 5, loss: 6.247880935668945\n",
      "epoch: 49,  batch step: 6, loss: 4.057594299316406\n",
      "epoch: 49,  batch step: 7, loss: 68.89291381835938\n",
      "epoch: 49,  batch step: 8, loss: 6.8883819580078125\n",
      "epoch: 49,  batch step: 9, loss: 5.80374813079834\n",
      "epoch: 49,  batch step: 10, loss: 20.600601196289062\n",
      "epoch: 49,  batch step: 11, loss: 3.457726001739502\n",
      "epoch: 49,  batch step: 12, loss: 19.513187408447266\n",
      "epoch: 49,  batch step: 13, loss: 4.405664443969727\n",
      "epoch: 49,  batch step: 14, loss: 6.989657878875732\n",
      "epoch: 49,  batch step: 15, loss: 32.27983093261719\n",
      "epoch: 49,  batch step: 16, loss: 19.955284118652344\n",
      "epoch: 49,  batch step: 17, loss: 11.827659606933594\n",
      "epoch: 49,  batch step: 18, loss: 25.291362762451172\n",
      "epoch: 49,  batch step: 19, loss: 6.268083572387695\n",
      "epoch: 49,  batch step: 20, loss: 46.151771545410156\n",
      "epoch: 49,  batch step: 21, loss: 31.787803649902344\n",
      "epoch: 49,  batch step: 22, loss: 69.48243713378906\n",
      "epoch: 49,  batch step: 23, loss: 3.855836868286133\n",
      "epoch: 49,  batch step: 24, loss: 23.122154235839844\n",
      "epoch: 49,  batch step: 25, loss: 17.364421844482422\n",
      "epoch: 49,  batch step: 26, loss: 4.137619495391846\n",
      "epoch: 49,  batch step: 27, loss: 5.1638031005859375\n",
      "epoch: 49,  batch step: 28, loss: 6.422387599945068\n",
      "epoch: 49,  batch step: 29, loss: 2.561770439147949\n",
      "epoch: 49,  batch step: 30, loss: 117.7079086303711\n",
      "epoch: 49,  batch step: 31, loss: 33.410682678222656\n",
      "epoch: 49,  batch step: 32, loss: 13.76942253112793\n",
      "epoch: 49,  batch step: 33, loss: 29.027786254882812\n",
      "epoch: 49,  batch step: 34, loss: 2.882519483566284\n",
      "epoch: 49,  batch step: 35, loss: 69.91922760009766\n",
      "epoch: 49,  batch step: 36, loss: 6.460538387298584\n",
      "epoch: 49,  batch step: 37, loss: 11.160334587097168\n",
      "epoch: 49,  batch step: 38, loss: 35.87714385986328\n",
      "epoch: 49,  batch step: 39, loss: 8.026373863220215\n",
      "epoch: 49,  batch step: 40, loss: 107.00606536865234\n",
      "epoch: 49,  batch step: 41, loss: 2.9683585166931152\n",
      "epoch: 49,  batch step: 42, loss: 3.7713027000427246\n",
      "epoch: 49,  batch step: 43, loss: 69.46745300292969\n",
      "epoch: 49,  batch step: 44, loss: 22.978788375854492\n",
      "epoch: 49,  batch step: 45, loss: 70.68153381347656\n",
      "epoch: 49,  batch step: 46, loss: 4.5943074226379395\n",
      "epoch: 49,  batch step: 47, loss: 4.616601943969727\n",
      "epoch: 49,  batch step: 48, loss: 9.355691909790039\n",
      "epoch: 49,  batch step: 49, loss: 3.7752342224121094\n",
      "epoch: 49,  batch step: 50, loss: 42.758819580078125\n",
      "epoch: 49,  batch step: 51, loss: 95.15357971191406\n",
      "epoch: 49,  batch step: 52, loss: 4.080029487609863\n",
      "epoch: 49,  batch step: 53, loss: 23.948585510253906\n",
      "epoch: 49,  batch step: 54, loss: 33.22053146362305\n",
      "epoch: 49,  batch step: 55, loss: 3.435905933380127\n",
      "epoch: 49,  batch step: 56, loss: 14.189208984375\n",
      "epoch: 49,  batch step: 57, loss: 8.674818992614746\n",
      "epoch: 49,  batch step: 58, loss: 3.297710418701172\n",
      "epoch: 49,  batch step: 59, loss: 4.418743133544922\n",
      "epoch: 49,  batch step: 60, loss: 4.403236389160156\n",
      "epoch: 49,  batch step: 61, loss: 59.31060791015625\n",
      "epoch: 49,  batch step: 62, loss: 5.722528457641602\n",
      "epoch: 49,  batch step: 63, loss: 49.040740966796875\n",
      "epoch: 49,  batch step: 64, loss: 4.443231105804443\n",
      "epoch: 49,  batch step: 65, loss: 4.226083755493164\n",
      "epoch: 49,  batch step: 66, loss: 2.644566774368286\n",
      "epoch: 49,  batch step: 67, loss: 13.88825798034668\n",
      "epoch: 49,  batch step: 68, loss: 4.123263835906982\n",
      "epoch: 49,  batch step: 69, loss: 5.807090759277344\n",
      "epoch: 49,  batch step: 70, loss: 8.622549057006836\n",
      "epoch: 49,  batch step: 71, loss: 5.8861002922058105\n",
      "epoch: 49,  batch step: 72, loss: 3.202970027923584\n",
      "epoch: 49,  batch step: 73, loss: 5.897327423095703\n",
      "epoch: 49,  batch step: 74, loss: 6.55157470703125\n",
      "epoch: 49,  batch step: 75, loss: 12.871356964111328\n",
      "epoch: 49,  batch step: 76, loss: 15.093647003173828\n",
      "epoch: 49,  batch step: 77, loss: 63.292381286621094\n",
      "epoch: 49,  batch step: 78, loss: 3.4043824672698975\n",
      "epoch: 49,  batch step: 79, loss: 2.955565929412842\n",
      "epoch: 49,  batch step: 80, loss: 2.676828145980835\n",
      "epoch: 49,  batch step: 81, loss: 5.944440841674805\n",
      "epoch: 49,  batch step: 82, loss: 74.34325408935547\n",
      "epoch: 49,  batch step: 83, loss: 8.368080139160156\n",
      "epoch: 49,  batch step: 84, loss: 4.233804702758789\n",
      "epoch: 49,  batch step: 85, loss: 18.63106918334961\n",
      "epoch: 49,  batch step: 86, loss: 12.480605125427246\n",
      "epoch: 49,  batch step: 87, loss: 3.3097681999206543\n",
      "epoch: 49,  batch step: 88, loss: 51.25481414794922\n",
      "epoch: 49,  batch step: 89, loss: 69.3606948852539\n",
      "epoch: 49,  batch step: 90, loss: 54.88230514526367\n",
      "epoch: 49,  batch step: 91, loss: 3.9201905727386475\n",
      "epoch: 49,  batch step: 92, loss: 13.101675033569336\n",
      "epoch: 49,  batch step: 93, loss: 31.85672378540039\n",
      "epoch: 49,  batch step: 94, loss: 4.153396129608154\n",
      "epoch: 49,  batch step: 95, loss: 119.50435638427734\n",
      "epoch: 49,  batch step: 96, loss: 32.352684020996094\n",
      "epoch: 49,  batch step: 97, loss: 87.18418884277344\n",
      "epoch: 49,  batch step: 98, loss: 14.347139358520508\n",
      "epoch: 49,  batch step: 99, loss: 20.26226806640625\n",
      "epoch: 49,  batch step: 100, loss: 5.972289085388184\n",
      "epoch: 49,  batch step: 101, loss: 5.350884437561035\n",
      "epoch: 49,  batch step: 102, loss: 3.8852920532226562\n",
      "epoch: 49,  batch step: 103, loss: 2.840815305709839\n",
      "epoch: 49,  batch step: 104, loss: 4.887618541717529\n",
      "epoch: 49,  batch step: 105, loss: 23.291484832763672\n",
      "epoch: 49,  batch step: 106, loss: 99.34408569335938\n",
      "epoch: 49,  batch step: 107, loss: 27.96971893310547\n",
      "epoch: 49,  batch step: 108, loss: 2.7262094020843506\n",
      "epoch: 49,  batch step: 109, loss: 4.2104926109313965\n",
      "epoch: 49,  batch step: 110, loss: 4.671507358551025\n",
      "epoch: 49,  batch step: 111, loss: 6.149199962615967\n",
      "epoch: 49,  batch step: 112, loss: 27.81340789794922\n",
      "epoch: 49,  batch step: 113, loss: 5.9999799728393555\n",
      "epoch: 49,  batch step: 114, loss: 147.3568878173828\n",
      "epoch: 49,  batch step: 115, loss: 9.235172271728516\n",
      "epoch: 49,  batch step: 116, loss: 4.931525707244873\n",
      "epoch: 49,  batch step: 117, loss: 33.18758010864258\n",
      "epoch: 49,  batch step: 118, loss: 22.88190269470215\n",
      "epoch: 49,  batch step: 119, loss: 22.732261657714844\n",
      "epoch: 49,  batch step: 120, loss: 20.613975524902344\n",
      "epoch: 49,  batch step: 121, loss: 47.0780143737793\n",
      "epoch: 49,  batch step: 122, loss: 30.600074768066406\n",
      "epoch: 49,  batch step: 123, loss: 5.693814754486084\n",
      "epoch: 49,  batch step: 124, loss: 45.555091857910156\n",
      "epoch: 49,  batch step: 125, loss: 7.432674407958984\n",
      "epoch: 49,  batch step: 126, loss: 5.956620693206787\n",
      "epoch: 49,  batch step: 127, loss: 5.15643835067749\n",
      "epoch: 49,  batch step: 128, loss: 4.552834987640381\n",
      "epoch: 49,  batch step: 129, loss: 8.85079288482666\n",
      "epoch: 49,  batch step: 130, loss: 4.952150344848633\n",
      "epoch: 49,  batch step: 131, loss: 4.485709190368652\n",
      "epoch: 49,  batch step: 132, loss: 3.394634246826172\n",
      "epoch: 49,  batch step: 133, loss: 7.514555931091309\n",
      "epoch: 49,  batch step: 134, loss: 18.777339935302734\n",
      "epoch: 49,  batch step: 135, loss: 30.151479721069336\n",
      "epoch: 49,  batch step: 136, loss: 15.938627243041992\n",
      "epoch: 49,  batch step: 137, loss: 3.4981141090393066\n",
      "epoch: 49,  batch step: 138, loss: 26.40652847290039\n",
      "epoch: 49,  batch step: 139, loss: 45.07316589355469\n",
      "epoch: 49,  batch step: 140, loss: 51.74009323120117\n",
      "epoch: 49,  batch step: 141, loss: 3.3041656017303467\n",
      "epoch: 49,  batch step: 142, loss: 37.419891357421875\n",
      "epoch: 49,  batch step: 143, loss: 38.850555419921875\n",
      "epoch: 49,  batch step: 144, loss: 30.69069480895996\n",
      "epoch: 49,  batch step: 145, loss: 14.716845512390137\n",
      "epoch: 49,  batch step: 146, loss: 22.270689010620117\n",
      "epoch: 49,  batch step: 147, loss: 14.548654556274414\n",
      "epoch: 49,  batch step: 148, loss: 27.509719848632812\n",
      "epoch: 49,  batch step: 149, loss: 5.829442501068115\n",
      "epoch: 49,  batch step: 150, loss: 5.015379905700684\n",
      "epoch: 49,  batch step: 151, loss: 29.987293243408203\n",
      "epoch: 49,  batch step: 152, loss: 18.121124267578125\n",
      "epoch: 49,  batch step: 153, loss: 3.547724485397339\n",
      "epoch: 49,  batch step: 154, loss: 10.457555770874023\n",
      "epoch: 49,  batch step: 155, loss: 6.135351181030273\n",
      "epoch: 49,  batch step: 156, loss: 33.04027557373047\n",
      "epoch: 49,  batch step: 157, loss: 2.8975377082824707\n",
      "epoch: 49,  batch step: 158, loss: 22.119144439697266\n",
      "epoch: 49,  batch step: 159, loss: 20.340003967285156\n",
      "epoch: 49,  batch step: 160, loss: 2.937814235687256\n",
      "epoch: 49,  batch step: 161, loss: 42.81281280517578\n",
      "epoch: 49,  batch step: 162, loss: 40.49718475341797\n",
      "epoch: 49,  batch step: 163, loss: 3.411159038543701\n",
      "epoch: 49,  batch step: 164, loss: 12.395484924316406\n",
      "epoch: 49,  batch step: 165, loss: 10.02072525024414\n",
      "epoch: 49,  batch step: 166, loss: 9.082175254821777\n",
      "epoch: 49,  batch step: 167, loss: 8.499534606933594\n",
      "epoch: 49,  batch step: 168, loss: 4.250131607055664\n",
      "epoch: 49,  batch step: 169, loss: 5.912358283996582\n",
      "epoch: 49,  batch step: 170, loss: 10.905008316040039\n",
      "epoch: 49,  batch step: 171, loss: 21.30457305908203\n",
      "epoch: 49,  batch step: 172, loss: 3.767524242401123\n",
      "epoch: 49,  batch step: 173, loss: 2.7048134803771973\n",
      "epoch: 49,  batch step: 174, loss: 31.76727867126465\n",
      "epoch: 49,  batch step: 175, loss: 36.679603576660156\n",
      "epoch: 49,  batch step: 176, loss: 7.811543941497803\n",
      "epoch: 49,  batch step: 177, loss: 19.095914840698242\n",
      "epoch: 49,  batch step: 178, loss: 7.868609428405762\n",
      "epoch: 49,  batch step: 179, loss: 3.8501460552215576\n",
      "epoch: 49,  batch step: 180, loss: 2.678671360015869\n",
      "epoch: 49,  batch step: 181, loss: 5.39713191986084\n",
      "epoch: 49,  batch step: 182, loss: 18.471464157104492\n",
      "epoch: 49,  batch step: 183, loss: 25.125511169433594\n",
      "epoch: 49,  batch step: 184, loss: 31.622671127319336\n",
      "epoch: 49,  batch step: 185, loss: 2.269402503967285\n",
      "epoch: 49,  batch step: 186, loss: 13.1076078414917\n",
      "epoch: 49,  batch step: 187, loss: 8.504056930541992\n",
      "epoch: 49,  batch step: 188, loss: 27.938156127929688\n",
      "epoch: 49,  batch step: 189, loss: 71.65919494628906\n",
      "epoch: 49,  batch step: 190, loss: 8.390701293945312\n",
      "epoch: 49,  batch step: 191, loss: 4.077271461486816\n",
      "epoch: 49,  batch step: 192, loss: 22.77759552001953\n",
      "epoch: 49,  batch step: 193, loss: 17.218870162963867\n",
      "epoch: 49,  batch step: 194, loss: 4.275903224945068\n",
      "epoch: 49,  batch step: 195, loss: 28.519466400146484\n",
      "epoch: 49,  batch step: 196, loss: 3.069326162338257\n",
      "epoch: 49,  batch step: 197, loss: 40.02532958984375\n",
      "epoch: 49,  batch step: 198, loss: 4.654541015625\n",
      "epoch: 49,  batch step: 199, loss: 74.45108795166016\n",
      "epoch: 49,  batch step: 200, loss: 28.28607749938965\n",
      "epoch: 49,  batch step: 201, loss: 2.795327663421631\n",
      "epoch: 49,  batch step: 202, loss: 2.7868614196777344\n",
      "epoch: 49,  batch step: 203, loss: 42.06545639038086\n",
      "epoch: 49,  batch step: 204, loss: 3.6794185638427734\n",
      "epoch: 49,  batch step: 205, loss: 86.31350708007812\n",
      "epoch: 49,  batch step: 206, loss: 33.01370620727539\n",
      "epoch: 49,  batch step: 207, loss: 3.90765643119812\n",
      "epoch: 49,  batch step: 208, loss: 33.9607048034668\n",
      "epoch: 49,  batch step: 209, loss: 37.47305679321289\n",
      "epoch: 49,  batch step: 210, loss: 57.11077117919922\n",
      "epoch: 49,  batch step: 211, loss: 4.590217113494873\n",
      "epoch: 49,  batch step: 212, loss: 22.66748046875\n",
      "epoch: 49,  batch step: 213, loss: 3.7938754558563232\n",
      "epoch: 49,  batch step: 214, loss: 33.25304412841797\n",
      "epoch: 49,  batch step: 215, loss: 9.318998336791992\n",
      "epoch: 49,  batch step: 216, loss: 26.041709899902344\n",
      "epoch: 49,  batch step: 217, loss: 4.606281280517578\n",
      "epoch: 49,  batch step: 218, loss: 5.412257194519043\n",
      "epoch: 49,  batch step: 219, loss: 11.430856704711914\n",
      "epoch: 49,  batch step: 220, loss: 48.15029525756836\n",
      "epoch: 49,  batch step: 221, loss: 4.960156440734863\n",
      "epoch: 49,  batch step: 222, loss: 3.4986894130706787\n",
      "epoch: 49,  batch step: 223, loss: 2.679564952850342\n",
      "epoch: 49,  batch step: 224, loss: 135.61111450195312\n",
      "epoch: 49,  batch step: 225, loss: 17.110065460205078\n",
      "epoch: 49,  batch step: 226, loss: 45.298133850097656\n",
      "epoch: 49,  batch step: 227, loss: 39.856117248535156\n",
      "epoch: 49,  batch step: 228, loss: 2.4649252891540527\n",
      "epoch: 49,  batch step: 229, loss: 27.020000457763672\n",
      "epoch: 49,  batch step: 230, loss: 4.87490177154541\n",
      "epoch: 49,  batch step: 231, loss: 26.472270965576172\n",
      "epoch: 49,  batch step: 232, loss: 3.8719069957733154\n",
      "epoch: 49,  batch step: 233, loss: 3.734773635864258\n",
      "epoch: 49,  batch step: 234, loss: 3.9822158813476562\n",
      "epoch: 49,  batch step: 235, loss: 18.641056060791016\n",
      "epoch: 49,  batch step: 236, loss: 7.700067043304443\n",
      "epoch: 49,  batch step: 237, loss: 23.65558433532715\n",
      "epoch: 49,  batch step: 238, loss: 2.865586042404175\n",
      "epoch: 49,  batch step: 239, loss: 62.22624206542969\n",
      "epoch: 49,  batch step: 240, loss: 7.95431661605835\n",
      "epoch: 49,  batch step: 241, loss: 19.398183822631836\n",
      "epoch: 49,  batch step: 242, loss: 12.001463890075684\n",
      "epoch: 49,  batch step: 243, loss: 6.646580696105957\n",
      "epoch: 49,  batch step: 244, loss: 3.2139673233032227\n",
      "epoch: 49,  batch step: 245, loss: 51.40493392944336\n",
      "epoch: 49,  batch step: 246, loss: 54.575889587402344\n",
      "epoch: 49,  batch step: 247, loss: 37.6525993347168\n",
      "epoch: 49,  batch step: 248, loss: 2.474949359893799\n",
      "epoch: 49,  batch step: 249, loss: 45.91318893432617\n",
      "epoch: 49,  batch step: 250, loss: 16.33802032470703\n",
      "epoch: 49,  batch step: 251, loss: 18.47710609436035\n",
      "finished saving checkpoints\n",
      "validation error epoch  49:    tensor(66.4249, device='cuda:0')\n",
      "316\n",
      "epoch: 50,  batch step: 0, loss: 2.239962577819824\n",
      "epoch: 50,  batch step: 1, loss: 4.099828243255615\n",
      "epoch: 50,  batch step: 2, loss: 5.735851287841797\n",
      "epoch: 50,  batch step: 3, loss: 2.6251816749572754\n",
      "epoch: 50,  batch step: 4, loss: 4.016604423522949\n",
      "epoch: 50,  batch step: 5, loss: 11.020645141601562\n",
      "epoch: 50,  batch step: 6, loss: 114.60860443115234\n",
      "epoch: 50,  batch step: 7, loss: 45.146087646484375\n",
      "epoch: 50,  batch step: 8, loss: 18.8255615234375\n",
      "epoch: 50,  batch step: 9, loss: 12.445709228515625\n",
      "epoch: 50,  batch step: 10, loss: 35.916053771972656\n",
      "epoch: 50,  batch step: 11, loss: 81.98161315917969\n",
      "epoch: 50,  batch step: 12, loss: 8.892724990844727\n",
      "epoch: 50,  batch step: 13, loss: 3.247025489807129\n",
      "epoch: 50,  batch step: 14, loss: 3.3299179077148438\n",
      "epoch: 50,  batch step: 15, loss: 30.601652145385742\n",
      "epoch: 50,  batch step: 16, loss: 3.947944164276123\n",
      "epoch: 50,  batch step: 17, loss: 23.44387435913086\n",
      "epoch: 50,  batch step: 18, loss: 41.871009826660156\n",
      "epoch: 50,  batch step: 19, loss: 50.083106994628906\n",
      "epoch: 50,  batch step: 20, loss: 5.018939971923828\n",
      "epoch: 50,  batch step: 21, loss: 6.781192302703857\n",
      "epoch: 50,  batch step: 22, loss: 49.55131149291992\n",
      "epoch: 50,  batch step: 23, loss: 44.8454704284668\n",
      "epoch: 50,  batch step: 24, loss: 25.873159408569336\n",
      "epoch: 50,  batch step: 25, loss: 22.210485458374023\n",
      "epoch: 50,  batch step: 26, loss: 6.218022346496582\n",
      "epoch: 50,  batch step: 27, loss: 32.25559997558594\n",
      "epoch: 50,  batch step: 28, loss: 6.626368522644043\n",
      "epoch: 50,  batch step: 29, loss: 32.049598693847656\n",
      "epoch: 50,  batch step: 30, loss: 2.9622983932495117\n",
      "epoch: 50,  batch step: 31, loss: 4.3719482421875\n",
      "epoch: 50,  batch step: 32, loss: 5.623573303222656\n",
      "epoch: 50,  batch step: 33, loss: 21.907238006591797\n",
      "epoch: 50,  batch step: 34, loss: 31.04413414001465\n",
      "epoch: 50,  batch step: 35, loss: 41.917755126953125\n",
      "epoch: 50,  batch step: 36, loss: 3.4618377685546875\n",
      "epoch: 50,  batch step: 37, loss: 50.507320404052734\n",
      "epoch: 50,  batch step: 38, loss: 41.703678131103516\n",
      "epoch: 50,  batch step: 39, loss: 10.050737380981445\n",
      "epoch: 50,  batch step: 40, loss: 16.907014846801758\n",
      "epoch: 50,  batch step: 41, loss: 3.3141746520996094\n",
      "epoch: 50,  batch step: 42, loss: 4.24708890914917\n",
      "epoch: 50,  batch step: 43, loss: 5.4576945304870605\n",
      "epoch: 50,  batch step: 44, loss: 2.8167378902435303\n",
      "epoch: 50,  batch step: 45, loss: 2.999659299850464\n",
      "epoch: 50,  batch step: 46, loss: 17.585311889648438\n",
      "epoch: 50,  batch step: 47, loss: 119.89290618896484\n",
      "epoch: 50,  batch step: 48, loss: 5.665849685668945\n",
      "epoch: 50,  batch step: 49, loss: 17.534988403320312\n",
      "epoch: 50,  batch step: 50, loss: 41.431480407714844\n",
      "epoch: 50,  batch step: 51, loss: 66.05003356933594\n",
      "epoch: 50,  batch step: 52, loss: 23.881383895874023\n",
      "epoch: 50,  batch step: 53, loss: 54.15386199951172\n",
      "epoch: 50,  batch step: 54, loss: 7.942070960998535\n",
      "epoch: 50,  batch step: 55, loss: 38.14746856689453\n",
      "epoch: 50,  batch step: 56, loss: 15.346328735351562\n",
      "epoch: 50,  batch step: 57, loss: 3.0625367164611816\n",
      "epoch: 50,  batch step: 58, loss: 18.620054244995117\n",
      "epoch: 50,  batch step: 59, loss: 32.853416442871094\n",
      "epoch: 50,  batch step: 60, loss: 19.256772994995117\n",
      "epoch: 50,  batch step: 61, loss: 5.915635585784912\n",
      "epoch: 50,  batch step: 62, loss: 4.650383949279785\n",
      "epoch: 50,  batch step: 63, loss: 5.176804542541504\n",
      "epoch: 50,  batch step: 64, loss: 28.308368682861328\n",
      "epoch: 50,  batch step: 65, loss: 9.620996475219727\n",
      "epoch: 50,  batch step: 66, loss: 5.93867826461792\n",
      "epoch: 50,  batch step: 67, loss: 19.653194427490234\n",
      "epoch: 50,  batch step: 68, loss: 21.0093994140625\n",
      "epoch: 50,  batch step: 69, loss: 14.385505676269531\n",
      "epoch: 50,  batch step: 70, loss: 14.468902587890625\n",
      "epoch: 50,  batch step: 71, loss: 34.327239990234375\n",
      "epoch: 50,  batch step: 72, loss: 9.066927909851074\n",
      "epoch: 50,  batch step: 73, loss: 2.4530506134033203\n",
      "epoch: 50,  batch step: 74, loss: 60.6119499206543\n",
      "epoch: 50,  batch step: 75, loss: 12.445865631103516\n",
      "epoch: 50,  batch step: 76, loss: 5.113907814025879\n",
      "epoch: 50,  batch step: 77, loss: 3.752000331878662\n",
      "epoch: 50,  batch step: 78, loss: 3.3596858978271484\n",
      "epoch: 50,  batch step: 79, loss: 3.7133090496063232\n",
      "epoch: 50,  batch step: 80, loss: 3.2113852500915527\n",
      "epoch: 50,  batch step: 81, loss: 17.467605590820312\n",
      "epoch: 50,  batch step: 82, loss: 7.402297496795654\n",
      "epoch: 50,  batch step: 83, loss: 53.050048828125\n",
      "epoch: 50,  batch step: 84, loss: 6.565365791320801\n",
      "epoch: 50,  batch step: 85, loss: 35.36781692504883\n",
      "epoch: 50,  batch step: 86, loss: 4.970003604888916\n",
      "epoch: 50,  batch step: 87, loss: 14.813623428344727\n",
      "epoch: 50,  batch step: 88, loss: 6.61434268951416\n",
      "epoch: 50,  batch step: 89, loss: 32.45183563232422\n",
      "epoch: 50,  batch step: 90, loss: 136.6802215576172\n",
      "epoch: 50,  batch step: 91, loss: 102.0272445678711\n",
      "epoch: 50,  batch step: 92, loss: 6.1172919273376465\n",
      "epoch: 50,  batch step: 93, loss: 50.382869720458984\n",
      "epoch: 50,  batch step: 94, loss: 11.274470329284668\n",
      "epoch: 50,  batch step: 95, loss: 4.900256156921387\n",
      "epoch: 50,  batch step: 96, loss: 17.73632049560547\n",
      "epoch: 50,  batch step: 97, loss: 4.11183500289917\n",
      "epoch: 50,  batch step: 98, loss: 26.59121322631836\n",
      "epoch: 50,  batch step: 99, loss: 26.319782257080078\n",
      "epoch: 50,  batch step: 100, loss: 17.337913513183594\n",
      "epoch: 50,  batch step: 101, loss: 3.969353437423706\n",
      "epoch: 50,  batch step: 102, loss: 2.6842942237854004\n",
      "epoch: 50,  batch step: 103, loss: 2.7687692642211914\n",
      "epoch: 50,  batch step: 104, loss: 24.656309127807617\n",
      "epoch: 50,  batch step: 105, loss: 16.082561492919922\n",
      "epoch: 50,  batch step: 106, loss: 20.748394012451172\n",
      "epoch: 50,  batch step: 107, loss: 5.00520658493042\n",
      "epoch: 50,  batch step: 108, loss: 25.019458770751953\n",
      "epoch: 50,  batch step: 109, loss: 42.195701599121094\n",
      "epoch: 50,  batch step: 110, loss: 4.841691017150879\n",
      "epoch: 50,  batch step: 111, loss: 3.7883739471435547\n",
      "epoch: 50,  batch step: 112, loss: 4.341318130493164\n",
      "epoch: 50,  batch step: 113, loss: 26.943037033081055\n",
      "epoch: 50,  batch step: 114, loss: 40.287471771240234\n",
      "epoch: 50,  batch step: 115, loss: 35.510292053222656\n",
      "epoch: 50,  batch step: 116, loss: 6.460727691650391\n",
      "epoch: 50,  batch step: 117, loss: 13.807943344116211\n",
      "epoch: 50,  batch step: 118, loss: 4.633531093597412\n",
      "epoch: 50,  batch step: 119, loss: 19.42055320739746\n",
      "epoch: 50,  batch step: 120, loss: 5.061214447021484\n",
      "epoch: 50,  batch step: 121, loss: 11.670797348022461\n",
      "epoch: 50,  batch step: 122, loss: 60.77939987182617\n",
      "epoch: 50,  batch step: 123, loss: 4.799324035644531\n",
      "epoch: 50,  batch step: 124, loss: 47.402549743652344\n",
      "epoch: 50,  batch step: 125, loss: 31.42816162109375\n",
      "epoch: 50,  batch step: 126, loss: 2.8686938285827637\n",
      "epoch: 50,  batch step: 127, loss: 18.091064453125\n",
      "epoch: 50,  batch step: 128, loss: 4.6332807540893555\n",
      "epoch: 50,  batch step: 129, loss: 24.237945556640625\n",
      "epoch: 50,  batch step: 130, loss: 38.868614196777344\n",
      "epoch: 50,  batch step: 131, loss: 8.22780704498291\n",
      "epoch: 50,  batch step: 132, loss: 6.401248931884766\n",
      "epoch: 50,  batch step: 133, loss: 58.49454879760742\n",
      "epoch: 50,  batch step: 134, loss: 4.2680559158325195\n",
      "epoch: 50,  batch step: 135, loss: 15.040054321289062\n",
      "epoch: 50,  batch step: 136, loss: 3.5043535232543945\n",
      "epoch: 50,  batch step: 137, loss: 44.58837127685547\n",
      "epoch: 50,  batch step: 138, loss: 7.989825248718262\n",
      "epoch: 50,  batch step: 139, loss: 4.928106307983398\n",
      "epoch: 50,  batch step: 140, loss: 70.76972961425781\n",
      "epoch: 50,  batch step: 141, loss: 7.827313423156738\n",
      "epoch: 50,  batch step: 142, loss: 69.77960968017578\n",
      "epoch: 50,  batch step: 143, loss: 53.614471435546875\n",
      "epoch: 50,  batch step: 144, loss: 5.028737545013428\n",
      "epoch: 50,  batch step: 145, loss: 4.381855010986328\n",
      "epoch: 50,  batch step: 146, loss: 4.729548931121826\n",
      "epoch: 50,  batch step: 147, loss: 7.904559135437012\n",
      "epoch: 50,  batch step: 148, loss: 10.688806533813477\n",
      "epoch: 50,  batch step: 149, loss: 41.120460510253906\n",
      "epoch: 50,  batch step: 150, loss: 23.32270050048828\n",
      "epoch: 50,  batch step: 151, loss: 5.097681999206543\n",
      "epoch: 50,  batch step: 152, loss: 4.793584823608398\n",
      "epoch: 50,  batch step: 153, loss: 15.503984451293945\n",
      "epoch: 50,  batch step: 154, loss: 4.161500930786133\n",
      "epoch: 50,  batch step: 155, loss: 6.008140563964844\n",
      "epoch: 50,  batch step: 156, loss: 5.5827460289001465\n",
      "epoch: 50,  batch step: 157, loss: 9.24765682220459\n",
      "epoch: 50,  batch step: 158, loss: 24.966033935546875\n",
      "epoch: 50,  batch step: 159, loss: 4.017609596252441\n",
      "epoch: 50,  batch step: 160, loss: 18.548555374145508\n",
      "epoch: 50,  batch step: 161, loss: 18.277450561523438\n",
      "epoch: 50,  batch step: 162, loss: 9.151459693908691\n",
      "epoch: 50,  batch step: 163, loss: 3.2834994792938232\n",
      "epoch: 50,  batch step: 164, loss: 4.712088108062744\n",
      "epoch: 50,  batch step: 165, loss: 6.103980541229248\n",
      "epoch: 50,  batch step: 166, loss: 6.613757133483887\n",
      "epoch: 50,  batch step: 167, loss: 5.407123565673828\n",
      "epoch: 50,  batch step: 168, loss: 126.88966369628906\n",
      "epoch: 50,  batch step: 169, loss: 14.680941581726074\n",
      "epoch: 50,  batch step: 170, loss: 45.26998519897461\n",
      "epoch: 50,  batch step: 171, loss: 20.206966400146484\n",
      "epoch: 50,  batch step: 172, loss: 3.120452404022217\n",
      "epoch: 50,  batch step: 173, loss: 5.096445560455322\n",
      "epoch: 50,  batch step: 174, loss: 3.995075225830078\n",
      "epoch: 50,  batch step: 175, loss: 35.7854118347168\n",
      "epoch: 50,  batch step: 176, loss: 27.93111801147461\n",
      "epoch: 50,  batch step: 177, loss: 16.48128890991211\n",
      "epoch: 50,  batch step: 178, loss: 8.028701782226562\n",
      "epoch: 50,  batch step: 179, loss: 4.107809066772461\n",
      "epoch: 50,  batch step: 180, loss: 17.17119789123535\n",
      "epoch: 50,  batch step: 181, loss: 3.4744603633880615\n",
      "epoch: 50,  batch step: 182, loss: 2.5780131816864014\n",
      "epoch: 50,  batch step: 183, loss: 3.7648820877075195\n",
      "epoch: 50,  batch step: 184, loss: 12.314067840576172\n",
      "epoch: 50,  batch step: 185, loss: 26.923980712890625\n",
      "epoch: 50,  batch step: 186, loss: 5.095502853393555\n",
      "epoch: 50,  batch step: 187, loss: 5.559351921081543\n",
      "epoch: 50,  batch step: 188, loss: 2.9274775981903076\n",
      "epoch: 50,  batch step: 189, loss: 6.799705505371094\n",
      "epoch: 50,  batch step: 190, loss: 50.57147979736328\n",
      "epoch: 50,  batch step: 191, loss: 5.021042823791504\n",
      "epoch: 50,  batch step: 192, loss: 2.2761669158935547\n",
      "epoch: 50,  batch step: 193, loss: 15.813045501708984\n",
      "epoch: 50,  batch step: 194, loss: 3.785884380340576\n",
      "epoch: 50,  batch step: 195, loss: 53.337032318115234\n",
      "epoch: 50,  batch step: 196, loss: 7.6901679039001465\n",
      "epoch: 50,  batch step: 197, loss: 53.654579162597656\n",
      "epoch: 50,  batch step: 198, loss: 2.762794017791748\n",
      "epoch: 50,  batch step: 199, loss: 15.38566780090332\n",
      "epoch: 50,  batch step: 200, loss: 23.718830108642578\n",
      "epoch: 50,  batch step: 201, loss: 8.629899978637695\n",
      "epoch: 50,  batch step: 202, loss: 4.728793144226074\n",
      "epoch: 50,  batch step: 203, loss: 3.6446533203125\n",
      "epoch: 50,  batch step: 204, loss: 3.5781545639038086\n",
      "epoch: 50,  batch step: 205, loss: 3.8320069313049316\n",
      "epoch: 50,  batch step: 206, loss: 5.410719871520996\n",
      "epoch: 50,  batch step: 207, loss: 105.51110076904297\n",
      "epoch: 50,  batch step: 208, loss: 3.3388900756835938\n",
      "epoch: 50,  batch step: 209, loss: 42.15814208984375\n",
      "epoch: 50,  batch step: 210, loss: 87.49610900878906\n",
      "epoch: 50,  batch step: 211, loss: 38.05113220214844\n",
      "epoch: 50,  batch step: 212, loss: 26.312358856201172\n",
      "epoch: 50,  batch step: 213, loss: 13.132280349731445\n",
      "epoch: 50,  batch step: 214, loss: 39.25984191894531\n",
      "epoch: 50,  batch step: 215, loss: 5.176703453063965\n",
      "epoch: 50,  batch step: 216, loss: 49.06855392456055\n",
      "epoch: 50,  batch step: 217, loss: 29.766395568847656\n",
      "epoch: 50,  batch step: 218, loss: 37.653560638427734\n",
      "epoch: 50,  batch step: 219, loss: 43.30668258666992\n",
      "epoch: 50,  batch step: 220, loss: 4.6230058670043945\n",
      "epoch: 50,  batch step: 221, loss: 6.05754280090332\n",
      "epoch: 50,  batch step: 222, loss: 3.5049281120300293\n",
      "epoch: 50,  batch step: 223, loss: 27.5495662689209\n",
      "epoch: 50,  batch step: 224, loss: 50.10972595214844\n",
      "epoch: 50,  batch step: 225, loss: 3.458256483078003\n",
      "epoch: 50,  batch step: 226, loss: 21.712146759033203\n",
      "epoch: 50,  batch step: 227, loss: 2.9119205474853516\n",
      "epoch: 50,  batch step: 228, loss: 8.699705123901367\n",
      "epoch: 50,  batch step: 229, loss: 32.03612518310547\n",
      "epoch: 50,  batch step: 230, loss: 3.5813510417938232\n",
      "epoch: 50,  batch step: 231, loss: 14.56281566619873\n",
      "epoch: 50,  batch step: 232, loss: 44.665977478027344\n",
      "epoch: 50,  batch step: 233, loss: 4.153053283691406\n",
      "epoch: 50,  batch step: 234, loss: 4.626452445983887\n",
      "epoch: 50,  batch step: 235, loss: 43.52022933959961\n",
      "epoch: 50,  batch step: 236, loss: 24.792951583862305\n",
      "epoch: 50,  batch step: 237, loss: 5.718108654022217\n",
      "epoch: 50,  batch step: 238, loss: 36.26417922973633\n",
      "epoch: 50,  batch step: 239, loss: 42.958824157714844\n",
      "epoch: 50,  batch step: 240, loss: 51.29913330078125\n",
      "epoch: 50,  batch step: 241, loss: 14.76349925994873\n",
      "epoch: 50,  batch step: 242, loss: 8.68040657043457\n",
      "epoch: 50,  batch step: 243, loss: 10.172858238220215\n",
      "epoch: 50,  batch step: 244, loss: 11.300021171569824\n",
      "epoch: 50,  batch step: 245, loss: 5.083981513977051\n",
      "epoch: 50,  batch step: 246, loss: 5.326480865478516\n",
      "epoch: 50,  batch step: 247, loss: 5.658135414123535\n",
      "epoch: 50,  batch step: 248, loss: 28.788436889648438\n",
      "epoch: 50,  batch step: 249, loss: 44.30656051635742\n",
      "epoch: 50,  batch step: 250, loss: 43.50681686401367\n",
      "epoch: 50,  batch step: 251, loss: 23.196033477783203\n",
      "validation error epoch  50:    tensor(66.8046, device='cuda:0')\n",
      "316\n",
      "epoch: 51,  batch step: 0, loss: 40.641754150390625\n",
      "epoch: 51,  batch step: 1, loss: 80.1989517211914\n",
      "epoch: 51,  batch step: 2, loss: 26.164350509643555\n",
      "epoch: 51,  batch step: 3, loss: 5.382970809936523\n",
      "epoch: 51,  batch step: 4, loss: 50.07594299316406\n",
      "epoch: 51,  batch step: 5, loss: 16.87607765197754\n",
      "epoch: 51,  batch step: 6, loss: 8.162578582763672\n",
      "epoch: 51,  batch step: 7, loss: 34.20240783691406\n",
      "epoch: 51,  batch step: 8, loss: 20.65483856201172\n",
      "epoch: 51,  batch step: 9, loss: 21.22283172607422\n",
      "epoch: 51,  batch step: 10, loss: 4.5835795402526855\n",
      "epoch: 51,  batch step: 11, loss: 5.156376838684082\n",
      "epoch: 51,  batch step: 12, loss: 5.997085094451904\n",
      "epoch: 51,  batch step: 13, loss: 32.83781433105469\n",
      "epoch: 51,  batch step: 14, loss: 49.11738204956055\n",
      "epoch: 51,  batch step: 15, loss: 9.616854667663574\n",
      "epoch: 51,  batch step: 16, loss: 27.655689239501953\n",
      "epoch: 51,  batch step: 17, loss: 8.12831974029541\n",
      "epoch: 51,  batch step: 18, loss: 29.791738510131836\n",
      "epoch: 51,  batch step: 19, loss: 97.19872283935547\n",
      "epoch: 51,  batch step: 20, loss: 8.686275482177734\n",
      "epoch: 51,  batch step: 21, loss: 6.422564506530762\n",
      "epoch: 51,  batch step: 22, loss: 4.236078262329102\n",
      "epoch: 51,  batch step: 23, loss: 7.477245330810547\n",
      "epoch: 51,  batch step: 24, loss: 10.687342643737793\n",
      "epoch: 51,  batch step: 25, loss: 10.163460731506348\n",
      "epoch: 51,  batch step: 26, loss: 44.67500305175781\n",
      "epoch: 51,  batch step: 27, loss: 107.15049743652344\n",
      "epoch: 51,  batch step: 28, loss: 6.291165351867676\n",
      "epoch: 51,  batch step: 29, loss: 45.2627067565918\n",
      "epoch: 51,  batch step: 30, loss: 44.18122100830078\n",
      "epoch: 51,  batch step: 31, loss: 37.25695037841797\n",
      "epoch: 51,  batch step: 32, loss: 45.737060546875\n",
      "epoch: 51,  batch step: 33, loss: 30.719181060791016\n",
      "epoch: 51,  batch step: 34, loss: 20.6962890625\n",
      "epoch: 51,  batch step: 35, loss: 17.10213279724121\n",
      "epoch: 51,  batch step: 36, loss: 8.929422378540039\n",
      "epoch: 51,  batch step: 37, loss: 6.830024719238281\n",
      "epoch: 51,  batch step: 38, loss: 5.677610397338867\n",
      "epoch: 51,  batch step: 39, loss: 35.07365036010742\n",
      "epoch: 51,  batch step: 40, loss: 21.070884704589844\n",
      "epoch: 51,  batch step: 41, loss: 5.112955570220947\n",
      "epoch: 51,  batch step: 42, loss: 9.204216003417969\n",
      "epoch: 51,  batch step: 43, loss: 10.861163139343262\n",
      "epoch: 51,  batch step: 44, loss: 4.776474475860596\n",
      "epoch: 51,  batch step: 45, loss: 48.55769348144531\n",
      "epoch: 51,  batch step: 46, loss: 21.609678268432617\n",
      "epoch: 51,  batch step: 47, loss: 54.94007110595703\n",
      "epoch: 51,  batch step: 48, loss: 22.739837646484375\n",
      "epoch: 51,  batch step: 49, loss: 7.211750030517578\n",
      "epoch: 51,  batch step: 50, loss: 7.237599849700928\n",
      "epoch: 51,  batch step: 51, loss: 9.218624114990234\n",
      "epoch: 51,  batch step: 52, loss: 18.638803482055664\n",
      "epoch: 51,  batch step: 53, loss: 22.10340690612793\n",
      "epoch: 51,  batch step: 54, loss: 6.310477256774902\n",
      "epoch: 51,  batch step: 55, loss: 6.644341945648193\n",
      "epoch: 51,  batch step: 56, loss: 158.18603515625\n",
      "epoch: 51,  batch step: 57, loss: 38.929473876953125\n",
      "epoch: 51,  batch step: 58, loss: 57.30292892456055\n",
      "epoch: 51,  batch step: 59, loss: 9.556831359863281\n",
      "epoch: 51,  batch step: 60, loss: 10.051965713500977\n",
      "epoch: 51,  batch step: 61, loss: 11.947774887084961\n",
      "epoch: 51,  batch step: 62, loss: 10.07252025604248\n",
      "epoch: 51,  batch step: 63, loss: 8.184576034545898\n",
      "epoch: 51,  batch step: 64, loss: 8.191953659057617\n",
      "epoch: 51,  batch step: 65, loss: 99.73775482177734\n",
      "epoch: 51,  batch step: 66, loss: 15.620344161987305\n",
      "epoch: 51,  batch step: 67, loss: 47.14167785644531\n",
      "epoch: 51,  batch step: 68, loss: 14.524177551269531\n",
      "epoch: 51,  batch step: 69, loss: 40.356658935546875\n",
      "epoch: 51,  batch step: 70, loss: 16.709569931030273\n",
      "epoch: 51,  batch step: 71, loss: 55.5385856628418\n",
      "epoch: 51,  batch step: 72, loss: 6.49442195892334\n",
      "epoch: 51,  batch step: 73, loss: 13.987319946289062\n",
      "epoch: 51,  batch step: 74, loss: 31.526241302490234\n",
      "epoch: 51,  batch step: 75, loss: 39.24601364135742\n",
      "epoch: 51,  batch step: 76, loss: 5.77583646774292\n",
      "epoch: 51,  batch step: 77, loss: 4.87971305847168\n",
      "epoch: 51,  batch step: 78, loss: 8.15245246887207\n",
      "epoch: 51,  batch step: 79, loss: 3.543722152709961\n",
      "epoch: 51,  batch step: 80, loss: 37.767364501953125\n",
      "epoch: 51,  batch step: 81, loss: 18.666118621826172\n",
      "epoch: 51,  batch step: 82, loss: 4.133583068847656\n",
      "epoch: 51,  batch step: 83, loss: 4.108072280883789\n",
      "epoch: 51,  batch step: 84, loss: 3.6793830394744873\n",
      "epoch: 51,  batch step: 85, loss: 8.208292007446289\n",
      "epoch: 51,  batch step: 86, loss: 83.01204681396484\n",
      "epoch: 51,  batch step: 87, loss: 19.67850112915039\n",
      "epoch: 51,  batch step: 88, loss: 18.09320640563965\n",
      "epoch: 51,  batch step: 89, loss: 20.256729125976562\n",
      "epoch: 51,  batch step: 90, loss: 120.59139251708984\n",
      "epoch: 51,  batch step: 91, loss: 4.943942070007324\n",
      "epoch: 51,  batch step: 92, loss: 6.045832633972168\n",
      "epoch: 51,  batch step: 93, loss: 34.720909118652344\n",
      "epoch: 51,  batch step: 94, loss: 10.39889907836914\n",
      "epoch: 51,  batch step: 95, loss: 5.103721618652344\n",
      "epoch: 51,  batch step: 96, loss: 40.093387603759766\n",
      "epoch: 51,  batch step: 97, loss: 8.236296653747559\n",
      "epoch: 51,  batch step: 98, loss: 85.86154174804688\n",
      "epoch: 51,  batch step: 99, loss: 5.93263053894043\n",
      "epoch: 51,  batch step: 100, loss: 9.940190315246582\n",
      "epoch: 51,  batch step: 101, loss: 28.957019805908203\n",
      "epoch: 51,  batch step: 102, loss: 36.42138671875\n",
      "epoch: 51,  batch step: 103, loss: 3.7603259086608887\n",
      "epoch: 51,  batch step: 104, loss: 3.8620500564575195\n",
      "epoch: 51,  batch step: 105, loss: 33.80445098876953\n",
      "epoch: 51,  batch step: 106, loss: 6.5322065353393555\n",
      "epoch: 51,  batch step: 107, loss: 92.29563903808594\n",
      "epoch: 51,  batch step: 108, loss: 8.103495597839355\n",
      "epoch: 51,  batch step: 109, loss: 89.2752914428711\n",
      "epoch: 51,  batch step: 110, loss: 46.44758224487305\n",
      "epoch: 51,  batch step: 111, loss: 7.691886901855469\n",
      "epoch: 51,  batch step: 112, loss: 11.978373527526855\n",
      "epoch: 51,  batch step: 113, loss: 63.80350875854492\n",
      "epoch: 51,  batch step: 114, loss: 6.076249599456787\n",
      "epoch: 51,  batch step: 115, loss: 4.49769926071167\n",
      "epoch: 51,  batch step: 116, loss: 5.5877814292907715\n",
      "epoch: 51,  batch step: 117, loss: 111.94783782958984\n",
      "epoch: 51,  batch step: 118, loss: 21.731090545654297\n",
      "epoch: 51,  batch step: 119, loss: 32.67523193359375\n",
      "epoch: 51,  batch step: 120, loss: 5.376855850219727\n",
      "epoch: 51,  batch step: 121, loss: 35.40507125854492\n",
      "epoch: 51,  batch step: 122, loss: 4.447133541107178\n",
      "epoch: 51,  batch step: 123, loss: 5.1438493728637695\n",
      "epoch: 51,  batch step: 124, loss: 4.5311737060546875\n",
      "epoch: 51,  batch step: 125, loss: 3.59785532951355\n",
      "epoch: 51,  batch step: 126, loss: 27.926918029785156\n",
      "epoch: 51,  batch step: 127, loss: 4.39621639251709\n",
      "epoch: 51,  batch step: 128, loss: 9.157816886901855\n",
      "epoch: 51,  batch step: 129, loss: 4.00369119644165\n",
      "epoch: 51,  batch step: 130, loss: 30.452133178710938\n",
      "epoch: 51,  batch step: 131, loss: 50.11788558959961\n",
      "epoch: 51,  batch step: 132, loss: 3.985592842102051\n",
      "epoch: 51,  batch step: 133, loss: 8.030860900878906\n",
      "epoch: 51,  batch step: 134, loss: 3.9992778301239014\n",
      "epoch: 51,  batch step: 135, loss: 6.768040657043457\n",
      "epoch: 51,  batch step: 136, loss: 57.36785125732422\n",
      "epoch: 51,  batch step: 137, loss: 38.54270553588867\n",
      "epoch: 51,  batch step: 138, loss: 3.5156917572021484\n",
      "epoch: 51,  batch step: 139, loss: 34.42267608642578\n",
      "epoch: 51,  batch step: 140, loss: 2.572277069091797\n",
      "epoch: 51,  batch step: 141, loss: 5.313811302185059\n",
      "epoch: 51,  batch step: 142, loss: 84.74030303955078\n",
      "epoch: 51,  batch step: 143, loss: 3.20743465423584\n",
      "epoch: 51,  batch step: 144, loss: 43.81418991088867\n",
      "epoch: 51,  batch step: 145, loss: 112.82018280029297\n",
      "epoch: 51,  batch step: 146, loss: 10.248022079467773\n",
      "epoch: 51,  batch step: 147, loss: 4.845102310180664\n",
      "epoch: 51,  batch step: 148, loss: 4.042895317077637\n",
      "epoch: 51,  batch step: 149, loss: 6.775766372680664\n",
      "epoch: 51,  batch step: 150, loss: 74.47661590576172\n",
      "epoch: 51,  batch step: 151, loss: 56.42705535888672\n",
      "epoch: 51,  batch step: 152, loss: 4.6696953773498535\n",
      "epoch: 51,  batch step: 153, loss: 7.540544033050537\n",
      "epoch: 51,  batch step: 154, loss: 52.81060028076172\n",
      "epoch: 51,  batch step: 155, loss: 17.446090698242188\n",
      "epoch: 51,  batch step: 156, loss: 6.474483013153076\n",
      "epoch: 51,  batch step: 157, loss: 6.878416061401367\n",
      "epoch: 51,  batch step: 158, loss: 156.79148864746094\n",
      "epoch: 51,  batch step: 159, loss: 4.857238292694092\n",
      "epoch: 51,  batch step: 160, loss: 43.45287322998047\n",
      "epoch: 51,  batch step: 161, loss: 26.19561767578125\n",
      "epoch: 51,  batch step: 162, loss: 38.03155517578125\n",
      "epoch: 51,  batch step: 163, loss: 28.445968627929688\n",
      "epoch: 51,  batch step: 164, loss: 25.806961059570312\n",
      "epoch: 51,  batch step: 165, loss: 78.73591613769531\n",
      "epoch: 51,  batch step: 166, loss: 26.189163208007812\n",
      "epoch: 51,  batch step: 167, loss: 69.31629943847656\n",
      "epoch: 51,  batch step: 168, loss: 9.884300231933594\n",
      "epoch: 51,  batch step: 169, loss: 8.843829154968262\n",
      "epoch: 51,  batch step: 170, loss: 7.785494804382324\n",
      "epoch: 51,  batch step: 171, loss: 7.720465660095215\n",
      "epoch: 51,  batch step: 172, loss: 18.295974731445312\n",
      "epoch: 51,  batch step: 173, loss: 5.93702507019043\n",
      "epoch: 51,  batch step: 174, loss: 29.165700912475586\n",
      "epoch: 51,  batch step: 175, loss: 30.808414459228516\n",
      "epoch: 51,  batch step: 176, loss: 4.986848831176758\n",
      "epoch: 51,  batch step: 177, loss: 6.245474815368652\n",
      "epoch: 51,  batch step: 178, loss: 6.19674825668335\n",
      "epoch: 51,  batch step: 179, loss: 41.85844039916992\n",
      "epoch: 51,  batch step: 180, loss: 11.918758392333984\n",
      "epoch: 51,  batch step: 181, loss: 2.9010143280029297\n",
      "epoch: 51,  batch step: 182, loss: 3.8612544536590576\n",
      "epoch: 51,  batch step: 183, loss: 50.19078826904297\n",
      "epoch: 51,  batch step: 184, loss: 3.938023328781128\n",
      "epoch: 51,  batch step: 185, loss: 3.3818061351776123\n",
      "epoch: 51,  batch step: 186, loss: 30.77448272705078\n",
      "epoch: 51,  batch step: 187, loss: 57.31756591796875\n",
      "epoch: 51,  batch step: 188, loss: 17.131391525268555\n",
      "epoch: 51,  batch step: 189, loss: 15.030657768249512\n",
      "epoch: 51,  batch step: 190, loss: 21.3929443359375\n",
      "epoch: 51,  batch step: 191, loss: 23.373149871826172\n",
      "epoch: 51,  batch step: 192, loss: 15.978632926940918\n",
      "epoch: 51,  batch step: 193, loss: 49.246803283691406\n",
      "epoch: 51,  batch step: 194, loss: 5.7533769607543945\n",
      "epoch: 51,  batch step: 195, loss: 41.349788665771484\n",
      "epoch: 51,  batch step: 196, loss: 52.77068328857422\n",
      "epoch: 51,  batch step: 197, loss: 7.286651611328125\n",
      "epoch: 51,  batch step: 198, loss: 34.32358932495117\n",
      "epoch: 51,  batch step: 199, loss: 30.281150817871094\n",
      "epoch: 51,  batch step: 200, loss: 10.936617851257324\n",
      "epoch: 51,  batch step: 201, loss: 6.045933723449707\n",
      "epoch: 51,  batch step: 202, loss: 3.485616683959961\n",
      "epoch: 51,  batch step: 203, loss: 23.377559661865234\n",
      "epoch: 51,  batch step: 204, loss: 5.167424201965332\n",
      "epoch: 51,  batch step: 205, loss: 16.14208984375\n",
      "epoch: 51,  batch step: 206, loss: 68.22682189941406\n",
      "epoch: 51,  batch step: 207, loss: 75.50016784667969\n",
      "epoch: 51,  batch step: 208, loss: 59.231201171875\n",
      "epoch: 51,  batch step: 209, loss: 22.088781356811523\n",
      "epoch: 51,  batch step: 210, loss: 40.264278411865234\n",
      "epoch: 51,  batch step: 211, loss: 9.795950889587402\n",
      "epoch: 51,  batch step: 212, loss: 3.6242215633392334\n",
      "epoch: 51,  batch step: 213, loss: 24.29863739013672\n",
      "epoch: 51,  batch step: 214, loss: 9.600166320800781\n",
      "epoch: 51,  batch step: 215, loss: 35.11430358886719\n",
      "epoch: 51,  batch step: 216, loss: 34.98375701904297\n",
      "epoch: 51,  batch step: 217, loss: 21.229841232299805\n",
      "epoch: 51,  batch step: 218, loss: 18.406383514404297\n",
      "epoch: 51,  batch step: 219, loss: 5.294105052947998\n",
      "epoch: 51,  batch step: 220, loss: 20.69956398010254\n",
      "epoch: 51,  batch step: 221, loss: 34.74738693237305\n",
      "epoch: 51,  batch step: 222, loss: 9.986908912658691\n",
      "epoch: 51,  batch step: 223, loss: 5.661366939544678\n",
      "epoch: 51,  batch step: 224, loss: 35.38502883911133\n",
      "epoch: 51,  batch step: 225, loss: 4.0085835456848145\n",
      "epoch: 51,  batch step: 226, loss: 3.5861165523529053\n",
      "epoch: 51,  batch step: 227, loss: 17.81134033203125\n",
      "epoch: 51,  batch step: 228, loss: 55.69135284423828\n",
      "epoch: 51,  batch step: 229, loss: 9.730142593383789\n",
      "epoch: 51,  batch step: 230, loss: 20.256874084472656\n",
      "epoch: 51,  batch step: 231, loss: 32.912139892578125\n",
      "epoch: 51,  batch step: 232, loss: 25.65481185913086\n",
      "epoch: 51,  batch step: 233, loss: 5.395282745361328\n",
      "epoch: 51,  batch step: 234, loss: 5.764962673187256\n",
      "epoch: 51,  batch step: 235, loss: 15.99180793762207\n",
      "epoch: 51,  batch step: 236, loss: 4.647902011871338\n",
      "epoch: 51,  batch step: 237, loss: 4.936844825744629\n",
      "epoch: 51,  batch step: 238, loss: 8.328760147094727\n",
      "epoch: 51,  batch step: 239, loss: 5.257335186004639\n",
      "epoch: 51,  batch step: 240, loss: 7.347712993621826\n",
      "epoch: 51,  batch step: 241, loss: 3.0079169273376465\n",
      "epoch: 51,  batch step: 242, loss: 2.8677725791931152\n",
      "epoch: 51,  batch step: 243, loss: 20.806241989135742\n",
      "epoch: 51,  batch step: 244, loss: 30.685537338256836\n",
      "epoch: 51,  batch step: 245, loss: 58.7598991394043\n",
      "epoch: 51,  batch step: 246, loss: 67.84161376953125\n",
      "epoch: 51,  batch step: 247, loss: 3.138106107711792\n",
      "epoch: 51,  batch step: 248, loss: 23.913959503173828\n",
      "epoch: 51,  batch step: 249, loss: 7.284524440765381\n",
      "epoch: 51,  batch step: 250, loss: 5.5476202964782715\n",
      "epoch: 51,  batch step: 251, loss: 96.20884704589844\n",
      "validation error epoch  51:    tensor(98.6755, device='cuda:0')\n",
      "316\n",
      "epoch: 52,  batch step: 0, loss: 26.946392059326172\n",
      "epoch: 52,  batch step: 1, loss: 4.88234806060791\n",
      "epoch: 52,  batch step: 2, loss: 12.755908966064453\n",
      "epoch: 52,  batch step: 3, loss: 34.34040451049805\n",
      "epoch: 52,  batch step: 4, loss: 148.47528076171875\n",
      "epoch: 52,  batch step: 5, loss: 5.532479286193848\n",
      "epoch: 52,  batch step: 6, loss: 13.567449569702148\n",
      "epoch: 52,  batch step: 7, loss: 49.71944808959961\n",
      "epoch: 52,  batch step: 8, loss: 25.407825469970703\n",
      "epoch: 52,  batch step: 9, loss: 115.53312683105469\n",
      "epoch: 52,  batch step: 10, loss: 14.994949340820312\n",
      "epoch: 52,  batch step: 11, loss: 11.787195205688477\n",
      "epoch: 52,  batch step: 12, loss: 3.9331977367401123\n",
      "epoch: 52,  batch step: 13, loss: 16.579345703125\n",
      "epoch: 52,  batch step: 14, loss: 10.11503791809082\n",
      "epoch: 52,  batch step: 15, loss: 41.911468505859375\n",
      "epoch: 52,  batch step: 16, loss: 65.44306945800781\n",
      "epoch: 52,  batch step: 17, loss: 5.646422386169434\n",
      "epoch: 52,  batch step: 18, loss: 30.667072296142578\n",
      "epoch: 52,  batch step: 19, loss: 4.819465160369873\n",
      "epoch: 52,  batch step: 20, loss: 4.1845831871032715\n",
      "epoch: 52,  batch step: 21, loss: 78.70147705078125\n",
      "epoch: 52,  batch step: 22, loss: 33.875274658203125\n",
      "epoch: 52,  batch step: 23, loss: 4.977977752685547\n",
      "epoch: 52,  batch step: 24, loss: 22.64901351928711\n",
      "epoch: 52,  batch step: 25, loss: 6.361397743225098\n",
      "epoch: 52,  batch step: 26, loss: 10.255009651184082\n",
      "epoch: 52,  batch step: 27, loss: 4.348712921142578\n",
      "epoch: 52,  batch step: 28, loss: 7.775416851043701\n",
      "epoch: 52,  batch step: 29, loss: 9.868968963623047\n",
      "epoch: 52,  batch step: 30, loss: 4.9342265129089355\n",
      "epoch: 52,  batch step: 31, loss: 4.009207725524902\n",
      "epoch: 52,  batch step: 32, loss: 5.48652458190918\n",
      "epoch: 52,  batch step: 33, loss: 6.383376121520996\n",
      "epoch: 52,  batch step: 34, loss: 3.573286771774292\n",
      "epoch: 52,  batch step: 35, loss: 44.821327209472656\n",
      "epoch: 52,  batch step: 36, loss: 3.196662425994873\n",
      "epoch: 52,  batch step: 37, loss: 5.357460975646973\n",
      "epoch: 52,  batch step: 38, loss: 4.785244941711426\n",
      "epoch: 52,  batch step: 39, loss: 8.713760375976562\n",
      "epoch: 52,  batch step: 40, loss: 44.16227722167969\n",
      "epoch: 52,  batch step: 41, loss: 66.26851654052734\n",
      "epoch: 52,  batch step: 42, loss: 37.52424621582031\n",
      "epoch: 52,  batch step: 43, loss: 21.763105392456055\n",
      "epoch: 52,  batch step: 44, loss: 24.95126724243164\n",
      "epoch: 52,  batch step: 45, loss: 10.863020896911621\n",
      "epoch: 52,  batch step: 46, loss: 15.784046173095703\n",
      "epoch: 52,  batch step: 47, loss: 4.461080551147461\n",
      "epoch: 52,  batch step: 48, loss: 7.744897365570068\n",
      "epoch: 52,  batch step: 49, loss: 3.455352783203125\n",
      "epoch: 52,  batch step: 50, loss: 32.329795837402344\n",
      "epoch: 52,  batch step: 51, loss: 48.047027587890625\n",
      "epoch: 52,  batch step: 52, loss: 7.482619285583496\n",
      "epoch: 52,  batch step: 53, loss: 12.893397331237793\n",
      "epoch: 52,  batch step: 54, loss: 6.412858963012695\n",
      "epoch: 52,  batch step: 55, loss: 26.695098876953125\n",
      "epoch: 52,  batch step: 56, loss: 4.650043487548828\n",
      "epoch: 52,  batch step: 57, loss: 8.895194053649902\n",
      "epoch: 52,  batch step: 58, loss: 4.284392356872559\n",
      "epoch: 52,  batch step: 59, loss: 31.52941131591797\n",
      "epoch: 52,  batch step: 60, loss: 14.496845245361328\n",
      "epoch: 52,  batch step: 61, loss: 67.0673828125\n",
      "epoch: 52,  batch step: 62, loss: 4.917803764343262\n",
      "epoch: 52,  batch step: 63, loss: 56.64331817626953\n",
      "epoch: 52,  batch step: 64, loss: 22.96962547302246\n",
      "epoch: 52,  batch step: 65, loss: 22.048131942749023\n",
      "epoch: 52,  batch step: 66, loss: 17.41092300415039\n",
      "epoch: 52,  batch step: 67, loss: 28.269577026367188\n",
      "epoch: 52,  batch step: 68, loss: 36.03852081298828\n",
      "epoch: 52,  batch step: 69, loss: 5.115046977996826\n",
      "epoch: 52,  batch step: 70, loss: 12.541987419128418\n",
      "epoch: 52,  batch step: 71, loss: 5.580221176147461\n",
      "epoch: 52,  batch step: 72, loss: 13.721874237060547\n",
      "epoch: 52,  batch step: 73, loss: 30.178340911865234\n",
      "epoch: 52,  batch step: 74, loss: 6.555059432983398\n",
      "epoch: 52,  batch step: 75, loss: 40.572547912597656\n",
      "epoch: 52,  batch step: 76, loss: 60.08834457397461\n",
      "epoch: 52,  batch step: 77, loss: 27.57990074157715\n",
      "epoch: 52,  batch step: 78, loss: 4.066570281982422\n",
      "epoch: 52,  batch step: 79, loss: 9.16903305053711\n",
      "epoch: 52,  batch step: 80, loss: 20.109167098999023\n",
      "epoch: 52,  batch step: 81, loss: 31.986160278320312\n",
      "epoch: 52,  batch step: 82, loss: 90.64161682128906\n",
      "epoch: 52,  batch step: 83, loss: 3.7953498363494873\n",
      "epoch: 52,  batch step: 84, loss: 104.74105834960938\n",
      "epoch: 52,  batch step: 85, loss: 9.283166885375977\n",
      "epoch: 52,  batch step: 86, loss: 3.9713821411132812\n",
      "epoch: 52,  batch step: 87, loss: 7.931232452392578\n",
      "epoch: 52,  batch step: 88, loss: 8.662178993225098\n",
      "epoch: 52,  batch step: 89, loss: 10.8621244430542\n",
      "epoch: 52,  batch step: 90, loss: 30.797035217285156\n",
      "epoch: 52,  batch step: 91, loss: 113.43035125732422\n",
      "epoch: 52,  batch step: 92, loss: 5.033025741577148\n",
      "epoch: 52,  batch step: 93, loss: 30.266376495361328\n",
      "epoch: 52,  batch step: 94, loss: 3.31445574760437\n",
      "epoch: 52,  batch step: 95, loss: 31.933349609375\n",
      "epoch: 52,  batch step: 96, loss: 60.75382614135742\n",
      "epoch: 52,  batch step: 97, loss: 16.114974975585938\n",
      "epoch: 52,  batch step: 98, loss: 6.097665309906006\n",
      "epoch: 52,  batch step: 99, loss: 8.196610450744629\n",
      "epoch: 52,  batch step: 100, loss: 4.542191028594971\n",
      "epoch: 52,  batch step: 101, loss: 22.459251403808594\n",
      "epoch: 52,  batch step: 102, loss: 54.76252746582031\n",
      "epoch: 52,  batch step: 103, loss: 3.819659948348999\n",
      "epoch: 52,  batch step: 104, loss: 4.625485420227051\n",
      "epoch: 52,  batch step: 105, loss: 4.311964988708496\n",
      "epoch: 52,  batch step: 106, loss: 6.2073822021484375\n",
      "epoch: 52,  batch step: 107, loss: 34.05512237548828\n",
      "epoch: 52,  batch step: 108, loss: 3.564404249191284\n",
      "epoch: 52,  batch step: 109, loss: 43.257484436035156\n",
      "epoch: 52,  batch step: 110, loss: 52.88939666748047\n",
      "epoch: 52,  batch step: 111, loss: 6.249797344207764\n",
      "epoch: 52,  batch step: 112, loss: 3.897653579711914\n",
      "epoch: 52,  batch step: 113, loss: 63.397850036621094\n",
      "epoch: 52,  batch step: 114, loss: 4.393304824829102\n",
      "epoch: 52,  batch step: 115, loss: 3.8264880180358887\n",
      "epoch: 52,  batch step: 116, loss: 5.65433406829834\n",
      "epoch: 52,  batch step: 117, loss: 22.07242202758789\n",
      "epoch: 52,  batch step: 118, loss: 39.396934509277344\n",
      "epoch: 52,  batch step: 119, loss: 4.5636396408081055\n",
      "epoch: 52,  batch step: 120, loss: 6.964334011077881\n",
      "epoch: 52,  batch step: 121, loss: 2.713083267211914\n",
      "epoch: 52,  batch step: 122, loss: 6.752364635467529\n",
      "epoch: 52,  batch step: 123, loss: 17.64697265625\n",
      "epoch: 52,  batch step: 124, loss: 5.930088043212891\n",
      "epoch: 52,  batch step: 125, loss: 4.186027526855469\n",
      "epoch: 52,  batch step: 126, loss: 3.4624838829040527\n",
      "epoch: 52,  batch step: 127, loss: 3.797995090484619\n",
      "epoch: 52,  batch step: 128, loss: 13.899325370788574\n",
      "epoch: 52,  batch step: 129, loss: 5.03043794631958\n",
      "epoch: 52,  batch step: 130, loss: 32.007423400878906\n",
      "epoch: 52,  batch step: 131, loss: 4.417423248291016\n",
      "epoch: 52,  batch step: 132, loss: 17.75189208984375\n",
      "epoch: 52,  batch step: 133, loss: 8.222105979919434\n",
      "epoch: 52,  batch step: 134, loss: 3.8934085369110107\n",
      "epoch: 52,  batch step: 135, loss: 3.229771375656128\n",
      "epoch: 52,  batch step: 136, loss: 6.816503047943115\n",
      "epoch: 52,  batch step: 137, loss: 13.798831939697266\n",
      "epoch: 52,  batch step: 138, loss: 47.826698303222656\n",
      "epoch: 52,  batch step: 139, loss: 49.288509368896484\n",
      "epoch: 52,  batch step: 140, loss: 82.39522552490234\n",
      "epoch: 52,  batch step: 141, loss: 2.7949893474578857\n",
      "epoch: 52,  batch step: 142, loss: 4.27018928527832\n",
      "epoch: 52,  batch step: 143, loss: 30.65109634399414\n",
      "epoch: 52,  batch step: 144, loss: 26.718746185302734\n",
      "epoch: 52,  batch step: 145, loss: 17.14334487915039\n",
      "epoch: 52,  batch step: 146, loss: 11.23197078704834\n",
      "epoch: 52,  batch step: 147, loss: 33.26105499267578\n",
      "epoch: 52,  batch step: 148, loss: 29.13875961303711\n",
      "epoch: 52,  batch step: 149, loss: 3.7903430461883545\n",
      "epoch: 52,  batch step: 150, loss: 4.221376895904541\n",
      "epoch: 52,  batch step: 151, loss: 121.59548950195312\n",
      "epoch: 52,  batch step: 152, loss: 3.6729540824890137\n",
      "epoch: 52,  batch step: 153, loss: 63.04108428955078\n",
      "epoch: 52,  batch step: 154, loss: 2.7909622192382812\n",
      "epoch: 52,  batch step: 155, loss: 4.9228925704956055\n",
      "epoch: 52,  batch step: 156, loss: 41.083370208740234\n",
      "epoch: 52,  batch step: 157, loss: 54.78734588623047\n",
      "epoch: 52,  batch step: 158, loss: 21.505430221557617\n",
      "epoch: 52,  batch step: 159, loss: 6.474822044372559\n",
      "epoch: 52,  batch step: 160, loss: 18.784034729003906\n",
      "epoch: 52,  batch step: 161, loss: 23.21233367919922\n",
      "epoch: 52,  batch step: 162, loss: 5.12180233001709\n",
      "epoch: 52,  batch step: 163, loss: 3.0143237113952637\n",
      "epoch: 52,  batch step: 164, loss: 15.329524040222168\n",
      "epoch: 52,  batch step: 165, loss: 45.984619140625\n",
      "epoch: 52,  batch step: 166, loss: 21.647388458251953\n",
      "epoch: 52,  batch step: 167, loss: 21.212623596191406\n",
      "epoch: 52,  batch step: 168, loss: 8.924924850463867\n",
      "epoch: 52,  batch step: 169, loss: 8.631429672241211\n",
      "epoch: 52,  batch step: 170, loss: 21.578094482421875\n",
      "epoch: 52,  batch step: 171, loss: 23.145017623901367\n",
      "epoch: 52,  batch step: 172, loss: 89.7694320678711\n",
      "epoch: 52,  batch step: 173, loss: 8.959848403930664\n",
      "epoch: 52,  batch step: 174, loss: 20.73764419555664\n",
      "epoch: 52,  batch step: 175, loss: 28.239059448242188\n",
      "epoch: 52,  batch step: 176, loss: 2.8060107231140137\n",
      "epoch: 52,  batch step: 177, loss: 14.271979331970215\n",
      "epoch: 52,  batch step: 178, loss: 49.78477096557617\n",
      "epoch: 52,  batch step: 179, loss: 4.896981239318848\n",
      "epoch: 52,  batch step: 180, loss: 33.64122772216797\n",
      "epoch: 52,  batch step: 181, loss: 52.41065979003906\n",
      "epoch: 52,  batch step: 182, loss: 3.1652679443359375\n",
      "epoch: 52,  batch step: 183, loss: 14.687053680419922\n",
      "epoch: 52,  batch step: 184, loss: 6.44840145111084\n",
      "epoch: 52,  batch step: 185, loss: 6.824150085449219\n",
      "epoch: 52,  batch step: 186, loss: 3.7995967864990234\n",
      "epoch: 52,  batch step: 187, loss: 4.533991813659668\n",
      "epoch: 52,  batch step: 188, loss: 2.5178658962249756\n",
      "epoch: 52,  batch step: 189, loss: 3.220158338546753\n",
      "epoch: 52,  batch step: 190, loss: 21.170841217041016\n",
      "epoch: 52,  batch step: 191, loss: 6.632268905639648\n",
      "epoch: 52,  batch step: 192, loss: 7.706695079803467\n",
      "epoch: 52,  batch step: 193, loss: 3.7500500679016113\n",
      "epoch: 52,  batch step: 194, loss: 4.559314250946045\n",
      "epoch: 52,  batch step: 195, loss: 110.1707992553711\n",
      "epoch: 52,  batch step: 196, loss: 79.35723114013672\n",
      "epoch: 52,  batch step: 197, loss: 51.601226806640625\n",
      "epoch: 52,  batch step: 198, loss: 35.791595458984375\n",
      "epoch: 52,  batch step: 199, loss: 5.511341094970703\n",
      "epoch: 52,  batch step: 200, loss: 6.630227565765381\n",
      "epoch: 52,  batch step: 201, loss: 18.862873077392578\n",
      "epoch: 52,  batch step: 202, loss: 46.38593673706055\n",
      "epoch: 52,  batch step: 203, loss: 4.921830654144287\n",
      "epoch: 52,  batch step: 204, loss: 37.41850280761719\n",
      "epoch: 52,  batch step: 205, loss: 25.284927368164062\n",
      "epoch: 52,  batch step: 206, loss: 28.989858627319336\n",
      "epoch: 52,  batch step: 207, loss: 4.571610450744629\n",
      "epoch: 52,  batch step: 208, loss: 46.44631576538086\n",
      "epoch: 52,  batch step: 209, loss: 70.99430084228516\n",
      "epoch: 52,  batch step: 210, loss: 4.803572654724121\n",
      "epoch: 52,  batch step: 211, loss: 33.94614028930664\n",
      "epoch: 52,  batch step: 212, loss: 62.02777099609375\n",
      "epoch: 52,  batch step: 213, loss: 50.501441955566406\n",
      "epoch: 52,  batch step: 214, loss: 23.8360652923584\n",
      "epoch: 52,  batch step: 215, loss: 3.525435447692871\n",
      "epoch: 52,  batch step: 216, loss: 7.459556579589844\n",
      "epoch: 52,  batch step: 217, loss: 16.823665618896484\n",
      "epoch: 52,  batch step: 218, loss: 50.18424606323242\n",
      "epoch: 52,  batch step: 219, loss: 33.33062744140625\n",
      "epoch: 52,  batch step: 220, loss: 5.262505531311035\n",
      "epoch: 52,  batch step: 221, loss: 31.044963836669922\n",
      "epoch: 52,  batch step: 222, loss: 5.917487144470215\n",
      "epoch: 52,  batch step: 223, loss: 19.02397918701172\n",
      "epoch: 52,  batch step: 224, loss: 3.6056323051452637\n",
      "epoch: 52,  batch step: 225, loss: 42.218101501464844\n",
      "epoch: 52,  batch step: 226, loss: 51.711578369140625\n",
      "epoch: 52,  batch step: 227, loss: 66.05349731445312\n",
      "epoch: 52,  batch step: 228, loss: 26.868412017822266\n",
      "epoch: 52,  batch step: 229, loss: 78.05509948730469\n",
      "epoch: 52,  batch step: 230, loss: 4.270869255065918\n",
      "epoch: 52,  batch step: 231, loss: 42.825965881347656\n",
      "epoch: 52,  batch step: 232, loss: 4.517650604248047\n",
      "epoch: 52,  batch step: 233, loss: 34.457942962646484\n",
      "epoch: 52,  batch step: 234, loss: 6.097011566162109\n",
      "epoch: 52,  batch step: 235, loss: 166.1497039794922\n",
      "epoch: 52,  batch step: 236, loss: 10.532486915588379\n",
      "epoch: 52,  batch step: 237, loss: 3.6240687370300293\n",
      "epoch: 52,  batch step: 238, loss: 7.738326072692871\n",
      "epoch: 52,  batch step: 239, loss: 5.717408657073975\n",
      "epoch: 52,  batch step: 240, loss: 4.690578937530518\n",
      "epoch: 52,  batch step: 241, loss: 25.892776489257812\n",
      "epoch: 52,  batch step: 242, loss: 13.942686080932617\n",
      "epoch: 52,  batch step: 243, loss: 3.6226367950439453\n",
      "epoch: 52,  batch step: 244, loss: 44.44050598144531\n",
      "epoch: 52,  batch step: 245, loss: 55.633384704589844\n",
      "epoch: 52,  batch step: 246, loss: 58.85399627685547\n",
      "epoch: 52,  batch step: 247, loss: 87.09843444824219\n",
      "epoch: 52,  batch step: 248, loss: 8.484100341796875\n",
      "epoch: 52,  batch step: 249, loss: 33.533748626708984\n",
      "epoch: 52,  batch step: 250, loss: 36.08328628540039\n",
      "epoch: 52,  batch step: 251, loss: 80.09132385253906\n",
      "validation error epoch  52:    tensor(67.7961, device='cuda:0')\n",
      "316\n",
      "epoch: 53,  batch step: 0, loss: 8.430380821228027\n",
      "epoch: 53,  batch step: 1, loss: 69.91214752197266\n",
      "epoch: 53,  batch step: 2, loss: 41.30063247680664\n",
      "epoch: 53,  batch step: 3, loss: 5.6487531661987305\n",
      "epoch: 53,  batch step: 4, loss: 69.09070587158203\n",
      "epoch: 53,  batch step: 5, loss: 29.550518035888672\n",
      "epoch: 53,  batch step: 6, loss: 6.527572154998779\n",
      "epoch: 53,  batch step: 7, loss: 9.620681762695312\n",
      "epoch: 53,  batch step: 8, loss: 3.0026891231536865\n",
      "epoch: 53,  batch step: 9, loss: 18.522796630859375\n",
      "epoch: 53,  batch step: 10, loss: 85.76891326904297\n",
      "epoch: 53,  batch step: 11, loss: 39.97260284423828\n",
      "epoch: 53,  batch step: 12, loss: 7.410312175750732\n",
      "epoch: 53,  batch step: 13, loss: 16.4476318359375\n",
      "epoch: 53,  batch step: 14, loss: 6.017386436462402\n",
      "epoch: 53,  batch step: 15, loss: 25.268692016601562\n",
      "epoch: 53,  batch step: 16, loss: 20.328426361083984\n",
      "epoch: 53,  batch step: 17, loss: 5.153165817260742\n",
      "epoch: 53,  batch step: 18, loss: 41.27729034423828\n",
      "epoch: 53,  batch step: 19, loss: 71.81017303466797\n",
      "epoch: 53,  batch step: 20, loss: 4.756999969482422\n",
      "epoch: 53,  batch step: 21, loss: 43.86578369140625\n",
      "epoch: 53,  batch step: 22, loss: 95.91867065429688\n",
      "epoch: 53,  batch step: 23, loss: 12.003939628601074\n",
      "epoch: 53,  batch step: 24, loss: 7.201605796813965\n",
      "epoch: 53,  batch step: 25, loss: 6.894245147705078\n",
      "epoch: 53,  batch step: 26, loss: 32.243751525878906\n",
      "epoch: 53,  batch step: 27, loss: 74.14756774902344\n",
      "epoch: 53,  batch step: 28, loss: 31.59406089782715\n",
      "epoch: 53,  batch step: 29, loss: 3.7449092864990234\n",
      "epoch: 53,  batch step: 30, loss: 6.385985374450684\n",
      "epoch: 53,  batch step: 31, loss: 38.88166427612305\n",
      "epoch: 53,  batch step: 32, loss: 2.840855360031128\n",
      "epoch: 53,  batch step: 33, loss: 6.884950160980225\n",
      "epoch: 53,  batch step: 34, loss: 20.73346710205078\n",
      "epoch: 53,  batch step: 35, loss: 8.878927230834961\n",
      "epoch: 53,  batch step: 36, loss: 4.728160381317139\n",
      "epoch: 53,  batch step: 37, loss: 26.110891342163086\n",
      "epoch: 53,  batch step: 38, loss: 11.760882377624512\n",
      "epoch: 53,  batch step: 39, loss: 4.5334601402282715\n",
      "epoch: 53,  batch step: 40, loss: 2.995360851287842\n",
      "epoch: 53,  batch step: 41, loss: 53.154991149902344\n",
      "epoch: 53,  batch step: 42, loss: 101.44902801513672\n",
      "epoch: 53,  batch step: 43, loss: 34.787899017333984\n",
      "epoch: 53,  batch step: 44, loss: 4.4765520095825195\n",
      "epoch: 53,  batch step: 45, loss: 11.031837463378906\n",
      "epoch: 53,  batch step: 46, loss: 25.515792846679688\n",
      "epoch: 53,  batch step: 47, loss: 30.822025299072266\n",
      "epoch: 53,  batch step: 48, loss: 39.091453552246094\n",
      "epoch: 53,  batch step: 49, loss: 6.583984375\n",
      "epoch: 53,  batch step: 50, loss: 3.566575050354004\n",
      "epoch: 53,  batch step: 51, loss: 19.151321411132812\n",
      "epoch: 53,  batch step: 52, loss: 32.64312744140625\n",
      "epoch: 53,  batch step: 53, loss: 2.9435667991638184\n",
      "epoch: 53,  batch step: 54, loss: 25.283283233642578\n",
      "epoch: 53,  batch step: 55, loss: 20.148801803588867\n",
      "epoch: 53,  batch step: 56, loss: 5.200191974639893\n",
      "epoch: 53,  batch step: 57, loss: 51.020416259765625\n",
      "epoch: 53,  batch step: 58, loss: 5.131308555603027\n",
      "epoch: 53,  batch step: 59, loss: 59.071495056152344\n",
      "epoch: 53,  batch step: 60, loss: 5.01143217086792\n",
      "epoch: 53,  batch step: 61, loss: 25.582332611083984\n",
      "epoch: 53,  batch step: 62, loss: 3.869683265686035\n",
      "epoch: 53,  batch step: 63, loss: 4.973870277404785\n",
      "epoch: 53,  batch step: 64, loss: 9.636942863464355\n",
      "epoch: 53,  batch step: 65, loss: 8.132014274597168\n",
      "epoch: 53,  batch step: 66, loss: 5.108818531036377\n",
      "epoch: 53,  batch step: 67, loss: 79.29158020019531\n",
      "epoch: 53,  batch step: 68, loss: 7.769631862640381\n",
      "epoch: 53,  batch step: 69, loss: 2.961865186691284\n",
      "epoch: 53,  batch step: 70, loss: 5.096658706665039\n",
      "epoch: 53,  batch step: 71, loss: 6.740836143493652\n",
      "epoch: 53,  batch step: 72, loss: 4.181873321533203\n",
      "epoch: 53,  batch step: 73, loss: 51.24021911621094\n",
      "epoch: 53,  batch step: 74, loss: 25.688732147216797\n",
      "epoch: 53,  batch step: 75, loss: 5.266323089599609\n",
      "epoch: 53,  batch step: 76, loss: 35.227020263671875\n",
      "epoch: 53,  batch step: 77, loss: 3.2807583808898926\n",
      "epoch: 53,  batch step: 78, loss: 25.9951229095459\n",
      "epoch: 53,  batch step: 79, loss: 71.47477722167969\n",
      "epoch: 53,  batch step: 80, loss: 5.3805670738220215\n",
      "epoch: 53,  batch step: 81, loss: 3.3850016593933105\n",
      "epoch: 53,  batch step: 82, loss: 37.68963623046875\n",
      "epoch: 53,  batch step: 83, loss: 5.090528964996338\n",
      "epoch: 53,  batch step: 84, loss: 70.91377258300781\n",
      "epoch: 53,  batch step: 85, loss: 76.12605285644531\n",
      "epoch: 53,  batch step: 86, loss: 141.77001953125\n",
      "epoch: 53,  batch step: 87, loss: 29.609909057617188\n",
      "epoch: 53,  batch step: 88, loss: 35.58229064941406\n",
      "epoch: 53,  batch step: 89, loss: 71.61785888671875\n",
      "epoch: 53,  batch step: 90, loss: 3.8579773902893066\n",
      "epoch: 53,  batch step: 91, loss: 66.94703674316406\n",
      "epoch: 53,  batch step: 92, loss: 7.116427421569824\n",
      "epoch: 53,  batch step: 93, loss: 5.717357635498047\n",
      "epoch: 53,  batch step: 94, loss: 14.105306625366211\n",
      "epoch: 53,  batch step: 95, loss: 5.959531307220459\n",
      "epoch: 53,  batch step: 96, loss: 63.883995056152344\n",
      "epoch: 53,  batch step: 97, loss: 55.58578109741211\n",
      "epoch: 53,  batch step: 98, loss: 33.69576644897461\n",
      "epoch: 53,  batch step: 99, loss: 13.249746322631836\n",
      "epoch: 53,  batch step: 100, loss: 4.588461875915527\n",
      "epoch: 53,  batch step: 101, loss: 6.186166286468506\n",
      "epoch: 53,  batch step: 102, loss: 4.2138447761535645\n",
      "epoch: 53,  batch step: 103, loss: 4.518453121185303\n",
      "epoch: 53,  batch step: 104, loss: 40.95471954345703\n",
      "epoch: 53,  batch step: 105, loss: 5.325671195983887\n",
      "epoch: 53,  batch step: 106, loss: 5.941260814666748\n",
      "epoch: 53,  batch step: 107, loss: 4.444714546203613\n",
      "epoch: 53,  batch step: 108, loss: 6.8967604637146\n",
      "epoch: 53,  batch step: 109, loss: 33.09027862548828\n",
      "epoch: 53,  batch step: 110, loss: 6.086719512939453\n",
      "epoch: 53,  batch step: 111, loss: 4.762246608734131\n",
      "epoch: 53,  batch step: 112, loss: 64.56014251708984\n",
      "epoch: 53,  batch step: 113, loss: 30.111082077026367\n",
      "epoch: 53,  batch step: 114, loss: 32.19593811035156\n",
      "epoch: 53,  batch step: 115, loss: 6.573136329650879\n",
      "epoch: 53,  batch step: 116, loss: 4.399339199066162\n",
      "epoch: 53,  batch step: 117, loss: 2.732983350753784\n",
      "epoch: 53,  batch step: 118, loss: 4.406972408294678\n",
      "epoch: 53,  batch step: 119, loss: 21.21467399597168\n",
      "epoch: 53,  batch step: 120, loss: 3.593601703643799\n",
      "epoch: 53,  batch step: 121, loss: 3.854398488998413\n",
      "epoch: 53,  batch step: 122, loss: 13.042404174804688\n",
      "epoch: 53,  batch step: 123, loss: 21.206863403320312\n",
      "epoch: 53,  batch step: 124, loss: 4.971855163574219\n",
      "epoch: 53,  batch step: 125, loss: 8.214372634887695\n",
      "epoch: 53,  batch step: 126, loss: 3.1318395137786865\n",
      "epoch: 53,  batch step: 127, loss: 6.484706401824951\n",
      "epoch: 53,  batch step: 128, loss: 2.236018657684326\n",
      "epoch: 53,  batch step: 129, loss: 4.852784156799316\n",
      "epoch: 53,  batch step: 130, loss: 3.582115888595581\n",
      "epoch: 53,  batch step: 131, loss: 2.884521961212158\n",
      "epoch: 53,  batch step: 132, loss: 71.84595489501953\n",
      "epoch: 53,  batch step: 133, loss: 31.465559005737305\n",
      "epoch: 53,  batch step: 134, loss: 90.40453338623047\n",
      "epoch: 53,  batch step: 135, loss: 4.8729329109191895\n",
      "epoch: 53,  batch step: 136, loss: 39.63236618041992\n",
      "epoch: 53,  batch step: 137, loss: 33.90036392211914\n",
      "epoch: 53,  batch step: 138, loss: 15.707972526550293\n",
      "epoch: 53,  batch step: 139, loss: 9.519474029541016\n",
      "epoch: 53,  batch step: 140, loss: 9.418859481811523\n",
      "epoch: 53,  batch step: 141, loss: 3.8890395164489746\n",
      "epoch: 53,  batch step: 142, loss: 9.256478309631348\n",
      "epoch: 53,  batch step: 143, loss: 4.800785064697266\n",
      "epoch: 53,  batch step: 144, loss: 7.796620845794678\n",
      "epoch: 53,  batch step: 145, loss: 4.162636756896973\n",
      "epoch: 53,  batch step: 146, loss: 3.29133677482605\n",
      "epoch: 53,  batch step: 147, loss: 44.97724151611328\n",
      "epoch: 53,  batch step: 148, loss: 9.74377727508545\n",
      "epoch: 53,  batch step: 149, loss: 38.51385498046875\n",
      "epoch: 53,  batch step: 150, loss: 19.254627227783203\n",
      "epoch: 53,  batch step: 151, loss: 76.20137023925781\n",
      "epoch: 53,  batch step: 152, loss: 6.9981689453125\n",
      "epoch: 53,  batch step: 153, loss: 5.643161773681641\n",
      "epoch: 53,  batch step: 154, loss: 6.849830627441406\n",
      "epoch: 53,  batch step: 155, loss: 42.24770736694336\n",
      "epoch: 53,  batch step: 156, loss: 3.2695460319519043\n",
      "epoch: 53,  batch step: 157, loss: 103.26420593261719\n",
      "epoch: 53,  batch step: 158, loss: 75.91334533691406\n",
      "epoch: 53,  batch step: 159, loss: 3.2049126625061035\n",
      "epoch: 53,  batch step: 160, loss: 32.97328186035156\n",
      "epoch: 53,  batch step: 161, loss: 40.319759368896484\n",
      "epoch: 53,  batch step: 162, loss: 5.691501617431641\n",
      "epoch: 53,  batch step: 163, loss: 6.724490165710449\n",
      "epoch: 53,  batch step: 164, loss: 38.08172607421875\n",
      "epoch: 53,  batch step: 165, loss: 67.80171203613281\n",
      "epoch: 53,  batch step: 166, loss: 5.615959167480469\n",
      "epoch: 53,  batch step: 167, loss: 30.062170028686523\n",
      "epoch: 53,  batch step: 168, loss: 5.436501979827881\n",
      "epoch: 53,  batch step: 169, loss: 3.247546672821045\n",
      "epoch: 53,  batch step: 170, loss: 7.397894859313965\n",
      "epoch: 53,  batch step: 171, loss: 4.99708366394043\n",
      "epoch: 53,  batch step: 172, loss: 50.2320556640625\n",
      "epoch: 53,  batch step: 173, loss: 10.939406394958496\n",
      "epoch: 53,  batch step: 174, loss: 2.0357351303100586\n",
      "epoch: 53,  batch step: 175, loss: 28.132200241088867\n",
      "epoch: 53,  batch step: 176, loss: 17.474241256713867\n",
      "epoch: 53,  batch step: 177, loss: 42.40399932861328\n",
      "epoch: 53,  batch step: 178, loss: 6.707732677459717\n",
      "epoch: 53,  batch step: 179, loss: 9.148893356323242\n",
      "epoch: 53,  batch step: 180, loss: 48.58153533935547\n",
      "epoch: 53,  batch step: 181, loss: 21.749195098876953\n",
      "epoch: 53,  batch step: 182, loss: 4.052970886230469\n",
      "epoch: 53,  batch step: 183, loss: 36.72484588623047\n",
      "epoch: 53,  batch step: 184, loss: 23.175792694091797\n",
      "epoch: 53,  batch step: 185, loss: 3.5480880737304688\n",
      "epoch: 53,  batch step: 186, loss: 5.303267478942871\n",
      "epoch: 53,  batch step: 187, loss: 10.02737045288086\n",
      "epoch: 53,  batch step: 188, loss: 73.3055419921875\n",
      "epoch: 53,  batch step: 189, loss: 6.218715190887451\n",
      "epoch: 53,  batch step: 190, loss: 38.121498107910156\n",
      "epoch: 53,  batch step: 191, loss: 30.309101104736328\n",
      "epoch: 53,  batch step: 192, loss: 86.77278900146484\n",
      "epoch: 53,  batch step: 193, loss: 6.982965469360352\n",
      "epoch: 53,  batch step: 194, loss: 6.43629789352417\n",
      "epoch: 53,  batch step: 195, loss: 21.433013916015625\n",
      "epoch: 53,  batch step: 196, loss: 4.83393669128418\n",
      "epoch: 53,  batch step: 197, loss: 18.857938766479492\n",
      "epoch: 53,  batch step: 198, loss: 15.557058334350586\n",
      "epoch: 53,  batch step: 199, loss: 44.302024841308594\n",
      "epoch: 53,  batch step: 200, loss: 4.420720100402832\n",
      "epoch: 53,  batch step: 201, loss: 19.375476837158203\n",
      "epoch: 53,  batch step: 202, loss: 18.782155990600586\n",
      "epoch: 53,  batch step: 203, loss: 14.094099044799805\n",
      "epoch: 53,  batch step: 204, loss: 15.263603210449219\n",
      "epoch: 53,  batch step: 205, loss: 72.35198211669922\n",
      "epoch: 53,  batch step: 206, loss: 33.59611511230469\n",
      "epoch: 53,  batch step: 207, loss: 3.735917091369629\n",
      "epoch: 53,  batch step: 208, loss: 87.54765319824219\n",
      "epoch: 53,  batch step: 209, loss: 3.4170894622802734\n",
      "epoch: 53,  batch step: 210, loss: 59.591033935546875\n",
      "epoch: 53,  batch step: 211, loss: 17.4281063079834\n",
      "epoch: 53,  batch step: 212, loss: 67.83094787597656\n",
      "epoch: 53,  batch step: 213, loss: 12.592367172241211\n",
      "epoch: 53,  batch step: 214, loss: 6.429038047790527\n",
      "epoch: 53,  batch step: 215, loss: 6.84623908996582\n",
      "epoch: 53,  batch step: 216, loss: 16.29559326171875\n",
      "epoch: 53,  batch step: 217, loss: 6.779414176940918\n",
      "epoch: 53,  batch step: 218, loss: 6.532772064208984\n",
      "epoch: 53,  batch step: 219, loss: 55.810482025146484\n",
      "epoch: 53,  batch step: 220, loss: 4.710812568664551\n",
      "epoch: 53,  batch step: 221, loss: 58.36796188354492\n",
      "epoch: 53,  batch step: 222, loss: 3.0354456901550293\n",
      "epoch: 53,  batch step: 223, loss: 26.061140060424805\n",
      "epoch: 53,  batch step: 224, loss: 11.094317436218262\n",
      "epoch: 53,  batch step: 225, loss: 5.135502815246582\n",
      "epoch: 53,  batch step: 226, loss: 37.91284942626953\n",
      "epoch: 53,  batch step: 227, loss: 3.1985177993774414\n",
      "epoch: 53,  batch step: 228, loss: 6.070067405700684\n",
      "epoch: 53,  batch step: 229, loss: 28.435192108154297\n",
      "epoch: 53,  batch step: 230, loss: 5.257443904876709\n",
      "epoch: 53,  batch step: 231, loss: 4.500779151916504\n",
      "epoch: 53,  batch step: 232, loss: 23.162853240966797\n",
      "epoch: 53,  batch step: 233, loss: 24.22347068786621\n",
      "epoch: 53,  batch step: 234, loss: 4.49537467956543\n",
      "epoch: 53,  batch step: 235, loss: 4.19677209854126\n",
      "epoch: 53,  batch step: 236, loss: 78.7615966796875\n",
      "epoch: 53,  batch step: 237, loss: 32.94220733642578\n",
      "epoch: 53,  batch step: 238, loss: 5.986322402954102\n",
      "epoch: 53,  batch step: 239, loss: 69.36328125\n",
      "epoch: 53,  batch step: 240, loss: 18.906604766845703\n",
      "epoch: 53,  batch step: 241, loss: 28.624374389648438\n",
      "epoch: 53,  batch step: 242, loss: 22.56794548034668\n",
      "epoch: 53,  batch step: 243, loss: 7.9292731285095215\n",
      "epoch: 53,  batch step: 244, loss: 24.13467788696289\n",
      "epoch: 53,  batch step: 245, loss: 5.673303127288818\n",
      "epoch: 53,  batch step: 246, loss: 10.127939224243164\n",
      "epoch: 53,  batch step: 247, loss: 47.1474609375\n",
      "epoch: 53,  batch step: 248, loss: 34.58189392089844\n",
      "epoch: 53,  batch step: 249, loss: 61.84953308105469\n",
      "epoch: 53,  batch step: 250, loss: 37.47357177734375\n",
      "epoch: 53,  batch step: 251, loss: 9.142088890075684\n",
      "validation error epoch  53:    tensor(69.2295, device='cuda:0')\n",
      "316\n",
      "epoch: 54,  batch step: 0, loss: 15.435310363769531\n",
      "epoch: 54,  batch step: 1, loss: 9.212700843811035\n",
      "epoch: 54,  batch step: 2, loss: 28.446043014526367\n",
      "epoch: 54,  batch step: 3, loss: 58.27478790283203\n",
      "epoch: 54,  batch step: 4, loss: 11.498457908630371\n",
      "epoch: 54,  batch step: 5, loss: 13.229137420654297\n",
      "epoch: 54,  batch step: 6, loss: 71.47273254394531\n",
      "epoch: 54,  batch step: 7, loss: 31.489219665527344\n",
      "epoch: 54,  batch step: 8, loss: 6.100344657897949\n",
      "epoch: 54,  batch step: 9, loss: 19.184741973876953\n",
      "epoch: 54,  batch step: 10, loss: 58.42301559448242\n",
      "epoch: 54,  batch step: 11, loss: 21.706050872802734\n",
      "epoch: 54,  batch step: 12, loss: 31.381683349609375\n",
      "epoch: 54,  batch step: 13, loss: 51.68709945678711\n",
      "epoch: 54,  batch step: 14, loss: 8.751269340515137\n",
      "epoch: 54,  batch step: 15, loss: 6.671206474304199\n",
      "epoch: 54,  batch step: 16, loss: 4.883070468902588\n",
      "epoch: 54,  batch step: 17, loss: 5.286752223968506\n",
      "epoch: 54,  batch step: 18, loss: 17.533214569091797\n",
      "epoch: 54,  batch step: 19, loss: 126.22835540771484\n",
      "epoch: 54,  batch step: 20, loss: 8.169086456298828\n",
      "epoch: 54,  batch step: 21, loss: 7.490715026855469\n",
      "epoch: 54,  batch step: 22, loss: 5.662522315979004\n",
      "epoch: 54,  batch step: 23, loss: 7.611320495605469\n",
      "epoch: 54,  batch step: 24, loss: 24.332576751708984\n",
      "epoch: 54,  batch step: 25, loss: 5.675652980804443\n",
      "epoch: 54,  batch step: 26, loss: 26.123022079467773\n",
      "epoch: 54,  batch step: 27, loss: 17.7938289642334\n",
      "epoch: 54,  batch step: 28, loss: 43.77241134643555\n",
      "epoch: 54,  batch step: 29, loss: 50.31983947753906\n",
      "epoch: 54,  batch step: 30, loss: 5.96619176864624\n",
      "epoch: 54,  batch step: 31, loss: 7.9902448654174805\n",
      "epoch: 54,  batch step: 32, loss: 8.424509048461914\n",
      "epoch: 54,  batch step: 33, loss: 35.065284729003906\n",
      "epoch: 54,  batch step: 34, loss: 3.5820207595825195\n",
      "epoch: 54,  batch step: 35, loss: 3.5788097381591797\n",
      "epoch: 54,  batch step: 36, loss: 5.256198883056641\n",
      "epoch: 54,  batch step: 37, loss: 3.7028892040252686\n",
      "epoch: 54,  batch step: 38, loss: 42.97392272949219\n",
      "epoch: 54,  batch step: 39, loss: 31.95394515991211\n",
      "epoch: 54,  batch step: 40, loss: 46.482749938964844\n",
      "epoch: 54,  batch step: 41, loss: 2.7995972633361816\n",
      "epoch: 54,  batch step: 42, loss: 24.191696166992188\n",
      "epoch: 54,  batch step: 43, loss: 6.233656883239746\n",
      "epoch: 54,  batch step: 44, loss: 20.804569244384766\n",
      "epoch: 54,  batch step: 45, loss: 4.269532203674316\n",
      "epoch: 54,  batch step: 46, loss: 45.65204620361328\n",
      "epoch: 54,  batch step: 47, loss: 5.618902206420898\n",
      "epoch: 54,  batch step: 48, loss: 3.2207038402557373\n",
      "epoch: 54,  batch step: 49, loss: 41.153316497802734\n",
      "epoch: 54,  batch step: 50, loss: 5.173815727233887\n",
      "epoch: 54,  batch step: 51, loss: 56.27338790893555\n",
      "epoch: 54,  batch step: 52, loss: 3.5639846324920654\n",
      "epoch: 54,  batch step: 53, loss: 55.92015075683594\n",
      "epoch: 54,  batch step: 54, loss: 27.616012573242188\n",
      "epoch: 54,  batch step: 55, loss: 33.239295959472656\n",
      "epoch: 54,  batch step: 56, loss: 13.697639465332031\n",
      "epoch: 54,  batch step: 57, loss: 10.100752830505371\n",
      "epoch: 54,  batch step: 58, loss: 3.2536869049072266\n",
      "epoch: 54,  batch step: 59, loss: 4.075124740600586\n",
      "epoch: 54,  batch step: 60, loss: 4.188534259796143\n",
      "epoch: 54,  batch step: 61, loss: 21.903724670410156\n",
      "epoch: 54,  batch step: 62, loss: 3.7779195308685303\n",
      "epoch: 54,  batch step: 63, loss: 5.517550945281982\n",
      "epoch: 54,  batch step: 64, loss: 27.66090202331543\n",
      "epoch: 54,  batch step: 65, loss: 4.890201568603516\n",
      "epoch: 54,  batch step: 66, loss: 4.005315780639648\n",
      "epoch: 54,  batch step: 67, loss: 57.40488815307617\n",
      "epoch: 54,  batch step: 68, loss: 37.52259063720703\n",
      "epoch: 54,  batch step: 69, loss: 3.4309239387512207\n",
      "epoch: 54,  batch step: 70, loss: 6.576319694519043\n",
      "epoch: 54,  batch step: 71, loss: 30.73029136657715\n",
      "epoch: 54,  batch step: 72, loss: 3.151432991027832\n",
      "epoch: 54,  batch step: 73, loss: 2.9161324501037598\n",
      "epoch: 54,  batch step: 74, loss: 4.155879497528076\n",
      "epoch: 54,  batch step: 75, loss: 4.350066184997559\n",
      "epoch: 54,  batch step: 76, loss: 10.269633293151855\n",
      "epoch: 54,  batch step: 77, loss: 35.427982330322266\n",
      "epoch: 54,  batch step: 78, loss: 61.38865661621094\n",
      "epoch: 54,  batch step: 79, loss: 4.047091007232666\n",
      "epoch: 54,  batch step: 80, loss: 3.5279932022094727\n",
      "epoch: 54,  batch step: 81, loss: 35.82393264770508\n",
      "epoch: 54,  batch step: 82, loss: 5.531192779541016\n",
      "epoch: 54,  batch step: 83, loss: 7.31702184677124\n",
      "epoch: 54,  batch step: 84, loss: 3.4861526489257812\n",
      "epoch: 54,  batch step: 85, loss: 3.110523223876953\n",
      "epoch: 54,  batch step: 86, loss: 3.896420478820801\n",
      "epoch: 54,  batch step: 87, loss: 16.744365692138672\n",
      "epoch: 54,  batch step: 88, loss: 54.89225387573242\n",
      "epoch: 54,  batch step: 89, loss: 4.087393760681152\n",
      "epoch: 54,  batch step: 90, loss: 8.061727523803711\n",
      "epoch: 54,  batch step: 91, loss: 9.735618591308594\n",
      "epoch: 54,  batch step: 92, loss: 27.47418212890625\n",
      "epoch: 54,  batch step: 93, loss: 3.784929037094116\n",
      "epoch: 54,  batch step: 94, loss: 51.19910430908203\n",
      "epoch: 54,  batch step: 95, loss: 3.6984739303588867\n",
      "epoch: 54,  batch step: 96, loss: 3.1883368492126465\n",
      "epoch: 54,  batch step: 97, loss: 4.727460861206055\n",
      "epoch: 54,  batch step: 98, loss: 3.9391419887542725\n",
      "epoch: 54,  batch step: 99, loss: 37.6594352722168\n",
      "epoch: 54,  batch step: 100, loss: 4.025395393371582\n",
      "epoch: 54,  batch step: 101, loss: 3.7899632453918457\n",
      "epoch: 54,  batch step: 102, loss: 50.82539367675781\n",
      "epoch: 54,  batch step: 103, loss: 31.11319351196289\n",
      "epoch: 54,  batch step: 104, loss: 20.413328170776367\n",
      "epoch: 54,  batch step: 105, loss: 3.870431661605835\n",
      "epoch: 54,  batch step: 106, loss: 16.314590454101562\n",
      "epoch: 54,  batch step: 107, loss: 3.754995346069336\n",
      "epoch: 54,  batch step: 108, loss: 27.33709716796875\n",
      "epoch: 54,  batch step: 109, loss: 15.533958435058594\n",
      "epoch: 54,  batch step: 110, loss: 49.38988494873047\n",
      "epoch: 54,  batch step: 111, loss: 3.037071704864502\n",
      "epoch: 54,  batch step: 112, loss: 3.796691417694092\n",
      "epoch: 54,  batch step: 113, loss: 29.486867904663086\n",
      "epoch: 54,  batch step: 114, loss: 30.43832015991211\n",
      "epoch: 54,  batch step: 115, loss: 29.022655487060547\n",
      "epoch: 54,  batch step: 116, loss: 33.58708572387695\n",
      "epoch: 54,  batch step: 117, loss: 7.267618179321289\n",
      "epoch: 54,  batch step: 118, loss: 2.7480616569519043\n",
      "epoch: 54,  batch step: 119, loss: 3.6225104331970215\n",
      "epoch: 54,  batch step: 120, loss: 19.428773880004883\n",
      "epoch: 54,  batch step: 121, loss: 6.720396995544434\n",
      "epoch: 54,  batch step: 122, loss: 3.189849376678467\n",
      "epoch: 54,  batch step: 123, loss: 4.230031490325928\n",
      "epoch: 54,  batch step: 124, loss: 33.953857421875\n",
      "epoch: 54,  batch step: 125, loss: 5.9550886154174805\n",
      "epoch: 54,  batch step: 126, loss: 4.590710639953613\n",
      "epoch: 54,  batch step: 127, loss: 48.22446823120117\n",
      "epoch: 54,  batch step: 128, loss: 5.6376495361328125\n",
      "epoch: 54,  batch step: 129, loss: 4.344535827636719\n",
      "epoch: 54,  batch step: 130, loss: 20.492490768432617\n",
      "epoch: 54,  batch step: 131, loss: 58.43130111694336\n",
      "epoch: 54,  batch step: 132, loss: 23.053295135498047\n",
      "epoch: 54,  batch step: 133, loss: 19.29958724975586\n",
      "epoch: 54,  batch step: 134, loss: 8.032275199890137\n",
      "epoch: 54,  batch step: 135, loss: 6.864873886108398\n",
      "epoch: 54,  batch step: 136, loss: 4.307482719421387\n",
      "epoch: 54,  batch step: 137, loss: 2.9166629314422607\n",
      "epoch: 54,  batch step: 138, loss: 2.504970073699951\n",
      "epoch: 54,  batch step: 139, loss: 25.50927734375\n",
      "epoch: 54,  batch step: 140, loss: 4.9005255699157715\n",
      "epoch: 54,  batch step: 141, loss: 12.96899700164795\n",
      "epoch: 54,  batch step: 142, loss: 5.126086235046387\n",
      "epoch: 54,  batch step: 143, loss: 19.957622528076172\n",
      "epoch: 54,  batch step: 144, loss: 35.83403778076172\n",
      "epoch: 54,  batch step: 145, loss: 36.19276428222656\n",
      "epoch: 54,  batch step: 146, loss: 11.167620658874512\n",
      "epoch: 54,  batch step: 147, loss: 2.7521281242370605\n",
      "epoch: 54,  batch step: 148, loss: 17.215370178222656\n",
      "epoch: 54,  batch step: 149, loss: 21.438100814819336\n",
      "epoch: 54,  batch step: 150, loss: 2.6462132930755615\n",
      "epoch: 54,  batch step: 151, loss: 50.33439254760742\n",
      "epoch: 54,  batch step: 152, loss: 156.05796813964844\n",
      "epoch: 54,  batch step: 153, loss: 4.503686904907227\n",
      "epoch: 54,  batch step: 154, loss: 63.044097900390625\n",
      "epoch: 54,  batch step: 155, loss: 35.52259826660156\n",
      "epoch: 54,  batch step: 156, loss: 5.482074737548828\n",
      "epoch: 54,  batch step: 157, loss: 9.545315742492676\n",
      "epoch: 54,  batch step: 158, loss: 81.77487182617188\n",
      "epoch: 54,  batch step: 159, loss: 44.350765228271484\n",
      "epoch: 54,  batch step: 160, loss: 82.26239776611328\n",
      "epoch: 54,  batch step: 161, loss: 37.50358581542969\n",
      "epoch: 54,  batch step: 162, loss: 10.321528434753418\n",
      "epoch: 54,  batch step: 163, loss: 45.326961517333984\n",
      "epoch: 54,  batch step: 164, loss: 32.66887664794922\n",
      "epoch: 54,  batch step: 165, loss: 51.73969650268555\n",
      "epoch: 54,  batch step: 166, loss: 52.79210662841797\n",
      "epoch: 54,  batch step: 167, loss: 20.10072898864746\n",
      "epoch: 54,  batch step: 168, loss: 51.73155975341797\n",
      "epoch: 54,  batch step: 169, loss: 5.724610805511475\n",
      "epoch: 54,  batch step: 170, loss: 5.355042457580566\n",
      "epoch: 54,  batch step: 171, loss: 4.371469497680664\n",
      "epoch: 54,  batch step: 172, loss: 9.779356002807617\n",
      "epoch: 54,  batch step: 173, loss: 5.766988277435303\n",
      "epoch: 54,  batch step: 174, loss: 36.49777603149414\n",
      "epoch: 54,  batch step: 175, loss: 5.487916469573975\n",
      "epoch: 54,  batch step: 176, loss: 4.23885440826416\n",
      "epoch: 54,  batch step: 177, loss: 11.220661163330078\n",
      "epoch: 54,  batch step: 178, loss: 4.742042541503906\n",
      "epoch: 54,  batch step: 179, loss: 4.212871551513672\n",
      "epoch: 54,  batch step: 180, loss: 4.360633373260498\n",
      "epoch: 54,  batch step: 181, loss: 41.675514221191406\n",
      "epoch: 54,  batch step: 182, loss: 2.822504997253418\n",
      "epoch: 54,  batch step: 183, loss: 16.949359893798828\n",
      "epoch: 54,  batch step: 184, loss: 29.84583282470703\n",
      "epoch: 54,  batch step: 185, loss: 2.927945137023926\n",
      "epoch: 54,  batch step: 186, loss: 22.628332138061523\n",
      "epoch: 54,  batch step: 187, loss: 7.0951457023620605\n",
      "epoch: 54,  batch step: 188, loss: 9.370826721191406\n",
      "epoch: 54,  batch step: 189, loss: 16.220478057861328\n",
      "epoch: 54,  batch step: 190, loss: 5.549341201782227\n",
      "epoch: 54,  batch step: 191, loss: 8.96807861328125\n",
      "epoch: 54,  batch step: 192, loss: 17.228740692138672\n",
      "epoch: 54,  batch step: 193, loss: 10.936603546142578\n",
      "epoch: 54,  batch step: 194, loss: 3.164029836654663\n",
      "epoch: 54,  batch step: 195, loss: 3.2582201957702637\n",
      "epoch: 54,  batch step: 196, loss: 19.21053695678711\n",
      "epoch: 54,  batch step: 197, loss: 6.129937171936035\n",
      "epoch: 54,  batch step: 198, loss: 32.63597869873047\n",
      "epoch: 54,  batch step: 199, loss: 3.0786819458007812\n",
      "epoch: 54,  batch step: 200, loss: 3.402174711227417\n",
      "epoch: 54,  batch step: 201, loss: 69.88485717773438\n",
      "epoch: 54,  batch step: 202, loss: 16.95065689086914\n",
      "epoch: 54,  batch step: 203, loss: 3.0812854766845703\n",
      "epoch: 54,  batch step: 204, loss: 5.578746795654297\n",
      "epoch: 54,  batch step: 205, loss: 12.082446098327637\n",
      "epoch: 54,  batch step: 206, loss: 26.703086853027344\n",
      "epoch: 54,  batch step: 207, loss: 3.2818074226379395\n",
      "epoch: 54,  batch step: 208, loss: 10.161968231201172\n",
      "epoch: 54,  batch step: 209, loss: 5.552791595458984\n",
      "epoch: 54,  batch step: 210, loss: 41.63526153564453\n",
      "epoch: 54,  batch step: 211, loss: 4.256618022918701\n",
      "epoch: 54,  batch step: 212, loss: 4.6276655197143555\n",
      "epoch: 54,  batch step: 213, loss: 17.678207397460938\n",
      "epoch: 54,  batch step: 214, loss: 4.882680416107178\n",
      "epoch: 54,  batch step: 215, loss: 23.838272094726562\n",
      "epoch: 54,  batch step: 216, loss: 34.88739013671875\n",
      "epoch: 54,  batch step: 217, loss: 56.158058166503906\n",
      "epoch: 54,  batch step: 218, loss: 46.88172912597656\n",
      "epoch: 54,  batch step: 219, loss: 23.200176239013672\n",
      "epoch: 54,  batch step: 220, loss: 12.694656372070312\n",
      "epoch: 54,  batch step: 221, loss: 26.15709686279297\n",
      "epoch: 54,  batch step: 222, loss: 3.514270782470703\n",
      "epoch: 54,  batch step: 223, loss: 23.261123657226562\n",
      "epoch: 54,  batch step: 224, loss: 3.808138608932495\n",
      "epoch: 54,  batch step: 225, loss: 3.334350109100342\n",
      "epoch: 54,  batch step: 226, loss: 19.198238372802734\n",
      "epoch: 54,  batch step: 227, loss: 4.919229507446289\n",
      "epoch: 54,  batch step: 228, loss: 9.912459373474121\n",
      "epoch: 54,  batch step: 229, loss: 3.8158111572265625\n",
      "epoch: 54,  batch step: 230, loss: 9.639226913452148\n",
      "epoch: 54,  batch step: 231, loss: 2.149949312210083\n",
      "epoch: 54,  batch step: 232, loss: 5.696380138397217\n",
      "epoch: 54,  batch step: 233, loss: 22.723419189453125\n",
      "epoch: 54,  batch step: 234, loss: 26.63714599609375\n",
      "epoch: 54,  batch step: 235, loss: 65.8734130859375\n",
      "epoch: 54,  batch step: 236, loss: 60.844696044921875\n",
      "epoch: 54,  batch step: 237, loss: 46.86357116699219\n",
      "epoch: 54,  batch step: 238, loss: 26.894689559936523\n",
      "epoch: 54,  batch step: 239, loss: 5.111098289489746\n",
      "epoch: 54,  batch step: 240, loss: 31.633743286132812\n",
      "epoch: 54,  batch step: 241, loss: 84.23243713378906\n",
      "epoch: 54,  batch step: 242, loss: 27.005191802978516\n",
      "epoch: 54,  batch step: 243, loss: 5.315964698791504\n",
      "epoch: 54,  batch step: 244, loss: 3.939990997314453\n",
      "epoch: 54,  batch step: 245, loss: 39.81006622314453\n",
      "epoch: 54,  batch step: 246, loss: 28.797622680664062\n",
      "epoch: 54,  batch step: 247, loss: 2.769033908843994\n",
      "epoch: 54,  batch step: 248, loss: 5.510207176208496\n",
      "epoch: 54,  batch step: 249, loss: 4.637887001037598\n",
      "epoch: 54,  batch step: 250, loss: 39.45954895019531\n",
      "epoch: 54,  batch step: 251, loss: 55.59900665283203\n",
      "validation error epoch  54:    tensor(68.8053, device='cuda:0')\n",
      "316\n",
      "epoch: 55,  batch step: 0, loss: 51.08655548095703\n",
      "epoch: 55,  batch step: 1, loss: 22.7180233001709\n",
      "epoch: 55,  batch step: 2, loss: 21.489070892333984\n",
      "epoch: 55,  batch step: 3, loss: 15.204218864440918\n",
      "epoch: 55,  batch step: 4, loss: 6.121993064880371\n",
      "epoch: 55,  batch step: 5, loss: 138.08956909179688\n",
      "epoch: 55,  batch step: 6, loss: 51.98619079589844\n",
      "epoch: 55,  batch step: 7, loss: 3.288972854614258\n",
      "epoch: 55,  batch step: 8, loss: 14.142057418823242\n",
      "epoch: 55,  batch step: 9, loss: 3.297210454940796\n",
      "epoch: 55,  batch step: 10, loss: 3.914135456085205\n",
      "epoch: 55,  batch step: 11, loss: 50.087005615234375\n",
      "epoch: 55,  batch step: 12, loss: 47.705535888671875\n",
      "epoch: 55,  batch step: 13, loss: 6.131465911865234\n",
      "epoch: 55,  batch step: 14, loss: 52.847801208496094\n",
      "epoch: 55,  batch step: 15, loss: 31.998212814331055\n",
      "epoch: 55,  batch step: 16, loss: 6.260008811950684\n",
      "epoch: 55,  batch step: 17, loss: 3.185758352279663\n",
      "epoch: 55,  batch step: 18, loss: 6.268912315368652\n",
      "epoch: 55,  batch step: 19, loss: 63.27265930175781\n",
      "epoch: 55,  batch step: 20, loss: 3.457674741744995\n",
      "epoch: 55,  batch step: 21, loss: 4.46920919418335\n",
      "epoch: 55,  batch step: 22, loss: 10.229262351989746\n",
      "epoch: 55,  batch step: 23, loss: 3.8895087242126465\n",
      "epoch: 55,  batch step: 24, loss: 11.103869438171387\n",
      "epoch: 55,  batch step: 25, loss: 8.62537670135498\n",
      "epoch: 55,  batch step: 26, loss: 5.954925060272217\n",
      "epoch: 55,  batch step: 27, loss: 5.266990661621094\n",
      "epoch: 55,  batch step: 28, loss: 52.32557678222656\n",
      "epoch: 55,  batch step: 29, loss: 5.021367073059082\n",
      "epoch: 55,  batch step: 30, loss: 5.455672264099121\n",
      "epoch: 55,  batch step: 31, loss: 2.623067855834961\n",
      "epoch: 55,  batch step: 32, loss: 49.56586837768555\n",
      "epoch: 55,  batch step: 33, loss: 4.742551803588867\n",
      "epoch: 55,  batch step: 34, loss: 94.0122299194336\n",
      "epoch: 55,  batch step: 35, loss: 22.799976348876953\n",
      "epoch: 55,  batch step: 36, loss: 2.1469297409057617\n",
      "epoch: 55,  batch step: 37, loss: 18.408782958984375\n",
      "epoch: 55,  batch step: 38, loss: 20.445301055908203\n",
      "epoch: 55,  batch step: 39, loss: 3.736457347869873\n",
      "epoch: 55,  batch step: 40, loss: 13.935369491577148\n",
      "epoch: 55,  batch step: 41, loss: 25.915000915527344\n",
      "epoch: 55,  batch step: 42, loss: 4.155512809753418\n",
      "epoch: 55,  batch step: 43, loss: 3.3466644287109375\n",
      "epoch: 55,  batch step: 44, loss: 2.819606304168701\n",
      "epoch: 55,  batch step: 45, loss: 8.824853897094727\n",
      "epoch: 55,  batch step: 46, loss: 28.943565368652344\n",
      "epoch: 55,  batch step: 47, loss: 23.325210571289062\n",
      "epoch: 55,  batch step: 48, loss: 21.023561477661133\n",
      "epoch: 55,  batch step: 49, loss: 3.8372628688812256\n",
      "epoch: 55,  batch step: 50, loss: 2.9323928356170654\n",
      "epoch: 55,  batch step: 51, loss: 6.724048614501953\n",
      "epoch: 55,  batch step: 52, loss: 5.731954574584961\n",
      "epoch: 55,  batch step: 53, loss: 53.29651641845703\n",
      "epoch: 55,  batch step: 54, loss: 5.155533790588379\n",
      "epoch: 55,  batch step: 55, loss: 5.081172943115234\n",
      "epoch: 55,  batch step: 56, loss: 73.44117736816406\n",
      "epoch: 55,  batch step: 57, loss: 14.92111873626709\n",
      "epoch: 55,  batch step: 58, loss: 11.366976737976074\n",
      "epoch: 55,  batch step: 59, loss: 3.8575332164764404\n",
      "epoch: 55,  batch step: 60, loss: 10.040374755859375\n",
      "epoch: 55,  batch step: 61, loss: 2.6154134273529053\n",
      "epoch: 55,  batch step: 62, loss: 48.63166809082031\n",
      "epoch: 55,  batch step: 63, loss: 18.389619827270508\n",
      "epoch: 55,  batch step: 64, loss: 3.2188422679901123\n",
      "epoch: 55,  batch step: 65, loss: 6.955018997192383\n",
      "epoch: 55,  batch step: 66, loss: 2.3191604614257812\n",
      "epoch: 55,  batch step: 67, loss: 35.64215087890625\n",
      "epoch: 55,  batch step: 68, loss: 3.3051910400390625\n",
      "epoch: 55,  batch step: 69, loss: 42.47526550292969\n",
      "epoch: 55,  batch step: 70, loss: 18.186588287353516\n",
      "epoch: 55,  batch step: 71, loss: 4.610830307006836\n",
      "epoch: 55,  batch step: 72, loss: 18.51004409790039\n",
      "epoch: 55,  batch step: 73, loss: 38.38217544555664\n",
      "epoch: 55,  batch step: 74, loss: 4.413764476776123\n",
      "epoch: 55,  batch step: 75, loss: 29.613388061523438\n",
      "epoch: 55,  batch step: 76, loss: 4.493846893310547\n",
      "epoch: 55,  batch step: 77, loss: 3.071380138397217\n",
      "epoch: 55,  batch step: 78, loss: 12.472904205322266\n",
      "epoch: 55,  batch step: 79, loss: 24.06148910522461\n",
      "epoch: 55,  batch step: 80, loss: 4.224468231201172\n",
      "epoch: 55,  batch step: 81, loss: 26.655492782592773\n",
      "epoch: 55,  batch step: 82, loss: 32.50740432739258\n",
      "epoch: 55,  batch step: 83, loss: 20.553485870361328\n",
      "epoch: 55,  batch step: 84, loss: 28.26259422302246\n",
      "epoch: 55,  batch step: 85, loss: 4.591651439666748\n",
      "epoch: 55,  batch step: 86, loss: 18.332958221435547\n",
      "epoch: 55,  batch step: 87, loss: 56.52590560913086\n",
      "epoch: 55,  batch step: 88, loss: 46.41619110107422\n",
      "epoch: 55,  batch step: 89, loss: 18.817691802978516\n",
      "epoch: 55,  batch step: 90, loss: 62.501991271972656\n",
      "epoch: 55,  batch step: 91, loss: 20.722454071044922\n",
      "epoch: 55,  batch step: 92, loss: 2.4858016967773438\n",
      "epoch: 55,  batch step: 93, loss: 103.89913177490234\n",
      "epoch: 55,  batch step: 94, loss: 15.883781433105469\n",
      "epoch: 55,  batch step: 95, loss: 2.5455589294433594\n",
      "epoch: 55,  batch step: 96, loss: 7.688485622406006\n",
      "epoch: 55,  batch step: 97, loss: 26.337026596069336\n",
      "epoch: 55,  batch step: 98, loss: 3.068356990814209\n",
      "epoch: 55,  batch step: 99, loss: 4.065278053283691\n",
      "epoch: 55,  batch step: 100, loss: 57.506805419921875\n",
      "epoch: 55,  batch step: 101, loss: 8.227458000183105\n",
      "epoch: 55,  batch step: 102, loss: 15.633517265319824\n",
      "epoch: 55,  batch step: 103, loss: 3.5086588859558105\n",
      "epoch: 55,  batch step: 104, loss: 42.82928466796875\n",
      "epoch: 55,  batch step: 105, loss: 29.919803619384766\n",
      "epoch: 55,  batch step: 106, loss: 20.267648696899414\n",
      "epoch: 55,  batch step: 107, loss: 3.7010579109191895\n",
      "epoch: 55,  batch step: 108, loss: 3.7841508388519287\n",
      "epoch: 55,  batch step: 109, loss: 56.726585388183594\n",
      "epoch: 55,  batch step: 110, loss: 20.94757080078125\n",
      "epoch: 55,  batch step: 111, loss: 33.047447204589844\n",
      "epoch: 55,  batch step: 112, loss: 17.099679946899414\n",
      "epoch: 55,  batch step: 113, loss: 6.541667938232422\n",
      "epoch: 55,  batch step: 114, loss: 3.827116012573242\n",
      "epoch: 55,  batch step: 115, loss: 21.619277954101562\n",
      "epoch: 55,  batch step: 116, loss: 3.2900073528289795\n",
      "epoch: 55,  batch step: 117, loss: 29.15632438659668\n",
      "epoch: 55,  batch step: 118, loss: 11.970332145690918\n",
      "epoch: 55,  batch step: 119, loss: 4.379236221313477\n",
      "epoch: 55,  batch step: 120, loss: 8.908295631408691\n",
      "epoch: 55,  batch step: 121, loss: 5.881181240081787\n",
      "epoch: 55,  batch step: 122, loss: 4.7165632247924805\n",
      "epoch: 55,  batch step: 123, loss: 19.33570098876953\n",
      "epoch: 55,  batch step: 124, loss: 3.6050782203674316\n",
      "epoch: 55,  batch step: 125, loss: 7.089152812957764\n",
      "epoch: 55,  batch step: 126, loss: 4.851004123687744\n",
      "epoch: 55,  batch step: 127, loss: 4.720568656921387\n",
      "epoch: 55,  batch step: 128, loss: 6.214761257171631\n",
      "epoch: 55,  batch step: 129, loss: 7.0347394943237305\n",
      "epoch: 55,  batch step: 130, loss: 34.79481506347656\n",
      "epoch: 55,  batch step: 131, loss: 23.568464279174805\n",
      "epoch: 55,  batch step: 132, loss: 2.633324146270752\n",
      "epoch: 55,  batch step: 133, loss: 4.278875350952148\n",
      "epoch: 55,  batch step: 134, loss: 3.674694776535034\n",
      "epoch: 55,  batch step: 135, loss: 6.1952972412109375\n",
      "epoch: 55,  batch step: 136, loss: 58.14674377441406\n",
      "epoch: 55,  batch step: 137, loss: 4.684518814086914\n",
      "epoch: 55,  batch step: 138, loss: 77.13568115234375\n",
      "epoch: 55,  batch step: 139, loss: 8.828597068786621\n",
      "epoch: 55,  batch step: 140, loss: 3.3341712951660156\n",
      "epoch: 55,  batch step: 141, loss: 36.104339599609375\n",
      "epoch: 55,  batch step: 142, loss: 5.036844730377197\n",
      "epoch: 55,  batch step: 143, loss: 21.915145874023438\n",
      "epoch: 55,  batch step: 144, loss: 19.554292678833008\n",
      "epoch: 55,  batch step: 145, loss: 4.633294105529785\n",
      "epoch: 55,  batch step: 146, loss: 31.737470626831055\n",
      "epoch: 55,  batch step: 147, loss: 57.400482177734375\n",
      "epoch: 55,  batch step: 148, loss: 37.454315185546875\n",
      "epoch: 55,  batch step: 149, loss: 16.501617431640625\n",
      "epoch: 55,  batch step: 150, loss: 5.244787216186523\n",
      "epoch: 55,  batch step: 151, loss: 20.090145111083984\n",
      "epoch: 55,  batch step: 152, loss: 5.842849254608154\n",
      "epoch: 55,  batch step: 153, loss: 4.11374044418335\n",
      "epoch: 55,  batch step: 154, loss: 71.9161376953125\n",
      "epoch: 55,  batch step: 155, loss: 22.779117584228516\n",
      "epoch: 55,  batch step: 156, loss: 48.28913497924805\n",
      "epoch: 55,  batch step: 157, loss: 2.993394613265991\n",
      "epoch: 55,  batch step: 158, loss: 3.13053035736084\n",
      "epoch: 55,  batch step: 159, loss: 60.01641082763672\n",
      "epoch: 55,  batch step: 160, loss: 13.666221618652344\n",
      "epoch: 55,  batch step: 161, loss: 3.051192283630371\n",
      "epoch: 55,  batch step: 162, loss: 3.489328384399414\n",
      "epoch: 55,  batch step: 163, loss: 3.253279447555542\n",
      "epoch: 55,  batch step: 164, loss: 3.1292853355407715\n",
      "epoch: 55,  batch step: 165, loss: 52.38551330566406\n",
      "epoch: 55,  batch step: 166, loss: 21.611774444580078\n",
      "epoch: 55,  batch step: 167, loss: 66.18912506103516\n",
      "epoch: 55,  batch step: 168, loss: 30.62129783630371\n",
      "epoch: 55,  batch step: 169, loss: 3.8298909664154053\n",
      "epoch: 55,  batch step: 170, loss: 22.477066040039062\n",
      "epoch: 55,  batch step: 171, loss: 7.353236675262451\n",
      "epoch: 55,  batch step: 172, loss: 3.7462048530578613\n",
      "epoch: 55,  batch step: 173, loss: 5.562716484069824\n",
      "epoch: 55,  batch step: 174, loss: 6.372941017150879\n",
      "epoch: 55,  batch step: 175, loss: 4.146892070770264\n",
      "epoch: 55,  batch step: 176, loss: 4.987002372741699\n",
      "epoch: 55,  batch step: 177, loss: 12.837766647338867\n",
      "epoch: 55,  batch step: 178, loss: 5.591808319091797\n",
      "epoch: 55,  batch step: 179, loss: 3.6660306453704834\n",
      "epoch: 55,  batch step: 180, loss: 10.642629623413086\n",
      "epoch: 55,  batch step: 181, loss: 14.954276084899902\n",
      "epoch: 55,  batch step: 182, loss: 4.597986221313477\n",
      "epoch: 55,  batch step: 183, loss: 26.685596466064453\n",
      "epoch: 55,  batch step: 184, loss: 6.523791313171387\n",
      "epoch: 55,  batch step: 185, loss: 37.65141296386719\n",
      "epoch: 55,  batch step: 186, loss: 47.38193893432617\n",
      "epoch: 55,  batch step: 187, loss: 7.488752365112305\n",
      "epoch: 55,  batch step: 188, loss: 17.8485107421875\n",
      "epoch: 55,  batch step: 189, loss: 3.851877212524414\n",
      "epoch: 55,  batch step: 190, loss: 14.029487609863281\n",
      "epoch: 55,  batch step: 191, loss: 7.477667808532715\n",
      "epoch: 55,  batch step: 192, loss: 27.056066513061523\n",
      "epoch: 55,  batch step: 193, loss: 2.109677791595459\n",
      "epoch: 55,  batch step: 194, loss: 39.284183502197266\n",
      "epoch: 55,  batch step: 195, loss: 70.2061538696289\n",
      "epoch: 55,  batch step: 196, loss: 39.68909454345703\n",
      "epoch: 55,  batch step: 197, loss: 21.226593017578125\n",
      "epoch: 55,  batch step: 198, loss: 5.038050651550293\n",
      "epoch: 55,  batch step: 199, loss: 4.588175296783447\n",
      "epoch: 55,  batch step: 200, loss: 6.510489463806152\n",
      "epoch: 55,  batch step: 201, loss: 25.63264274597168\n",
      "epoch: 55,  batch step: 202, loss: 62.131256103515625\n",
      "epoch: 55,  batch step: 203, loss: 3.3822028636932373\n",
      "epoch: 55,  batch step: 204, loss: 4.750338554382324\n",
      "epoch: 55,  batch step: 205, loss: 6.788069725036621\n",
      "epoch: 55,  batch step: 206, loss: 21.033878326416016\n",
      "epoch: 55,  batch step: 207, loss: 17.291526794433594\n",
      "epoch: 55,  batch step: 208, loss: 17.460674285888672\n",
      "epoch: 55,  batch step: 209, loss: 4.964555263519287\n",
      "epoch: 55,  batch step: 210, loss: 34.84697723388672\n",
      "epoch: 55,  batch step: 211, loss: 31.3367919921875\n",
      "epoch: 55,  batch step: 212, loss: 3.376034736633301\n",
      "epoch: 55,  batch step: 213, loss: 4.638592720031738\n",
      "epoch: 55,  batch step: 214, loss: 5.26060152053833\n",
      "epoch: 55,  batch step: 215, loss: 25.466228485107422\n",
      "epoch: 55,  batch step: 216, loss: 30.411733627319336\n",
      "epoch: 55,  batch step: 217, loss: 16.112876892089844\n",
      "epoch: 55,  batch step: 218, loss: 9.681095123291016\n",
      "epoch: 55,  batch step: 219, loss: 29.307661056518555\n",
      "epoch: 55,  batch step: 220, loss: 3.2320456504821777\n",
      "epoch: 55,  batch step: 221, loss: 62.0970573425293\n",
      "epoch: 55,  batch step: 222, loss: 3.19651198387146\n",
      "epoch: 55,  batch step: 223, loss: 39.14400863647461\n",
      "epoch: 55,  batch step: 224, loss: 3.824003219604492\n",
      "epoch: 55,  batch step: 225, loss: 72.19850158691406\n",
      "epoch: 55,  batch step: 226, loss: 16.144325256347656\n",
      "epoch: 55,  batch step: 227, loss: 15.898348808288574\n",
      "epoch: 55,  batch step: 228, loss: 65.22087097167969\n",
      "epoch: 55,  batch step: 229, loss: 10.66638469696045\n",
      "epoch: 55,  batch step: 230, loss: 4.407605171203613\n",
      "epoch: 55,  batch step: 231, loss: 30.8968505859375\n",
      "epoch: 55,  batch step: 232, loss: 10.250677108764648\n",
      "epoch: 55,  batch step: 233, loss: 9.409866333007812\n",
      "epoch: 55,  batch step: 234, loss: 34.01332473754883\n",
      "epoch: 55,  batch step: 235, loss: 22.581668853759766\n",
      "epoch: 55,  batch step: 236, loss: 4.138317584991455\n",
      "epoch: 55,  batch step: 237, loss: 6.019701957702637\n",
      "epoch: 55,  batch step: 238, loss: 7.025725364685059\n",
      "epoch: 55,  batch step: 239, loss: 16.088258743286133\n",
      "epoch: 55,  batch step: 240, loss: 13.129794120788574\n",
      "epoch: 55,  batch step: 241, loss: 4.420092582702637\n",
      "epoch: 55,  batch step: 242, loss: 3.9934821128845215\n",
      "epoch: 55,  batch step: 243, loss: 29.700084686279297\n",
      "epoch: 55,  batch step: 244, loss: 44.71369934082031\n",
      "epoch: 55,  batch step: 245, loss: 62.3243408203125\n",
      "epoch: 55,  batch step: 246, loss: 21.230838775634766\n",
      "epoch: 55,  batch step: 247, loss: 2.1934566497802734\n",
      "epoch: 55,  batch step: 248, loss: 3.218841552734375\n",
      "epoch: 55,  batch step: 249, loss: 3.0602567195892334\n",
      "epoch: 55,  batch step: 250, loss: 31.00064468383789\n",
      "epoch: 55,  batch step: 251, loss: 19.63090705871582\n",
      "validation error epoch  55:    tensor(66.7443, device='cuda:0')\n",
      "316\n",
      "epoch: 56,  batch step: 0, loss: 3.8028578758239746\n",
      "epoch: 56,  batch step: 1, loss: 14.155200958251953\n",
      "epoch: 56,  batch step: 2, loss: 5.560101509094238\n",
      "epoch: 56,  batch step: 3, loss: 34.89427185058594\n",
      "epoch: 56,  batch step: 4, loss: 20.489839553833008\n",
      "epoch: 56,  batch step: 5, loss: 14.102317810058594\n",
      "epoch: 56,  batch step: 6, loss: 36.85143280029297\n",
      "epoch: 56,  batch step: 7, loss: 52.38663864135742\n",
      "epoch: 56,  batch step: 8, loss: 3.96405291557312\n",
      "epoch: 56,  batch step: 9, loss: 9.686563491821289\n",
      "epoch: 56,  batch step: 10, loss: 12.187057495117188\n",
      "epoch: 56,  batch step: 11, loss: 5.093901634216309\n",
      "epoch: 56,  batch step: 12, loss: 8.94200325012207\n",
      "epoch: 56,  batch step: 13, loss: 23.062374114990234\n",
      "epoch: 56,  batch step: 14, loss: 7.503304958343506\n",
      "epoch: 56,  batch step: 15, loss: 55.623146057128906\n",
      "epoch: 56,  batch step: 16, loss: 2.3371264934539795\n",
      "epoch: 56,  batch step: 17, loss: 7.113776206970215\n",
      "epoch: 56,  batch step: 18, loss: 48.40839767456055\n",
      "epoch: 56,  batch step: 19, loss: 38.22293472290039\n",
      "epoch: 56,  batch step: 20, loss: 9.359429359436035\n",
      "epoch: 56,  batch step: 21, loss: 32.450523376464844\n",
      "epoch: 56,  batch step: 22, loss: 15.064550399780273\n",
      "epoch: 56,  batch step: 23, loss: 4.330620765686035\n",
      "epoch: 56,  batch step: 24, loss: 4.471116542816162\n",
      "epoch: 56,  batch step: 25, loss: 4.169151782989502\n",
      "epoch: 56,  batch step: 26, loss: 28.062833786010742\n",
      "epoch: 56,  batch step: 27, loss: 3.240853786468506\n",
      "epoch: 56,  batch step: 28, loss: 4.682241916656494\n",
      "epoch: 56,  batch step: 29, loss: 14.498198509216309\n",
      "epoch: 56,  batch step: 30, loss: 65.2894287109375\n",
      "epoch: 56,  batch step: 31, loss: 30.082500457763672\n",
      "epoch: 56,  batch step: 32, loss: 3.5619468688964844\n",
      "epoch: 56,  batch step: 33, loss: 3.411311626434326\n",
      "epoch: 56,  batch step: 34, loss: 16.636991500854492\n",
      "epoch: 56,  batch step: 35, loss: 42.303985595703125\n",
      "epoch: 56,  batch step: 36, loss: 42.587974548339844\n",
      "epoch: 56,  batch step: 37, loss: 19.62056541442871\n",
      "epoch: 56,  batch step: 38, loss: 4.21449089050293\n",
      "epoch: 56,  batch step: 39, loss: 3.435105085372925\n",
      "epoch: 56,  batch step: 40, loss: 25.94470977783203\n",
      "epoch: 56,  batch step: 41, loss: 34.34976577758789\n",
      "epoch: 56,  batch step: 42, loss: 6.53842830657959\n",
      "epoch: 56,  batch step: 43, loss: 7.476577281951904\n",
      "epoch: 56,  batch step: 44, loss: 4.044231414794922\n",
      "epoch: 56,  batch step: 45, loss: 2.4166858196258545\n",
      "epoch: 56,  batch step: 46, loss: 7.306046962738037\n",
      "epoch: 56,  batch step: 47, loss: 84.08504486083984\n",
      "epoch: 56,  batch step: 48, loss: 58.705810546875\n",
      "epoch: 56,  batch step: 49, loss: 38.98712158203125\n",
      "epoch: 56,  batch step: 50, loss: 21.99880027770996\n",
      "epoch: 56,  batch step: 51, loss: 42.23663330078125\n",
      "epoch: 56,  batch step: 52, loss: 42.642845153808594\n",
      "epoch: 56,  batch step: 53, loss: 143.3217010498047\n",
      "epoch: 56,  batch step: 54, loss: 51.04684066772461\n",
      "epoch: 56,  batch step: 55, loss: 3.6291494369506836\n",
      "epoch: 56,  batch step: 56, loss: 108.30210876464844\n",
      "epoch: 56,  batch step: 57, loss: 8.88543701171875\n",
      "epoch: 56,  batch step: 58, loss: 6.078242301940918\n",
      "epoch: 56,  batch step: 59, loss: 14.16042709350586\n",
      "epoch: 56,  batch step: 60, loss: 4.194670677185059\n",
      "epoch: 56,  batch step: 61, loss: 14.029531478881836\n",
      "epoch: 56,  batch step: 62, loss: 5.206918239593506\n",
      "epoch: 56,  batch step: 63, loss: 2.222647190093994\n",
      "epoch: 56,  batch step: 64, loss: 46.88739013671875\n",
      "epoch: 56,  batch step: 65, loss: 21.777748107910156\n",
      "epoch: 56,  batch step: 66, loss: 3.7302663326263428\n",
      "epoch: 56,  batch step: 67, loss: 4.309103012084961\n",
      "epoch: 56,  batch step: 68, loss: 6.498039245605469\n",
      "epoch: 56,  batch step: 69, loss: 4.115695476531982\n",
      "epoch: 56,  batch step: 70, loss: 24.01777458190918\n",
      "epoch: 56,  batch step: 71, loss: 17.69996452331543\n",
      "epoch: 56,  batch step: 72, loss: 11.877924919128418\n",
      "epoch: 56,  batch step: 73, loss: 25.28237533569336\n",
      "epoch: 56,  batch step: 74, loss: 40.80417251586914\n",
      "epoch: 56,  batch step: 75, loss: 3.4845075607299805\n",
      "epoch: 56,  batch step: 76, loss: 19.559755325317383\n",
      "epoch: 56,  batch step: 77, loss: 135.4344940185547\n",
      "epoch: 56,  batch step: 78, loss: 7.220860481262207\n",
      "epoch: 56,  batch step: 79, loss: 58.39527893066406\n",
      "epoch: 56,  batch step: 80, loss: 29.80897331237793\n",
      "epoch: 56,  batch step: 81, loss: 8.646418571472168\n",
      "epoch: 56,  batch step: 82, loss: 25.226482391357422\n",
      "epoch: 56,  batch step: 83, loss: 29.612167358398438\n",
      "epoch: 56,  batch step: 84, loss: 28.001117706298828\n",
      "epoch: 56,  batch step: 85, loss: 81.92903900146484\n",
      "epoch: 56,  batch step: 86, loss: 54.16534423828125\n",
      "epoch: 56,  batch step: 87, loss: 5.908361434936523\n",
      "epoch: 56,  batch step: 88, loss: 6.942037105560303\n",
      "epoch: 56,  batch step: 89, loss: 4.552029609680176\n",
      "epoch: 56,  batch step: 90, loss: 30.02460479736328\n",
      "epoch: 56,  batch step: 91, loss: 31.022123336791992\n",
      "epoch: 56,  batch step: 92, loss: 3.266080617904663\n",
      "epoch: 56,  batch step: 93, loss: 6.119477272033691\n",
      "epoch: 56,  batch step: 94, loss: 32.97615051269531\n",
      "epoch: 56,  batch step: 95, loss: 74.47066497802734\n",
      "epoch: 56,  batch step: 96, loss: 4.5087666511535645\n",
      "epoch: 56,  batch step: 97, loss: 29.514633178710938\n",
      "epoch: 56,  batch step: 98, loss: 5.828988075256348\n",
      "epoch: 56,  batch step: 99, loss: 43.71856689453125\n",
      "epoch: 56,  batch step: 100, loss: 4.7821149826049805\n",
      "epoch: 56,  batch step: 101, loss: 27.752277374267578\n",
      "epoch: 56,  batch step: 102, loss: 6.228343963623047\n",
      "epoch: 56,  batch step: 103, loss: 52.426143646240234\n",
      "epoch: 56,  batch step: 104, loss: 65.26292419433594\n",
      "epoch: 56,  batch step: 105, loss: 33.94479751586914\n",
      "epoch: 56,  batch step: 106, loss: 4.453511714935303\n",
      "epoch: 56,  batch step: 107, loss: 7.198221206665039\n",
      "epoch: 56,  batch step: 108, loss: 5.26400089263916\n",
      "epoch: 56,  batch step: 109, loss: 27.71703338623047\n",
      "epoch: 56,  batch step: 110, loss: 33.52111053466797\n",
      "epoch: 56,  batch step: 111, loss: 5.26641845703125\n",
      "epoch: 56,  batch step: 112, loss: 5.406560897827148\n",
      "epoch: 56,  batch step: 113, loss: 13.44233226776123\n",
      "epoch: 56,  batch step: 114, loss: 7.283950328826904\n",
      "epoch: 56,  batch step: 115, loss: 10.541936874389648\n",
      "epoch: 56,  batch step: 116, loss: 38.41007614135742\n",
      "epoch: 56,  batch step: 117, loss: 45.49713134765625\n",
      "epoch: 56,  batch step: 118, loss: 121.06539154052734\n",
      "epoch: 56,  batch step: 119, loss: 3.860386848449707\n",
      "epoch: 56,  batch step: 120, loss: 7.626964092254639\n",
      "epoch: 56,  batch step: 121, loss: 8.333982467651367\n",
      "epoch: 56,  batch step: 122, loss: 12.76571273803711\n",
      "epoch: 56,  batch step: 123, loss: 20.039875030517578\n",
      "epoch: 56,  batch step: 124, loss: 16.359619140625\n",
      "epoch: 56,  batch step: 125, loss: 5.401753902435303\n",
      "epoch: 56,  batch step: 126, loss: 3.962984085083008\n",
      "epoch: 56,  batch step: 127, loss: 11.301046371459961\n",
      "epoch: 56,  batch step: 128, loss: 7.768450736999512\n",
      "epoch: 56,  batch step: 129, loss: 19.73503875732422\n",
      "epoch: 56,  batch step: 130, loss: 100.60320281982422\n",
      "epoch: 56,  batch step: 131, loss: 10.264652252197266\n",
      "epoch: 56,  batch step: 132, loss: 6.777130126953125\n",
      "epoch: 56,  batch step: 133, loss: 5.159848690032959\n",
      "epoch: 56,  batch step: 134, loss: 38.648704528808594\n",
      "epoch: 56,  batch step: 135, loss: 5.409758567810059\n",
      "epoch: 56,  batch step: 136, loss: 5.051476001739502\n",
      "epoch: 56,  batch step: 137, loss: 4.431886196136475\n",
      "epoch: 56,  batch step: 138, loss: 6.244780540466309\n",
      "epoch: 56,  batch step: 139, loss: 4.232269287109375\n",
      "epoch: 56,  batch step: 140, loss: 8.943022727966309\n",
      "epoch: 56,  batch step: 141, loss: 35.698707580566406\n",
      "epoch: 56,  batch step: 142, loss: 42.225040435791016\n",
      "epoch: 56,  batch step: 143, loss: 39.9114990234375\n",
      "epoch: 56,  batch step: 144, loss: 32.938228607177734\n",
      "epoch: 56,  batch step: 145, loss: 4.660017013549805\n",
      "epoch: 56,  batch step: 146, loss: 38.788692474365234\n",
      "epoch: 56,  batch step: 147, loss: 3.706454038619995\n",
      "epoch: 56,  batch step: 148, loss: 14.538573265075684\n",
      "epoch: 56,  batch step: 149, loss: 46.69948959350586\n",
      "epoch: 56,  batch step: 150, loss: 5.557042121887207\n",
      "epoch: 56,  batch step: 151, loss: 3.254392147064209\n",
      "epoch: 56,  batch step: 152, loss: 3.0277841091156006\n",
      "epoch: 56,  batch step: 153, loss: 15.661558151245117\n",
      "epoch: 56,  batch step: 154, loss: 55.15834045410156\n",
      "epoch: 56,  batch step: 155, loss: 3.6602654457092285\n",
      "epoch: 56,  batch step: 156, loss: 10.089212417602539\n",
      "epoch: 56,  batch step: 157, loss: 12.892779350280762\n",
      "epoch: 56,  batch step: 158, loss: 7.257100582122803\n",
      "epoch: 56,  batch step: 159, loss: 18.03603744506836\n",
      "epoch: 56,  batch step: 160, loss: 27.038047790527344\n",
      "epoch: 56,  batch step: 161, loss: 52.475364685058594\n",
      "epoch: 56,  batch step: 162, loss: 69.34046936035156\n",
      "epoch: 56,  batch step: 163, loss: 4.183407783508301\n",
      "epoch: 56,  batch step: 164, loss: 2.3885364532470703\n",
      "epoch: 56,  batch step: 165, loss: 2.7314517498016357\n",
      "epoch: 56,  batch step: 166, loss: 4.476325988769531\n",
      "epoch: 56,  batch step: 167, loss: 2.7093091011047363\n",
      "epoch: 56,  batch step: 168, loss: 38.219966888427734\n",
      "epoch: 56,  batch step: 169, loss: 78.53799438476562\n",
      "epoch: 56,  batch step: 170, loss: 75.70834350585938\n",
      "epoch: 56,  batch step: 171, loss: 30.358442306518555\n",
      "epoch: 56,  batch step: 172, loss: 4.508886337280273\n",
      "epoch: 56,  batch step: 173, loss: 5.312087059020996\n",
      "epoch: 56,  batch step: 174, loss: 44.11176681518555\n",
      "epoch: 56,  batch step: 175, loss: 5.8495941162109375\n",
      "epoch: 56,  batch step: 176, loss: 31.571264266967773\n",
      "epoch: 56,  batch step: 177, loss: 4.092470169067383\n",
      "epoch: 56,  batch step: 178, loss: 8.678506851196289\n",
      "epoch: 56,  batch step: 179, loss: 4.439927577972412\n",
      "epoch: 56,  batch step: 180, loss: 20.87880516052246\n",
      "epoch: 56,  batch step: 181, loss: 25.70661735534668\n",
      "epoch: 56,  batch step: 182, loss: 5.362032890319824\n",
      "epoch: 56,  batch step: 183, loss: 2.511477470397949\n",
      "epoch: 56,  batch step: 184, loss: 9.238826751708984\n",
      "epoch: 56,  batch step: 185, loss: 3.600905179977417\n",
      "epoch: 56,  batch step: 186, loss: 4.440281867980957\n",
      "epoch: 56,  batch step: 187, loss: 27.428897857666016\n",
      "epoch: 56,  batch step: 188, loss: 29.765064239501953\n",
      "epoch: 56,  batch step: 189, loss: 122.3911361694336\n",
      "epoch: 56,  batch step: 190, loss: 75.23682403564453\n",
      "epoch: 56,  batch step: 191, loss: 23.525360107421875\n",
      "epoch: 56,  batch step: 192, loss: 6.686892986297607\n",
      "epoch: 56,  batch step: 193, loss: 34.40251541137695\n",
      "epoch: 56,  batch step: 194, loss: 5.349365234375\n",
      "epoch: 56,  batch step: 195, loss: 60.02297592163086\n",
      "epoch: 56,  batch step: 196, loss: 97.73503112792969\n",
      "epoch: 56,  batch step: 197, loss: 13.646224021911621\n",
      "epoch: 56,  batch step: 198, loss: 15.958845138549805\n",
      "epoch: 56,  batch step: 199, loss: 4.844973564147949\n",
      "epoch: 56,  batch step: 200, loss: 17.31644058227539\n",
      "epoch: 56,  batch step: 201, loss: 4.695965766906738\n",
      "epoch: 56,  batch step: 202, loss: 52.30279541015625\n",
      "epoch: 56,  batch step: 203, loss: 37.40795135498047\n",
      "epoch: 56,  batch step: 204, loss: 46.61262130737305\n",
      "epoch: 56,  batch step: 205, loss: 7.591167449951172\n",
      "epoch: 56,  batch step: 206, loss: 14.12375259399414\n",
      "epoch: 56,  batch step: 207, loss: 5.083739280700684\n",
      "epoch: 56,  batch step: 208, loss: 18.304344177246094\n",
      "epoch: 56,  batch step: 209, loss: 5.926403045654297\n",
      "epoch: 56,  batch step: 210, loss: 25.663326263427734\n",
      "epoch: 56,  batch step: 211, loss: 88.04338836669922\n",
      "epoch: 56,  batch step: 212, loss: 7.546252727508545\n",
      "epoch: 56,  batch step: 213, loss: 7.987923622131348\n",
      "epoch: 56,  batch step: 214, loss: 26.636058807373047\n",
      "epoch: 56,  batch step: 215, loss: 58.14680099487305\n",
      "epoch: 56,  batch step: 216, loss: 25.661296844482422\n",
      "epoch: 56,  batch step: 217, loss: 26.266469955444336\n",
      "epoch: 56,  batch step: 218, loss: 22.502758026123047\n",
      "epoch: 56,  batch step: 219, loss: 15.325679779052734\n",
      "epoch: 56,  batch step: 220, loss: 9.57596206665039\n",
      "epoch: 56,  batch step: 221, loss: 6.179414749145508\n",
      "epoch: 56,  batch step: 222, loss: 18.55289077758789\n",
      "epoch: 56,  batch step: 223, loss: 4.79872465133667\n",
      "epoch: 56,  batch step: 224, loss: 5.279851913452148\n",
      "epoch: 56,  batch step: 225, loss: 24.566242218017578\n",
      "epoch: 56,  batch step: 226, loss: 9.171914100646973\n",
      "epoch: 56,  batch step: 227, loss: 3.7260117530822754\n",
      "epoch: 56,  batch step: 228, loss: 3.4274699687957764\n",
      "epoch: 56,  batch step: 229, loss: 53.96291732788086\n",
      "epoch: 56,  batch step: 230, loss: 6.480832576751709\n",
      "epoch: 56,  batch step: 231, loss: 8.868351936340332\n",
      "epoch: 56,  batch step: 232, loss: 13.933717727661133\n",
      "epoch: 56,  batch step: 233, loss: 28.98116111755371\n",
      "epoch: 56,  batch step: 234, loss: 15.788335800170898\n",
      "epoch: 56,  batch step: 235, loss: 5.5968427658081055\n",
      "epoch: 56,  batch step: 236, loss: 19.158432006835938\n",
      "epoch: 56,  batch step: 237, loss: 40.854637145996094\n",
      "epoch: 56,  batch step: 238, loss: 23.372770309448242\n",
      "epoch: 56,  batch step: 239, loss: 7.223983287811279\n",
      "epoch: 56,  batch step: 240, loss: 2.802541732788086\n",
      "epoch: 56,  batch step: 241, loss: 33.25167465209961\n",
      "epoch: 56,  batch step: 242, loss: 4.07865571975708\n",
      "epoch: 56,  batch step: 243, loss: 42.6978874206543\n",
      "epoch: 56,  batch step: 244, loss: 30.269851684570312\n",
      "epoch: 56,  batch step: 245, loss: 4.10526180267334\n",
      "epoch: 56,  batch step: 246, loss: 28.560848236083984\n",
      "epoch: 56,  batch step: 247, loss: 14.479270935058594\n",
      "epoch: 56,  batch step: 248, loss: 22.834684371948242\n",
      "epoch: 56,  batch step: 249, loss: 4.226643085479736\n",
      "epoch: 56,  batch step: 250, loss: 76.16317749023438\n",
      "epoch: 56,  batch step: 251, loss: 13.408891677856445\n",
      "validation error epoch  56:    tensor(66.8889, device='cuda:0')\n",
      "316\n",
      "epoch: 57,  batch step: 0, loss: 9.612646102905273\n",
      "epoch: 57,  batch step: 1, loss: 23.46832847595215\n",
      "epoch: 57,  batch step: 2, loss: 2.914918899536133\n",
      "epoch: 57,  batch step: 3, loss: 45.26293182373047\n",
      "epoch: 57,  batch step: 4, loss: 26.73206329345703\n",
      "epoch: 57,  batch step: 5, loss: 47.728782653808594\n",
      "epoch: 57,  batch step: 6, loss: 8.50076675415039\n",
      "epoch: 57,  batch step: 7, loss: 15.385377883911133\n",
      "epoch: 57,  batch step: 8, loss: 18.58714485168457\n",
      "epoch: 57,  batch step: 9, loss: 3.0335302352905273\n",
      "epoch: 57,  batch step: 10, loss: 2.4773709774017334\n",
      "epoch: 57,  batch step: 11, loss: 16.81387710571289\n",
      "epoch: 57,  batch step: 12, loss: 46.622432708740234\n",
      "epoch: 57,  batch step: 13, loss: 3.1228187084198\n",
      "epoch: 57,  batch step: 14, loss: 4.202698230743408\n",
      "epoch: 57,  batch step: 15, loss: 28.254051208496094\n",
      "epoch: 57,  batch step: 16, loss: 3.0027177333831787\n",
      "epoch: 57,  batch step: 17, loss: 3.2220816612243652\n",
      "epoch: 57,  batch step: 18, loss: 3.0056614875793457\n",
      "epoch: 57,  batch step: 19, loss: 38.272003173828125\n",
      "epoch: 57,  batch step: 20, loss: 3.713886022567749\n",
      "epoch: 57,  batch step: 21, loss: 2.802608013153076\n",
      "epoch: 57,  batch step: 22, loss: 3.862177848815918\n",
      "epoch: 57,  batch step: 23, loss: 45.6334342956543\n",
      "epoch: 57,  batch step: 24, loss: 6.375410079956055\n",
      "epoch: 57,  batch step: 25, loss: 3.7335729598999023\n",
      "epoch: 57,  batch step: 26, loss: 15.306666374206543\n",
      "epoch: 57,  batch step: 27, loss: 12.547232627868652\n",
      "epoch: 57,  batch step: 28, loss: 21.604156494140625\n",
      "epoch: 57,  batch step: 29, loss: 30.400381088256836\n",
      "epoch: 57,  batch step: 30, loss: 5.744047164916992\n",
      "epoch: 57,  batch step: 31, loss: 4.02513313293457\n",
      "epoch: 57,  batch step: 32, loss: 6.559808254241943\n",
      "epoch: 57,  batch step: 33, loss: 18.19329071044922\n",
      "epoch: 57,  batch step: 34, loss: 32.09727478027344\n",
      "epoch: 57,  batch step: 35, loss: 4.635556697845459\n",
      "epoch: 57,  batch step: 36, loss: 29.58782196044922\n",
      "epoch: 57,  batch step: 37, loss: 5.337143898010254\n",
      "epoch: 57,  batch step: 38, loss: 13.558080673217773\n",
      "epoch: 57,  batch step: 39, loss: 33.304847717285156\n",
      "epoch: 57,  batch step: 40, loss: 18.3901309967041\n",
      "epoch: 57,  batch step: 41, loss: 4.645196914672852\n",
      "epoch: 57,  batch step: 42, loss: 16.807706832885742\n",
      "epoch: 57,  batch step: 43, loss: 2.8583314418792725\n",
      "epoch: 57,  batch step: 44, loss: 5.596722602844238\n",
      "epoch: 57,  batch step: 45, loss: 39.28642272949219\n",
      "epoch: 57,  batch step: 46, loss: 45.735103607177734\n",
      "epoch: 57,  batch step: 47, loss: 8.396991729736328\n",
      "epoch: 57,  batch step: 48, loss: 21.134939193725586\n",
      "epoch: 57,  batch step: 49, loss: 57.88893127441406\n",
      "epoch: 57,  batch step: 50, loss: 11.507487297058105\n",
      "epoch: 57,  batch step: 51, loss: 4.797320365905762\n",
      "epoch: 57,  batch step: 52, loss: 4.496204376220703\n",
      "epoch: 57,  batch step: 53, loss: 19.049510955810547\n",
      "epoch: 57,  batch step: 54, loss: 18.254470825195312\n",
      "epoch: 57,  batch step: 55, loss: 3.752653121948242\n",
      "epoch: 57,  batch step: 56, loss: 3.84375\n",
      "epoch: 57,  batch step: 57, loss: 4.370017051696777\n",
      "epoch: 57,  batch step: 58, loss: 3.775658369064331\n",
      "epoch: 57,  batch step: 59, loss: 6.298501491546631\n",
      "epoch: 57,  batch step: 60, loss: 10.72292709350586\n",
      "epoch: 57,  batch step: 61, loss: 25.920242309570312\n",
      "epoch: 57,  batch step: 62, loss: 17.979766845703125\n",
      "epoch: 57,  batch step: 63, loss: 48.15313720703125\n",
      "epoch: 57,  batch step: 64, loss: 23.13787841796875\n",
      "epoch: 57,  batch step: 65, loss: 3.548255681991577\n",
      "epoch: 57,  batch step: 66, loss: 2.284925937652588\n",
      "epoch: 57,  batch step: 67, loss: 41.60578155517578\n",
      "epoch: 57,  batch step: 68, loss: 17.017526626586914\n",
      "epoch: 57,  batch step: 69, loss: 74.70609283447266\n",
      "epoch: 57,  batch step: 70, loss: 9.21395492553711\n",
      "epoch: 57,  batch step: 71, loss: 21.91195297241211\n",
      "epoch: 57,  batch step: 72, loss: 79.26979064941406\n",
      "epoch: 57,  batch step: 73, loss: 53.29642868041992\n",
      "epoch: 57,  batch step: 74, loss: 2.929478168487549\n",
      "epoch: 57,  batch step: 75, loss: 80.82736206054688\n",
      "epoch: 57,  batch step: 76, loss: 6.1894636154174805\n",
      "epoch: 57,  batch step: 77, loss: 4.053753852844238\n",
      "epoch: 57,  batch step: 78, loss: 50.522186279296875\n",
      "epoch: 57,  batch step: 79, loss: 3.7815375328063965\n",
      "epoch: 57,  batch step: 80, loss: 43.64106369018555\n",
      "epoch: 57,  batch step: 81, loss: 55.217647552490234\n",
      "epoch: 57,  batch step: 82, loss: 3.9595277309417725\n",
      "epoch: 57,  batch step: 83, loss: 12.464400291442871\n",
      "epoch: 57,  batch step: 84, loss: 5.651998043060303\n",
      "epoch: 57,  batch step: 85, loss: 74.66217041015625\n",
      "epoch: 57,  batch step: 86, loss: 4.928555488586426\n",
      "epoch: 57,  batch step: 87, loss: 19.804964065551758\n",
      "epoch: 57,  batch step: 88, loss: 3.406660556793213\n",
      "epoch: 57,  batch step: 89, loss: 20.94672393798828\n",
      "epoch: 57,  batch step: 90, loss: 2.004915237426758\n",
      "epoch: 57,  batch step: 91, loss: 17.91615867614746\n",
      "epoch: 57,  batch step: 92, loss: 1.807727336883545\n",
      "epoch: 57,  batch step: 93, loss: 5.2022705078125\n",
      "epoch: 57,  batch step: 94, loss: 5.307167053222656\n",
      "epoch: 57,  batch step: 95, loss: 3.706045627593994\n",
      "epoch: 57,  batch step: 96, loss: 3.6467056274414062\n",
      "epoch: 57,  batch step: 97, loss: 15.253050804138184\n",
      "epoch: 57,  batch step: 98, loss: 29.843252182006836\n",
      "epoch: 57,  batch step: 99, loss: 2.333369731903076\n",
      "epoch: 57,  batch step: 100, loss: 3.170361280441284\n",
      "epoch: 57,  batch step: 101, loss: 2.1477746963500977\n",
      "epoch: 57,  batch step: 102, loss: 9.662525177001953\n",
      "epoch: 57,  batch step: 103, loss: 16.185333251953125\n",
      "epoch: 57,  batch step: 104, loss: 46.1212158203125\n",
      "epoch: 57,  batch step: 105, loss: 16.1710147857666\n",
      "epoch: 57,  batch step: 106, loss: 13.134001731872559\n",
      "epoch: 57,  batch step: 107, loss: 11.652191162109375\n",
      "epoch: 57,  batch step: 108, loss: 3.288442611694336\n",
      "epoch: 57,  batch step: 109, loss: 25.044647216796875\n",
      "epoch: 57,  batch step: 110, loss: 9.96450424194336\n",
      "epoch: 57,  batch step: 111, loss: 10.131092071533203\n",
      "epoch: 57,  batch step: 112, loss: 16.219669342041016\n",
      "epoch: 57,  batch step: 113, loss: 27.739622116088867\n",
      "epoch: 57,  batch step: 114, loss: 15.320754051208496\n",
      "epoch: 57,  batch step: 115, loss: 18.812213897705078\n",
      "epoch: 57,  batch step: 116, loss: 4.161227703094482\n",
      "epoch: 57,  batch step: 117, loss: 23.359622955322266\n",
      "epoch: 57,  batch step: 118, loss: 16.08255386352539\n",
      "epoch: 57,  batch step: 119, loss: 28.095012664794922\n",
      "epoch: 57,  batch step: 120, loss: 4.3382463455200195\n",
      "epoch: 57,  batch step: 121, loss: 14.596259117126465\n",
      "epoch: 57,  batch step: 122, loss: 3.3117799758911133\n",
      "epoch: 57,  batch step: 123, loss: 2.1430068016052246\n",
      "epoch: 57,  batch step: 124, loss: 3.245286464691162\n",
      "epoch: 57,  batch step: 125, loss: 2.2850804328918457\n",
      "epoch: 57,  batch step: 126, loss: 21.6202449798584\n",
      "epoch: 57,  batch step: 127, loss: 24.851469039916992\n",
      "epoch: 57,  batch step: 128, loss: 3.6672987937927246\n",
      "epoch: 57,  batch step: 129, loss: 31.15949058532715\n",
      "epoch: 57,  batch step: 130, loss: 17.635372161865234\n",
      "epoch: 57,  batch step: 131, loss: 72.6630859375\n",
      "epoch: 57,  batch step: 132, loss: 4.341336250305176\n",
      "epoch: 57,  batch step: 133, loss: 6.186267852783203\n",
      "epoch: 57,  batch step: 134, loss: 87.02061462402344\n",
      "epoch: 57,  batch step: 135, loss: 2.813333511352539\n",
      "epoch: 57,  batch step: 136, loss: 18.45216941833496\n",
      "epoch: 57,  batch step: 137, loss: 10.10085678100586\n",
      "epoch: 57,  batch step: 138, loss: 2.2328972816467285\n",
      "epoch: 57,  batch step: 139, loss: 22.95730972290039\n",
      "epoch: 57,  batch step: 140, loss: 4.646489143371582\n",
      "epoch: 57,  batch step: 141, loss: 46.27891159057617\n",
      "epoch: 57,  batch step: 142, loss: 35.85570526123047\n",
      "epoch: 57,  batch step: 143, loss: 3.389829635620117\n",
      "epoch: 57,  batch step: 144, loss: 23.435070037841797\n",
      "epoch: 57,  batch step: 145, loss: 6.465767860412598\n",
      "epoch: 57,  batch step: 146, loss: 22.978918075561523\n",
      "epoch: 57,  batch step: 147, loss: 8.19488525390625\n",
      "epoch: 57,  batch step: 148, loss: 23.46900177001953\n",
      "epoch: 57,  batch step: 149, loss: 56.65873336791992\n",
      "epoch: 57,  batch step: 150, loss: 6.045045375823975\n",
      "epoch: 57,  batch step: 151, loss: 6.619246482849121\n",
      "epoch: 57,  batch step: 152, loss: 3.517681121826172\n",
      "epoch: 57,  batch step: 153, loss: 44.64750671386719\n",
      "epoch: 57,  batch step: 154, loss: 14.230101585388184\n",
      "epoch: 57,  batch step: 155, loss: 55.35637664794922\n",
      "epoch: 57,  batch step: 156, loss: 3.336665630340576\n",
      "epoch: 57,  batch step: 157, loss: 31.16547393798828\n",
      "epoch: 57,  batch step: 158, loss: 2.266169786453247\n",
      "epoch: 57,  batch step: 159, loss: 6.801571846008301\n",
      "epoch: 57,  batch step: 160, loss: 24.691421508789062\n",
      "epoch: 57,  batch step: 161, loss: 10.474258422851562\n",
      "epoch: 57,  batch step: 162, loss: 15.430912017822266\n",
      "epoch: 57,  batch step: 163, loss: 5.26269006729126\n",
      "epoch: 57,  batch step: 164, loss: 8.90533447265625\n",
      "epoch: 57,  batch step: 165, loss: 2.290337562561035\n",
      "epoch: 57,  batch step: 166, loss: 4.684140205383301\n",
      "epoch: 57,  batch step: 167, loss: 17.53963851928711\n",
      "epoch: 57,  batch step: 168, loss: 16.90425682067871\n",
      "epoch: 57,  batch step: 169, loss: 59.846038818359375\n",
      "epoch: 57,  batch step: 170, loss: 3.083343505859375\n",
      "epoch: 57,  batch step: 171, loss: 32.44008255004883\n",
      "epoch: 57,  batch step: 172, loss: 48.67388916015625\n",
      "epoch: 57,  batch step: 173, loss: 19.15929412841797\n",
      "epoch: 57,  batch step: 174, loss: 25.093801498413086\n",
      "epoch: 57,  batch step: 175, loss: 6.565321922302246\n",
      "epoch: 57,  batch step: 176, loss: 2.672693967819214\n",
      "epoch: 57,  batch step: 177, loss: 3.837735652923584\n",
      "epoch: 57,  batch step: 178, loss: 1.9648396968841553\n",
      "epoch: 57,  batch step: 179, loss: 18.804157257080078\n",
      "epoch: 57,  batch step: 180, loss: 6.971320152282715\n",
      "epoch: 57,  batch step: 181, loss: 3.4865338802337646\n",
      "epoch: 57,  batch step: 182, loss: 4.400840759277344\n",
      "epoch: 57,  batch step: 183, loss: 54.53261947631836\n",
      "epoch: 57,  batch step: 184, loss: 3.875945568084717\n",
      "epoch: 57,  batch step: 185, loss: 7.935818672180176\n",
      "epoch: 57,  batch step: 186, loss: 23.660221099853516\n",
      "epoch: 57,  batch step: 187, loss: 70.93038940429688\n",
      "epoch: 57,  batch step: 188, loss: 3.5492188930511475\n",
      "epoch: 57,  batch step: 189, loss: 4.060598373413086\n",
      "epoch: 57,  batch step: 190, loss: 64.13238525390625\n",
      "epoch: 57,  batch step: 191, loss: 3.8931050300598145\n",
      "epoch: 57,  batch step: 192, loss: 11.892191886901855\n",
      "epoch: 57,  batch step: 193, loss: 44.24147033691406\n",
      "epoch: 57,  batch step: 194, loss: 3.0638155937194824\n",
      "epoch: 57,  batch step: 195, loss: 5.441393852233887\n",
      "epoch: 57,  batch step: 196, loss: 3.529843330383301\n",
      "epoch: 57,  batch step: 197, loss: 28.53555679321289\n",
      "epoch: 57,  batch step: 198, loss: 7.878734588623047\n",
      "epoch: 57,  batch step: 199, loss: 32.219078063964844\n",
      "epoch: 57,  batch step: 200, loss: 24.74956512451172\n",
      "epoch: 57,  batch step: 201, loss: 53.74304962158203\n",
      "epoch: 57,  batch step: 202, loss: 3.1377029418945312\n",
      "epoch: 57,  batch step: 203, loss: 17.781496047973633\n",
      "epoch: 57,  batch step: 204, loss: 11.699782371520996\n",
      "epoch: 57,  batch step: 205, loss: 7.248480796813965\n",
      "epoch: 57,  batch step: 206, loss: 8.946329116821289\n",
      "epoch: 57,  batch step: 207, loss: 13.788213729858398\n",
      "epoch: 57,  batch step: 208, loss: 19.40618133544922\n",
      "epoch: 57,  batch step: 209, loss: 62.386131286621094\n",
      "epoch: 57,  batch step: 210, loss: 8.44940185546875\n",
      "epoch: 57,  batch step: 211, loss: 5.031726837158203\n",
      "epoch: 57,  batch step: 212, loss: 6.900179862976074\n",
      "epoch: 57,  batch step: 213, loss: 24.477155685424805\n",
      "epoch: 57,  batch step: 214, loss: 5.140764236450195\n",
      "epoch: 57,  batch step: 215, loss: 8.182889938354492\n",
      "epoch: 57,  batch step: 216, loss: 4.220298767089844\n",
      "epoch: 57,  batch step: 217, loss: 4.489180564880371\n",
      "epoch: 57,  batch step: 218, loss: 5.927989959716797\n",
      "epoch: 57,  batch step: 219, loss: 15.52338695526123\n",
      "epoch: 57,  batch step: 220, loss: 3.8743858337402344\n",
      "epoch: 57,  batch step: 221, loss: 4.128700256347656\n",
      "epoch: 57,  batch step: 222, loss: 7.604422569274902\n",
      "epoch: 57,  batch step: 223, loss: 5.119077682495117\n",
      "epoch: 57,  batch step: 224, loss: 4.324064254760742\n",
      "epoch: 57,  batch step: 225, loss: 4.1242194175720215\n",
      "epoch: 57,  batch step: 226, loss: 30.057876586914062\n",
      "epoch: 57,  batch step: 227, loss: 3.2216243743896484\n",
      "epoch: 57,  batch step: 228, loss: 34.10115051269531\n",
      "epoch: 57,  batch step: 229, loss: 8.951751708984375\n",
      "epoch: 57,  batch step: 230, loss: 4.576318740844727\n",
      "epoch: 57,  batch step: 231, loss: 4.274971008300781\n",
      "epoch: 57,  batch step: 232, loss: 253.12109375\n",
      "epoch: 57,  batch step: 233, loss: 20.92506980895996\n",
      "epoch: 57,  batch step: 234, loss: 6.256593227386475\n",
      "epoch: 57,  batch step: 235, loss: 4.713682174682617\n",
      "epoch: 57,  batch step: 236, loss: 57.2511100769043\n",
      "epoch: 57,  batch step: 237, loss: 8.891145706176758\n",
      "epoch: 57,  batch step: 238, loss: 27.725860595703125\n",
      "epoch: 57,  batch step: 239, loss: 32.93563461303711\n",
      "epoch: 57,  batch step: 240, loss: 5.44197940826416\n",
      "epoch: 57,  batch step: 241, loss: 30.696269989013672\n",
      "epoch: 57,  batch step: 242, loss: 4.441783905029297\n",
      "epoch: 57,  batch step: 243, loss: 5.151926040649414\n",
      "epoch: 57,  batch step: 244, loss: 25.605361938476562\n",
      "epoch: 57,  batch step: 245, loss: 32.1695442199707\n",
      "epoch: 57,  batch step: 246, loss: 18.656448364257812\n",
      "epoch: 57,  batch step: 247, loss: 2.998530864715576\n",
      "epoch: 57,  batch step: 248, loss: 6.199174880981445\n",
      "epoch: 57,  batch step: 249, loss: 143.22372436523438\n",
      "epoch: 57,  batch step: 250, loss: 33.86930847167969\n",
      "epoch: 57,  batch step: 251, loss: 26.10089111328125\n",
      "validation error epoch  57:    tensor(70.6436, device='cuda:0')\n",
      "316\n",
      "epoch: 58,  batch step: 0, loss: 7.897933483123779\n",
      "epoch: 58,  batch step: 1, loss: 3.672377347946167\n",
      "epoch: 58,  batch step: 2, loss: 37.09004211425781\n",
      "epoch: 58,  batch step: 3, loss: 7.949799537658691\n",
      "epoch: 58,  batch step: 4, loss: 34.47650909423828\n",
      "epoch: 58,  batch step: 5, loss: 4.309001445770264\n",
      "epoch: 58,  batch step: 6, loss: 44.08116912841797\n",
      "epoch: 58,  batch step: 7, loss: 6.013606071472168\n",
      "epoch: 58,  batch step: 8, loss: 26.206756591796875\n",
      "epoch: 58,  batch step: 9, loss: 55.42963790893555\n",
      "epoch: 58,  batch step: 10, loss: 19.46013832092285\n",
      "epoch: 58,  batch step: 11, loss: 5.652469158172607\n",
      "epoch: 58,  batch step: 12, loss: 23.364341735839844\n",
      "epoch: 58,  batch step: 13, loss: 43.77882385253906\n",
      "epoch: 58,  batch step: 14, loss: 28.8770751953125\n",
      "epoch: 58,  batch step: 15, loss: 4.690675258636475\n",
      "epoch: 58,  batch step: 16, loss: 15.15796184539795\n",
      "epoch: 58,  batch step: 17, loss: 3.767979145050049\n",
      "epoch: 58,  batch step: 18, loss: 28.73088836669922\n",
      "epoch: 58,  batch step: 19, loss: 18.41834259033203\n",
      "epoch: 58,  batch step: 20, loss: 49.9215087890625\n",
      "epoch: 58,  batch step: 21, loss: 5.969167709350586\n",
      "epoch: 58,  batch step: 22, loss: 4.491422653198242\n",
      "epoch: 58,  batch step: 23, loss: 74.6302490234375\n",
      "epoch: 58,  batch step: 24, loss: 25.77273941040039\n",
      "epoch: 58,  batch step: 25, loss: 4.56719970703125\n",
      "epoch: 58,  batch step: 26, loss: 2.8000400066375732\n",
      "epoch: 58,  batch step: 27, loss: 2.1059961318969727\n",
      "epoch: 58,  batch step: 28, loss: 5.6460418701171875\n",
      "epoch: 58,  batch step: 29, loss: 7.576351165771484\n",
      "epoch: 58,  batch step: 30, loss: 3.7241837978363037\n",
      "epoch: 58,  batch step: 31, loss: 4.961016654968262\n",
      "epoch: 58,  batch step: 32, loss: 32.7574462890625\n",
      "epoch: 58,  batch step: 33, loss: 7.936879634857178\n",
      "epoch: 58,  batch step: 34, loss: 24.571640014648438\n",
      "epoch: 58,  batch step: 35, loss: 32.06134033203125\n",
      "epoch: 58,  batch step: 36, loss: 34.439918518066406\n",
      "epoch: 58,  batch step: 37, loss: 43.187400817871094\n",
      "epoch: 58,  batch step: 38, loss: 4.104555130004883\n",
      "epoch: 58,  batch step: 39, loss: 4.042952060699463\n",
      "epoch: 58,  batch step: 40, loss: 166.68548583984375\n",
      "epoch: 58,  batch step: 41, loss: 44.340545654296875\n",
      "epoch: 58,  batch step: 42, loss: 4.409581661224365\n",
      "epoch: 58,  batch step: 43, loss: 25.87567901611328\n",
      "epoch: 58,  batch step: 44, loss: 4.916697025299072\n",
      "epoch: 58,  batch step: 45, loss: 71.36753845214844\n",
      "epoch: 58,  batch step: 46, loss: 8.320194244384766\n",
      "epoch: 58,  batch step: 47, loss: 12.07144832611084\n",
      "epoch: 58,  batch step: 48, loss: 4.777522563934326\n",
      "epoch: 58,  batch step: 49, loss: 9.479667663574219\n",
      "epoch: 58,  batch step: 50, loss: 2.8260326385498047\n",
      "epoch: 58,  batch step: 51, loss: 5.7191572189331055\n",
      "epoch: 58,  batch step: 52, loss: 6.85507869720459\n",
      "epoch: 58,  batch step: 53, loss: 4.097933769226074\n",
      "epoch: 58,  batch step: 54, loss: 22.208587646484375\n",
      "epoch: 58,  batch step: 55, loss: 3.0441770553588867\n",
      "epoch: 58,  batch step: 56, loss: 3.099693775177002\n",
      "epoch: 58,  batch step: 57, loss: 4.780837059020996\n",
      "epoch: 58,  batch step: 58, loss: 32.410526275634766\n",
      "epoch: 58,  batch step: 59, loss: 28.993648529052734\n",
      "epoch: 58,  batch step: 60, loss: 2.597126007080078\n",
      "epoch: 58,  batch step: 61, loss: 16.86481285095215\n",
      "epoch: 58,  batch step: 62, loss: 26.286907196044922\n",
      "epoch: 58,  batch step: 63, loss: 13.191823959350586\n",
      "epoch: 58,  batch step: 64, loss: 18.099472045898438\n",
      "epoch: 58,  batch step: 65, loss: 9.162960052490234\n",
      "epoch: 58,  batch step: 66, loss: 13.31433391571045\n",
      "epoch: 58,  batch step: 67, loss: 3.4108054637908936\n",
      "epoch: 58,  batch step: 68, loss: 51.539608001708984\n",
      "epoch: 58,  batch step: 69, loss: 3.641737937927246\n",
      "epoch: 58,  batch step: 70, loss: 28.48560333251953\n",
      "epoch: 58,  batch step: 71, loss: 18.87006950378418\n",
      "epoch: 58,  batch step: 72, loss: 18.601869583129883\n",
      "epoch: 58,  batch step: 73, loss: 6.185832977294922\n",
      "epoch: 58,  batch step: 74, loss: 2.134537696838379\n",
      "epoch: 58,  batch step: 75, loss: 32.486602783203125\n",
      "epoch: 58,  batch step: 76, loss: 14.858622550964355\n",
      "epoch: 58,  batch step: 77, loss: 4.10609769821167\n",
      "epoch: 58,  batch step: 78, loss: 2.7148702144622803\n",
      "epoch: 58,  batch step: 79, loss: 9.51701545715332\n",
      "epoch: 58,  batch step: 80, loss: 6.87755012512207\n",
      "epoch: 58,  batch step: 81, loss: 24.5281982421875\n",
      "epoch: 58,  batch step: 82, loss: 4.613386631011963\n",
      "epoch: 58,  batch step: 83, loss: 9.988364219665527\n",
      "epoch: 58,  batch step: 84, loss: 27.829730987548828\n",
      "epoch: 58,  batch step: 85, loss: 5.123666763305664\n",
      "epoch: 58,  batch step: 86, loss: 33.01786422729492\n",
      "epoch: 58,  batch step: 87, loss: 2.639798164367676\n",
      "epoch: 58,  batch step: 88, loss: 13.467954635620117\n",
      "epoch: 58,  batch step: 89, loss: 1.7363908290863037\n",
      "epoch: 58,  batch step: 90, loss: 7.009849548339844\n",
      "epoch: 58,  batch step: 91, loss: 16.97136116027832\n",
      "epoch: 58,  batch step: 92, loss: 2.7033743858337402\n",
      "epoch: 58,  batch step: 93, loss: 12.051408767700195\n",
      "epoch: 58,  batch step: 94, loss: 4.649683475494385\n",
      "epoch: 58,  batch step: 95, loss: 20.790607452392578\n",
      "epoch: 58,  batch step: 96, loss: 2.883899688720703\n",
      "epoch: 58,  batch step: 97, loss: 2.670266628265381\n",
      "epoch: 58,  batch step: 98, loss: 3.0213375091552734\n",
      "epoch: 58,  batch step: 99, loss: 2.9740941524505615\n",
      "epoch: 58,  batch step: 100, loss: 2.5169527530670166\n",
      "epoch: 58,  batch step: 101, loss: 2.4114036560058594\n",
      "epoch: 58,  batch step: 102, loss: 3.244792938232422\n",
      "epoch: 58,  batch step: 103, loss: 3.071537971496582\n",
      "epoch: 58,  batch step: 104, loss: 2.2751071453094482\n",
      "epoch: 58,  batch step: 105, loss: 29.600841522216797\n",
      "epoch: 58,  batch step: 106, loss: 3.2140955924987793\n",
      "epoch: 58,  batch step: 107, loss: 2.2269039154052734\n",
      "epoch: 58,  batch step: 108, loss: 37.470458984375\n",
      "epoch: 58,  batch step: 109, loss: 18.90453338623047\n",
      "epoch: 58,  batch step: 110, loss: 3.9566197395324707\n",
      "epoch: 58,  batch step: 111, loss: 46.48004913330078\n",
      "epoch: 58,  batch step: 112, loss: 5.176054000854492\n",
      "epoch: 58,  batch step: 113, loss: 5.010471343994141\n",
      "epoch: 58,  batch step: 114, loss: 2.7219626903533936\n",
      "epoch: 58,  batch step: 115, loss: 31.134292602539062\n",
      "epoch: 58,  batch step: 116, loss: 13.467411041259766\n",
      "epoch: 58,  batch step: 117, loss: 31.932830810546875\n",
      "epoch: 58,  batch step: 118, loss: 40.4824333190918\n",
      "epoch: 58,  batch step: 119, loss: 2.638913154602051\n",
      "epoch: 58,  batch step: 120, loss: 2.821599006652832\n",
      "epoch: 58,  batch step: 121, loss: 10.140559196472168\n",
      "epoch: 58,  batch step: 122, loss: 4.477826118469238\n",
      "epoch: 58,  batch step: 123, loss: 2.9416797161102295\n",
      "epoch: 58,  batch step: 124, loss: 19.346708297729492\n",
      "epoch: 58,  batch step: 125, loss: 42.31156921386719\n",
      "epoch: 58,  batch step: 126, loss: 3.2179946899414062\n",
      "epoch: 58,  batch step: 127, loss: 23.28978729248047\n",
      "epoch: 58,  batch step: 128, loss: 2.7244906425476074\n",
      "epoch: 58,  batch step: 129, loss: 2.535581111907959\n",
      "epoch: 58,  batch step: 130, loss: 40.01915740966797\n",
      "epoch: 58,  batch step: 131, loss: 2.7550106048583984\n",
      "epoch: 58,  batch step: 132, loss: 7.713220596313477\n",
      "epoch: 58,  batch step: 133, loss: 35.319339752197266\n",
      "epoch: 58,  batch step: 134, loss: 64.70193481445312\n",
      "epoch: 58,  batch step: 135, loss: 69.22908782958984\n",
      "epoch: 58,  batch step: 136, loss: 5.155837059020996\n",
      "epoch: 58,  batch step: 137, loss: 2.5667152404785156\n",
      "epoch: 58,  batch step: 138, loss: 3.7905325889587402\n",
      "epoch: 58,  batch step: 139, loss: 20.304475784301758\n",
      "epoch: 58,  batch step: 140, loss: 81.19883728027344\n",
      "epoch: 58,  batch step: 141, loss: 27.05879783630371\n",
      "epoch: 58,  batch step: 142, loss: 5.995955467224121\n",
      "epoch: 58,  batch step: 143, loss: 13.531827926635742\n",
      "epoch: 58,  batch step: 144, loss: 3.458296537399292\n",
      "epoch: 58,  batch step: 145, loss: 29.123592376708984\n",
      "epoch: 58,  batch step: 146, loss: 33.344608306884766\n",
      "epoch: 58,  batch step: 147, loss: 18.079383850097656\n",
      "epoch: 58,  batch step: 148, loss: 17.741825103759766\n",
      "epoch: 58,  batch step: 149, loss: 3.2807271480560303\n",
      "epoch: 58,  batch step: 150, loss: 2.905151844024658\n",
      "epoch: 58,  batch step: 151, loss: 3.893937587738037\n",
      "epoch: 58,  batch step: 152, loss: 2.745784282684326\n",
      "epoch: 58,  batch step: 153, loss: 20.734249114990234\n",
      "epoch: 58,  batch step: 154, loss: 4.400533199310303\n",
      "epoch: 58,  batch step: 155, loss: 8.04073715209961\n",
      "epoch: 58,  batch step: 156, loss: 5.371490478515625\n",
      "epoch: 58,  batch step: 157, loss: 21.8238582611084\n",
      "epoch: 58,  batch step: 158, loss: 3.188385009765625\n",
      "epoch: 58,  batch step: 159, loss: 11.668115615844727\n",
      "epoch: 58,  batch step: 160, loss: 7.641533851623535\n",
      "epoch: 58,  batch step: 161, loss: 2.5770998001098633\n",
      "epoch: 58,  batch step: 162, loss: 2.926356792449951\n",
      "epoch: 58,  batch step: 163, loss: 39.820735931396484\n",
      "epoch: 58,  batch step: 164, loss: 4.750033378601074\n",
      "epoch: 58,  batch step: 165, loss: 54.515663146972656\n",
      "epoch: 58,  batch step: 166, loss: 11.621072769165039\n",
      "epoch: 58,  batch step: 167, loss: 3.191831588745117\n",
      "epoch: 58,  batch step: 168, loss: 23.174484252929688\n",
      "epoch: 58,  batch step: 169, loss: 84.43614196777344\n",
      "epoch: 58,  batch step: 170, loss: 3.091930866241455\n",
      "epoch: 58,  batch step: 171, loss: 38.1077995300293\n",
      "epoch: 58,  batch step: 172, loss: 2.9331483840942383\n",
      "epoch: 58,  batch step: 173, loss: 53.60590744018555\n",
      "epoch: 58,  batch step: 174, loss: 3.0975184440612793\n",
      "epoch: 58,  batch step: 175, loss: 11.238677024841309\n",
      "epoch: 58,  batch step: 176, loss: 3.924211025238037\n",
      "epoch: 58,  batch step: 177, loss: 4.612695217132568\n",
      "epoch: 58,  batch step: 178, loss: 29.00307846069336\n",
      "epoch: 58,  batch step: 179, loss: 86.8272933959961\n",
      "epoch: 58,  batch step: 180, loss: 16.585994720458984\n",
      "epoch: 58,  batch step: 181, loss: 2.9337902069091797\n",
      "epoch: 58,  batch step: 182, loss: 32.65650177001953\n",
      "epoch: 58,  batch step: 183, loss: 89.98051452636719\n",
      "epoch: 58,  batch step: 184, loss: 2.818171501159668\n",
      "epoch: 58,  batch step: 185, loss: 20.639202117919922\n",
      "epoch: 58,  batch step: 186, loss: 115.28630065917969\n",
      "epoch: 58,  batch step: 187, loss: 4.819016933441162\n",
      "epoch: 58,  batch step: 188, loss: 14.982928276062012\n",
      "epoch: 58,  batch step: 189, loss: 12.913459777832031\n",
      "epoch: 58,  batch step: 190, loss: 4.246580123901367\n",
      "epoch: 58,  batch step: 191, loss: 52.903465270996094\n",
      "epoch: 58,  batch step: 192, loss: 50.3454704284668\n",
      "epoch: 58,  batch step: 193, loss: 2.9020800590515137\n",
      "epoch: 58,  batch step: 194, loss: 5.088350296020508\n",
      "epoch: 58,  batch step: 195, loss: 2.241304636001587\n",
      "epoch: 58,  batch step: 196, loss: 19.014766693115234\n",
      "epoch: 58,  batch step: 197, loss: 20.549814224243164\n",
      "epoch: 58,  batch step: 198, loss: 3.513425588607788\n",
      "epoch: 58,  batch step: 199, loss: 3.8714587688446045\n",
      "epoch: 58,  batch step: 200, loss: 6.0521240234375\n",
      "epoch: 58,  batch step: 201, loss: 12.948129653930664\n",
      "epoch: 58,  batch step: 202, loss: 3.3044934272766113\n",
      "epoch: 58,  batch step: 203, loss: 26.422321319580078\n",
      "epoch: 58,  batch step: 204, loss: 2.3164806365966797\n",
      "epoch: 58,  batch step: 205, loss: 19.38044548034668\n",
      "epoch: 58,  batch step: 206, loss: 3.2646090984344482\n",
      "epoch: 58,  batch step: 207, loss: 20.734134674072266\n",
      "epoch: 58,  batch step: 208, loss: 5.09211540222168\n",
      "epoch: 58,  batch step: 209, loss: 18.30650520324707\n",
      "epoch: 58,  batch step: 210, loss: 78.3812255859375\n",
      "epoch: 58,  batch step: 211, loss: 44.54488754272461\n",
      "epoch: 58,  batch step: 212, loss: 18.61052131652832\n",
      "epoch: 58,  batch step: 213, loss: 24.337371826171875\n",
      "epoch: 58,  batch step: 214, loss: 4.541511058807373\n",
      "epoch: 58,  batch step: 215, loss: 4.84055233001709\n",
      "epoch: 58,  batch step: 216, loss: 22.63011932373047\n",
      "epoch: 58,  batch step: 217, loss: 23.979774475097656\n",
      "epoch: 58,  batch step: 218, loss: 34.01035690307617\n",
      "epoch: 58,  batch step: 219, loss: 5.384761810302734\n",
      "epoch: 58,  batch step: 220, loss: 37.35201644897461\n",
      "epoch: 58,  batch step: 221, loss: 16.668121337890625\n",
      "epoch: 58,  batch step: 222, loss: 31.314697265625\n",
      "epoch: 58,  batch step: 223, loss: 27.176366806030273\n",
      "epoch: 58,  batch step: 224, loss: 28.73114585876465\n",
      "epoch: 58,  batch step: 225, loss: 60.896263122558594\n",
      "epoch: 58,  batch step: 226, loss: 2.286588430404663\n",
      "epoch: 58,  batch step: 227, loss: 12.229175567626953\n",
      "epoch: 58,  batch step: 228, loss: 6.786003112792969\n",
      "epoch: 58,  batch step: 229, loss: 82.05204010009766\n",
      "epoch: 58,  batch step: 230, loss: 48.542755126953125\n",
      "epoch: 58,  batch step: 231, loss: 22.215002059936523\n",
      "epoch: 58,  batch step: 232, loss: 15.192548751831055\n",
      "epoch: 58,  batch step: 233, loss: 17.832462310791016\n",
      "epoch: 58,  batch step: 234, loss: 4.059188365936279\n",
      "epoch: 58,  batch step: 235, loss: 3.1968836784362793\n",
      "epoch: 58,  batch step: 236, loss: 5.9314117431640625\n",
      "epoch: 58,  batch step: 237, loss: 5.336852073669434\n",
      "epoch: 58,  batch step: 238, loss: 4.6160478591918945\n",
      "epoch: 58,  batch step: 239, loss: 18.032947540283203\n",
      "epoch: 58,  batch step: 240, loss: 4.775753021240234\n",
      "epoch: 58,  batch step: 241, loss: 8.063620567321777\n",
      "epoch: 58,  batch step: 242, loss: 74.6558837890625\n",
      "epoch: 58,  batch step: 243, loss: 17.364822387695312\n",
      "epoch: 58,  batch step: 244, loss: 3.381725788116455\n",
      "epoch: 58,  batch step: 245, loss: 15.890054702758789\n",
      "epoch: 58,  batch step: 246, loss: 5.364209175109863\n",
      "epoch: 58,  batch step: 247, loss: 3.736937999725342\n",
      "epoch: 58,  batch step: 248, loss: 31.822277069091797\n",
      "epoch: 58,  batch step: 249, loss: 22.320035934448242\n",
      "epoch: 58,  batch step: 250, loss: 3.7144436836242676\n",
      "epoch: 58,  batch step: 251, loss: 68.16102600097656\n",
      "validation error epoch  58:    tensor(70.6203, device='cuda:0')\n",
      "316\n",
      "epoch: 59,  batch step: 0, loss: 27.25469207763672\n",
      "epoch: 59,  batch step: 1, loss: 3.260822057723999\n",
      "epoch: 59,  batch step: 2, loss: 52.80589294433594\n",
      "epoch: 59,  batch step: 3, loss: 7.200736045837402\n",
      "epoch: 59,  batch step: 4, loss: 13.176698684692383\n",
      "epoch: 59,  batch step: 5, loss: 3.276374578475952\n",
      "epoch: 59,  batch step: 6, loss: 26.171554565429688\n",
      "epoch: 59,  batch step: 7, loss: 4.106851100921631\n",
      "epoch: 59,  batch step: 8, loss: 57.65460968017578\n",
      "epoch: 59,  batch step: 9, loss: 3.7052478790283203\n",
      "epoch: 59,  batch step: 10, loss: 61.13037872314453\n",
      "epoch: 59,  batch step: 11, loss: 43.661407470703125\n",
      "epoch: 59,  batch step: 12, loss: 4.186514854431152\n",
      "epoch: 59,  batch step: 13, loss: 6.606484413146973\n",
      "epoch: 59,  batch step: 14, loss: 32.17033767700195\n",
      "epoch: 59,  batch step: 15, loss: 8.064132690429688\n",
      "epoch: 59,  batch step: 16, loss: 24.084209442138672\n",
      "epoch: 59,  batch step: 17, loss: 20.856658935546875\n",
      "epoch: 59,  batch step: 18, loss: 3.7295284271240234\n",
      "epoch: 59,  batch step: 19, loss: 4.605403423309326\n",
      "epoch: 59,  batch step: 20, loss: 3.177759885787964\n",
      "epoch: 59,  batch step: 21, loss: 4.79170036315918\n",
      "epoch: 59,  batch step: 22, loss: 21.759639739990234\n",
      "epoch: 59,  batch step: 23, loss: 32.373077392578125\n",
      "epoch: 59,  batch step: 24, loss: 9.094413757324219\n",
      "epoch: 59,  batch step: 25, loss: 3.3278346061706543\n",
      "epoch: 59,  batch step: 26, loss: 7.055140495300293\n",
      "epoch: 59,  batch step: 27, loss: 2.8242576122283936\n",
      "epoch: 59,  batch step: 28, loss: 3.3026962280273438\n",
      "epoch: 59,  batch step: 29, loss: 4.899164199829102\n",
      "epoch: 59,  batch step: 30, loss: 6.62164831161499\n",
      "epoch: 59,  batch step: 31, loss: 35.62775802612305\n",
      "epoch: 59,  batch step: 32, loss: 19.458019256591797\n",
      "epoch: 59,  batch step: 33, loss: 3.1188981533050537\n",
      "epoch: 59,  batch step: 34, loss: 37.972511291503906\n",
      "epoch: 59,  batch step: 35, loss: 23.462642669677734\n",
      "epoch: 59,  batch step: 36, loss: 48.61229705810547\n",
      "epoch: 59,  batch step: 37, loss: 11.286104202270508\n",
      "epoch: 59,  batch step: 38, loss: 18.30424690246582\n",
      "epoch: 59,  batch step: 39, loss: 3.522447109222412\n",
      "epoch: 59,  batch step: 40, loss: 10.23984146118164\n",
      "epoch: 59,  batch step: 41, loss: 4.527940273284912\n",
      "epoch: 59,  batch step: 42, loss: 14.932146072387695\n",
      "epoch: 59,  batch step: 43, loss: 14.198119163513184\n",
      "epoch: 59,  batch step: 44, loss: 86.9974594116211\n",
      "epoch: 59,  batch step: 45, loss: 42.26752471923828\n",
      "epoch: 59,  batch step: 46, loss: 3.9909729957580566\n",
      "epoch: 59,  batch step: 47, loss: 46.36921691894531\n",
      "epoch: 59,  batch step: 48, loss: 29.10570526123047\n",
      "epoch: 59,  batch step: 49, loss: 7.364377021789551\n",
      "epoch: 59,  batch step: 50, loss: 5.859231472015381\n",
      "epoch: 59,  batch step: 51, loss: 2.2971692085266113\n",
      "epoch: 59,  batch step: 52, loss: 11.119399070739746\n",
      "epoch: 59,  batch step: 53, loss: 30.665184020996094\n",
      "epoch: 59,  batch step: 54, loss: 52.038631439208984\n",
      "epoch: 59,  batch step: 55, loss: 27.332242965698242\n",
      "epoch: 59,  batch step: 56, loss: 5.320374965667725\n",
      "epoch: 59,  batch step: 57, loss: 33.702667236328125\n",
      "epoch: 59,  batch step: 58, loss: 9.590975761413574\n",
      "epoch: 59,  batch step: 59, loss: 4.815241813659668\n",
      "epoch: 59,  batch step: 60, loss: 51.46238708496094\n",
      "epoch: 59,  batch step: 61, loss: 4.665694713592529\n",
      "epoch: 59,  batch step: 62, loss: 60.137908935546875\n",
      "epoch: 59,  batch step: 63, loss: 23.761747360229492\n",
      "epoch: 59,  batch step: 64, loss: 34.532466888427734\n",
      "epoch: 59,  batch step: 65, loss: 5.154338359832764\n",
      "epoch: 59,  batch step: 66, loss: 5.866820812225342\n",
      "epoch: 59,  batch step: 67, loss: 52.855865478515625\n",
      "epoch: 59,  batch step: 68, loss: 18.34514808654785\n",
      "epoch: 59,  batch step: 69, loss: 6.8245697021484375\n",
      "epoch: 59,  batch step: 70, loss: 21.86737632751465\n",
      "epoch: 59,  batch step: 71, loss: 19.26972198486328\n",
      "epoch: 59,  batch step: 72, loss: 39.6341552734375\n",
      "epoch: 59,  batch step: 73, loss: 7.821166038513184\n",
      "epoch: 59,  batch step: 74, loss: 18.09446907043457\n",
      "epoch: 59,  batch step: 75, loss: 3.768681287765503\n",
      "epoch: 59,  batch step: 76, loss: 6.56695556640625\n",
      "epoch: 59,  batch step: 77, loss: 11.154109001159668\n",
      "epoch: 59,  batch step: 78, loss: 6.428569793701172\n",
      "epoch: 59,  batch step: 79, loss: 12.287925720214844\n",
      "epoch: 59,  batch step: 80, loss: 5.451688289642334\n",
      "epoch: 59,  batch step: 81, loss: 7.2106428146362305\n",
      "epoch: 59,  batch step: 82, loss: 14.946688652038574\n",
      "epoch: 59,  batch step: 83, loss: 64.49637603759766\n",
      "epoch: 59,  batch step: 84, loss: 14.556131362915039\n",
      "epoch: 59,  batch step: 85, loss: 5.3849005699157715\n",
      "epoch: 59,  batch step: 86, loss: 30.439899444580078\n",
      "epoch: 59,  batch step: 87, loss: 5.358707427978516\n",
      "epoch: 59,  batch step: 88, loss: 10.187844276428223\n",
      "epoch: 59,  batch step: 89, loss: 4.365589141845703\n",
      "epoch: 59,  batch step: 90, loss: 5.403582572937012\n",
      "epoch: 59,  batch step: 91, loss: 31.020328521728516\n",
      "epoch: 59,  batch step: 92, loss: 6.263980865478516\n",
      "epoch: 59,  batch step: 93, loss: 10.846673965454102\n",
      "epoch: 59,  batch step: 94, loss: 29.580978393554688\n",
      "epoch: 59,  batch step: 95, loss: 2.62166166305542\n",
      "epoch: 59,  batch step: 96, loss: 4.247139930725098\n",
      "epoch: 59,  batch step: 97, loss: 3.5802016258239746\n",
      "epoch: 59,  batch step: 98, loss: 40.25908279418945\n",
      "epoch: 59,  batch step: 99, loss: 19.24042510986328\n",
      "epoch: 59,  batch step: 100, loss: 26.516347885131836\n",
      "epoch: 59,  batch step: 101, loss: 3.0613512992858887\n",
      "epoch: 59,  batch step: 102, loss: 3.4362730979919434\n",
      "epoch: 59,  batch step: 103, loss: 4.7324018478393555\n",
      "epoch: 59,  batch step: 104, loss: 29.92489242553711\n",
      "epoch: 59,  batch step: 105, loss: 6.5926833152771\n",
      "epoch: 59,  batch step: 106, loss: 14.746034622192383\n",
      "epoch: 59,  batch step: 107, loss: 39.899505615234375\n",
      "epoch: 59,  batch step: 108, loss: 5.1627197265625\n",
      "epoch: 59,  batch step: 109, loss: 3.4930922985076904\n",
      "epoch: 59,  batch step: 110, loss: 3.011137008666992\n",
      "epoch: 59,  batch step: 111, loss: 29.02395248413086\n",
      "epoch: 59,  batch step: 112, loss: 21.632339477539062\n",
      "epoch: 59,  batch step: 113, loss: 50.02305603027344\n",
      "epoch: 59,  batch step: 114, loss: 24.007360458374023\n",
      "epoch: 59,  batch step: 115, loss: 4.993200302124023\n",
      "epoch: 59,  batch step: 116, loss: 5.3027238845825195\n",
      "epoch: 59,  batch step: 117, loss: 7.583643913269043\n",
      "epoch: 59,  batch step: 118, loss: 5.191868782043457\n",
      "epoch: 59,  batch step: 119, loss: 3.679457664489746\n",
      "epoch: 59,  batch step: 120, loss: 90.93524169921875\n",
      "epoch: 59,  batch step: 121, loss: 3.567242383956909\n",
      "epoch: 59,  batch step: 122, loss: 19.006528854370117\n",
      "epoch: 59,  batch step: 123, loss: 2.062431573867798\n",
      "epoch: 59,  batch step: 124, loss: 6.818360805511475\n",
      "epoch: 59,  batch step: 125, loss: 3.324944496154785\n",
      "epoch: 59,  batch step: 126, loss: 3.960681438446045\n",
      "epoch: 59,  batch step: 127, loss: 23.7460994720459\n",
      "epoch: 59,  batch step: 128, loss: 170.4774627685547\n",
      "epoch: 59,  batch step: 129, loss: 4.856413841247559\n",
      "epoch: 59,  batch step: 130, loss: 14.629369735717773\n",
      "epoch: 59,  batch step: 131, loss: 22.933889389038086\n",
      "epoch: 59,  batch step: 132, loss: 4.0175018310546875\n",
      "epoch: 59,  batch step: 133, loss: 24.092771530151367\n",
      "epoch: 59,  batch step: 134, loss: 5.836730480194092\n",
      "epoch: 59,  batch step: 135, loss: 4.095643520355225\n",
      "epoch: 59,  batch step: 136, loss: 5.665875434875488\n",
      "epoch: 59,  batch step: 137, loss: 27.376081466674805\n",
      "epoch: 59,  batch step: 138, loss: 6.329165935516357\n",
      "epoch: 59,  batch step: 139, loss: 2.8153369426727295\n",
      "epoch: 59,  batch step: 140, loss: 5.835878372192383\n",
      "epoch: 59,  batch step: 141, loss: 50.930145263671875\n",
      "epoch: 59,  batch step: 142, loss: 6.03964376449585\n",
      "epoch: 59,  batch step: 143, loss: 4.363637924194336\n",
      "epoch: 59,  batch step: 144, loss: 3.1310901641845703\n",
      "epoch: 59,  batch step: 145, loss: 3.940082550048828\n",
      "epoch: 59,  batch step: 146, loss: 3.7748517990112305\n",
      "epoch: 59,  batch step: 147, loss: 63.10554122924805\n",
      "epoch: 59,  batch step: 148, loss: 3.5051429271698\n",
      "epoch: 59,  batch step: 149, loss: 21.326122283935547\n",
      "epoch: 59,  batch step: 150, loss: 56.940147399902344\n",
      "epoch: 59,  batch step: 151, loss: 17.01104164123535\n",
      "epoch: 59,  batch step: 152, loss: 46.270076751708984\n",
      "epoch: 59,  batch step: 153, loss: 106.75169372558594\n",
      "epoch: 59,  batch step: 154, loss: 4.129757881164551\n",
      "epoch: 59,  batch step: 155, loss: 42.24077224731445\n",
      "epoch: 59,  batch step: 156, loss: 4.056700706481934\n",
      "epoch: 59,  batch step: 157, loss: 23.085338592529297\n",
      "epoch: 59,  batch step: 158, loss: 26.985546112060547\n",
      "epoch: 59,  batch step: 159, loss: 40.23649978637695\n",
      "epoch: 59,  batch step: 160, loss: 2.3101730346679688\n",
      "epoch: 59,  batch step: 161, loss: 17.198436737060547\n",
      "epoch: 59,  batch step: 162, loss: 5.412380218505859\n",
      "epoch: 59,  batch step: 163, loss: 4.779902458190918\n",
      "epoch: 59,  batch step: 164, loss: 29.49984359741211\n",
      "epoch: 59,  batch step: 165, loss: 22.550567626953125\n",
      "epoch: 59,  batch step: 166, loss: 27.251068115234375\n",
      "epoch: 59,  batch step: 167, loss: 35.0214958190918\n",
      "epoch: 59,  batch step: 168, loss: 13.11212158203125\n",
      "epoch: 59,  batch step: 169, loss: 4.24444055557251\n",
      "epoch: 59,  batch step: 170, loss: 19.08643341064453\n",
      "epoch: 59,  batch step: 171, loss: 5.855748176574707\n",
      "epoch: 59,  batch step: 172, loss: 8.218552589416504\n",
      "epoch: 59,  batch step: 173, loss: 13.605825424194336\n",
      "epoch: 59,  batch step: 174, loss: 45.972862243652344\n",
      "epoch: 59,  batch step: 175, loss: 51.59324645996094\n",
      "epoch: 59,  batch step: 176, loss: 4.14813232421875\n",
      "epoch: 59,  batch step: 177, loss: 30.821971893310547\n",
      "epoch: 59,  batch step: 178, loss: 39.64769744873047\n",
      "epoch: 59,  batch step: 179, loss: 3.5377206802368164\n",
      "epoch: 59,  batch step: 180, loss: 5.244906425476074\n",
      "epoch: 59,  batch step: 181, loss: 6.01800537109375\n",
      "epoch: 59,  batch step: 182, loss: 9.019841194152832\n",
      "epoch: 59,  batch step: 183, loss: 3.578190565109253\n",
      "epoch: 59,  batch step: 184, loss: 15.477374076843262\n",
      "epoch: 59,  batch step: 185, loss: 3.1308562755584717\n",
      "epoch: 59,  batch step: 186, loss: 3.2729499340057373\n",
      "epoch: 59,  batch step: 187, loss: 11.675653457641602\n",
      "epoch: 59,  batch step: 188, loss: 3.3960225582122803\n",
      "epoch: 59,  batch step: 189, loss: 28.87684440612793\n",
      "epoch: 59,  batch step: 190, loss: 5.634172439575195\n",
      "epoch: 59,  batch step: 191, loss: 4.801031112670898\n",
      "epoch: 59,  batch step: 192, loss: 30.020668029785156\n",
      "epoch: 59,  batch step: 193, loss: 106.572021484375\n",
      "epoch: 59,  batch step: 194, loss: 6.592431545257568\n",
      "epoch: 59,  batch step: 195, loss: 28.072402954101562\n",
      "epoch: 59,  batch step: 196, loss: 3.9347341060638428\n",
      "epoch: 59,  batch step: 197, loss: 6.442420482635498\n",
      "epoch: 59,  batch step: 198, loss: 18.938804626464844\n",
      "epoch: 59,  batch step: 199, loss: 37.49930953979492\n",
      "epoch: 59,  batch step: 200, loss: 51.23710250854492\n",
      "epoch: 59,  batch step: 201, loss: 6.393115520477295\n",
      "epoch: 59,  batch step: 202, loss: 34.549827575683594\n",
      "epoch: 59,  batch step: 203, loss: 6.449399948120117\n",
      "epoch: 59,  batch step: 204, loss: 8.259673118591309\n",
      "epoch: 59,  batch step: 205, loss: 19.561399459838867\n",
      "epoch: 59,  batch step: 206, loss: 34.993038177490234\n",
      "epoch: 59,  batch step: 207, loss: 30.81252670288086\n",
      "epoch: 59,  batch step: 208, loss: 7.905910491943359\n",
      "epoch: 59,  batch step: 209, loss: 7.218797206878662\n",
      "epoch: 59,  batch step: 210, loss: 40.30902862548828\n",
      "epoch: 59,  batch step: 211, loss: 64.22843170166016\n",
      "epoch: 59,  batch step: 212, loss: 53.493648529052734\n",
      "epoch: 59,  batch step: 213, loss: 110.22615814208984\n",
      "epoch: 59,  batch step: 214, loss: 6.265120506286621\n",
      "epoch: 59,  batch step: 215, loss: 40.98101043701172\n",
      "epoch: 59,  batch step: 216, loss: 5.463688850402832\n",
      "epoch: 59,  batch step: 217, loss: 6.096385955810547\n",
      "epoch: 59,  batch step: 218, loss: 5.219064712524414\n",
      "epoch: 59,  batch step: 219, loss: 37.53346633911133\n",
      "epoch: 59,  batch step: 220, loss: 5.880903244018555\n",
      "epoch: 59,  batch step: 221, loss: 16.140016555786133\n",
      "epoch: 59,  batch step: 222, loss: 4.13871955871582\n",
      "epoch: 59,  batch step: 223, loss: 29.444686889648438\n",
      "epoch: 59,  batch step: 224, loss: 4.471744537353516\n",
      "epoch: 59,  batch step: 225, loss: 3.8374621868133545\n",
      "epoch: 59,  batch step: 226, loss: 23.756450653076172\n",
      "epoch: 59,  batch step: 227, loss: 23.83058738708496\n",
      "epoch: 59,  batch step: 228, loss: 3.8047285079956055\n",
      "epoch: 59,  batch step: 229, loss: 49.681312561035156\n",
      "epoch: 59,  batch step: 230, loss: 3.262909412384033\n",
      "epoch: 59,  batch step: 231, loss: 3.6879048347473145\n",
      "epoch: 59,  batch step: 232, loss: 3.7018661499023438\n",
      "epoch: 59,  batch step: 233, loss: 5.32574987411499\n",
      "epoch: 59,  batch step: 234, loss: 27.849468231201172\n",
      "epoch: 59,  batch step: 235, loss: 34.962120056152344\n",
      "epoch: 59,  batch step: 236, loss: 3.5811586380004883\n",
      "epoch: 59,  batch step: 237, loss: 3.2329955101013184\n",
      "epoch: 59,  batch step: 238, loss: 20.919904708862305\n",
      "epoch: 59,  batch step: 239, loss: 2.645477294921875\n",
      "epoch: 59,  batch step: 240, loss: 27.898941040039062\n",
      "epoch: 59,  batch step: 241, loss: 3.1141467094421387\n",
      "epoch: 59,  batch step: 242, loss: 20.717811584472656\n",
      "epoch: 59,  batch step: 243, loss: 24.33606719970703\n",
      "epoch: 59,  batch step: 244, loss: 1.644439935684204\n",
      "epoch: 59,  batch step: 245, loss: 16.717205047607422\n",
      "epoch: 59,  batch step: 246, loss: 4.25749397277832\n",
      "epoch: 59,  batch step: 247, loss: 2.608645439147949\n",
      "epoch: 59,  batch step: 248, loss: 34.6855354309082\n",
      "epoch: 59,  batch step: 249, loss: 3.5869688987731934\n",
      "epoch: 59,  batch step: 250, loss: 29.402332305908203\n",
      "epoch: 59,  batch step: 251, loss: 109.68011474609375\n",
      "finished saving checkpoints\n",
      "validation error epoch  59:    tensor(64.3304, device='cuda:0')\n",
      "316\n",
      "epoch: 60,  batch step: 0, loss: 26.125539779663086\n",
      "epoch: 60,  batch step: 1, loss: 15.99586296081543\n",
      "epoch: 60,  batch step: 2, loss: 18.532676696777344\n",
      "epoch: 60,  batch step: 3, loss: 21.607406616210938\n",
      "epoch: 60,  batch step: 4, loss: 11.013148307800293\n",
      "epoch: 60,  batch step: 5, loss: 42.65218734741211\n",
      "epoch: 60,  batch step: 6, loss: 3.687474012374878\n",
      "epoch: 60,  batch step: 7, loss: 41.82273864746094\n",
      "epoch: 60,  batch step: 8, loss: 5.7677812576293945\n",
      "epoch: 60,  batch step: 9, loss: 30.835086822509766\n",
      "epoch: 60,  batch step: 10, loss: 12.142265319824219\n",
      "epoch: 60,  batch step: 11, loss: 7.389533996582031\n",
      "epoch: 60,  batch step: 12, loss: 6.249318599700928\n",
      "epoch: 60,  batch step: 13, loss: 7.877837657928467\n",
      "epoch: 60,  batch step: 14, loss: 10.05891227722168\n",
      "epoch: 60,  batch step: 15, loss: 6.377601623535156\n",
      "epoch: 60,  batch step: 16, loss: 23.855438232421875\n",
      "epoch: 60,  batch step: 17, loss: 10.293301582336426\n",
      "epoch: 60,  batch step: 18, loss: 4.269432067871094\n",
      "epoch: 60,  batch step: 19, loss: 27.067913055419922\n",
      "epoch: 60,  batch step: 20, loss: 22.167205810546875\n",
      "epoch: 60,  batch step: 21, loss: 11.700698852539062\n",
      "epoch: 60,  batch step: 22, loss: 4.812374114990234\n",
      "epoch: 60,  batch step: 23, loss: 11.89161205291748\n",
      "epoch: 60,  batch step: 24, loss: 3.1471517086029053\n",
      "epoch: 60,  batch step: 25, loss: 16.47437286376953\n",
      "epoch: 60,  batch step: 26, loss: 2.9471328258514404\n",
      "epoch: 60,  batch step: 27, loss: 8.597444534301758\n",
      "epoch: 60,  batch step: 28, loss: 2.4474334716796875\n",
      "epoch: 60,  batch step: 29, loss: 89.52020263671875\n",
      "epoch: 60,  batch step: 30, loss: 35.84956741333008\n",
      "epoch: 60,  batch step: 31, loss: 25.409259796142578\n",
      "epoch: 60,  batch step: 32, loss: 36.41252136230469\n",
      "epoch: 60,  batch step: 33, loss: 19.628707885742188\n",
      "epoch: 60,  batch step: 34, loss: 5.951127529144287\n",
      "epoch: 60,  batch step: 35, loss: 7.054954528808594\n",
      "epoch: 60,  batch step: 36, loss: 6.089076519012451\n",
      "epoch: 60,  batch step: 37, loss: 18.967113494873047\n",
      "epoch: 60,  batch step: 38, loss: 13.070001602172852\n",
      "epoch: 60,  batch step: 39, loss: 38.51571273803711\n",
      "epoch: 60,  batch step: 40, loss: 3.2680277824401855\n",
      "epoch: 60,  batch step: 41, loss: 3.854623794555664\n",
      "epoch: 60,  batch step: 42, loss: 10.672106742858887\n",
      "epoch: 60,  batch step: 43, loss: 5.050799369812012\n",
      "epoch: 60,  batch step: 44, loss: 15.678506851196289\n",
      "epoch: 60,  batch step: 45, loss: 9.498889923095703\n",
      "epoch: 60,  batch step: 46, loss: 23.241485595703125\n",
      "epoch: 60,  batch step: 47, loss: 29.931259155273438\n",
      "epoch: 60,  batch step: 48, loss: 5.569827556610107\n",
      "epoch: 60,  batch step: 49, loss: 5.120434284210205\n",
      "epoch: 60,  batch step: 50, loss: 68.24309539794922\n",
      "epoch: 60,  batch step: 51, loss: 2.2704572677612305\n",
      "epoch: 60,  batch step: 52, loss: 37.90106201171875\n",
      "epoch: 60,  batch step: 53, loss: 54.07374572753906\n",
      "epoch: 60,  batch step: 54, loss: 38.023311614990234\n",
      "epoch: 60,  batch step: 55, loss: 5.6121931076049805\n",
      "epoch: 60,  batch step: 56, loss: 2.899627447128296\n",
      "epoch: 60,  batch step: 57, loss: 3.7653045654296875\n",
      "epoch: 60,  batch step: 58, loss: 4.225332260131836\n",
      "epoch: 60,  batch step: 59, loss: 16.58625030517578\n",
      "epoch: 60,  batch step: 60, loss: 8.968320846557617\n",
      "epoch: 60,  batch step: 61, loss: 32.10826110839844\n",
      "epoch: 60,  batch step: 62, loss: 2.1643028259277344\n",
      "epoch: 60,  batch step: 63, loss: 4.626310348510742\n",
      "epoch: 60,  batch step: 64, loss: 4.325043678283691\n",
      "epoch: 60,  batch step: 65, loss: 12.774602890014648\n",
      "epoch: 60,  batch step: 66, loss: 4.52485990524292\n",
      "epoch: 60,  batch step: 67, loss: 2.465884208679199\n",
      "epoch: 60,  batch step: 68, loss: 26.353857040405273\n",
      "epoch: 60,  batch step: 69, loss: 3.8274993896484375\n",
      "epoch: 60,  batch step: 70, loss: 3.863431930541992\n",
      "epoch: 60,  batch step: 71, loss: 56.92527389526367\n",
      "epoch: 60,  batch step: 72, loss: 19.820833206176758\n",
      "epoch: 60,  batch step: 73, loss: 5.852110862731934\n",
      "epoch: 60,  batch step: 74, loss: 20.164785385131836\n",
      "epoch: 60,  batch step: 75, loss: 3.6201388835906982\n",
      "epoch: 60,  batch step: 76, loss: 6.418159484863281\n",
      "epoch: 60,  batch step: 77, loss: 3.480135679244995\n",
      "epoch: 60,  batch step: 78, loss: 3.5156381130218506\n",
      "epoch: 60,  batch step: 79, loss: 42.95029830932617\n",
      "epoch: 60,  batch step: 80, loss: 32.850547790527344\n",
      "epoch: 60,  batch step: 81, loss: 3.8320345878601074\n",
      "epoch: 60,  batch step: 82, loss: 4.191871166229248\n",
      "epoch: 60,  batch step: 83, loss: 15.46883773803711\n",
      "epoch: 60,  batch step: 84, loss: 4.050239086151123\n",
      "epoch: 60,  batch step: 85, loss: 3.2251944541931152\n",
      "epoch: 60,  batch step: 86, loss: 23.056835174560547\n",
      "epoch: 60,  batch step: 87, loss: 2.587101459503174\n",
      "epoch: 60,  batch step: 88, loss: 28.396709442138672\n",
      "epoch: 60,  batch step: 89, loss: 5.107512950897217\n",
      "epoch: 60,  batch step: 90, loss: 4.833902359008789\n",
      "epoch: 60,  batch step: 91, loss: 5.251103401184082\n",
      "epoch: 60,  batch step: 92, loss: 55.44091033935547\n",
      "epoch: 60,  batch step: 93, loss: 3.9478795528411865\n",
      "epoch: 60,  batch step: 94, loss: 4.445222854614258\n",
      "epoch: 60,  batch step: 95, loss: 14.861444473266602\n",
      "epoch: 60,  batch step: 96, loss: 13.294731140136719\n",
      "epoch: 60,  batch step: 97, loss: 2.9005515575408936\n",
      "epoch: 60,  batch step: 98, loss: 29.896564483642578\n",
      "epoch: 60,  batch step: 99, loss: 4.600699424743652\n",
      "epoch: 60,  batch step: 100, loss: 3.464181423187256\n",
      "epoch: 60,  batch step: 101, loss: 5.678786277770996\n",
      "epoch: 60,  batch step: 102, loss: 15.509309768676758\n",
      "epoch: 60,  batch step: 103, loss: 5.531130790710449\n",
      "epoch: 60,  batch step: 104, loss: 31.892988204956055\n",
      "epoch: 60,  batch step: 105, loss: 4.414926528930664\n",
      "epoch: 60,  batch step: 106, loss: 2.5157246589660645\n",
      "epoch: 60,  batch step: 107, loss: 16.61382484436035\n",
      "epoch: 60,  batch step: 108, loss: 1.9765472412109375\n",
      "epoch: 60,  batch step: 109, loss: 2.68853759765625\n",
      "epoch: 60,  batch step: 110, loss: 4.392274856567383\n",
      "epoch: 60,  batch step: 111, loss: 3.591073751449585\n",
      "epoch: 60,  batch step: 112, loss: 6.1089701652526855\n",
      "epoch: 60,  batch step: 113, loss: 28.54970932006836\n",
      "epoch: 60,  batch step: 114, loss: 24.12417221069336\n",
      "epoch: 60,  batch step: 115, loss: 3.0042271614074707\n",
      "epoch: 60,  batch step: 116, loss: 48.02263641357422\n",
      "epoch: 60,  batch step: 117, loss: 21.545438766479492\n",
      "epoch: 60,  batch step: 118, loss: 7.501672744750977\n",
      "epoch: 60,  batch step: 119, loss: 38.24443054199219\n",
      "epoch: 60,  batch step: 120, loss: 59.72819137573242\n",
      "epoch: 60,  batch step: 121, loss: 3.076023817062378\n",
      "epoch: 60,  batch step: 122, loss: 10.10184097290039\n",
      "epoch: 60,  batch step: 123, loss: 24.54910659790039\n",
      "epoch: 60,  batch step: 124, loss: 15.16111946105957\n",
      "epoch: 60,  batch step: 125, loss: 2.9162025451660156\n",
      "epoch: 60,  batch step: 126, loss: 61.89055633544922\n",
      "epoch: 60,  batch step: 127, loss: 6.806979179382324\n",
      "epoch: 60,  batch step: 128, loss: 5.148456573486328\n",
      "epoch: 60,  batch step: 129, loss: 32.6405029296875\n",
      "epoch: 60,  batch step: 130, loss: 25.422523498535156\n",
      "epoch: 60,  batch step: 131, loss: 45.39167022705078\n",
      "epoch: 60,  batch step: 132, loss: 24.62495994567871\n",
      "epoch: 60,  batch step: 133, loss: 3.2070062160491943\n",
      "epoch: 60,  batch step: 134, loss: 3.323429822921753\n",
      "epoch: 60,  batch step: 135, loss: 4.91530179977417\n",
      "epoch: 60,  batch step: 136, loss: 15.03570556640625\n",
      "epoch: 60,  batch step: 137, loss: 28.843196868896484\n",
      "epoch: 60,  batch step: 138, loss: 43.457725524902344\n",
      "epoch: 60,  batch step: 139, loss: 6.0230817794799805\n",
      "epoch: 60,  batch step: 140, loss: 11.43952465057373\n",
      "epoch: 60,  batch step: 141, loss: 4.570703983306885\n",
      "epoch: 60,  batch step: 142, loss: 3.818279266357422\n",
      "epoch: 60,  batch step: 143, loss: 8.659181594848633\n",
      "epoch: 60,  batch step: 144, loss: 8.069976806640625\n",
      "epoch: 60,  batch step: 145, loss: 20.933456420898438\n",
      "epoch: 60,  batch step: 146, loss: 20.704877853393555\n",
      "epoch: 60,  batch step: 147, loss: 4.697717189788818\n",
      "epoch: 60,  batch step: 148, loss: 28.449888229370117\n",
      "epoch: 60,  batch step: 149, loss: 25.684682846069336\n",
      "epoch: 60,  batch step: 150, loss: 16.547452926635742\n",
      "epoch: 60,  batch step: 151, loss: 5.539323329925537\n",
      "epoch: 60,  batch step: 152, loss: 14.754034996032715\n",
      "epoch: 60,  batch step: 153, loss: 2.692486524581909\n",
      "epoch: 60,  batch step: 154, loss: 51.04371643066406\n",
      "epoch: 60,  batch step: 155, loss: 30.22751235961914\n",
      "epoch: 60,  batch step: 156, loss: 34.14252471923828\n",
      "epoch: 60,  batch step: 157, loss: 5.533756256103516\n",
      "epoch: 60,  batch step: 158, loss: 4.296375274658203\n",
      "epoch: 60,  batch step: 159, loss: 23.527891159057617\n",
      "epoch: 60,  batch step: 160, loss: 4.631900310516357\n",
      "epoch: 60,  batch step: 161, loss: 6.181328773498535\n",
      "epoch: 60,  batch step: 162, loss: 46.18793869018555\n",
      "epoch: 60,  batch step: 163, loss: 4.981740474700928\n",
      "epoch: 60,  batch step: 164, loss: 23.387121200561523\n",
      "epoch: 60,  batch step: 165, loss: 52.615821838378906\n",
      "epoch: 60,  batch step: 166, loss: 46.47093963623047\n",
      "epoch: 60,  batch step: 167, loss: 16.318180084228516\n",
      "epoch: 60,  batch step: 168, loss: 6.503901481628418\n",
      "epoch: 60,  batch step: 169, loss: 49.938255310058594\n",
      "epoch: 60,  batch step: 170, loss: 8.545711517333984\n",
      "epoch: 60,  batch step: 171, loss: 16.27296257019043\n",
      "epoch: 60,  batch step: 172, loss: 4.4360432624816895\n",
      "epoch: 60,  batch step: 173, loss: 37.87066650390625\n",
      "epoch: 60,  batch step: 174, loss: 3.7894656658172607\n",
      "epoch: 60,  batch step: 175, loss: 4.070925712585449\n",
      "epoch: 60,  batch step: 176, loss: 11.147590637207031\n",
      "epoch: 60,  batch step: 177, loss: 21.615283966064453\n",
      "epoch: 60,  batch step: 178, loss: 4.101212501525879\n",
      "epoch: 60,  batch step: 179, loss: 36.07389831542969\n",
      "epoch: 60,  batch step: 180, loss: 7.440756797790527\n",
      "epoch: 60,  batch step: 181, loss: 3.573981761932373\n",
      "epoch: 60,  batch step: 182, loss: 7.225892543792725\n",
      "epoch: 60,  batch step: 183, loss: 4.238134384155273\n",
      "epoch: 60,  batch step: 184, loss: 34.79194641113281\n",
      "epoch: 60,  batch step: 185, loss: 65.35678100585938\n",
      "epoch: 60,  batch step: 186, loss: 4.988266944885254\n",
      "epoch: 60,  batch step: 187, loss: 57.331642150878906\n",
      "epoch: 60,  batch step: 188, loss: 3.4323697090148926\n",
      "epoch: 60,  batch step: 189, loss: 60.31367492675781\n",
      "epoch: 60,  batch step: 190, loss: 9.175602912902832\n",
      "epoch: 60,  batch step: 191, loss: 7.149434566497803\n",
      "epoch: 60,  batch step: 192, loss: 26.6091365814209\n",
      "epoch: 60,  batch step: 193, loss: 17.741212844848633\n",
      "epoch: 60,  batch step: 194, loss: 62.726409912109375\n",
      "epoch: 60,  batch step: 195, loss: 4.17390775680542\n",
      "epoch: 60,  batch step: 196, loss: 51.356468200683594\n",
      "epoch: 60,  batch step: 197, loss: 9.875319480895996\n",
      "epoch: 60,  batch step: 198, loss: 24.792272567749023\n",
      "epoch: 60,  batch step: 199, loss: 52.198875427246094\n",
      "epoch: 60,  batch step: 200, loss: 52.223854064941406\n",
      "epoch: 60,  batch step: 201, loss: 35.04026794433594\n",
      "epoch: 60,  batch step: 202, loss: 6.9911723136901855\n",
      "epoch: 60,  batch step: 203, loss: 4.783406734466553\n",
      "epoch: 60,  batch step: 204, loss: 4.8495683670043945\n",
      "epoch: 60,  batch step: 205, loss: 24.375839233398438\n",
      "epoch: 60,  batch step: 206, loss: 41.59452819824219\n",
      "epoch: 60,  batch step: 207, loss: 3.889305591583252\n",
      "epoch: 60,  batch step: 208, loss: 27.988449096679688\n",
      "epoch: 60,  batch step: 209, loss: 20.75084686279297\n",
      "epoch: 60,  batch step: 210, loss: 33.601932525634766\n",
      "epoch: 60,  batch step: 211, loss: 136.7633819580078\n",
      "epoch: 60,  batch step: 212, loss: 37.62434005737305\n",
      "epoch: 60,  batch step: 213, loss: 7.2797980308532715\n",
      "epoch: 60,  batch step: 214, loss: 52.384178161621094\n",
      "epoch: 60,  batch step: 215, loss: 5.280388832092285\n",
      "epoch: 60,  batch step: 216, loss: 11.02789306640625\n",
      "epoch: 60,  batch step: 217, loss: 18.3564453125\n",
      "epoch: 60,  batch step: 218, loss: 44.911766052246094\n",
      "epoch: 60,  batch step: 219, loss: 5.203354835510254\n",
      "epoch: 60,  batch step: 220, loss: 49.60594940185547\n",
      "epoch: 60,  batch step: 221, loss: 23.602773666381836\n",
      "epoch: 60,  batch step: 222, loss: 18.366369247436523\n",
      "epoch: 60,  batch step: 223, loss: 16.238739013671875\n",
      "epoch: 60,  batch step: 224, loss: 33.85137176513672\n",
      "epoch: 60,  batch step: 225, loss: 22.659666061401367\n",
      "epoch: 60,  batch step: 226, loss: 3.6669540405273438\n",
      "epoch: 60,  batch step: 227, loss: 13.260988235473633\n",
      "epoch: 60,  batch step: 228, loss: 5.198137283325195\n",
      "epoch: 60,  batch step: 229, loss: 54.424461364746094\n",
      "epoch: 60,  batch step: 230, loss: 2.522217273712158\n",
      "epoch: 60,  batch step: 231, loss: 13.831314086914062\n",
      "epoch: 60,  batch step: 232, loss: 3.667586088180542\n",
      "epoch: 60,  batch step: 233, loss: 47.20091247558594\n",
      "epoch: 60,  batch step: 234, loss: 4.990289688110352\n",
      "epoch: 60,  batch step: 235, loss: 4.23980712890625\n",
      "epoch: 60,  batch step: 236, loss: 4.0178327560424805\n",
      "epoch: 60,  batch step: 237, loss: 35.7476921081543\n",
      "epoch: 60,  batch step: 238, loss: 31.614612579345703\n",
      "epoch: 60,  batch step: 239, loss: 17.468475341796875\n",
      "epoch: 60,  batch step: 240, loss: 23.07215690612793\n",
      "epoch: 60,  batch step: 241, loss: 42.46318054199219\n",
      "epoch: 60,  batch step: 242, loss: 24.323486328125\n",
      "epoch: 60,  batch step: 243, loss: 4.225334167480469\n",
      "epoch: 60,  batch step: 244, loss: 22.031667709350586\n",
      "epoch: 60,  batch step: 245, loss: 19.044570922851562\n",
      "epoch: 60,  batch step: 246, loss: 3.2871313095092773\n",
      "epoch: 60,  batch step: 247, loss: 6.017615795135498\n",
      "epoch: 60,  batch step: 248, loss: 29.757522583007812\n",
      "epoch: 60,  batch step: 249, loss: 5.609145641326904\n",
      "epoch: 60,  batch step: 250, loss: 12.452154159545898\n",
      "epoch: 60,  batch step: 251, loss: 8.029684066772461\n",
      "validation error epoch  60:    tensor(72.1127, device='cuda:0')\n",
      "316\n",
      "epoch: 61,  batch step: 0, loss: 3.768857479095459\n",
      "epoch: 61,  batch step: 1, loss: 3.815511703491211\n",
      "epoch: 61,  batch step: 2, loss: 5.812844753265381\n",
      "epoch: 61,  batch step: 3, loss: 12.767450332641602\n",
      "epoch: 61,  batch step: 4, loss: 10.00196647644043\n",
      "epoch: 61,  batch step: 5, loss: 4.8634562492370605\n",
      "epoch: 61,  batch step: 6, loss: 4.054732799530029\n",
      "epoch: 61,  batch step: 7, loss: 90.55555725097656\n",
      "epoch: 61,  batch step: 8, loss: 43.39707565307617\n",
      "epoch: 61,  batch step: 9, loss: 35.210411071777344\n",
      "epoch: 61,  batch step: 10, loss: 5.859676361083984\n",
      "epoch: 61,  batch step: 11, loss: 27.08734130859375\n",
      "epoch: 61,  batch step: 12, loss: 36.935333251953125\n",
      "epoch: 61,  batch step: 13, loss: 5.133953094482422\n",
      "epoch: 61,  batch step: 14, loss: 3.923542022705078\n",
      "epoch: 61,  batch step: 15, loss: 52.72364807128906\n",
      "epoch: 61,  batch step: 16, loss: 3.2473769187927246\n",
      "epoch: 61,  batch step: 17, loss: 3.6354851722717285\n",
      "epoch: 61,  batch step: 18, loss: 21.308774948120117\n",
      "epoch: 61,  batch step: 19, loss: 2.7020792961120605\n",
      "epoch: 61,  batch step: 20, loss: 24.01128387451172\n",
      "epoch: 61,  batch step: 21, loss: 11.728414535522461\n",
      "epoch: 61,  batch step: 22, loss: 4.527348518371582\n",
      "epoch: 61,  batch step: 23, loss: 4.492836952209473\n",
      "epoch: 61,  batch step: 24, loss: 8.551963806152344\n",
      "epoch: 61,  batch step: 25, loss: 29.606414794921875\n",
      "epoch: 61,  batch step: 26, loss: 4.764535427093506\n",
      "epoch: 61,  batch step: 27, loss: 5.142675399780273\n",
      "epoch: 61,  batch step: 28, loss: 31.838773727416992\n",
      "epoch: 61,  batch step: 29, loss: 17.564491271972656\n",
      "epoch: 61,  batch step: 30, loss: 27.31757926940918\n",
      "epoch: 61,  batch step: 31, loss: 4.029567718505859\n",
      "epoch: 61,  batch step: 32, loss: 7.8176422119140625\n",
      "epoch: 61,  batch step: 33, loss: 3.9907760620117188\n",
      "epoch: 61,  batch step: 34, loss: 23.075429916381836\n",
      "epoch: 61,  batch step: 35, loss: 21.571979522705078\n",
      "epoch: 61,  batch step: 36, loss: 3.8453195095062256\n",
      "epoch: 61,  batch step: 37, loss: 24.01253890991211\n",
      "epoch: 61,  batch step: 38, loss: 5.286140441894531\n",
      "epoch: 61,  batch step: 39, loss: 15.42221450805664\n",
      "epoch: 61,  batch step: 40, loss: 2.718791961669922\n",
      "epoch: 61,  batch step: 41, loss: 3.0671727657318115\n",
      "epoch: 61,  batch step: 42, loss: 108.76937866210938\n",
      "epoch: 61,  batch step: 43, loss: 23.92055320739746\n",
      "epoch: 61,  batch step: 44, loss: 3.3373775482177734\n",
      "epoch: 61,  batch step: 45, loss: 34.47886657714844\n",
      "epoch: 61,  batch step: 46, loss: 4.489157676696777\n",
      "epoch: 61,  batch step: 47, loss: 30.1026668548584\n",
      "epoch: 61,  batch step: 48, loss: 50.443397521972656\n",
      "epoch: 61,  batch step: 49, loss: 4.3220953941345215\n",
      "epoch: 61,  batch step: 50, loss: 17.35320472717285\n",
      "epoch: 61,  batch step: 51, loss: 18.261016845703125\n",
      "epoch: 61,  batch step: 52, loss: 9.0014066696167\n",
      "epoch: 61,  batch step: 53, loss: 7.482532024383545\n",
      "epoch: 61,  batch step: 54, loss: 4.277826309204102\n",
      "epoch: 61,  batch step: 55, loss: 17.004915237426758\n",
      "epoch: 61,  batch step: 56, loss: 19.605670928955078\n",
      "epoch: 61,  batch step: 57, loss: 3.3270907402038574\n",
      "epoch: 61,  batch step: 58, loss: 31.29050064086914\n",
      "epoch: 61,  batch step: 59, loss: 15.073271751403809\n",
      "epoch: 61,  batch step: 60, loss: 13.431549072265625\n",
      "epoch: 61,  batch step: 61, loss: 4.585189342498779\n",
      "epoch: 61,  batch step: 62, loss: 13.347990036010742\n",
      "epoch: 61,  batch step: 63, loss: 4.763397216796875\n",
      "epoch: 61,  batch step: 64, loss: 26.433086395263672\n",
      "epoch: 61,  batch step: 65, loss: 9.161263465881348\n",
      "epoch: 61,  batch step: 66, loss: 6.154747009277344\n",
      "epoch: 61,  batch step: 67, loss: 6.2351508140563965\n",
      "epoch: 61,  batch step: 68, loss: 4.181736946105957\n",
      "epoch: 61,  batch step: 69, loss: 13.407535552978516\n",
      "epoch: 61,  batch step: 70, loss: 24.488370895385742\n",
      "epoch: 61,  batch step: 71, loss: 3.933645248413086\n",
      "epoch: 61,  batch step: 72, loss: 26.71389389038086\n",
      "epoch: 61,  batch step: 73, loss: 3.6339564323425293\n",
      "epoch: 61,  batch step: 74, loss: 17.27676010131836\n",
      "epoch: 61,  batch step: 75, loss: 7.931850433349609\n",
      "epoch: 61,  batch step: 76, loss: 12.881060600280762\n",
      "epoch: 61,  batch step: 77, loss: 11.529502868652344\n",
      "epoch: 61,  batch step: 78, loss: 22.369338989257812\n",
      "epoch: 61,  batch step: 79, loss: 37.62059783935547\n",
      "epoch: 61,  batch step: 80, loss: 19.99852752685547\n",
      "epoch: 61,  batch step: 81, loss: 4.270404815673828\n",
      "epoch: 61,  batch step: 82, loss: 2.7201602458953857\n",
      "epoch: 61,  batch step: 83, loss: 3.0652785301208496\n",
      "epoch: 61,  batch step: 84, loss: 4.401731014251709\n",
      "epoch: 61,  batch step: 85, loss: 4.008758068084717\n",
      "epoch: 61,  batch step: 86, loss: 15.961130142211914\n",
      "epoch: 61,  batch step: 87, loss: 3.2677383422851562\n",
      "epoch: 61,  batch step: 88, loss: 3.6776680946350098\n",
      "epoch: 61,  batch step: 89, loss: 8.143342018127441\n",
      "epoch: 61,  batch step: 90, loss: 50.40389633178711\n",
      "epoch: 61,  batch step: 91, loss: 3.2461307048797607\n",
      "epoch: 61,  batch step: 92, loss: 3.343182325363159\n",
      "epoch: 61,  batch step: 93, loss: 62.879878997802734\n",
      "epoch: 61,  batch step: 94, loss: 4.716287612915039\n",
      "epoch: 61,  batch step: 95, loss: 63.31523895263672\n",
      "epoch: 61,  batch step: 96, loss: 5.574782848358154\n",
      "epoch: 61,  batch step: 97, loss: 32.36396789550781\n",
      "epoch: 61,  batch step: 98, loss: 10.659296035766602\n",
      "epoch: 61,  batch step: 99, loss: 31.729076385498047\n",
      "epoch: 61,  batch step: 100, loss: 3.4686875343322754\n",
      "epoch: 61,  batch step: 101, loss: 15.332552909851074\n",
      "epoch: 61,  batch step: 102, loss: 34.50554656982422\n",
      "epoch: 61,  batch step: 103, loss: 67.40203857421875\n",
      "epoch: 61,  batch step: 104, loss: 43.81528854370117\n",
      "epoch: 61,  batch step: 105, loss: 4.757755279541016\n",
      "epoch: 61,  batch step: 106, loss: 17.069942474365234\n",
      "epoch: 61,  batch step: 107, loss: 5.391269207000732\n",
      "epoch: 61,  batch step: 108, loss: 134.597900390625\n",
      "epoch: 61,  batch step: 109, loss: 7.435787200927734\n",
      "epoch: 61,  batch step: 110, loss: 2.5473949909210205\n",
      "epoch: 61,  batch step: 111, loss: 4.067634105682373\n",
      "epoch: 61,  batch step: 112, loss: 7.067998886108398\n",
      "epoch: 61,  batch step: 113, loss: 4.1301188468933105\n",
      "epoch: 61,  batch step: 114, loss: 8.258514404296875\n",
      "epoch: 61,  batch step: 115, loss: 3.826171875\n",
      "epoch: 61,  batch step: 116, loss: 22.748371124267578\n",
      "epoch: 61,  batch step: 117, loss: 24.187973022460938\n",
      "epoch: 61,  batch step: 118, loss: 18.418991088867188\n",
      "epoch: 61,  batch step: 119, loss: 16.767480850219727\n",
      "epoch: 61,  batch step: 120, loss: 41.84764862060547\n",
      "epoch: 61,  batch step: 121, loss: 15.499762535095215\n",
      "epoch: 61,  batch step: 122, loss: 5.611941814422607\n",
      "epoch: 61,  batch step: 123, loss: 18.922147750854492\n",
      "epoch: 61,  batch step: 124, loss: 17.05677032470703\n",
      "epoch: 61,  batch step: 125, loss: 22.781253814697266\n",
      "epoch: 61,  batch step: 126, loss: 2.5581517219543457\n",
      "epoch: 61,  batch step: 127, loss: 63.03273010253906\n",
      "epoch: 61,  batch step: 128, loss: 24.27145004272461\n",
      "epoch: 61,  batch step: 129, loss: 8.477628707885742\n",
      "epoch: 61,  batch step: 130, loss: 21.144790649414062\n",
      "epoch: 61,  batch step: 131, loss: 41.91336441040039\n",
      "epoch: 61,  batch step: 132, loss: 16.392419815063477\n",
      "epoch: 61,  batch step: 133, loss: 27.19227409362793\n",
      "epoch: 61,  batch step: 134, loss: 2.4286797046661377\n",
      "epoch: 61,  batch step: 135, loss: 5.112354278564453\n",
      "epoch: 61,  batch step: 136, loss: 59.063472747802734\n",
      "epoch: 61,  batch step: 137, loss: 4.26121711730957\n",
      "epoch: 61,  batch step: 138, loss: 62.764793395996094\n",
      "epoch: 61,  batch step: 139, loss: 62.86888122558594\n",
      "epoch: 61,  batch step: 140, loss: 28.958110809326172\n",
      "epoch: 61,  batch step: 141, loss: 6.857104778289795\n",
      "epoch: 61,  batch step: 142, loss: 6.146716117858887\n",
      "epoch: 61,  batch step: 143, loss: 16.03595542907715\n",
      "epoch: 61,  batch step: 144, loss: 5.838387489318848\n",
      "epoch: 61,  batch step: 145, loss: 18.338855743408203\n",
      "epoch: 61,  batch step: 146, loss: 5.161888122558594\n",
      "epoch: 61,  batch step: 147, loss: 66.05099487304688\n",
      "epoch: 61,  batch step: 148, loss: 5.29642391204834\n",
      "epoch: 61,  batch step: 149, loss: 5.538499355316162\n",
      "epoch: 61,  batch step: 150, loss: 3.8806395530700684\n",
      "epoch: 61,  batch step: 151, loss: 28.053848266601562\n",
      "epoch: 61,  batch step: 152, loss: 25.3249568939209\n",
      "epoch: 61,  batch step: 153, loss: 5.899990081787109\n",
      "epoch: 61,  batch step: 154, loss: 3.8146562576293945\n",
      "epoch: 61,  batch step: 155, loss: 4.087224960327148\n",
      "epoch: 61,  batch step: 156, loss: 19.710166931152344\n",
      "epoch: 61,  batch step: 157, loss: 7.496781826019287\n",
      "epoch: 61,  batch step: 158, loss: 18.282873153686523\n",
      "epoch: 61,  batch step: 159, loss: 61.48735809326172\n",
      "epoch: 61,  batch step: 160, loss: 40.57478332519531\n",
      "epoch: 61,  batch step: 161, loss: 16.806377410888672\n",
      "epoch: 61,  batch step: 162, loss: 75.52770233154297\n",
      "epoch: 61,  batch step: 163, loss: 3.275420665740967\n",
      "epoch: 61,  batch step: 164, loss: 4.723220348358154\n",
      "epoch: 61,  batch step: 165, loss: 68.60769653320312\n",
      "epoch: 61,  batch step: 166, loss: 25.171667098999023\n",
      "epoch: 61,  batch step: 167, loss: 6.347970485687256\n",
      "epoch: 61,  batch step: 168, loss: 5.212647438049316\n",
      "epoch: 61,  batch step: 169, loss: 5.177607536315918\n",
      "epoch: 61,  batch step: 170, loss: 13.738237380981445\n",
      "epoch: 61,  batch step: 171, loss: 7.084710597991943\n",
      "epoch: 61,  batch step: 172, loss: 92.28559875488281\n",
      "epoch: 61,  batch step: 173, loss: 5.78715705871582\n",
      "epoch: 61,  batch step: 174, loss: 4.6999030113220215\n",
      "epoch: 61,  batch step: 175, loss: 7.280714988708496\n",
      "epoch: 61,  batch step: 176, loss: 73.23751831054688\n",
      "epoch: 61,  batch step: 177, loss: 4.288351058959961\n",
      "epoch: 61,  batch step: 178, loss: 7.08447790145874\n",
      "epoch: 61,  batch step: 179, loss: 15.008391380310059\n",
      "epoch: 61,  batch step: 180, loss: 8.277700424194336\n",
      "epoch: 61,  batch step: 181, loss: 31.413227081298828\n",
      "epoch: 61,  batch step: 182, loss: 36.04956817626953\n",
      "epoch: 61,  batch step: 183, loss: 3.571949005126953\n",
      "epoch: 61,  batch step: 184, loss: 5.792306423187256\n",
      "epoch: 61,  batch step: 185, loss: 4.16646146774292\n",
      "epoch: 61,  batch step: 186, loss: 24.386343002319336\n",
      "epoch: 61,  batch step: 187, loss: 18.981163024902344\n",
      "epoch: 61,  batch step: 188, loss: 4.392337799072266\n",
      "epoch: 61,  batch step: 189, loss: 5.0223493576049805\n",
      "epoch: 61,  batch step: 190, loss: 34.31672668457031\n",
      "epoch: 61,  batch step: 191, loss: 3.285656452178955\n",
      "epoch: 61,  batch step: 192, loss: 41.0822868347168\n",
      "epoch: 61,  batch step: 193, loss: 157.08560180664062\n",
      "epoch: 61,  batch step: 194, loss: 2.888629913330078\n",
      "epoch: 61,  batch step: 195, loss: 19.070072174072266\n",
      "epoch: 61,  batch step: 196, loss: 6.036372184753418\n",
      "epoch: 61,  batch step: 197, loss: 38.5201416015625\n",
      "epoch: 61,  batch step: 198, loss: 9.381742477416992\n",
      "epoch: 61,  batch step: 199, loss: 102.0128402709961\n",
      "epoch: 61,  batch step: 200, loss: 64.09590148925781\n",
      "epoch: 61,  batch step: 201, loss: 4.800297260284424\n",
      "epoch: 61,  batch step: 202, loss: 17.08550262451172\n",
      "epoch: 61,  batch step: 203, loss: 39.71965789794922\n",
      "epoch: 61,  batch step: 204, loss: 25.386571884155273\n",
      "epoch: 61,  batch step: 205, loss: 6.139695644378662\n",
      "epoch: 61,  batch step: 206, loss: 7.517350196838379\n",
      "epoch: 61,  batch step: 207, loss: 16.69831657409668\n",
      "epoch: 61,  batch step: 208, loss: 10.468526840209961\n",
      "epoch: 61,  batch step: 209, loss: 44.19916534423828\n",
      "epoch: 61,  batch step: 210, loss: 120.8772201538086\n",
      "epoch: 61,  batch step: 211, loss: 37.04959487915039\n",
      "epoch: 61,  batch step: 212, loss: 7.169341087341309\n",
      "epoch: 61,  batch step: 213, loss: 5.374702453613281\n",
      "epoch: 61,  batch step: 214, loss: 4.09363317489624\n",
      "epoch: 61,  batch step: 215, loss: 5.292943477630615\n",
      "epoch: 61,  batch step: 216, loss: 10.993927001953125\n",
      "epoch: 61,  batch step: 217, loss: 24.02490234375\n",
      "epoch: 61,  batch step: 218, loss: 3.7980594635009766\n",
      "epoch: 61,  batch step: 219, loss: 8.117639541625977\n",
      "epoch: 61,  batch step: 220, loss: 3.509186267852783\n",
      "epoch: 61,  batch step: 221, loss: 5.081000328063965\n",
      "epoch: 61,  batch step: 222, loss: 27.604408264160156\n",
      "epoch: 61,  batch step: 223, loss: 42.33294677734375\n",
      "epoch: 61,  batch step: 224, loss: 2.877204418182373\n",
      "epoch: 61,  batch step: 225, loss: 107.32418060302734\n",
      "epoch: 61,  batch step: 226, loss: 5.909024715423584\n",
      "epoch: 61,  batch step: 227, loss: 4.671021938323975\n",
      "epoch: 61,  batch step: 228, loss: 4.54361629486084\n",
      "epoch: 61,  batch step: 229, loss: 2.9878623485565186\n",
      "epoch: 61,  batch step: 230, loss: 2.7806193828582764\n",
      "epoch: 61,  batch step: 231, loss: 11.380996704101562\n",
      "epoch: 61,  batch step: 232, loss: 18.66779327392578\n",
      "epoch: 61,  batch step: 233, loss: 28.124860763549805\n",
      "epoch: 61,  batch step: 234, loss: 44.61186981201172\n",
      "epoch: 61,  batch step: 235, loss: 2.7130515575408936\n",
      "epoch: 61,  batch step: 236, loss: 31.202678680419922\n",
      "epoch: 61,  batch step: 237, loss: 9.666431427001953\n",
      "epoch: 61,  batch step: 238, loss: 30.264644622802734\n",
      "epoch: 61,  batch step: 239, loss: 19.50227165222168\n",
      "epoch: 61,  batch step: 240, loss: 3.242748975753784\n",
      "epoch: 61,  batch step: 241, loss: 58.125579833984375\n",
      "epoch: 61,  batch step: 242, loss: 38.99427795410156\n",
      "epoch: 61,  batch step: 243, loss: 3.474482297897339\n",
      "epoch: 61,  batch step: 244, loss: 37.61720275878906\n",
      "epoch: 61,  batch step: 245, loss: 29.712583541870117\n",
      "epoch: 61,  batch step: 246, loss: 6.414942741394043\n",
      "epoch: 61,  batch step: 247, loss: 30.610746383666992\n",
      "epoch: 61,  batch step: 248, loss: 6.175711631774902\n",
      "epoch: 61,  batch step: 249, loss: 41.474403381347656\n",
      "epoch: 61,  batch step: 250, loss: 3.315415859222412\n",
      "epoch: 61,  batch step: 251, loss: 121.38653564453125\n",
      "validation error epoch  61:    tensor(78.3186, device='cuda:0')\n",
      "316\n",
      "epoch: 62,  batch step: 0, loss: 4.6751251220703125\n",
      "epoch: 62,  batch step: 1, loss: 5.769227504730225\n",
      "epoch: 62,  batch step: 2, loss: 11.34256362915039\n",
      "epoch: 62,  batch step: 3, loss: 5.087316513061523\n",
      "epoch: 62,  batch step: 4, loss: 29.952856063842773\n",
      "epoch: 62,  batch step: 5, loss: 51.02276611328125\n",
      "epoch: 62,  batch step: 6, loss: 25.588241577148438\n",
      "epoch: 62,  batch step: 7, loss: 11.18991756439209\n",
      "epoch: 62,  batch step: 8, loss: 45.22059631347656\n",
      "epoch: 62,  batch step: 9, loss: 9.88260555267334\n",
      "epoch: 62,  batch step: 10, loss: 119.39999389648438\n",
      "epoch: 62,  batch step: 11, loss: 5.810171127319336\n",
      "epoch: 62,  batch step: 12, loss: 8.58535385131836\n",
      "epoch: 62,  batch step: 13, loss: 13.804193496704102\n",
      "epoch: 62,  batch step: 14, loss: 56.242271423339844\n",
      "epoch: 62,  batch step: 15, loss: 22.80592155456543\n",
      "epoch: 62,  batch step: 16, loss: 7.051694393157959\n",
      "epoch: 62,  batch step: 17, loss: 4.6128034591674805\n",
      "epoch: 62,  batch step: 18, loss: 18.0867862701416\n",
      "epoch: 62,  batch step: 19, loss: 8.593194961547852\n",
      "epoch: 62,  batch step: 20, loss: 45.10367965698242\n",
      "epoch: 62,  batch step: 21, loss: 53.53440856933594\n",
      "epoch: 62,  batch step: 22, loss: 6.082643032073975\n",
      "epoch: 62,  batch step: 23, loss: 13.888069152832031\n",
      "epoch: 62,  batch step: 24, loss: 26.204940795898438\n",
      "epoch: 62,  batch step: 25, loss: 38.7758674621582\n",
      "epoch: 62,  batch step: 26, loss: 16.422657012939453\n",
      "epoch: 62,  batch step: 27, loss: 6.046730041503906\n",
      "epoch: 62,  batch step: 28, loss: 45.33063888549805\n",
      "epoch: 62,  batch step: 29, loss: 5.6852922439575195\n",
      "epoch: 62,  batch step: 30, loss: 17.670461654663086\n",
      "epoch: 62,  batch step: 31, loss: 26.32050323486328\n",
      "epoch: 62,  batch step: 32, loss: 36.5433464050293\n",
      "epoch: 62,  batch step: 33, loss: 6.747710227966309\n",
      "epoch: 62,  batch step: 34, loss: 9.881427764892578\n",
      "epoch: 62,  batch step: 35, loss: 24.62645721435547\n",
      "epoch: 62,  batch step: 36, loss: 8.30759048461914\n",
      "epoch: 62,  batch step: 37, loss: 4.680537700653076\n",
      "epoch: 62,  batch step: 38, loss: 19.177480697631836\n",
      "epoch: 62,  batch step: 39, loss: 4.566291809082031\n",
      "epoch: 62,  batch step: 40, loss: 94.61614227294922\n",
      "epoch: 62,  batch step: 41, loss: 28.065261840820312\n",
      "epoch: 62,  batch step: 42, loss: 5.125547885894775\n",
      "epoch: 62,  batch step: 43, loss: 58.66776657104492\n",
      "epoch: 62,  batch step: 44, loss: 7.128896713256836\n",
      "epoch: 62,  batch step: 45, loss: 42.242523193359375\n",
      "epoch: 62,  batch step: 46, loss: 3.965676784515381\n",
      "epoch: 62,  batch step: 47, loss: 30.079143524169922\n",
      "epoch: 62,  batch step: 48, loss: 7.112707138061523\n",
      "epoch: 62,  batch step: 49, loss: 5.675471305847168\n",
      "epoch: 62,  batch step: 50, loss: 12.310919761657715\n",
      "epoch: 62,  batch step: 51, loss: 66.27459716796875\n",
      "epoch: 62,  batch step: 52, loss: 30.98033905029297\n",
      "epoch: 62,  batch step: 53, loss: 9.338296890258789\n",
      "epoch: 62,  batch step: 54, loss: 6.575797080993652\n",
      "epoch: 62,  batch step: 55, loss: 5.838565349578857\n",
      "epoch: 62,  batch step: 56, loss: 59.25362777709961\n",
      "epoch: 62,  batch step: 57, loss: 24.81549072265625\n",
      "epoch: 62,  batch step: 58, loss: 5.6166791915893555\n",
      "epoch: 62,  batch step: 59, loss: 4.667774677276611\n",
      "epoch: 62,  batch step: 60, loss: 3.7439229488372803\n",
      "epoch: 62,  batch step: 61, loss: 6.207490921020508\n",
      "epoch: 62,  batch step: 62, loss: 6.7576727867126465\n",
      "epoch: 62,  batch step: 63, loss: 9.140237808227539\n",
      "epoch: 62,  batch step: 64, loss: 27.306053161621094\n",
      "epoch: 62,  batch step: 65, loss: 7.000100135803223\n",
      "epoch: 62,  batch step: 66, loss: 37.3659553527832\n",
      "epoch: 62,  batch step: 67, loss: 6.087009429931641\n",
      "epoch: 62,  batch step: 68, loss: 4.471738815307617\n",
      "epoch: 62,  batch step: 69, loss: 7.30750846862793\n",
      "epoch: 62,  batch step: 70, loss: 7.481208801269531\n",
      "epoch: 62,  batch step: 71, loss: 6.383270263671875\n",
      "epoch: 62,  batch step: 72, loss: 3.780822277069092\n",
      "epoch: 62,  batch step: 73, loss: 168.89834594726562\n",
      "epoch: 62,  batch step: 74, loss: 44.493072509765625\n",
      "epoch: 62,  batch step: 75, loss: 14.734670639038086\n",
      "epoch: 62,  batch step: 76, loss: 2.8705596923828125\n",
      "epoch: 62,  batch step: 77, loss: 38.91966247558594\n",
      "epoch: 62,  batch step: 78, loss: 69.90208435058594\n",
      "epoch: 62,  batch step: 79, loss: 51.76637268066406\n",
      "epoch: 62,  batch step: 80, loss: 65.38614654541016\n",
      "epoch: 62,  batch step: 81, loss: 3.430222988128662\n",
      "epoch: 62,  batch step: 82, loss: 6.517049789428711\n",
      "epoch: 62,  batch step: 83, loss: 28.602163314819336\n",
      "epoch: 62,  batch step: 84, loss: 3.8674190044403076\n",
      "epoch: 62,  batch step: 85, loss: 30.89089012145996\n",
      "epoch: 62,  batch step: 86, loss: 72.57536315917969\n",
      "epoch: 62,  batch step: 87, loss: 2.91811466217041\n",
      "epoch: 62,  batch step: 88, loss: 8.566213607788086\n",
      "epoch: 62,  batch step: 89, loss: 22.653827667236328\n",
      "epoch: 62,  batch step: 90, loss: 4.4855427742004395\n",
      "epoch: 62,  batch step: 91, loss: 3.9112088680267334\n",
      "epoch: 62,  batch step: 92, loss: 18.510456085205078\n",
      "epoch: 62,  batch step: 93, loss: 3.355990171432495\n",
      "epoch: 62,  batch step: 94, loss: 2.920192241668701\n",
      "epoch: 62,  batch step: 95, loss: 2.8800811767578125\n",
      "epoch: 62,  batch step: 96, loss: 3.2246313095092773\n",
      "epoch: 62,  batch step: 97, loss: 3.0200564861297607\n",
      "epoch: 62,  batch step: 98, loss: 8.686737060546875\n",
      "epoch: 62,  batch step: 99, loss: 49.823638916015625\n",
      "epoch: 62,  batch step: 100, loss: 4.147656440734863\n",
      "epoch: 62,  batch step: 101, loss: 41.41935729980469\n",
      "epoch: 62,  batch step: 102, loss: 50.232513427734375\n",
      "epoch: 62,  batch step: 103, loss: 6.41957950592041\n",
      "epoch: 62,  batch step: 104, loss: 4.058597564697266\n",
      "epoch: 62,  batch step: 105, loss: 3.280254364013672\n",
      "epoch: 62,  batch step: 106, loss: 2.9698362350463867\n",
      "epoch: 62,  batch step: 107, loss: 3.5972304344177246\n",
      "epoch: 62,  batch step: 108, loss: 6.617118835449219\n",
      "epoch: 62,  batch step: 109, loss: 5.189431190490723\n",
      "epoch: 62,  batch step: 110, loss: 14.194023132324219\n",
      "epoch: 62,  batch step: 111, loss: 34.47706604003906\n",
      "epoch: 62,  batch step: 112, loss: 16.73200035095215\n",
      "epoch: 62,  batch step: 113, loss: 3.219853162765503\n",
      "epoch: 62,  batch step: 114, loss: 3.6203861236572266\n",
      "epoch: 62,  batch step: 115, loss: 21.100994110107422\n",
      "epoch: 62,  batch step: 116, loss: 5.244164943695068\n",
      "epoch: 62,  batch step: 117, loss: 16.899600982666016\n",
      "epoch: 62,  batch step: 118, loss: 3.018646240234375\n",
      "epoch: 62,  batch step: 119, loss: 42.420597076416016\n",
      "epoch: 62,  batch step: 120, loss: 3.2404322624206543\n",
      "epoch: 62,  batch step: 121, loss: 20.26000213623047\n",
      "epoch: 62,  batch step: 122, loss: 65.64456176757812\n",
      "epoch: 62,  batch step: 123, loss: 27.77362632751465\n",
      "epoch: 62,  batch step: 124, loss: 9.6241455078125\n",
      "epoch: 62,  batch step: 125, loss: 2.811948537826538\n",
      "epoch: 62,  batch step: 126, loss: 2.981727123260498\n",
      "epoch: 62,  batch step: 127, loss: 9.948587417602539\n",
      "epoch: 62,  batch step: 128, loss: 33.12571716308594\n",
      "epoch: 62,  batch step: 129, loss: 2.9270095825195312\n",
      "epoch: 62,  batch step: 130, loss: 31.824825286865234\n",
      "epoch: 62,  batch step: 131, loss: 67.65495300292969\n",
      "epoch: 62,  batch step: 132, loss: 33.45541000366211\n",
      "epoch: 62,  batch step: 133, loss: 61.71656799316406\n",
      "epoch: 62,  batch step: 134, loss: 31.416160583496094\n",
      "epoch: 62,  batch step: 135, loss: 3.2577171325683594\n",
      "epoch: 62,  batch step: 136, loss: 80.13229370117188\n",
      "epoch: 62,  batch step: 137, loss: 4.734297752380371\n",
      "epoch: 62,  batch step: 138, loss: 6.061590194702148\n",
      "epoch: 62,  batch step: 139, loss: 6.925093650817871\n",
      "epoch: 62,  batch step: 140, loss: 25.005300521850586\n",
      "epoch: 62,  batch step: 141, loss: 24.301307678222656\n",
      "epoch: 62,  batch step: 142, loss: 24.75942611694336\n",
      "epoch: 62,  batch step: 143, loss: 5.807206630706787\n",
      "epoch: 62,  batch step: 144, loss: 17.47660255432129\n",
      "epoch: 62,  batch step: 145, loss: 30.492145538330078\n",
      "epoch: 62,  batch step: 146, loss: 2.9409091472625732\n",
      "epoch: 62,  batch step: 147, loss: 4.222902774810791\n",
      "epoch: 62,  batch step: 148, loss: 26.015213012695312\n",
      "epoch: 62,  batch step: 149, loss: 5.797118186950684\n",
      "epoch: 62,  batch step: 150, loss: 3.410295248031616\n",
      "epoch: 62,  batch step: 151, loss: 14.829684257507324\n",
      "epoch: 62,  batch step: 152, loss: 26.05971908569336\n",
      "epoch: 62,  batch step: 153, loss: 5.520370006561279\n",
      "epoch: 62,  batch step: 154, loss: 19.212257385253906\n",
      "epoch: 62,  batch step: 155, loss: 4.045650482177734\n",
      "epoch: 62,  batch step: 156, loss: 50.32579040527344\n",
      "epoch: 62,  batch step: 157, loss: 2.6362533569335938\n",
      "epoch: 62,  batch step: 158, loss: 5.389840602874756\n",
      "epoch: 62,  batch step: 159, loss: 3.916961193084717\n",
      "epoch: 62,  batch step: 160, loss: 5.075761318206787\n",
      "epoch: 62,  batch step: 161, loss: 13.696654319763184\n",
      "epoch: 62,  batch step: 162, loss: 3.089878797531128\n",
      "epoch: 62,  batch step: 163, loss: 8.89061164855957\n",
      "epoch: 62,  batch step: 164, loss: 7.867352485656738\n",
      "epoch: 62,  batch step: 165, loss: 2.734405040740967\n",
      "epoch: 62,  batch step: 166, loss: 19.34764862060547\n",
      "epoch: 62,  batch step: 167, loss: 12.825569152832031\n",
      "epoch: 62,  batch step: 168, loss: 21.818862915039062\n",
      "epoch: 62,  batch step: 169, loss: 17.468860626220703\n",
      "epoch: 62,  batch step: 170, loss: 24.840559005737305\n",
      "epoch: 62,  batch step: 171, loss: 8.731581687927246\n",
      "epoch: 62,  batch step: 172, loss: 36.14476013183594\n",
      "epoch: 62,  batch step: 173, loss: 19.465469360351562\n",
      "epoch: 62,  batch step: 174, loss: 10.027288436889648\n",
      "epoch: 62,  batch step: 175, loss: 19.16080665588379\n",
      "epoch: 62,  batch step: 176, loss: 19.01506805419922\n",
      "epoch: 62,  batch step: 177, loss: 9.756758689880371\n",
      "epoch: 62,  batch step: 178, loss: 6.491677761077881\n",
      "epoch: 62,  batch step: 179, loss: 28.156875610351562\n",
      "epoch: 62,  batch step: 180, loss: 4.74244499206543\n",
      "epoch: 62,  batch step: 181, loss: 5.086568832397461\n",
      "epoch: 62,  batch step: 182, loss: 5.480529308319092\n",
      "epoch: 62,  batch step: 183, loss: 2.4036011695861816\n",
      "epoch: 62,  batch step: 184, loss: 13.691648483276367\n",
      "epoch: 62,  batch step: 185, loss: 3.383492946624756\n",
      "epoch: 62,  batch step: 186, loss: 4.1109538078308105\n",
      "epoch: 62,  batch step: 187, loss: 4.880026817321777\n",
      "epoch: 62,  batch step: 188, loss: 3.8039393424987793\n",
      "epoch: 62,  batch step: 189, loss: 21.081287384033203\n",
      "epoch: 62,  batch step: 190, loss: 32.92649459838867\n",
      "epoch: 62,  batch step: 191, loss: 28.956382751464844\n",
      "epoch: 62,  batch step: 192, loss: 30.968597412109375\n",
      "epoch: 62,  batch step: 193, loss: 17.0184326171875\n",
      "epoch: 62,  batch step: 194, loss: 36.98933410644531\n",
      "epoch: 62,  batch step: 195, loss: 2.3875041007995605\n",
      "epoch: 62,  batch step: 196, loss: 36.189842224121094\n",
      "epoch: 62,  batch step: 197, loss: 15.59274673461914\n",
      "epoch: 62,  batch step: 198, loss: 2.4383389949798584\n",
      "epoch: 62,  batch step: 199, loss: 51.242881774902344\n",
      "epoch: 62,  batch step: 200, loss: 4.8744916915893555\n",
      "epoch: 62,  batch step: 201, loss: 2.5998122692108154\n",
      "epoch: 62,  batch step: 202, loss: 20.151906967163086\n",
      "epoch: 62,  batch step: 203, loss: 3.8667380809783936\n",
      "epoch: 62,  batch step: 204, loss: 2.8741068840026855\n",
      "epoch: 62,  batch step: 205, loss: 4.441959381103516\n",
      "epoch: 62,  batch step: 206, loss: 19.585731506347656\n",
      "epoch: 62,  batch step: 207, loss: 18.695308685302734\n",
      "epoch: 62,  batch step: 208, loss: 4.415689468383789\n",
      "epoch: 62,  batch step: 209, loss: 9.24484634399414\n",
      "epoch: 62,  batch step: 210, loss: 14.598648071289062\n",
      "epoch: 62,  batch step: 211, loss: 18.888442993164062\n",
      "epoch: 62,  batch step: 212, loss: 20.016149520874023\n",
      "epoch: 62,  batch step: 213, loss: 2.7275123596191406\n",
      "epoch: 62,  batch step: 214, loss: 4.4760236740112305\n",
      "epoch: 62,  batch step: 215, loss: 3.2668352127075195\n",
      "epoch: 62,  batch step: 216, loss: 16.510089874267578\n",
      "epoch: 62,  batch step: 217, loss: 2.333894968032837\n",
      "epoch: 62,  batch step: 218, loss: 8.095497131347656\n",
      "epoch: 62,  batch step: 219, loss: 2.670166015625\n",
      "epoch: 62,  batch step: 220, loss: 3.437244415283203\n",
      "epoch: 62,  batch step: 221, loss: 3.504127264022827\n",
      "epoch: 62,  batch step: 222, loss: 58.104793548583984\n",
      "epoch: 62,  batch step: 223, loss: 81.7502670288086\n",
      "epoch: 62,  batch step: 224, loss: 26.20989990234375\n",
      "epoch: 62,  batch step: 225, loss: 1.7545298337936401\n",
      "epoch: 62,  batch step: 226, loss: 2.833686351776123\n",
      "epoch: 62,  batch step: 227, loss: 71.60897827148438\n",
      "epoch: 62,  batch step: 228, loss: 55.46080780029297\n",
      "epoch: 62,  batch step: 229, loss: 4.383406639099121\n",
      "epoch: 62,  batch step: 230, loss: 15.80002212524414\n",
      "epoch: 62,  batch step: 231, loss: 5.596707820892334\n",
      "epoch: 62,  batch step: 232, loss: 22.433385848999023\n",
      "epoch: 62,  batch step: 233, loss: 2.7888267040252686\n",
      "epoch: 62,  batch step: 234, loss: 6.5023698806762695\n",
      "epoch: 62,  batch step: 235, loss: 32.22066116333008\n",
      "epoch: 62,  batch step: 236, loss: 3.9615161418914795\n",
      "epoch: 62,  batch step: 237, loss: 40.602455139160156\n",
      "epoch: 62,  batch step: 238, loss: 4.662613868713379\n",
      "epoch: 62,  batch step: 239, loss: 50.987586975097656\n",
      "epoch: 62,  batch step: 240, loss: 47.0959587097168\n",
      "epoch: 62,  batch step: 241, loss: 4.158320903778076\n",
      "epoch: 62,  batch step: 242, loss: 3.609347105026245\n",
      "epoch: 62,  batch step: 243, loss: 3.7276086807250977\n",
      "epoch: 62,  batch step: 244, loss: 18.784984588623047\n",
      "epoch: 62,  batch step: 245, loss: 2.264836072921753\n",
      "epoch: 62,  batch step: 246, loss: 2.4916293621063232\n",
      "epoch: 62,  batch step: 247, loss: 6.847692012786865\n",
      "epoch: 62,  batch step: 248, loss: 3.404291868209839\n",
      "epoch: 62,  batch step: 249, loss: 30.930776596069336\n",
      "epoch: 62,  batch step: 250, loss: 48.97045135498047\n",
      "epoch: 62,  batch step: 251, loss: 141.83261108398438\n",
      "validation error epoch  62:    tensor(65.7738, device='cuda:0')\n",
      "316\n",
      "epoch: 63,  batch step: 0, loss: 17.669418334960938\n",
      "epoch: 63,  batch step: 1, loss: 3.9057586193084717\n",
      "epoch: 63,  batch step: 2, loss: 8.937471389770508\n",
      "epoch: 63,  batch step: 3, loss: 12.192779541015625\n",
      "epoch: 63,  batch step: 4, loss: 10.298583984375\n",
      "epoch: 63,  batch step: 5, loss: 3.825087070465088\n",
      "epoch: 63,  batch step: 6, loss: 26.360530853271484\n",
      "epoch: 63,  batch step: 7, loss: 25.28664207458496\n",
      "epoch: 63,  batch step: 8, loss: 8.469944953918457\n",
      "epoch: 63,  batch step: 9, loss: 14.252462387084961\n",
      "epoch: 63,  batch step: 10, loss: 30.84626579284668\n",
      "epoch: 63,  batch step: 11, loss: 5.563793182373047\n",
      "epoch: 63,  batch step: 12, loss: 84.99710083007812\n",
      "epoch: 63,  batch step: 13, loss: 5.43741512298584\n",
      "epoch: 63,  batch step: 14, loss: 20.861228942871094\n",
      "epoch: 63,  batch step: 15, loss: 49.57282257080078\n",
      "epoch: 63,  batch step: 16, loss: 43.639095306396484\n",
      "epoch: 63,  batch step: 17, loss: 6.70499324798584\n",
      "epoch: 63,  batch step: 18, loss: 4.509352684020996\n",
      "epoch: 63,  batch step: 19, loss: 6.034326076507568\n",
      "epoch: 63,  batch step: 20, loss: 4.1201491355896\n",
      "epoch: 63,  batch step: 21, loss: 16.63585090637207\n",
      "epoch: 63,  batch step: 22, loss: 12.190502166748047\n",
      "epoch: 63,  batch step: 23, loss: 28.295093536376953\n",
      "epoch: 63,  batch step: 24, loss: 24.050453186035156\n",
      "epoch: 63,  batch step: 25, loss: 22.60322380065918\n",
      "epoch: 63,  batch step: 26, loss: 2.7171239852905273\n",
      "epoch: 63,  batch step: 27, loss: 5.729269027709961\n",
      "epoch: 63,  batch step: 28, loss: 10.704318046569824\n",
      "epoch: 63,  batch step: 29, loss: 2.0731863975524902\n",
      "epoch: 63,  batch step: 30, loss: 25.008106231689453\n",
      "epoch: 63,  batch step: 31, loss: 2.5608859062194824\n",
      "epoch: 63,  batch step: 32, loss: 26.953754425048828\n",
      "epoch: 63,  batch step: 33, loss: 31.30646324157715\n",
      "epoch: 63,  batch step: 34, loss: 4.534213066101074\n",
      "epoch: 63,  batch step: 35, loss: 11.944778442382812\n",
      "epoch: 63,  batch step: 36, loss: 19.918651580810547\n",
      "epoch: 63,  batch step: 37, loss: 15.531286239624023\n",
      "epoch: 63,  batch step: 38, loss: 9.161452293395996\n",
      "epoch: 63,  batch step: 39, loss: 5.732041835784912\n",
      "epoch: 63,  batch step: 40, loss: 9.211271286010742\n",
      "epoch: 63,  batch step: 41, loss: 4.171322822570801\n",
      "epoch: 63,  batch step: 42, loss: 5.034385681152344\n",
      "epoch: 63,  batch step: 43, loss: 6.651065826416016\n",
      "epoch: 63,  batch step: 44, loss: 52.26750183105469\n",
      "epoch: 63,  batch step: 45, loss: 19.404441833496094\n",
      "epoch: 63,  batch step: 46, loss: 33.68638610839844\n",
      "epoch: 63,  batch step: 47, loss: 5.487135887145996\n",
      "epoch: 63,  batch step: 48, loss: 62.886409759521484\n",
      "epoch: 63,  batch step: 49, loss: 20.296897888183594\n",
      "epoch: 63,  batch step: 50, loss: 18.97078514099121\n",
      "epoch: 63,  batch step: 51, loss: 12.630739212036133\n",
      "epoch: 63,  batch step: 52, loss: 32.452850341796875\n",
      "epoch: 63,  batch step: 53, loss: 18.545944213867188\n",
      "epoch: 63,  batch step: 54, loss: 5.103047847747803\n",
      "epoch: 63,  batch step: 55, loss: 3.741224765777588\n",
      "epoch: 63,  batch step: 56, loss: 2.8366498947143555\n",
      "epoch: 63,  batch step: 57, loss: 33.041019439697266\n",
      "epoch: 63,  batch step: 58, loss: 4.837886810302734\n",
      "epoch: 63,  batch step: 59, loss: 3.323857307434082\n",
      "epoch: 63,  batch step: 60, loss: 24.37704849243164\n",
      "epoch: 63,  batch step: 61, loss: 62.46064376831055\n",
      "epoch: 63,  batch step: 62, loss: 58.6589469909668\n",
      "epoch: 63,  batch step: 63, loss: 32.34498596191406\n",
      "epoch: 63,  batch step: 64, loss: 3.6611099243164062\n",
      "epoch: 63,  batch step: 65, loss: 4.7321271896362305\n",
      "epoch: 63,  batch step: 66, loss: 3.474790096282959\n",
      "epoch: 63,  batch step: 67, loss: 54.94563293457031\n",
      "epoch: 63,  batch step: 68, loss: 2.0612521171569824\n",
      "epoch: 63,  batch step: 69, loss: 20.928077697753906\n",
      "epoch: 63,  batch step: 70, loss: 10.882804870605469\n",
      "epoch: 63,  batch step: 71, loss: 64.90450286865234\n",
      "epoch: 63,  batch step: 72, loss: 6.352808952331543\n",
      "epoch: 63,  batch step: 73, loss: 3.0085906982421875\n",
      "epoch: 63,  batch step: 74, loss: 3.7110393047332764\n",
      "epoch: 63,  batch step: 75, loss: 12.446727752685547\n",
      "epoch: 63,  batch step: 76, loss: 23.84772491455078\n",
      "epoch: 63,  batch step: 77, loss: 2.9360885620117188\n",
      "epoch: 63,  batch step: 78, loss: 22.470359802246094\n",
      "epoch: 63,  batch step: 79, loss: 4.647184371948242\n",
      "epoch: 63,  batch step: 80, loss: 26.604068756103516\n",
      "epoch: 63,  batch step: 81, loss: 6.3188700675964355\n",
      "epoch: 63,  batch step: 82, loss: 2.772054433822632\n",
      "epoch: 63,  batch step: 83, loss: 32.21488952636719\n",
      "epoch: 63,  batch step: 84, loss: 27.792402267456055\n",
      "epoch: 63,  batch step: 85, loss: 3.1492233276367188\n",
      "epoch: 63,  batch step: 86, loss: 8.770137786865234\n",
      "epoch: 63,  batch step: 87, loss: 11.621805191040039\n",
      "epoch: 63,  batch step: 88, loss: 33.40081024169922\n",
      "epoch: 63,  batch step: 89, loss: 5.655298233032227\n",
      "epoch: 63,  batch step: 90, loss: 32.00065612792969\n",
      "epoch: 63,  batch step: 91, loss: 1.8096323013305664\n",
      "epoch: 63,  batch step: 92, loss: 21.981077194213867\n",
      "epoch: 63,  batch step: 93, loss: 52.14021301269531\n",
      "epoch: 63,  batch step: 94, loss: 12.664098739624023\n",
      "epoch: 63,  batch step: 95, loss: 23.622650146484375\n",
      "epoch: 63,  batch step: 96, loss: 3.318561315536499\n",
      "epoch: 63,  batch step: 97, loss: 27.5839900970459\n",
      "epoch: 63,  batch step: 98, loss: 8.495351791381836\n",
      "epoch: 63,  batch step: 99, loss: 8.857502937316895\n",
      "epoch: 63,  batch step: 100, loss: 3.2562294006347656\n",
      "epoch: 63,  batch step: 101, loss: 20.940776824951172\n",
      "epoch: 63,  batch step: 102, loss: 3.7119789123535156\n",
      "epoch: 63,  batch step: 103, loss: 58.856319427490234\n",
      "epoch: 63,  batch step: 104, loss: 22.919782638549805\n",
      "epoch: 63,  batch step: 105, loss: 2.5987508296966553\n",
      "epoch: 63,  batch step: 106, loss: 20.343538284301758\n",
      "epoch: 63,  batch step: 107, loss: 2.520524501800537\n",
      "epoch: 63,  batch step: 108, loss: 12.855669975280762\n",
      "epoch: 63,  batch step: 109, loss: 3.586350202560425\n",
      "epoch: 63,  batch step: 110, loss: 6.240215301513672\n",
      "epoch: 63,  batch step: 111, loss: 15.881214141845703\n",
      "epoch: 63,  batch step: 112, loss: 3.319779872894287\n",
      "epoch: 63,  batch step: 113, loss: 5.101012229919434\n",
      "epoch: 63,  batch step: 114, loss: 3.383232831954956\n",
      "epoch: 63,  batch step: 115, loss: 2.402526617050171\n",
      "epoch: 63,  batch step: 116, loss: 20.405635833740234\n",
      "epoch: 63,  batch step: 117, loss: 15.453550338745117\n",
      "epoch: 63,  batch step: 118, loss: 3.9786250591278076\n",
      "epoch: 63,  batch step: 119, loss: 12.431220054626465\n",
      "epoch: 63,  batch step: 120, loss: 4.979114532470703\n",
      "epoch: 63,  batch step: 121, loss: 36.07447814941406\n",
      "epoch: 63,  batch step: 122, loss: 14.927566528320312\n",
      "epoch: 63,  batch step: 123, loss: 47.626007080078125\n",
      "epoch: 63,  batch step: 124, loss: 15.418098449707031\n",
      "epoch: 63,  batch step: 125, loss: 4.384666442871094\n",
      "epoch: 63,  batch step: 126, loss: 18.30852508544922\n",
      "epoch: 63,  batch step: 127, loss: 2.891408920288086\n",
      "epoch: 63,  batch step: 128, loss: 27.08496856689453\n",
      "epoch: 63,  batch step: 129, loss: 13.816524505615234\n",
      "epoch: 63,  batch step: 130, loss: 36.79521179199219\n",
      "epoch: 63,  batch step: 131, loss: 13.808963775634766\n",
      "epoch: 63,  batch step: 132, loss: 2.4755539894104004\n",
      "epoch: 63,  batch step: 133, loss: 2.924813747406006\n",
      "epoch: 63,  batch step: 134, loss: 4.434869289398193\n",
      "epoch: 63,  batch step: 135, loss: 32.05549621582031\n",
      "epoch: 63,  batch step: 136, loss: 121.71788024902344\n",
      "epoch: 63,  batch step: 137, loss: 32.5053825378418\n",
      "epoch: 63,  batch step: 138, loss: 5.942159175872803\n",
      "epoch: 63,  batch step: 139, loss: 4.6684417724609375\n",
      "epoch: 63,  batch step: 140, loss: 2.250669479370117\n",
      "epoch: 63,  batch step: 141, loss: 5.693662166595459\n",
      "epoch: 63,  batch step: 142, loss: 23.977191925048828\n",
      "epoch: 63,  batch step: 143, loss: 153.7351531982422\n",
      "epoch: 63,  batch step: 144, loss: 5.948321342468262\n",
      "epoch: 63,  batch step: 145, loss: 23.16897201538086\n",
      "epoch: 63,  batch step: 146, loss: 22.497251510620117\n",
      "epoch: 63,  batch step: 147, loss: 20.253454208374023\n",
      "epoch: 63,  batch step: 148, loss: 13.215812683105469\n",
      "epoch: 63,  batch step: 149, loss: 17.666685104370117\n",
      "epoch: 63,  batch step: 150, loss: 4.136443138122559\n",
      "epoch: 63,  batch step: 151, loss: 9.295950889587402\n",
      "epoch: 63,  batch step: 152, loss: 33.87504196166992\n",
      "epoch: 63,  batch step: 153, loss: 2.5630311965942383\n",
      "epoch: 63,  batch step: 154, loss: 19.599498748779297\n",
      "epoch: 63,  batch step: 155, loss: 36.66698455810547\n",
      "epoch: 63,  batch step: 156, loss: 2.593013286590576\n",
      "epoch: 63,  batch step: 157, loss: 36.37102508544922\n",
      "epoch: 63,  batch step: 158, loss: 2.5896425247192383\n",
      "epoch: 63,  batch step: 159, loss: 5.057812690734863\n",
      "epoch: 63,  batch step: 160, loss: 2.9444055557250977\n",
      "epoch: 63,  batch step: 161, loss: 41.09592056274414\n",
      "epoch: 63,  batch step: 162, loss: 23.843854904174805\n",
      "epoch: 63,  batch step: 163, loss: 6.125922203063965\n",
      "epoch: 63,  batch step: 164, loss: 51.11238098144531\n",
      "epoch: 63,  batch step: 165, loss: 3.297182083129883\n",
      "epoch: 63,  batch step: 166, loss: 4.300374507904053\n",
      "epoch: 63,  batch step: 167, loss: 5.914344787597656\n",
      "epoch: 63,  batch step: 168, loss: 21.397701263427734\n",
      "epoch: 63,  batch step: 169, loss: 6.379051208496094\n",
      "epoch: 63,  batch step: 170, loss: 3.807860851287842\n",
      "epoch: 63,  batch step: 171, loss: 4.166358947753906\n",
      "epoch: 63,  batch step: 172, loss: 17.75347137451172\n",
      "epoch: 63,  batch step: 173, loss: 7.627919673919678\n",
      "epoch: 63,  batch step: 174, loss: 7.996649742126465\n",
      "epoch: 63,  batch step: 175, loss: 3.3159921169281006\n",
      "epoch: 63,  batch step: 176, loss: 2.683586835861206\n",
      "epoch: 63,  batch step: 177, loss: 4.265025615692139\n",
      "epoch: 63,  batch step: 178, loss: 45.500762939453125\n",
      "epoch: 63,  batch step: 179, loss: 7.970923900604248\n",
      "epoch: 63,  batch step: 180, loss: 51.49009704589844\n",
      "epoch: 63,  batch step: 181, loss: 4.487921714782715\n",
      "epoch: 63,  batch step: 182, loss: 15.96100902557373\n",
      "epoch: 63,  batch step: 183, loss: 7.347414970397949\n",
      "epoch: 63,  batch step: 184, loss: 7.301560878753662\n",
      "epoch: 63,  batch step: 185, loss: 1.9621505737304688\n",
      "epoch: 63,  batch step: 186, loss: 16.413543701171875\n",
      "epoch: 63,  batch step: 187, loss: 2.265462875366211\n",
      "epoch: 63,  batch step: 188, loss: 3.3742470741271973\n",
      "epoch: 63,  batch step: 189, loss: 3.500359296798706\n",
      "epoch: 63,  batch step: 190, loss: 8.539216995239258\n",
      "epoch: 63,  batch step: 191, loss: 13.094810485839844\n",
      "epoch: 63,  batch step: 192, loss: 28.295372009277344\n",
      "epoch: 63,  batch step: 193, loss: 3.211728811264038\n",
      "epoch: 63,  batch step: 194, loss: 30.55116081237793\n",
      "epoch: 63,  batch step: 195, loss: 14.973690032958984\n",
      "epoch: 63,  batch step: 196, loss: 22.8530330657959\n",
      "epoch: 63,  batch step: 197, loss: 6.6247053146362305\n",
      "epoch: 63,  batch step: 198, loss: 49.8485107421875\n",
      "epoch: 63,  batch step: 199, loss: 6.905551433563232\n",
      "epoch: 63,  batch step: 200, loss: 35.748252868652344\n",
      "epoch: 63,  batch step: 201, loss: 12.403200149536133\n",
      "epoch: 63,  batch step: 202, loss: 2.914132595062256\n",
      "epoch: 63,  batch step: 203, loss: 31.140689849853516\n",
      "epoch: 63,  batch step: 204, loss: 37.263031005859375\n",
      "epoch: 63,  batch step: 205, loss: 12.22276496887207\n",
      "epoch: 63,  batch step: 206, loss: 4.914765357971191\n",
      "epoch: 63,  batch step: 207, loss: 12.691892623901367\n",
      "epoch: 63,  batch step: 208, loss: 12.121901512145996\n",
      "epoch: 63,  batch step: 209, loss: 18.760356903076172\n",
      "epoch: 63,  batch step: 210, loss: 95.44280242919922\n",
      "epoch: 63,  batch step: 211, loss: 5.413112640380859\n",
      "epoch: 63,  batch step: 212, loss: 3.0740737915039062\n",
      "epoch: 63,  batch step: 213, loss: 3.3266332149505615\n",
      "epoch: 63,  batch step: 214, loss: 7.285144805908203\n",
      "epoch: 63,  batch step: 215, loss: 39.85772705078125\n",
      "epoch: 63,  batch step: 216, loss: 20.223873138427734\n",
      "epoch: 63,  batch step: 217, loss: 15.388065338134766\n",
      "epoch: 63,  batch step: 218, loss: 4.603397369384766\n",
      "epoch: 63,  batch step: 219, loss: 34.628299713134766\n",
      "epoch: 63,  batch step: 220, loss: 24.354381561279297\n",
      "epoch: 63,  batch step: 221, loss: 39.24526596069336\n",
      "epoch: 63,  batch step: 222, loss: 7.092711448669434\n",
      "epoch: 63,  batch step: 223, loss: 7.257967948913574\n",
      "epoch: 63,  batch step: 224, loss: 3.988858938217163\n",
      "epoch: 63,  batch step: 225, loss: 25.60358428955078\n",
      "epoch: 63,  batch step: 226, loss: 4.349436283111572\n",
      "epoch: 63,  batch step: 227, loss: 6.922355651855469\n",
      "epoch: 63,  batch step: 228, loss: 11.97290325164795\n",
      "epoch: 63,  batch step: 229, loss: 3.455821990966797\n",
      "epoch: 63,  batch step: 230, loss: 3.554105281829834\n",
      "epoch: 63,  batch step: 231, loss: 4.8770599365234375\n",
      "epoch: 63,  batch step: 232, loss: 100.300048828125\n",
      "epoch: 63,  batch step: 233, loss: 46.30564498901367\n",
      "epoch: 63,  batch step: 234, loss: 40.71842956542969\n",
      "epoch: 63,  batch step: 235, loss: 31.219966888427734\n",
      "epoch: 63,  batch step: 236, loss: 28.945430755615234\n",
      "epoch: 63,  batch step: 237, loss: 27.13313102722168\n",
      "epoch: 63,  batch step: 238, loss: 4.6469268798828125\n",
      "epoch: 63,  batch step: 239, loss: 42.551483154296875\n",
      "epoch: 63,  batch step: 240, loss: 19.276212692260742\n",
      "epoch: 63,  batch step: 241, loss: 23.179662704467773\n",
      "epoch: 63,  batch step: 242, loss: 2.750673294067383\n",
      "epoch: 63,  batch step: 243, loss: 5.172539710998535\n",
      "epoch: 63,  batch step: 244, loss: 6.201936721801758\n",
      "epoch: 63,  batch step: 245, loss: 7.102010726928711\n",
      "epoch: 63,  batch step: 246, loss: 15.997108459472656\n",
      "epoch: 63,  batch step: 247, loss: 9.809965133666992\n",
      "epoch: 63,  batch step: 248, loss: 4.6999831199646\n",
      "epoch: 63,  batch step: 249, loss: 66.46236419677734\n",
      "epoch: 63,  batch step: 250, loss: 3.5216472148895264\n",
      "epoch: 63,  batch step: 251, loss: 4.840481758117676\n",
      "validation error epoch  63:    tensor(68.7309, device='cuda:0')\n",
      "316\n",
      "epoch: 64,  batch step: 0, loss: 5.59477424621582\n",
      "epoch: 64,  batch step: 1, loss: 6.85125732421875\n",
      "epoch: 64,  batch step: 2, loss: 20.02577018737793\n",
      "epoch: 64,  batch step: 3, loss: 2.9693970680236816\n",
      "epoch: 64,  batch step: 4, loss: 6.77699613571167\n",
      "epoch: 64,  batch step: 5, loss: 7.814332485198975\n",
      "epoch: 64,  batch step: 6, loss: 9.887849807739258\n",
      "epoch: 64,  batch step: 7, loss: 38.93260955810547\n",
      "epoch: 64,  batch step: 8, loss: 9.197759628295898\n",
      "epoch: 64,  batch step: 9, loss: 3.5099549293518066\n",
      "epoch: 64,  batch step: 10, loss: 21.335111618041992\n",
      "epoch: 64,  batch step: 11, loss: 3.051333427429199\n",
      "epoch: 64,  batch step: 12, loss: 11.087359428405762\n",
      "epoch: 64,  batch step: 13, loss: 2.47918438911438\n",
      "epoch: 64,  batch step: 14, loss: 21.26874542236328\n",
      "epoch: 64,  batch step: 15, loss: 3.054943799972534\n",
      "epoch: 64,  batch step: 16, loss: 5.879077911376953\n",
      "epoch: 64,  batch step: 17, loss: 3.0018889904022217\n",
      "epoch: 64,  batch step: 18, loss: 31.441497802734375\n",
      "epoch: 64,  batch step: 19, loss: 50.92921447753906\n",
      "epoch: 64,  batch step: 20, loss: 18.493141174316406\n",
      "epoch: 64,  batch step: 21, loss: 38.67604446411133\n",
      "epoch: 64,  batch step: 22, loss: 4.018775463104248\n",
      "epoch: 64,  batch step: 23, loss: 2.7131097316741943\n",
      "epoch: 64,  batch step: 24, loss: 3.135476589202881\n",
      "epoch: 64,  batch step: 25, loss: 3.7135112285614014\n",
      "epoch: 64,  batch step: 26, loss: 43.35190963745117\n",
      "epoch: 64,  batch step: 27, loss: 46.00238037109375\n",
      "epoch: 64,  batch step: 28, loss: 19.197242736816406\n",
      "epoch: 64,  batch step: 29, loss: 15.79340934753418\n",
      "epoch: 64,  batch step: 30, loss: 2.836620330810547\n",
      "epoch: 64,  batch step: 31, loss: 2.4973723888397217\n",
      "epoch: 64,  batch step: 32, loss: 3.666018486022949\n",
      "epoch: 64,  batch step: 33, loss: 46.87559127807617\n",
      "epoch: 64,  batch step: 34, loss: 3.872880697250366\n",
      "epoch: 64,  batch step: 35, loss: 3.3881099224090576\n",
      "epoch: 64,  batch step: 36, loss: 17.15603256225586\n",
      "epoch: 64,  batch step: 37, loss: 7.744109153747559\n",
      "epoch: 64,  batch step: 38, loss: 18.145065307617188\n",
      "epoch: 64,  batch step: 39, loss: 4.557450771331787\n",
      "epoch: 64,  batch step: 40, loss: 3.1219983100891113\n",
      "epoch: 64,  batch step: 41, loss: 3.036494016647339\n",
      "epoch: 64,  batch step: 42, loss: 32.33050537109375\n",
      "epoch: 64,  batch step: 43, loss: 19.700843811035156\n",
      "epoch: 64,  batch step: 44, loss: 70.95169067382812\n",
      "epoch: 64,  batch step: 45, loss: 3.262319564819336\n",
      "epoch: 64,  batch step: 46, loss: 2.652639150619507\n",
      "epoch: 64,  batch step: 47, loss: 29.464271545410156\n",
      "epoch: 64,  batch step: 48, loss: 3.369652271270752\n",
      "epoch: 64,  batch step: 49, loss: 4.946836471557617\n",
      "epoch: 64,  batch step: 50, loss: 2.879424810409546\n",
      "epoch: 64,  batch step: 51, loss: 38.24381637573242\n",
      "epoch: 64,  batch step: 52, loss: 12.481338500976562\n",
      "epoch: 64,  batch step: 53, loss: 31.288673400878906\n",
      "epoch: 64,  batch step: 54, loss: 17.997026443481445\n",
      "epoch: 64,  batch step: 55, loss: 31.650192260742188\n",
      "epoch: 64,  batch step: 56, loss: 14.551889419555664\n",
      "epoch: 64,  batch step: 57, loss: 17.365550994873047\n",
      "epoch: 64,  batch step: 58, loss: 4.014920234680176\n",
      "epoch: 64,  batch step: 59, loss: 17.988927841186523\n",
      "epoch: 64,  batch step: 60, loss: 5.002676963806152\n",
      "epoch: 64,  batch step: 61, loss: 67.26228332519531\n",
      "epoch: 64,  batch step: 62, loss: 12.221319198608398\n",
      "epoch: 64,  batch step: 63, loss: 30.67267608642578\n",
      "epoch: 64,  batch step: 64, loss: 73.57559204101562\n",
      "epoch: 64,  batch step: 65, loss: 8.50676441192627\n",
      "epoch: 64,  batch step: 66, loss: 4.095215320587158\n",
      "epoch: 64,  batch step: 67, loss: 4.510103225708008\n",
      "epoch: 64,  batch step: 68, loss: 17.558578491210938\n",
      "epoch: 64,  batch step: 69, loss: 2.7602109909057617\n",
      "epoch: 64,  batch step: 70, loss: 3.2959718704223633\n",
      "epoch: 64,  batch step: 71, loss: 16.245986938476562\n",
      "epoch: 64,  batch step: 72, loss: 39.19631576538086\n",
      "epoch: 64,  batch step: 73, loss: 4.107461452484131\n",
      "epoch: 64,  batch step: 74, loss: 38.7523307800293\n",
      "epoch: 64,  batch step: 75, loss: 19.238815307617188\n",
      "epoch: 64,  batch step: 76, loss: 3.05334734916687\n",
      "epoch: 64,  batch step: 77, loss: 27.741779327392578\n",
      "epoch: 64,  batch step: 78, loss: 24.3956298828125\n",
      "epoch: 64,  batch step: 79, loss: 19.74155044555664\n",
      "epoch: 64,  batch step: 80, loss: 48.37662887573242\n",
      "epoch: 64,  batch step: 81, loss: 36.09804153442383\n",
      "epoch: 64,  batch step: 82, loss: 31.854103088378906\n",
      "epoch: 64,  batch step: 83, loss: 6.763060569763184\n",
      "epoch: 64,  batch step: 84, loss: 11.55094051361084\n",
      "epoch: 64,  batch step: 85, loss: 4.2563323974609375\n",
      "epoch: 64,  batch step: 86, loss: 3.6033554077148438\n",
      "epoch: 64,  batch step: 87, loss: 14.862091064453125\n",
      "epoch: 64,  batch step: 88, loss: 3.9392004013061523\n",
      "epoch: 64,  batch step: 89, loss: 5.217985153198242\n",
      "epoch: 64,  batch step: 90, loss: 33.77342224121094\n",
      "epoch: 64,  batch step: 91, loss: 6.601646423339844\n",
      "epoch: 64,  batch step: 92, loss: 24.181079864501953\n",
      "epoch: 64,  batch step: 93, loss: 7.4159064292907715\n",
      "epoch: 64,  batch step: 94, loss: 26.715648651123047\n",
      "epoch: 64,  batch step: 95, loss: 6.1433844566345215\n",
      "epoch: 64,  batch step: 96, loss: 2.7960495948791504\n",
      "epoch: 64,  batch step: 97, loss: 13.759772300720215\n",
      "epoch: 64,  batch step: 98, loss: 53.65717697143555\n",
      "epoch: 64,  batch step: 99, loss: 5.681755065917969\n",
      "epoch: 64,  batch step: 100, loss: 35.71159362792969\n",
      "epoch: 64,  batch step: 101, loss: 21.923620223999023\n",
      "epoch: 64,  batch step: 102, loss: 4.520068168640137\n",
      "epoch: 64,  batch step: 103, loss: 13.037989616394043\n",
      "epoch: 64,  batch step: 104, loss: 10.97314739227295\n",
      "epoch: 64,  batch step: 105, loss: 43.70852279663086\n",
      "epoch: 64,  batch step: 106, loss: 11.381938934326172\n",
      "epoch: 64,  batch step: 107, loss: 3.2972412109375\n",
      "epoch: 64,  batch step: 108, loss: 3.503812313079834\n",
      "epoch: 64,  batch step: 109, loss: 4.047911643981934\n",
      "epoch: 64,  batch step: 110, loss: 16.373777389526367\n",
      "epoch: 64,  batch step: 111, loss: 36.59909439086914\n",
      "epoch: 64,  batch step: 112, loss: 43.15031051635742\n",
      "epoch: 64,  batch step: 113, loss: 33.517433166503906\n",
      "epoch: 64,  batch step: 114, loss: 3.392242431640625\n",
      "epoch: 64,  batch step: 115, loss: 14.9994535446167\n",
      "epoch: 64,  batch step: 116, loss: 3.428431987762451\n",
      "epoch: 64,  batch step: 117, loss: 13.545339584350586\n",
      "epoch: 64,  batch step: 118, loss: 40.026611328125\n",
      "epoch: 64,  batch step: 119, loss: 43.6410026550293\n",
      "epoch: 64,  batch step: 120, loss: 6.781169891357422\n",
      "epoch: 64,  batch step: 121, loss: 33.12409591674805\n",
      "epoch: 64,  batch step: 122, loss: 15.679304122924805\n",
      "epoch: 64,  batch step: 123, loss: 16.42398452758789\n",
      "epoch: 64,  batch step: 124, loss: 27.344295501708984\n",
      "epoch: 64,  batch step: 125, loss: 4.252772808074951\n",
      "epoch: 64,  batch step: 126, loss: 37.18049621582031\n",
      "epoch: 64,  batch step: 127, loss: 3.648869276046753\n",
      "epoch: 64,  batch step: 128, loss: 3.676558017730713\n",
      "epoch: 64,  batch step: 129, loss: 3.564518690109253\n",
      "epoch: 64,  batch step: 130, loss: 2.990445852279663\n",
      "epoch: 64,  batch step: 131, loss: 6.1265106201171875\n",
      "epoch: 64,  batch step: 132, loss: 67.41273498535156\n",
      "epoch: 64,  batch step: 133, loss: 12.520380020141602\n",
      "epoch: 64,  batch step: 134, loss: 59.32267761230469\n",
      "epoch: 64,  batch step: 135, loss: 3.323911190032959\n",
      "epoch: 64,  batch step: 136, loss: 31.07290267944336\n",
      "epoch: 64,  batch step: 137, loss: 25.00644302368164\n",
      "epoch: 64,  batch step: 138, loss: 3.0529398918151855\n",
      "epoch: 64,  batch step: 139, loss: 20.856605529785156\n",
      "epoch: 64,  batch step: 140, loss: 2.7242724895477295\n",
      "epoch: 64,  batch step: 141, loss: 8.531181335449219\n",
      "epoch: 64,  batch step: 142, loss: 52.18962860107422\n",
      "epoch: 64,  batch step: 143, loss: 44.203758239746094\n",
      "epoch: 64,  batch step: 144, loss: 7.41715145111084\n",
      "epoch: 64,  batch step: 145, loss: 6.728908538818359\n",
      "epoch: 64,  batch step: 146, loss: 30.705020904541016\n",
      "epoch: 64,  batch step: 147, loss: 15.131495475769043\n",
      "epoch: 64,  batch step: 148, loss: 19.652587890625\n",
      "epoch: 64,  batch step: 149, loss: 3.040893316268921\n",
      "epoch: 64,  batch step: 150, loss: 30.588048934936523\n",
      "epoch: 64,  batch step: 151, loss: 5.443694591522217\n",
      "epoch: 64,  batch step: 152, loss: 3.684159994125366\n",
      "epoch: 64,  batch step: 153, loss: 3.1959357261657715\n",
      "epoch: 64,  batch step: 154, loss: 12.188413619995117\n",
      "epoch: 64,  batch step: 155, loss: 5.198573112487793\n",
      "epoch: 64,  batch step: 156, loss: 37.93131637573242\n",
      "epoch: 64,  batch step: 157, loss: 2.5668113231658936\n",
      "epoch: 64,  batch step: 158, loss: 2.347992420196533\n",
      "epoch: 64,  batch step: 159, loss: 4.098676681518555\n",
      "epoch: 64,  batch step: 160, loss: 4.304781913757324\n",
      "epoch: 64,  batch step: 161, loss: 15.829059600830078\n",
      "epoch: 64,  batch step: 162, loss: 2.9524784088134766\n",
      "epoch: 64,  batch step: 163, loss: 8.119614601135254\n",
      "epoch: 64,  batch step: 164, loss: 76.13580322265625\n",
      "epoch: 64,  batch step: 165, loss: 14.721789360046387\n",
      "epoch: 64,  batch step: 166, loss: 5.716180801391602\n",
      "epoch: 64,  batch step: 167, loss: 53.266632080078125\n",
      "epoch: 64,  batch step: 168, loss: 2.1959824562072754\n",
      "epoch: 64,  batch step: 169, loss: 5.632335662841797\n",
      "epoch: 64,  batch step: 170, loss: 65.30888366699219\n",
      "epoch: 64,  batch step: 171, loss: 11.874125480651855\n",
      "epoch: 64,  batch step: 172, loss: 14.290125846862793\n",
      "epoch: 64,  batch step: 173, loss: 12.873374938964844\n",
      "epoch: 64,  batch step: 174, loss: 4.012008190155029\n",
      "epoch: 64,  batch step: 175, loss: 6.856869697570801\n",
      "epoch: 64,  batch step: 176, loss: 11.219587326049805\n",
      "epoch: 64,  batch step: 177, loss: 3.5142364501953125\n",
      "epoch: 64,  batch step: 178, loss: 48.299095153808594\n",
      "epoch: 64,  batch step: 179, loss: 5.943798065185547\n",
      "epoch: 64,  batch step: 180, loss: 3.872854709625244\n",
      "epoch: 64,  batch step: 181, loss: 3.988600730895996\n",
      "epoch: 64,  batch step: 182, loss: 3.2238497734069824\n",
      "epoch: 64,  batch step: 183, loss: 4.281529426574707\n",
      "epoch: 64,  batch step: 184, loss: 32.91535186767578\n",
      "epoch: 64,  batch step: 185, loss: 3.08433198928833\n",
      "epoch: 64,  batch step: 186, loss: 18.478288650512695\n",
      "epoch: 64,  batch step: 187, loss: 16.828140258789062\n",
      "epoch: 64,  batch step: 188, loss: 17.635215759277344\n",
      "epoch: 64,  batch step: 189, loss: 3.3059887886047363\n",
      "epoch: 64,  batch step: 190, loss: 2.361398220062256\n",
      "epoch: 64,  batch step: 191, loss: 3.1182620525360107\n",
      "epoch: 64,  batch step: 192, loss: 3.708420753479004\n",
      "epoch: 64,  batch step: 193, loss: 15.081459045410156\n",
      "epoch: 64,  batch step: 194, loss: 2.1762075424194336\n",
      "epoch: 64,  batch step: 195, loss: 133.6201171875\n",
      "epoch: 64,  batch step: 196, loss: 13.922904014587402\n",
      "epoch: 64,  batch step: 197, loss: 17.104270935058594\n",
      "epoch: 64,  batch step: 198, loss: 4.092198848724365\n",
      "epoch: 64,  batch step: 199, loss: 2.5382795333862305\n",
      "epoch: 64,  batch step: 200, loss: 3.748335599899292\n",
      "epoch: 64,  batch step: 201, loss: 3.058542251586914\n",
      "epoch: 64,  batch step: 202, loss: 5.975001335144043\n",
      "epoch: 64,  batch step: 203, loss: 79.48281860351562\n",
      "epoch: 64,  batch step: 204, loss: 4.106008052825928\n",
      "epoch: 64,  batch step: 205, loss: 40.28969955444336\n",
      "epoch: 64,  batch step: 206, loss: 3.373817205429077\n",
      "epoch: 64,  batch step: 207, loss: 15.826212882995605\n",
      "epoch: 64,  batch step: 208, loss: 4.57131290435791\n",
      "epoch: 64,  batch step: 209, loss: 27.037948608398438\n",
      "epoch: 64,  batch step: 210, loss: 2.0737502574920654\n",
      "epoch: 64,  batch step: 211, loss: 43.631282806396484\n",
      "epoch: 64,  batch step: 212, loss: 9.792827606201172\n",
      "epoch: 64,  batch step: 213, loss: 16.016380310058594\n",
      "epoch: 64,  batch step: 214, loss: 4.430049419403076\n",
      "epoch: 64,  batch step: 215, loss: 46.05906295776367\n",
      "epoch: 64,  batch step: 216, loss: 5.73602819442749\n",
      "epoch: 64,  batch step: 217, loss: 6.045180797576904\n",
      "epoch: 64,  batch step: 218, loss: 3.167919874191284\n",
      "epoch: 64,  batch step: 219, loss: 5.204156398773193\n",
      "epoch: 64,  batch step: 220, loss: 8.507108688354492\n",
      "epoch: 64,  batch step: 221, loss: 17.99519920349121\n",
      "epoch: 64,  batch step: 222, loss: 2.918947219848633\n",
      "epoch: 64,  batch step: 223, loss: 41.80762481689453\n",
      "epoch: 64,  batch step: 224, loss: 29.417478561401367\n",
      "epoch: 64,  batch step: 225, loss: 54.321502685546875\n",
      "epoch: 64,  batch step: 226, loss: 36.44007873535156\n",
      "epoch: 64,  batch step: 227, loss: 15.91126823425293\n",
      "epoch: 64,  batch step: 228, loss: 3.133162260055542\n",
      "epoch: 64,  batch step: 229, loss: 2.6313071250915527\n",
      "epoch: 64,  batch step: 230, loss: 4.254223346710205\n",
      "epoch: 64,  batch step: 231, loss: 4.39860725402832\n",
      "epoch: 64,  batch step: 232, loss: 4.092860698699951\n",
      "epoch: 64,  batch step: 233, loss: 9.248433113098145\n",
      "epoch: 64,  batch step: 234, loss: 5.5769782066345215\n",
      "epoch: 64,  batch step: 235, loss: 11.117691040039062\n",
      "epoch: 64,  batch step: 236, loss: 14.879396438598633\n",
      "epoch: 64,  batch step: 237, loss: 3.9499268531799316\n",
      "epoch: 64,  batch step: 238, loss: 20.48654556274414\n",
      "epoch: 64,  batch step: 239, loss: 20.207338333129883\n",
      "epoch: 64,  batch step: 240, loss: 3.514486789703369\n",
      "epoch: 64,  batch step: 241, loss: 38.940948486328125\n",
      "epoch: 64,  batch step: 242, loss: 13.637426376342773\n",
      "epoch: 64,  batch step: 243, loss: 3.0262389183044434\n",
      "epoch: 64,  batch step: 244, loss: 9.973751068115234\n",
      "epoch: 64,  batch step: 245, loss: 18.256980895996094\n",
      "epoch: 64,  batch step: 246, loss: 22.776884078979492\n",
      "epoch: 64,  batch step: 247, loss: 51.61285400390625\n",
      "epoch: 64,  batch step: 248, loss: 50.74177551269531\n",
      "epoch: 64,  batch step: 249, loss: 3.769885540008545\n",
      "epoch: 64,  batch step: 250, loss: 26.179122924804688\n",
      "epoch: 64,  batch step: 251, loss: 6.858283042907715\n",
      "validation error epoch  64:    tensor(64.2804, device='cuda:0')\n",
      "316\n",
      "epoch: 65,  batch step: 0, loss: 23.31940269470215\n",
      "epoch: 65,  batch step: 1, loss: 10.476731300354004\n",
      "epoch: 65,  batch step: 2, loss: 24.778121948242188\n",
      "epoch: 65,  batch step: 3, loss: 9.865596771240234\n",
      "epoch: 65,  batch step: 4, loss: 20.038066864013672\n",
      "epoch: 65,  batch step: 5, loss: 25.703908920288086\n",
      "epoch: 65,  batch step: 6, loss: 1.843290090560913\n",
      "epoch: 65,  batch step: 7, loss: 3.046912670135498\n",
      "epoch: 65,  batch step: 8, loss: 2.1074674129486084\n",
      "epoch: 65,  batch step: 9, loss: 29.895404815673828\n",
      "epoch: 65,  batch step: 10, loss: 17.382450103759766\n",
      "epoch: 65,  batch step: 11, loss: 30.47896957397461\n",
      "epoch: 65,  batch step: 12, loss: 136.26712036132812\n",
      "epoch: 65,  batch step: 13, loss: 68.83683776855469\n",
      "epoch: 65,  batch step: 14, loss: 4.203136920928955\n",
      "epoch: 65,  batch step: 15, loss: 41.03535461425781\n",
      "epoch: 65,  batch step: 16, loss: 6.182888031005859\n",
      "epoch: 65,  batch step: 17, loss: 9.53380012512207\n",
      "epoch: 65,  batch step: 18, loss: 2.9174602031707764\n",
      "epoch: 65,  batch step: 19, loss: 23.599584579467773\n",
      "epoch: 65,  batch step: 20, loss: 4.567004203796387\n",
      "epoch: 65,  batch step: 21, loss: 60.52506637573242\n",
      "epoch: 65,  batch step: 22, loss: 15.778966903686523\n",
      "epoch: 65,  batch step: 23, loss: 3.4185760021209717\n",
      "epoch: 65,  batch step: 24, loss: 4.1388773918151855\n",
      "epoch: 65,  batch step: 25, loss: 43.417625427246094\n",
      "epoch: 65,  batch step: 26, loss: 33.97697830200195\n",
      "epoch: 65,  batch step: 27, loss: 35.419612884521484\n",
      "epoch: 65,  batch step: 28, loss: 35.56382751464844\n",
      "epoch: 65,  batch step: 29, loss: 4.268344879150391\n",
      "epoch: 65,  batch step: 30, loss: 2.9818999767303467\n",
      "epoch: 65,  batch step: 31, loss: 3.7575595378875732\n",
      "epoch: 65,  batch step: 32, loss: 18.909427642822266\n",
      "epoch: 65,  batch step: 33, loss: 2.9913203716278076\n",
      "epoch: 65,  batch step: 34, loss: 24.93653106689453\n",
      "epoch: 65,  batch step: 35, loss: 28.716110229492188\n",
      "epoch: 65,  batch step: 36, loss: 9.56611442565918\n",
      "epoch: 65,  batch step: 37, loss: 8.619650840759277\n",
      "epoch: 65,  batch step: 38, loss: 3.248900890350342\n",
      "epoch: 65,  batch step: 39, loss: 16.625017166137695\n",
      "epoch: 65,  batch step: 40, loss: 29.258947372436523\n",
      "epoch: 65,  batch step: 41, loss: 37.352874755859375\n",
      "epoch: 65,  batch step: 42, loss: 3.2371582984924316\n",
      "epoch: 65,  batch step: 43, loss: 2.6443567276000977\n",
      "epoch: 65,  batch step: 44, loss: 3.40643572807312\n",
      "epoch: 65,  batch step: 45, loss: 12.677488327026367\n",
      "epoch: 65,  batch step: 46, loss: 15.723834037780762\n",
      "epoch: 65,  batch step: 47, loss: 27.579029083251953\n",
      "epoch: 65,  batch step: 48, loss: 3.739598035812378\n",
      "epoch: 65,  batch step: 49, loss: 22.788105010986328\n",
      "epoch: 65,  batch step: 50, loss: 15.771202087402344\n",
      "epoch: 65,  batch step: 51, loss: 3.1955087184906006\n",
      "epoch: 65,  batch step: 52, loss: 5.9564619064331055\n",
      "epoch: 65,  batch step: 53, loss: 3.208395481109619\n",
      "epoch: 65,  batch step: 54, loss: 13.17772102355957\n",
      "epoch: 65,  batch step: 55, loss: 3.7420156002044678\n",
      "epoch: 65,  batch step: 56, loss: 3.3531274795532227\n",
      "epoch: 65,  batch step: 57, loss: 66.10435485839844\n",
      "epoch: 65,  batch step: 58, loss: 2.896547794342041\n",
      "epoch: 65,  batch step: 59, loss: 8.130549430847168\n",
      "epoch: 65,  batch step: 60, loss: 13.799094200134277\n",
      "epoch: 65,  batch step: 61, loss: 20.925491333007812\n",
      "epoch: 65,  batch step: 62, loss: 2.5449109077453613\n",
      "epoch: 65,  batch step: 63, loss: 13.18115520477295\n",
      "epoch: 65,  batch step: 64, loss: 3.8056588172912598\n",
      "epoch: 65,  batch step: 65, loss: 3.2898635864257812\n",
      "epoch: 65,  batch step: 66, loss: 2.1973443031311035\n",
      "epoch: 65,  batch step: 67, loss: 31.321693420410156\n",
      "epoch: 65,  batch step: 68, loss: 3.1581735610961914\n",
      "epoch: 65,  batch step: 69, loss: 42.42995071411133\n",
      "epoch: 65,  batch step: 70, loss: 3.272163152694702\n",
      "epoch: 65,  batch step: 71, loss: 29.83209228515625\n",
      "epoch: 65,  batch step: 72, loss: 3.278118848800659\n",
      "epoch: 65,  batch step: 73, loss: 5.764443397521973\n",
      "epoch: 65,  batch step: 74, loss: 14.152109146118164\n",
      "epoch: 65,  batch step: 75, loss: 13.21970272064209\n",
      "epoch: 65,  batch step: 76, loss: 2.7605807781219482\n",
      "epoch: 65,  batch step: 77, loss: 4.890918731689453\n",
      "epoch: 65,  batch step: 78, loss: 57.199913024902344\n",
      "epoch: 65,  batch step: 79, loss: 3.330901622772217\n",
      "epoch: 65,  batch step: 80, loss: 5.100276470184326\n",
      "epoch: 65,  batch step: 81, loss: 58.236595153808594\n",
      "epoch: 65,  batch step: 82, loss: 4.374989986419678\n",
      "epoch: 65,  batch step: 83, loss: 3.3445377349853516\n",
      "epoch: 65,  batch step: 84, loss: 115.73509216308594\n",
      "epoch: 65,  batch step: 85, loss: 3.5293831825256348\n",
      "epoch: 65,  batch step: 86, loss: 52.69697570800781\n",
      "epoch: 65,  batch step: 87, loss: 39.511714935302734\n",
      "epoch: 65,  batch step: 88, loss: 3.5132761001586914\n",
      "epoch: 65,  batch step: 89, loss: 19.467449188232422\n",
      "epoch: 65,  batch step: 90, loss: 4.044098854064941\n",
      "epoch: 65,  batch step: 91, loss: 5.537135124206543\n",
      "epoch: 65,  batch step: 92, loss: 5.055145263671875\n",
      "epoch: 65,  batch step: 93, loss: 19.17966079711914\n",
      "epoch: 65,  batch step: 94, loss: 53.08173751831055\n",
      "epoch: 65,  batch step: 95, loss: 6.621325492858887\n",
      "epoch: 65,  batch step: 96, loss: 27.97089385986328\n",
      "epoch: 65,  batch step: 97, loss: 6.051077842712402\n",
      "epoch: 65,  batch step: 98, loss: 8.420157432556152\n",
      "epoch: 65,  batch step: 99, loss: 25.282032012939453\n",
      "epoch: 65,  batch step: 100, loss: 8.327493667602539\n",
      "epoch: 65,  batch step: 101, loss: 42.8050651550293\n",
      "epoch: 65,  batch step: 102, loss: 18.587141036987305\n",
      "epoch: 65,  batch step: 103, loss: 3.779085636138916\n",
      "epoch: 65,  batch step: 104, loss: 4.164313316345215\n",
      "epoch: 65,  batch step: 105, loss: 11.738234519958496\n",
      "epoch: 65,  batch step: 106, loss: 10.976774215698242\n",
      "epoch: 65,  batch step: 107, loss: 33.74675750732422\n",
      "epoch: 65,  batch step: 108, loss: 9.387472152709961\n",
      "epoch: 65,  batch step: 109, loss: 15.658846855163574\n",
      "epoch: 65,  batch step: 110, loss: 25.57472801208496\n",
      "epoch: 65,  batch step: 111, loss: 52.419410705566406\n",
      "epoch: 65,  batch step: 112, loss: 20.445528030395508\n",
      "epoch: 65,  batch step: 113, loss: 52.406185150146484\n",
      "epoch: 65,  batch step: 114, loss: 4.376446723937988\n",
      "epoch: 65,  batch step: 115, loss: 16.466596603393555\n",
      "epoch: 65,  batch step: 116, loss: 30.102317810058594\n",
      "epoch: 65,  batch step: 117, loss: 38.71565628051758\n",
      "epoch: 65,  batch step: 118, loss: 19.980009078979492\n",
      "epoch: 65,  batch step: 119, loss: 47.05602264404297\n",
      "epoch: 65,  batch step: 120, loss: 3.2574222087860107\n",
      "epoch: 65,  batch step: 121, loss: 14.134421348571777\n",
      "epoch: 65,  batch step: 122, loss: 17.946739196777344\n",
      "epoch: 65,  batch step: 123, loss: 4.996680736541748\n",
      "epoch: 65,  batch step: 124, loss: 18.127269744873047\n",
      "epoch: 65,  batch step: 125, loss: 42.723297119140625\n",
      "epoch: 65,  batch step: 126, loss: 3.3066821098327637\n",
      "epoch: 65,  batch step: 127, loss: 3.9137284755706787\n",
      "epoch: 65,  batch step: 128, loss: 3.6583974361419678\n",
      "epoch: 65,  batch step: 129, loss: 7.561095714569092\n",
      "epoch: 65,  batch step: 130, loss: 15.973371505737305\n",
      "epoch: 65,  batch step: 131, loss: 3.2366881370544434\n",
      "epoch: 65,  batch step: 132, loss: 14.550272941589355\n",
      "epoch: 65,  batch step: 133, loss: 35.965232849121094\n",
      "epoch: 65,  batch step: 134, loss: 2.834747314453125\n",
      "epoch: 65,  batch step: 135, loss: 11.17679214477539\n",
      "epoch: 65,  batch step: 136, loss: 5.623696804046631\n",
      "epoch: 65,  batch step: 137, loss: 23.3973388671875\n",
      "epoch: 65,  batch step: 138, loss: 47.17876434326172\n",
      "epoch: 65,  batch step: 139, loss: 3.912351131439209\n",
      "epoch: 65,  batch step: 140, loss: 68.97332763671875\n",
      "epoch: 65,  batch step: 141, loss: 2.490938663482666\n",
      "epoch: 65,  batch step: 142, loss: 3.591276168823242\n",
      "epoch: 65,  batch step: 143, loss: 39.595314025878906\n",
      "epoch: 65,  batch step: 144, loss: 3.5990543365478516\n",
      "epoch: 65,  batch step: 145, loss: 12.79234504699707\n",
      "epoch: 65,  batch step: 146, loss: 20.953983306884766\n",
      "epoch: 65,  batch step: 147, loss: 9.725954055786133\n",
      "epoch: 65,  batch step: 148, loss: 1.8944810628890991\n",
      "epoch: 65,  batch step: 149, loss: 17.672298431396484\n",
      "epoch: 65,  batch step: 150, loss: 6.605978012084961\n",
      "epoch: 65,  batch step: 151, loss: 23.84328842163086\n",
      "epoch: 65,  batch step: 152, loss: 9.527633666992188\n",
      "epoch: 65,  batch step: 153, loss: 11.138858795166016\n",
      "epoch: 65,  batch step: 154, loss: 10.713698387145996\n",
      "epoch: 65,  batch step: 155, loss: 3.47390079498291\n",
      "epoch: 65,  batch step: 156, loss: 85.31712341308594\n",
      "epoch: 65,  batch step: 157, loss: 26.674015045166016\n",
      "epoch: 65,  batch step: 158, loss: 3.391178607940674\n",
      "epoch: 65,  batch step: 159, loss: 2.2502169609069824\n",
      "epoch: 65,  batch step: 160, loss: 3.640101194381714\n",
      "epoch: 65,  batch step: 161, loss: 4.2028937339782715\n",
      "epoch: 65,  batch step: 162, loss: 4.352075576782227\n",
      "epoch: 65,  batch step: 163, loss: 63.37445068359375\n",
      "epoch: 65,  batch step: 164, loss: 15.293798446655273\n",
      "epoch: 65,  batch step: 165, loss: 2.4592208862304688\n",
      "epoch: 65,  batch step: 166, loss: 58.48806381225586\n",
      "epoch: 65,  batch step: 167, loss: 21.969768524169922\n",
      "epoch: 65,  batch step: 168, loss: 9.537805557250977\n",
      "epoch: 65,  batch step: 169, loss: 3.9325404167175293\n",
      "epoch: 65,  batch step: 170, loss: 11.8421049118042\n",
      "epoch: 65,  batch step: 171, loss: 4.054751873016357\n",
      "epoch: 65,  batch step: 172, loss: 16.79793357849121\n",
      "epoch: 65,  batch step: 173, loss: 3.4004523754119873\n",
      "epoch: 65,  batch step: 174, loss: 24.838056564331055\n",
      "epoch: 65,  batch step: 175, loss: 2.886113166809082\n",
      "epoch: 65,  batch step: 176, loss: 15.855360984802246\n",
      "epoch: 65,  batch step: 177, loss: 3.298064708709717\n",
      "epoch: 65,  batch step: 178, loss: 51.273433685302734\n",
      "epoch: 65,  batch step: 179, loss: 12.288383483886719\n",
      "epoch: 65,  batch step: 180, loss: 16.178787231445312\n",
      "epoch: 65,  batch step: 181, loss: 2.5429837703704834\n",
      "epoch: 65,  batch step: 182, loss: 2.2578282356262207\n",
      "epoch: 65,  batch step: 183, loss: 3.926436185836792\n",
      "epoch: 65,  batch step: 184, loss: 12.819262504577637\n",
      "epoch: 65,  batch step: 185, loss: 5.559194087982178\n",
      "epoch: 65,  batch step: 186, loss: 2.5039215087890625\n",
      "epoch: 65,  batch step: 187, loss: 48.244468688964844\n",
      "epoch: 65,  batch step: 188, loss: 1.912524938583374\n",
      "epoch: 65,  batch step: 189, loss: 4.7374186515808105\n",
      "epoch: 65,  batch step: 190, loss: 3.356173515319824\n",
      "epoch: 65,  batch step: 191, loss: 3.3836655616760254\n",
      "epoch: 65,  batch step: 192, loss: 10.204511642456055\n",
      "epoch: 65,  batch step: 193, loss: 7.440391540527344\n",
      "epoch: 65,  batch step: 194, loss: 38.62467956542969\n",
      "epoch: 65,  batch step: 195, loss: 16.801795959472656\n",
      "epoch: 65,  batch step: 196, loss: 9.166215896606445\n",
      "epoch: 65,  batch step: 197, loss: 3.5906875133514404\n",
      "epoch: 65,  batch step: 198, loss: 19.177024841308594\n",
      "epoch: 65,  batch step: 199, loss: 13.48611068725586\n",
      "epoch: 65,  batch step: 200, loss: 7.3091840744018555\n",
      "epoch: 65,  batch step: 201, loss: 5.100015640258789\n",
      "epoch: 65,  batch step: 202, loss: 3.0353927612304688\n",
      "epoch: 65,  batch step: 203, loss: 3.1445350646972656\n",
      "epoch: 65,  batch step: 204, loss: 3.5276260375976562\n",
      "epoch: 65,  batch step: 205, loss: 2.3224377632141113\n",
      "epoch: 65,  batch step: 206, loss: 24.337947845458984\n",
      "epoch: 65,  batch step: 207, loss: 2.4406392574310303\n",
      "epoch: 65,  batch step: 208, loss: 28.867591857910156\n",
      "epoch: 65,  batch step: 209, loss: 36.85491943359375\n",
      "epoch: 65,  batch step: 210, loss: 3.721158027648926\n",
      "epoch: 65,  batch step: 211, loss: 2.122634172439575\n",
      "epoch: 65,  batch step: 212, loss: 17.16200828552246\n",
      "epoch: 65,  batch step: 213, loss: 11.544351577758789\n",
      "epoch: 65,  batch step: 214, loss: 2.4857029914855957\n",
      "epoch: 65,  batch step: 215, loss: 15.996321678161621\n",
      "epoch: 65,  batch step: 216, loss: 23.593015670776367\n",
      "epoch: 65,  batch step: 217, loss: 26.13579559326172\n",
      "epoch: 65,  batch step: 218, loss: 63.183074951171875\n",
      "epoch: 65,  batch step: 219, loss: 28.364696502685547\n",
      "epoch: 65,  batch step: 220, loss: 16.84206771850586\n",
      "epoch: 65,  batch step: 221, loss: 10.718966484069824\n",
      "epoch: 65,  batch step: 222, loss: 17.402368545532227\n",
      "epoch: 65,  batch step: 223, loss: 3.930083751678467\n",
      "epoch: 65,  batch step: 224, loss: 45.540687561035156\n",
      "epoch: 65,  batch step: 225, loss: 11.088435173034668\n",
      "epoch: 65,  batch step: 226, loss: 25.785449981689453\n",
      "epoch: 65,  batch step: 227, loss: 7.288545608520508\n",
      "epoch: 65,  batch step: 228, loss: 15.35097885131836\n",
      "epoch: 65,  batch step: 229, loss: 2.4601857662200928\n",
      "epoch: 65,  batch step: 230, loss: 16.442859649658203\n",
      "epoch: 65,  batch step: 231, loss: 22.817646026611328\n",
      "epoch: 65,  batch step: 232, loss: 33.01586151123047\n",
      "epoch: 65,  batch step: 233, loss: 27.14735221862793\n",
      "epoch: 65,  batch step: 234, loss: 17.411056518554688\n",
      "epoch: 65,  batch step: 235, loss: 4.677581310272217\n",
      "epoch: 65,  batch step: 236, loss: 2.2201344966888428\n",
      "epoch: 65,  batch step: 237, loss: 2.691889762878418\n",
      "epoch: 65,  batch step: 238, loss: 3.117844581604004\n",
      "epoch: 65,  batch step: 239, loss: 3.6010794639587402\n",
      "epoch: 65,  batch step: 240, loss: 4.417713165283203\n",
      "epoch: 65,  batch step: 241, loss: 4.520018577575684\n",
      "epoch: 65,  batch step: 242, loss: 3.9869678020477295\n",
      "epoch: 65,  batch step: 243, loss: 3.0139307975769043\n",
      "epoch: 65,  batch step: 244, loss: 2.416118860244751\n",
      "epoch: 65,  batch step: 245, loss: 3.156388759613037\n",
      "epoch: 65,  batch step: 246, loss: 2.525733232498169\n",
      "epoch: 65,  batch step: 247, loss: 2.9397644996643066\n",
      "epoch: 65,  batch step: 248, loss: 10.068675994873047\n",
      "epoch: 65,  batch step: 249, loss: 14.092041015625\n",
      "epoch: 65,  batch step: 250, loss: 11.342853546142578\n",
      "epoch: 65,  batch step: 251, loss: 4.458262920379639\n",
      "validation error epoch  65:    tensor(73.4066, device='cuda:0')\n",
      "316\n",
      "epoch: 66,  batch step: 0, loss: 7.010256767272949\n",
      "epoch: 66,  batch step: 1, loss: 57.60383224487305\n",
      "epoch: 66,  batch step: 2, loss: 5.0569891929626465\n",
      "epoch: 66,  batch step: 3, loss: 14.72230339050293\n",
      "epoch: 66,  batch step: 4, loss: 4.053701400756836\n",
      "epoch: 66,  batch step: 5, loss: 18.462181091308594\n",
      "epoch: 66,  batch step: 6, loss: 14.61758041381836\n",
      "epoch: 66,  batch step: 7, loss: 21.956905364990234\n",
      "epoch: 66,  batch step: 8, loss: 3.6954097747802734\n",
      "epoch: 66,  batch step: 9, loss: 21.52774429321289\n",
      "epoch: 66,  batch step: 10, loss: 49.81654357910156\n",
      "epoch: 66,  batch step: 11, loss: 2.522138833999634\n",
      "epoch: 66,  batch step: 12, loss: 4.249545574188232\n",
      "epoch: 66,  batch step: 13, loss: 16.030834197998047\n",
      "epoch: 66,  batch step: 14, loss: 15.02389907836914\n",
      "epoch: 66,  batch step: 15, loss: 3.858382225036621\n",
      "epoch: 66,  batch step: 16, loss: 4.20607852935791\n",
      "epoch: 66,  batch step: 17, loss: 68.50244903564453\n",
      "epoch: 66,  batch step: 18, loss: 17.088071823120117\n",
      "epoch: 66,  batch step: 19, loss: 3.5476126670837402\n",
      "epoch: 66,  batch step: 20, loss: 25.17378044128418\n",
      "epoch: 66,  batch step: 21, loss: 44.45155334472656\n",
      "epoch: 66,  batch step: 22, loss: 3.2811594009399414\n",
      "epoch: 66,  batch step: 23, loss: 2.72934889793396\n",
      "epoch: 66,  batch step: 24, loss: 19.90167236328125\n",
      "epoch: 66,  batch step: 25, loss: 7.3460211753845215\n",
      "epoch: 66,  batch step: 26, loss: 3.2517058849334717\n",
      "epoch: 66,  batch step: 27, loss: 2.9389781951904297\n",
      "epoch: 66,  batch step: 28, loss: 4.619170188903809\n",
      "epoch: 66,  batch step: 29, loss: 2.183683395385742\n",
      "epoch: 66,  batch step: 30, loss: 2.4731147289276123\n",
      "epoch: 66,  batch step: 31, loss: 7.227267265319824\n",
      "epoch: 66,  batch step: 32, loss: 50.51597213745117\n",
      "epoch: 66,  batch step: 33, loss: 3.871938705444336\n",
      "epoch: 66,  batch step: 34, loss: 19.481910705566406\n",
      "epoch: 66,  batch step: 35, loss: 3.574364423751831\n",
      "epoch: 66,  batch step: 36, loss: 26.276493072509766\n",
      "epoch: 66,  batch step: 37, loss: 16.00715446472168\n",
      "epoch: 66,  batch step: 38, loss: 24.02240753173828\n",
      "epoch: 66,  batch step: 39, loss: 26.00606346130371\n",
      "epoch: 66,  batch step: 40, loss: 2.7572803497314453\n",
      "epoch: 66,  batch step: 41, loss: 15.497207641601562\n",
      "epoch: 66,  batch step: 42, loss: 15.109885215759277\n",
      "epoch: 66,  batch step: 43, loss: 2.512777805328369\n",
      "epoch: 66,  batch step: 44, loss: 2.4781575202941895\n",
      "epoch: 66,  batch step: 45, loss: 3.556523323059082\n",
      "epoch: 66,  batch step: 46, loss: 3.7813401222229004\n",
      "epoch: 66,  batch step: 47, loss: 4.765247344970703\n",
      "epoch: 66,  batch step: 48, loss: 16.795921325683594\n",
      "epoch: 66,  batch step: 49, loss: 11.34384822845459\n",
      "epoch: 66,  batch step: 50, loss: 14.001951217651367\n",
      "epoch: 66,  batch step: 51, loss: 30.67043113708496\n",
      "epoch: 66,  batch step: 52, loss: 22.30867576599121\n",
      "epoch: 66,  batch step: 53, loss: 2.6006369590759277\n",
      "epoch: 66,  batch step: 54, loss: 5.433811187744141\n",
      "epoch: 66,  batch step: 55, loss: 3.0671279430389404\n",
      "epoch: 66,  batch step: 56, loss: 71.27556610107422\n",
      "epoch: 66,  batch step: 57, loss: 9.56298828125\n",
      "epoch: 66,  batch step: 58, loss: 3.0305116176605225\n",
      "epoch: 66,  batch step: 59, loss: 4.011845588684082\n",
      "epoch: 66,  batch step: 60, loss: 16.40420913696289\n",
      "epoch: 66,  batch step: 61, loss: 21.49504852294922\n",
      "epoch: 66,  batch step: 62, loss: 3.8172616958618164\n",
      "epoch: 66,  batch step: 63, loss: 34.480247497558594\n",
      "epoch: 66,  batch step: 64, loss: 10.596881866455078\n",
      "epoch: 66,  batch step: 65, loss: 9.711465835571289\n",
      "epoch: 66,  batch step: 66, loss: 4.700616359710693\n",
      "epoch: 66,  batch step: 67, loss: 4.367709636688232\n",
      "epoch: 66,  batch step: 68, loss: 3.3849191665649414\n",
      "epoch: 66,  batch step: 69, loss: 15.277101516723633\n",
      "epoch: 66,  batch step: 70, loss: 40.30613708496094\n",
      "epoch: 66,  batch step: 71, loss: 2.889033794403076\n",
      "epoch: 66,  batch step: 72, loss: 8.146441459655762\n",
      "epoch: 66,  batch step: 73, loss: 3.2036292552948\n",
      "epoch: 66,  batch step: 74, loss: 14.929180145263672\n",
      "epoch: 66,  batch step: 75, loss: 12.930994033813477\n",
      "epoch: 66,  batch step: 76, loss: 16.060348510742188\n",
      "epoch: 66,  batch step: 77, loss: 2.635927200317383\n",
      "epoch: 66,  batch step: 78, loss: 3.373894453048706\n",
      "epoch: 66,  batch step: 79, loss: 14.702052116394043\n",
      "epoch: 66,  batch step: 80, loss: 15.646400451660156\n",
      "epoch: 66,  batch step: 81, loss: 15.703959465026855\n",
      "epoch: 66,  batch step: 82, loss: 12.36048698425293\n",
      "epoch: 66,  batch step: 83, loss: 12.490550994873047\n",
      "epoch: 66,  batch step: 84, loss: 10.846916198730469\n",
      "epoch: 66,  batch step: 85, loss: 2.6936025619506836\n",
      "epoch: 66,  batch step: 86, loss: 2.714045524597168\n",
      "epoch: 66,  batch step: 87, loss: 3.479867935180664\n",
      "epoch: 66,  batch step: 88, loss: 3.8458304405212402\n",
      "epoch: 66,  batch step: 89, loss: 2.461179494857788\n",
      "epoch: 66,  batch step: 90, loss: 14.08038330078125\n",
      "epoch: 66,  batch step: 91, loss: 32.604042053222656\n",
      "epoch: 66,  batch step: 92, loss: 10.919848442077637\n",
      "epoch: 66,  batch step: 93, loss: 2.7372045516967773\n",
      "epoch: 66,  batch step: 94, loss: 31.533626556396484\n",
      "epoch: 66,  batch step: 95, loss: 15.624717712402344\n",
      "epoch: 66,  batch step: 96, loss: 2.081852912902832\n",
      "epoch: 66,  batch step: 97, loss: 3.214569091796875\n",
      "epoch: 66,  batch step: 98, loss: 29.071819305419922\n",
      "epoch: 66,  batch step: 99, loss: 42.5217399597168\n",
      "epoch: 66,  batch step: 100, loss: 3.886972188949585\n",
      "epoch: 66,  batch step: 101, loss: 2.6073310375213623\n",
      "epoch: 66,  batch step: 102, loss: 10.178564071655273\n",
      "epoch: 66,  batch step: 103, loss: 22.545711517333984\n",
      "epoch: 66,  batch step: 104, loss: 15.917118072509766\n",
      "epoch: 66,  batch step: 105, loss: 5.365303039550781\n",
      "epoch: 66,  batch step: 106, loss: 11.412649154663086\n",
      "epoch: 66,  batch step: 107, loss: 7.365670680999756\n",
      "epoch: 66,  batch step: 108, loss: 19.090776443481445\n",
      "epoch: 66,  batch step: 109, loss: 19.091899871826172\n",
      "epoch: 66,  batch step: 110, loss: 50.517784118652344\n",
      "epoch: 66,  batch step: 111, loss: 1.9963314533233643\n",
      "epoch: 66,  batch step: 112, loss: 4.030413627624512\n",
      "epoch: 66,  batch step: 113, loss: 9.001497268676758\n",
      "epoch: 66,  batch step: 114, loss: 3.6336727142333984\n",
      "epoch: 66,  batch step: 115, loss: 2.8093135356903076\n",
      "epoch: 66,  batch step: 116, loss: 3.5046334266662598\n",
      "epoch: 66,  batch step: 117, loss: 2.9905595779418945\n",
      "epoch: 66,  batch step: 118, loss: 16.03250503540039\n",
      "epoch: 66,  batch step: 119, loss: 1.8335679769515991\n",
      "epoch: 66,  batch step: 120, loss: 7.676506996154785\n",
      "epoch: 66,  batch step: 121, loss: 39.65257263183594\n",
      "epoch: 66,  batch step: 122, loss: 3.0472354888916016\n",
      "epoch: 66,  batch step: 123, loss: 4.3045878410339355\n",
      "epoch: 66,  batch step: 124, loss: 31.397186279296875\n",
      "epoch: 66,  batch step: 125, loss: 2.620673894882202\n",
      "epoch: 66,  batch step: 126, loss: 2.370365619659424\n",
      "epoch: 66,  batch step: 127, loss: 2.774698257446289\n",
      "epoch: 66,  batch step: 128, loss: 2.740859031677246\n",
      "epoch: 66,  batch step: 129, loss: 13.968719482421875\n",
      "epoch: 66,  batch step: 130, loss: 6.111085414886475\n",
      "epoch: 66,  batch step: 131, loss: 40.66088104248047\n",
      "epoch: 66,  batch step: 132, loss: 2.6429429054260254\n",
      "epoch: 66,  batch step: 133, loss: 26.923717498779297\n",
      "epoch: 66,  batch step: 134, loss: 54.395263671875\n",
      "epoch: 66,  batch step: 135, loss: 31.814056396484375\n",
      "epoch: 66,  batch step: 136, loss: 3.6639246940612793\n",
      "epoch: 66,  batch step: 137, loss: 20.890562057495117\n",
      "epoch: 66,  batch step: 138, loss: 27.503448486328125\n",
      "epoch: 66,  batch step: 139, loss: 17.769039154052734\n",
      "epoch: 66,  batch step: 140, loss: 3.7342123985290527\n",
      "epoch: 66,  batch step: 141, loss: 9.381567001342773\n",
      "epoch: 66,  batch step: 142, loss: 31.390859603881836\n",
      "epoch: 66,  batch step: 143, loss: 2.5428123474121094\n",
      "epoch: 66,  batch step: 144, loss: 3.342909336090088\n",
      "epoch: 66,  batch step: 145, loss: 9.804234504699707\n",
      "epoch: 66,  batch step: 146, loss: 2.822476863861084\n",
      "epoch: 66,  batch step: 147, loss: 33.847373962402344\n",
      "epoch: 66,  batch step: 148, loss: 4.202840805053711\n",
      "epoch: 66,  batch step: 149, loss: 40.81932067871094\n",
      "epoch: 66,  batch step: 150, loss: 17.54021644592285\n",
      "epoch: 66,  batch step: 151, loss: 2.859920024871826\n",
      "epoch: 66,  batch step: 152, loss: 3.8882625102996826\n",
      "epoch: 66,  batch step: 153, loss: 2.9032530784606934\n",
      "epoch: 66,  batch step: 154, loss: 62.626556396484375\n",
      "epoch: 66,  batch step: 155, loss: 3.159419536590576\n",
      "epoch: 66,  batch step: 156, loss: 2.5256567001342773\n",
      "epoch: 66,  batch step: 157, loss: 6.405460357666016\n",
      "epoch: 66,  batch step: 158, loss: 4.017272472381592\n",
      "epoch: 66,  batch step: 159, loss: 25.110363006591797\n",
      "epoch: 66,  batch step: 160, loss: 5.3918681144714355\n",
      "epoch: 66,  batch step: 161, loss: 51.96446228027344\n",
      "epoch: 66,  batch step: 162, loss: 4.355738639831543\n",
      "epoch: 66,  batch step: 163, loss: 3.2260797023773193\n",
      "epoch: 66,  batch step: 164, loss: 3.063920736312866\n",
      "epoch: 66,  batch step: 165, loss: 29.03639793395996\n",
      "epoch: 66,  batch step: 166, loss: 9.338513374328613\n",
      "epoch: 66,  batch step: 167, loss: 2.867769718170166\n",
      "epoch: 66,  batch step: 168, loss: 21.760086059570312\n",
      "epoch: 66,  batch step: 169, loss: 3.4246909618377686\n",
      "epoch: 66,  batch step: 170, loss: 4.082948684692383\n",
      "epoch: 66,  batch step: 171, loss: 47.73814392089844\n",
      "epoch: 66,  batch step: 172, loss: 22.85199546813965\n",
      "epoch: 66,  batch step: 173, loss: 3.8225343227386475\n",
      "epoch: 66,  batch step: 174, loss: 24.47626495361328\n",
      "epoch: 66,  batch step: 175, loss: 4.034578323364258\n",
      "epoch: 66,  batch step: 176, loss: 3.8375296592712402\n",
      "epoch: 66,  batch step: 177, loss: 30.00499725341797\n",
      "epoch: 66,  batch step: 178, loss: 82.84459686279297\n",
      "epoch: 66,  batch step: 179, loss: 3.2504663467407227\n",
      "epoch: 66,  batch step: 180, loss: 3.0126700401306152\n",
      "epoch: 66,  batch step: 181, loss: 4.485400199890137\n",
      "epoch: 66,  batch step: 182, loss: 70.72933197021484\n",
      "epoch: 66,  batch step: 183, loss: 3.4398343563079834\n",
      "epoch: 66,  batch step: 184, loss: 4.715848445892334\n",
      "epoch: 66,  batch step: 185, loss: 29.36574363708496\n",
      "epoch: 66,  batch step: 186, loss: 3.8485209941864014\n",
      "epoch: 66,  batch step: 187, loss: 2.3989315032958984\n",
      "epoch: 66,  batch step: 188, loss: 1.7219035625457764\n",
      "epoch: 66,  batch step: 189, loss: 2.8841209411621094\n",
      "epoch: 66,  batch step: 190, loss: 3.4220128059387207\n",
      "epoch: 66,  batch step: 191, loss: 7.317605495452881\n",
      "epoch: 66,  batch step: 192, loss: 43.884239196777344\n",
      "epoch: 66,  batch step: 193, loss: 4.877716064453125\n",
      "epoch: 66,  batch step: 194, loss: 8.387251853942871\n",
      "epoch: 66,  batch step: 195, loss: 2.3402481079101562\n",
      "epoch: 66,  batch step: 196, loss: 41.77748489379883\n",
      "epoch: 66,  batch step: 197, loss: 11.607874870300293\n",
      "epoch: 66,  batch step: 198, loss: 31.602333068847656\n",
      "epoch: 66,  batch step: 199, loss: 3.030672073364258\n",
      "epoch: 66,  batch step: 200, loss: 133.85568237304688\n",
      "epoch: 66,  batch step: 201, loss: 3.5826163291931152\n",
      "epoch: 66,  batch step: 202, loss: 13.665755271911621\n",
      "epoch: 66,  batch step: 203, loss: 27.40449333190918\n",
      "epoch: 66,  batch step: 204, loss: 2.9316470623016357\n",
      "epoch: 66,  batch step: 205, loss: 10.228668212890625\n",
      "epoch: 66,  batch step: 206, loss: 31.956863403320312\n",
      "epoch: 66,  batch step: 207, loss: 1.9163532257080078\n",
      "epoch: 66,  batch step: 208, loss: 16.61325454711914\n",
      "epoch: 66,  batch step: 209, loss: 67.21836853027344\n",
      "epoch: 66,  batch step: 210, loss: 2.357790470123291\n",
      "epoch: 66,  batch step: 211, loss: 7.160415172576904\n",
      "epoch: 66,  batch step: 212, loss: 28.551321029663086\n",
      "epoch: 66,  batch step: 213, loss: 10.643515586853027\n",
      "epoch: 66,  batch step: 214, loss: 2.1594693660736084\n",
      "epoch: 66,  batch step: 215, loss: 8.378496170043945\n",
      "epoch: 66,  batch step: 216, loss: 1.4488856792449951\n",
      "epoch: 66,  batch step: 217, loss: 10.100910186767578\n",
      "epoch: 66,  batch step: 218, loss: 65.92821502685547\n",
      "epoch: 66,  batch step: 219, loss: 22.831518173217773\n",
      "epoch: 66,  batch step: 220, loss: 11.17518138885498\n",
      "epoch: 66,  batch step: 221, loss: 34.09818649291992\n",
      "epoch: 66,  batch step: 222, loss: 63.42040252685547\n",
      "epoch: 66,  batch step: 223, loss: 12.510862350463867\n",
      "epoch: 66,  batch step: 224, loss: 18.364599227905273\n",
      "epoch: 66,  batch step: 225, loss: 3.849923610687256\n",
      "epoch: 66,  batch step: 226, loss: 22.47553062438965\n",
      "epoch: 66,  batch step: 227, loss: 18.5126953125\n",
      "epoch: 66,  batch step: 228, loss: 2.5454444885253906\n",
      "epoch: 66,  batch step: 229, loss: 13.714110374450684\n",
      "epoch: 66,  batch step: 230, loss: 17.27508544921875\n",
      "epoch: 66,  batch step: 231, loss: 2.190899610519409\n",
      "epoch: 66,  batch step: 232, loss: 3.459873914718628\n",
      "epoch: 66,  batch step: 233, loss: 10.967416763305664\n",
      "epoch: 66,  batch step: 234, loss: 30.381778717041016\n",
      "epoch: 66,  batch step: 235, loss: 2.8716018199920654\n",
      "epoch: 66,  batch step: 236, loss: 4.121006488800049\n",
      "epoch: 66,  batch step: 237, loss: 40.60810089111328\n",
      "epoch: 66,  batch step: 238, loss: 12.374626159667969\n",
      "epoch: 66,  batch step: 239, loss: 11.753210067749023\n",
      "epoch: 66,  batch step: 240, loss: 30.782569885253906\n",
      "epoch: 66,  batch step: 241, loss: 51.28968048095703\n",
      "epoch: 66,  batch step: 242, loss: 14.301389694213867\n",
      "epoch: 66,  batch step: 243, loss: 3.374922513961792\n",
      "epoch: 66,  batch step: 244, loss: 3.025338888168335\n",
      "epoch: 66,  batch step: 245, loss: 3.789883613586426\n",
      "epoch: 66,  batch step: 246, loss: 4.001555919647217\n",
      "epoch: 66,  batch step: 247, loss: 22.23107147216797\n",
      "epoch: 66,  batch step: 248, loss: 2.76784086227417\n",
      "epoch: 66,  batch step: 249, loss: 8.16427230834961\n",
      "epoch: 66,  batch step: 250, loss: 5.7179059982299805\n",
      "epoch: 66,  batch step: 251, loss: 14.96668815612793\n",
      "validation error epoch  66:    tensor(66.2941, device='cuda:0')\n",
      "316\n",
      "epoch: 67,  batch step: 0, loss: 6.091723442077637\n",
      "epoch: 67,  batch step: 1, loss: 67.72091674804688\n",
      "epoch: 67,  batch step: 2, loss: 3.923522472381592\n",
      "epoch: 67,  batch step: 3, loss: 3.960329055786133\n",
      "epoch: 67,  batch step: 4, loss: 3.862879514694214\n",
      "epoch: 67,  batch step: 5, loss: 2.738576889038086\n",
      "epoch: 67,  batch step: 6, loss: 2.561415195465088\n",
      "epoch: 67,  batch step: 7, loss: 15.430106163024902\n",
      "epoch: 67,  batch step: 8, loss: 23.540908813476562\n",
      "epoch: 67,  batch step: 9, loss: 26.881317138671875\n",
      "epoch: 67,  batch step: 10, loss: 27.737899780273438\n",
      "epoch: 67,  batch step: 11, loss: 16.5274600982666\n",
      "epoch: 67,  batch step: 12, loss: 22.41701889038086\n",
      "epoch: 67,  batch step: 13, loss: 28.38582992553711\n",
      "epoch: 67,  batch step: 14, loss: 2.4766488075256348\n",
      "epoch: 67,  batch step: 15, loss: 3.0381031036376953\n",
      "epoch: 67,  batch step: 16, loss: 4.671485424041748\n",
      "epoch: 67,  batch step: 17, loss: 2.711054563522339\n",
      "epoch: 67,  batch step: 18, loss: 3.5434730052948\n",
      "epoch: 67,  batch step: 19, loss: 3.494385242462158\n",
      "epoch: 67,  batch step: 20, loss: 18.05870246887207\n",
      "epoch: 67,  batch step: 21, loss: 54.11408233642578\n",
      "epoch: 67,  batch step: 22, loss: 2.2101478576660156\n",
      "epoch: 67,  batch step: 23, loss: 16.41476821899414\n",
      "epoch: 67,  batch step: 24, loss: 3.3742518424987793\n",
      "epoch: 67,  batch step: 25, loss: 22.03473663330078\n",
      "epoch: 67,  batch step: 26, loss: 29.703184127807617\n",
      "epoch: 67,  batch step: 27, loss: 2.007751226425171\n",
      "epoch: 67,  batch step: 28, loss: 17.550994873046875\n",
      "epoch: 67,  batch step: 29, loss: 61.815853118896484\n",
      "epoch: 67,  batch step: 30, loss: 4.02070426940918\n",
      "epoch: 67,  batch step: 31, loss: 19.240102767944336\n",
      "epoch: 67,  batch step: 32, loss: 5.007071018218994\n",
      "epoch: 67,  batch step: 33, loss: 3.4416730403900146\n",
      "epoch: 67,  batch step: 34, loss: 7.117441177368164\n",
      "epoch: 67,  batch step: 35, loss: 44.458370208740234\n",
      "epoch: 67,  batch step: 36, loss: 32.38402557373047\n",
      "epoch: 67,  batch step: 37, loss: 3.847287654876709\n",
      "epoch: 67,  batch step: 38, loss: 2.302250385284424\n",
      "epoch: 67,  batch step: 39, loss: 31.13257598876953\n",
      "epoch: 67,  batch step: 40, loss: 3.340148448944092\n",
      "epoch: 67,  batch step: 41, loss: 20.316789627075195\n",
      "epoch: 67,  batch step: 42, loss: 5.643715858459473\n",
      "epoch: 67,  batch step: 43, loss: 28.278209686279297\n",
      "epoch: 67,  batch step: 44, loss: 27.791332244873047\n",
      "epoch: 67,  batch step: 45, loss: 10.346098899841309\n",
      "epoch: 67,  batch step: 46, loss: 4.086931228637695\n",
      "epoch: 67,  batch step: 47, loss: 4.42348051071167\n",
      "epoch: 67,  batch step: 48, loss: 26.112865447998047\n",
      "epoch: 67,  batch step: 49, loss: 55.95527648925781\n",
      "epoch: 67,  batch step: 50, loss: 5.732307434082031\n",
      "epoch: 67,  batch step: 51, loss: 2.866645336151123\n",
      "epoch: 67,  batch step: 52, loss: 4.428618907928467\n",
      "epoch: 67,  batch step: 53, loss: 12.101736068725586\n",
      "epoch: 67,  batch step: 54, loss: 20.400985717773438\n",
      "epoch: 67,  batch step: 55, loss: 35.532615661621094\n",
      "epoch: 67,  batch step: 56, loss: 11.403093338012695\n",
      "epoch: 67,  batch step: 57, loss: 67.44009399414062\n",
      "epoch: 67,  batch step: 58, loss: 3.330796957015991\n",
      "epoch: 67,  batch step: 59, loss: 17.36733627319336\n",
      "epoch: 67,  batch step: 60, loss: 49.08711242675781\n",
      "epoch: 67,  batch step: 61, loss: 19.442611694335938\n",
      "epoch: 67,  batch step: 62, loss: 4.260830879211426\n",
      "epoch: 67,  batch step: 63, loss: 16.445465087890625\n",
      "epoch: 67,  batch step: 64, loss: 21.65225601196289\n",
      "epoch: 67,  batch step: 65, loss: 73.18632507324219\n",
      "epoch: 67,  batch step: 66, loss: 12.27642822265625\n",
      "epoch: 67,  batch step: 67, loss: 44.352352142333984\n",
      "epoch: 67,  batch step: 68, loss: 3.7100296020507812\n",
      "epoch: 67,  batch step: 69, loss: 3.8435988426208496\n",
      "epoch: 67,  batch step: 70, loss: 26.649869918823242\n",
      "epoch: 67,  batch step: 71, loss: 3.0512828826904297\n",
      "epoch: 67,  batch step: 72, loss: 15.973621368408203\n",
      "epoch: 67,  batch step: 73, loss: 3.393810987472534\n",
      "epoch: 67,  batch step: 74, loss: 4.7065935134887695\n",
      "epoch: 67,  batch step: 75, loss: 3.9254660606384277\n",
      "epoch: 67,  batch step: 76, loss: 2.7293710708618164\n",
      "epoch: 67,  batch step: 77, loss: 48.737823486328125\n",
      "epoch: 67,  batch step: 78, loss: 3.9849443435668945\n",
      "epoch: 67,  batch step: 79, loss: 13.302790641784668\n",
      "epoch: 67,  batch step: 80, loss: 4.813889026641846\n",
      "epoch: 67,  batch step: 81, loss: 17.186782836914062\n",
      "epoch: 67,  batch step: 82, loss: 5.496335506439209\n",
      "epoch: 67,  batch step: 83, loss: 5.008542060852051\n",
      "epoch: 67,  batch step: 84, loss: 6.987346649169922\n",
      "epoch: 67,  batch step: 85, loss: 5.6455230712890625\n",
      "epoch: 67,  batch step: 86, loss: 10.405684471130371\n",
      "epoch: 67,  batch step: 87, loss: 50.872127532958984\n",
      "epoch: 67,  batch step: 88, loss: 4.961971282958984\n",
      "epoch: 67,  batch step: 89, loss: 13.142646789550781\n",
      "epoch: 67,  batch step: 90, loss: 3.534943103790283\n",
      "epoch: 67,  batch step: 91, loss: 9.815488815307617\n",
      "epoch: 67,  batch step: 92, loss: 2.551431894302368\n",
      "epoch: 67,  batch step: 93, loss: 4.639876365661621\n",
      "epoch: 67,  batch step: 94, loss: 3.3758254051208496\n",
      "epoch: 67,  batch step: 95, loss: 18.029876708984375\n",
      "epoch: 67,  batch step: 96, loss: 3.2006773948669434\n",
      "epoch: 67,  batch step: 97, loss: 11.894447326660156\n",
      "epoch: 67,  batch step: 98, loss: 6.254636287689209\n",
      "epoch: 67,  batch step: 99, loss: 8.625831604003906\n",
      "epoch: 67,  batch step: 100, loss: 3.834176778793335\n",
      "epoch: 67,  batch step: 101, loss: 17.516250610351562\n",
      "epoch: 67,  batch step: 102, loss: 12.292486190795898\n",
      "epoch: 67,  batch step: 103, loss: 2.768421173095703\n",
      "epoch: 67,  batch step: 104, loss: 2.6907272338867188\n",
      "epoch: 67,  batch step: 105, loss: 2.607705593109131\n",
      "epoch: 67,  batch step: 106, loss: 16.080196380615234\n",
      "epoch: 67,  batch step: 107, loss: 3.6988589763641357\n",
      "epoch: 67,  batch step: 108, loss: 13.800186157226562\n",
      "epoch: 67,  batch step: 109, loss: 70.20863342285156\n",
      "epoch: 67,  batch step: 110, loss: 17.7896728515625\n",
      "epoch: 67,  batch step: 111, loss: 3.407519817352295\n",
      "epoch: 67,  batch step: 112, loss: 3.7726151943206787\n",
      "epoch: 67,  batch step: 113, loss: 24.172138214111328\n",
      "epoch: 67,  batch step: 114, loss: 2.9524593353271484\n",
      "epoch: 67,  batch step: 115, loss: 17.718896865844727\n",
      "epoch: 67,  batch step: 116, loss: 19.29102897644043\n",
      "epoch: 67,  batch step: 117, loss: 24.506668090820312\n",
      "epoch: 67,  batch step: 118, loss: 2.3666229248046875\n",
      "epoch: 67,  batch step: 119, loss: 1.7320334911346436\n",
      "epoch: 67,  batch step: 120, loss: 18.94012451171875\n",
      "epoch: 67,  batch step: 121, loss: 12.759432792663574\n",
      "epoch: 67,  batch step: 122, loss: 3.5962820053100586\n",
      "epoch: 67,  batch step: 123, loss: 27.1325740814209\n",
      "epoch: 67,  batch step: 124, loss: 2.262274742126465\n",
      "epoch: 67,  batch step: 125, loss: 2.5614070892333984\n",
      "epoch: 67,  batch step: 126, loss: 7.186206817626953\n",
      "epoch: 67,  batch step: 127, loss: 12.60601806640625\n",
      "epoch: 67,  batch step: 128, loss: 2.345438003540039\n",
      "epoch: 67,  batch step: 129, loss: 3.9109840393066406\n",
      "epoch: 67,  batch step: 130, loss: 9.825440406799316\n",
      "epoch: 67,  batch step: 131, loss: 5.710219383239746\n",
      "epoch: 67,  batch step: 132, loss: 2.2402358055114746\n",
      "epoch: 67,  batch step: 133, loss: 2.494156837463379\n",
      "epoch: 67,  batch step: 134, loss: 16.36940574645996\n",
      "epoch: 67,  batch step: 135, loss: 40.07703399658203\n",
      "epoch: 67,  batch step: 136, loss: 2.610765218734741\n",
      "epoch: 67,  batch step: 137, loss: 7.255700588226318\n",
      "epoch: 67,  batch step: 138, loss: 27.659709930419922\n",
      "epoch: 67,  batch step: 139, loss: 6.814552307128906\n",
      "epoch: 67,  batch step: 140, loss: 25.523910522460938\n",
      "epoch: 67,  batch step: 141, loss: 15.23383617401123\n",
      "epoch: 67,  batch step: 142, loss: 10.635763168334961\n",
      "epoch: 67,  batch step: 143, loss: 21.91817283630371\n",
      "epoch: 67,  batch step: 144, loss: 9.651708602905273\n",
      "epoch: 67,  batch step: 145, loss: 5.1791582107543945\n",
      "epoch: 67,  batch step: 146, loss: 11.278398513793945\n",
      "epoch: 67,  batch step: 147, loss: 9.409819602966309\n",
      "epoch: 67,  batch step: 148, loss: 2.0578746795654297\n",
      "epoch: 67,  batch step: 149, loss: 3.6704025268554688\n",
      "epoch: 67,  batch step: 150, loss: 23.61493682861328\n",
      "epoch: 67,  batch step: 151, loss: 4.491139888763428\n",
      "epoch: 67,  batch step: 152, loss: 13.521941184997559\n",
      "epoch: 67,  batch step: 153, loss: 24.997426986694336\n",
      "epoch: 67,  batch step: 154, loss: 10.477805137634277\n",
      "epoch: 67,  batch step: 155, loss: 2.7094295024871826\n",
      "epoch: 67,  batch step: 156, loss: 34.375\n",
      "epoch: 67,  batch step: 157, loss: 2.883188247680664\n",
      "epoch: 67,  batch step: 158, loss: 3.6204919815063477\n",
      "epoch: 67,  batch step: 159, loss: 2.3716373443603516\n",
      "epoch: 67,  batch step: 160, loss: 88.46879577636719\n",
      "epoch: 67,  batch step: 161, loss: 2.453064441680908\n",
      "epoch: 67,  batch step: 162, loss: 7.494139671325684\n",
      "epoch: 67,  batch step: 163, loss: 13.157503128051758\n",
      "epoch: 67,  batch step: 164, loss: 5.591569423675537\n",
      "epoch: 67,  batch step: 165, loss: 97.56338500976562\n",
      "epoch: 67,  batch step: 166, loss: 33.285926818847656\n",
      "epoch: 67,  batch step: 167, loss: 4.87905216217041\n",
      "epoch: 67,  batch step: 168, loss: 2.8364598751068115\n",
      "epoch: 67,  batch step: 169, loss: 17.39753532409668\n",
      "epoch: 67,  batch step: 170, loss: 19.038211822509766\n",
      "epoch: 67,  batch step: 171, loss: 37.59890365600586\n",
      "epoch: 67,  batch step: 172, loss: 10.539341926574707\n",
      "epoch: 67,  batch step: 173, loss: 146.9593048095703\n",
      "epoch: 67,  batch step: 174, loss: 4.454733848571777\n",
      "epoch: 67,  batch step: 175, loss: 18.11761474609375\n",
      "epoch: 67,  batch step: 176, loss: 4.462964057922363\n",
      "epoch: 67,  batch step: 177, loss: 3.3488426208496094\n",
      "epoch: 67,  batch step: 178, loss: 7.343188285827637\n",
      "epoch: 67,  batch step: 179, loss: 2.899959087371826\n",
      "epoch: 67,  batch step: 180, loss: 3.2751998901367188\n",
      "epoch: 67,  batch step: 181, loss: 4.399287223815918\n",
      "epoch: 67,  batch step: 182, loss: 43.64812469482422\n",
      "epoch: 67,  batch step: 183, loss: 16.401763916015625\n",
      "epoch: 67,  batch step: 184, loss: 12.112842559814453\n",
      "epoch: 67,  batch step: 185, loss: 76.86589813232422\n",
      "epoch: 67,  batch step: 186, loss: 133.83193969726562\n",
      "epoch: 67,  batch step: 187, loss: 4.050073146820068\n",
      "epoch: 67,  batch step: 188, loss: 30.63758087158203\n",
      "epoch: 67,  batch step: 189, loss: 4.80152702331543\n",
      "epoch: 67,  batch step: 190, loss: 17.834468841552734\n",
      "epoch: 67,  batch step: 191, loss: 4.744815349578857\n",
      "epoch: 67,  batch step: 192, loss: 66.513671875\n",
      "epoch: 67,  batch step: 193, loss: 29.388507843017578\n",
      "epoch: 67,  batch step: 194, loss: 29.288177490234375\n",
      "epoch: 67,  batch step: 195, loss: 4.096602439880371\n",
      "epoch: 67,  batch step: 196, loss: 5.396171569824219\n",
      "epoch: 67,  batch step: 197, loss: 31.731319427490234\n",
      "epoch: 67,  batch step: 198, loss: 31.427303314208984\n",
      "epoch: 67,  batch step: 199, loss: 2.6059679985046387\n",
      "epoch: 67,  batch step: 200, loss: 41.700927734375\n",
      "epoch: 67,  batch step: 201, loss: 6.542518138885498\n",
      "epoch: 67,  batch step: 202, loss: 2.652104377746582\n",
      "epoch: 67,  batch step: 203, loss: 13.096121788024902\n",
      "epoch: 67,  batch step: 204, loss: 29.55645751953125\n",
      "epoch: 67,  batch step: 205, loss: 40.49224090576172\n",
      "epoch: 67,  batch step: 206, loss: 3.0088279247283936\n",
      "epoch: 67,  batch step: 207, loss: 13.050182342529297\n",
      "epoch: 67,  batch step: 208, loss: 3.377286672592163\n",
      "epoch: 67,  batch step: 209, loss: 14.539234161376953\n",
      "epoch: 67,  batch step: 210, loss: 11.315544128417969\n",
      "epoch: 67,  batch step: 211, loss: 3.352543592453003\n",
      "epoch: 67,  batch step: 212, loss: 3.8830087184906006\n",
      "epoch: 67,  batch step: 213, loss: 19.878276824951172\n",
      "epoch: 67,  batch step: 214, loss: 6.113351821899414\n",
      "epoch: 67,  batch step: 215, loss: 4.371716499328613\n",
      "epoch: 67,  batch step: 216, loss: 14.827194213867188\n",
      "epoch: 67,  batch step: 217, loss: 3.9132192134857178\n",
      "epoch: 67,  batch step: 218, loss: 41.76042938232422\n",
      "epoch: 67,  batch step: 219, loss: 4.568470478057861\n",
      "epoch: 67,  batch step: 220, loss: 2.5512986183166504\n",
      "epoch: 67,  batch step: 221, loss: 4.444938659667969\n",
      "epoch: 67,  batch step: 222, loss: 5.532892227172852\n",
      "epoch: 67,  batch step: 223, loss: 3.045743465423584\n",
      "epoch: 67,  batch step: 224, loss: 21.986167907714844\n",
      "epoch: 67,  batch step: 225, loss: 10.245978355407715\n",
      "epoch: 67,  batch step: 226, loss: 9.64737606048584\n",
      "epoch: 67,  batch step: 227, loss: 7.178440093994141\n",
      "epoch: 67,  batch step: 228, loss: 5.843013763427734\n",
      "epoch: 67,  batch step: 229, loss: 32.021888732910156\n",
      "epoch: 67,  batch step: 230, loss: 3.320378065109253\n",
      "epoch: 67,  batch step: 231, loss: 14.88209342956543\n",
      "epoch: 67,  batch step: 232, loss: 4.184839725494385\n",
      "epoch: 67,  batch step: 233, loss: 3.4285507202148438\n",
      "epoch: 67,  batch step: 234, loss: 17.74928092956543\n",
      "epoch: 67,  batch step: 235, loss: 20.119632720947266\n",
      "epoch: 67,  batch step: 236, loss: 24.722034454345703\n",
      "epoch: 67,  batch step: 237, loss: 5.570127010345459\n",
      "epoch: 67,  batch step: 238, loss: 5.463084697723389\n",
      "epoch: 67,  batch step: 239, loss: 51.30912780761719\n",
      "epoch: 67,  batch step: 240, loss: 42.51636505126953\n",
      "epoch: 67,  batch step: 241, loss: 2.8738887310028076\n",
      "epoch: 67,  batch step: 242, loss: 2.417375087738037\n",
      "epoch: 67,  batch step: 243, loss: 20.35081672668457\n",
      "epoch: 67,  batch step: 244, loss: 8.879417419433594\n",
      "epoch: 67,  batch step: 245, loss: 3.939305067062378\n",
      "epoch: 67,  batch step: 246, loss: 3.4456708431243896\n",
      "epoch: 67,  batch step: 247, loss: 7.859981536865234\n",
      "epoch: 67,  batch step: 248, loss: 17.604511260986328\n",
      "epoch: 67,  batch step: 249, loss: 16.50726890563965\n",
      "epoch: 67,  batch step: 250, loss: 44.341941833496094\n",
      "epoch: 67,  batch step: 251, loss: 9.37656021118164\n",
      "validation error epoch  67:    tensor(71.5216, device='cuda:0')\n",
      "316\n",
      "epoch: 68,  batch step: 0, loss: 22.718585968017578\n",
      "epoch: 68,  batch step: 1, loss: 8.750986099243164\n",
      "epoch: 68,  batch step: 2, loss: 6.245471477508545\n",
      "epoch: 68,  batch step: 3, loss: 25.9920711517334\n",
      "epoch: 68,  batch step: 4, loss: 9.446152687072754\n",
      "epoch: 68,  batch step: 5, loss: 20.269760131835938\n",
      "epoch: 68,  batch step: 6, loss: 33.88398361206055\n",
      "epoch: 68,  batch step: 7, loss: 26.550064086914062\n",
      "epoch: 68,  batch step: 8, loss: 3.2923431396484375\n",
      "epoch: 68,  batch step: 9, loss: 2.286996841430664\n",
      "epoch: 68,  batch step: 10, loss: 16.698701858520508\n",
      "epoch: 68,  batch step: 11, loss: 32.01028060913086\n",
      "epoch: 68,  batch step: 12, loss: 12.669111251831055\n",
      "epoch: 68,  batch step: 13, loss: 9.305397987365723\n",
      "epoch: 68,  batch step: 14, loss: 3.6650896072387695\n",
      "epoch: 68,  batch step: 15, loss: 2.9594924449920654\n",
      "epoch: 68,  batch step: 16, loss: 15.219401359558105\n",
      "epoch: 68,  batch step: 17, loss: 4.179449081420898\n",
      "epoch: 68,  batch step: 18, loss: 3.5863780975341797\n",
      "epoch: 68,  batch step: 19, loss: 18.150611877441406\n",
      "epoch: 68,  batch step: 20, loss: 9.684248924255371\n",
      "epoch: 68,  batch step: 21, loss: 2.5823726654052734\n",
      "epoch: 68,  batch step: 22, loss: 17.01573371887207\n",
      "epoch: 68,  batch step: 23, loss: 145.85919189453125\n",
      "epoch: 68,  batch step: 24, loss: 15.232913970947266\n",
      "epoch: 68,  batch step: 25, loss: 4.152938365936279\n",
      "epoch: 68,  batch step: 26, loss: 16.095359802246094\n",
      "epoch: 68,  batch step: 27, loss: 16.509437561035156\n",
      "epoch: 68,  batch step: 28, loss: 3.7806499004364014\n",
      "epoch: 68,  batch step: 29, loss: 9.955902099609375\n",
      "epoch: 68,  batch step: 30, loss: 47.433616638183594\n",
      "epoch: 68,  batch step: 31, loss: 2.532813787460327\n",
      "epoch: 68,  batch step: 32, loss: 9.094608306884766\n",
      "epoch: 68,  batch step: 33, loss: 9.24487018585205\n",
      "epoch: 68,  batch step: 34, loss: 5.422101974487305\n",
      "epoch: 68,  batch step: 35, loss: 1.9577083587646484\n",
      "epoch: 68,  batch step: 36, loss: 37.55938720703125\n",
      "epoch: 68,  batch step: 37, loss: 24.199260711669922\n",
      "epoch: 68,  batch step: 38, loss: 3.5446128845214844\n",
      "epoch: 68,  batch step: 39, loss: 15.366893768310547\n",
      "epoch: 68,  batch step: 40, loss: 18.086292266845703\n",
      "epoch: 68,  batch step: 41, loss: 14.774829864501953\n",
      "epoch: 68,  batch step: 42, loss: 21.239534378051758\n",
      "epoch: 68,  batch step: 43, loss: 4.503037929534912\n",
      "epoch: 68,  batch step: 44, loss: 4.683088302612305\n",
      "epoch: 68,  batch step: 45, loss: 11.078768730163574\n",
      "epoch: 68,  batch step: 46, loss: 15.525238037109375\n",
      "epoch: 68,  batch step: 47, loss: 3.3079700469970703\n",
      "epoch: 68,  batch step: 48, loss: 21.26182746887207\n",
      "epoch: 68,  batch step: 49, loss: 26.300579071044922\n",
      "epoch: 68,  batch step: 50, loss: 2.113863706588745\n",
      "epoch: 68,  batch step: 51, loss: 2.6054635047912598\n",
      "epoch: 68,  batch step: 52, loss: 2.550157070159912\n",
      "epoch: 68,  batch step: 53, loss: 18.951492309570312\n",
      "epoch: 68,  batch step: 54, loss: 46.04042053222656\n",
      "epoch: 68,  batch step: 55, loss: 1.8055033683776855\n",
      "epoch: 68,  batch step: 56, loss: 49.497886657714844\n",
      "epoch: 68,  batch step: 57, loss: 67.22012329101562\n",
      "epoch: 68,  batch step: 58, loss: 7.094566345214844\n",
      "epoch: 68,  batch step: 59, loss: 3.344581365585327\n",
      "epoch: 68,  batch step: 60, loss: 9.851311683654785\n",
      "epoch: 68,  batch step: 61, loss: 2.5248303413391113\n",
      "epoch: 68,  batch step: 62, loss: 38.89715576171875\n",
      "epoch: 68,  batch step: 63, loss: 11.925092697143555\n",
      "epoch: 68,  batch step: 64, loss: 6.1843767166137695\n",
      "epoch: 68,  batch step: 65, loss: 2.5213541984558105\n",
      "epoch: 68,  batch step: 66, loss: 39.08211135864258\n",
      "epoch: 68,  batch step: 67, loss: 3.865950345993042\n",
      "epoch: 68,  batch step: 68, loss: 22.483959197998047\n",
      "epoch: 68,  batch step: 69, loss: 11.48618221282959\n",
      "epoch: 68,  batch step: 70, loss: 5.568797588348389\n",
      "epoch: 68,  batch step: 71, loss: 41.566925048828125\n",
      "epoch: 68,  batch step: 72, loss: 51.85708236694336\n",
      "epoch: 68,  batch step: 73, loss: 11.303572654724121\n",
      "epoch: 68,  batch step: 74, loss: 3.064868450164795\n",
      "epoch: 68,  batch step: 75, loss: 7.098775386810303\n",
      "epoch: 68,  batch step: 76, loss: 24.566137313842773\n",
      "epoch: 68,  batch step: 77, loss: 4.638731956481934\n",
      "epoch: 68,  batch step: 78, loss: 5.891208171844482\n",
      "epoch: 68,  batch step: 79, loss: 33.90667724609375\n",
      "epoch: 68,  batch step: 80, loss: 43.32990646362305\n",
      "epoch: 68,  batch step: 81, loss: 56.321388244628906\n",
      "epoch: 68,  batch step: 82, loss: 17.71324920654297\n",
      "epoch: 68,  batch step: 83, loss: 37.04811096191406\n",
      "epoch: 68,  batch step: 84, loss: 4.183642387390137\n",
      "epoch: 68,  batch step: 85, loss: 21.883655548095703\n",
      "epoch: 68,  batch step: 86, loss: 2.8533225059509277\n",
      "epoch: 68,  batch step: 87, loss: 10.359127044677734\n",
      "epoch: 68,  batch step: 88, loss: 3.2071914672851562\n",
      "epoch: 68,  batch step: 89, loss: 34.04215621948242\n",
      "epoch: 68,  batch step: 90, loss: 7.756501197814941\n",
      "epoch: 68,  batch step: 91, loss: 106.28160095214844\n",
      "epoch: 68,  batch step: 92, loss: 4.712151527404785\n",
      "epoch: 68,  batch step: 93, loss: 20.433876037597656\n",
      "epoch: 68,  batch step: 94, loss: 2.905130386352539\n",
      "epoch: 68,  batch step: 95, loss: 5.030057907104492\n",
      "epoch: 68,  batch step: 96, loss: 3.074326992034912\n",
      "epoch: 68,  batch step: 97, loss: 2.505074977874756\n",
      "epoch: 68,  batch step: 98, loss: 15.462656021118164\n",
      "epoch: 68,  batch step: 99, loss: 4.279099464416504\n",
      "epoch: 68,  batch step: 100, loss: 2.7827048301696777\n",
      "epoch: 68,  batch step: 101, loss: 3.209280014038086\n",
      "epoch: 68,  batch step: 102, loss: 3.9415812492370605\n",
      "epoch: 68,  batch step: 103, loss: 7.383277416229248\n",
      "epoch: 68,  batch step: 104, loss: 2.6912689208984375\n",
      "epoch: 68,  batch step: 105, loss: 7.318426132202148\n",
      "epoch: 68,  batch step: 106, loss: 5.4264068603515625\n",
      "epoch: 68,  batch step: 107, loss: 29.416133880615234\n",
      "epoch: 68,  batch step: 108, loss: 2.642942190170288\n",
      "epoch: 68,  batch step: 109, loss: 5.1840057373046875\n",
      "epoch: 68,  batch step: 110, loss: 31.982154846191406\n",
      "epoch: 68,  batch step: 111, loss: 2.9262874126434326\n",
      "epoch: 68,  batch step: 112, loss: 23.939777374267578\n",
      "epoch: 68,  batch step: 113, loss: 5.405914306640625\n",
      "epoch: 68,  batch step: 114, loss: 6.400120735168457\n",
      "epoch: 68,  batch step: 115, loss: 2.405121326446533\n",
      "epoch: 68,  batch step: 116, loss: 5.066112518310547\n",
      "epoch: 68,  batch step: 117, loss: 3.6841673851013184\n",
      "epoch: 68,  batch step: 118, loss: 12.30093002319336\n",
      "epoch: 68,  batch step: 119, loss: 19.766571044921875\n",
      "epoch: 68,  batch step: 120, loss: 31.195335388183594\n",
      "epoch: 68,  batch step: 121, loss: 5.449563980102539\n",
      "epoch: 68,  batch step: 122, loss: 3.0392656326293945\n",
      "epoch: 68,  batch step: 123, loss: 16.70413589477539\n",
      "epoch: 68,  batch step: 124, loss: 18.783432006835938\n",
      "epoch: 68,  batch step: 125, loss: 34.67847442626953\n",
      "epoch: 68,  batch step: 126, loss: 16.76439094543457\n",
      "epoch: 68,  batch step: 127, loss: 2.0010170936584473\n",
      "epoch: 68,  batch step: 128, loss: 51.42597198486328\n",
      "epoch: 68,  batch step: 129, loss: 8.231639862060547\n",
      "epoch: 68,  batch step: 130, loss: 2.1463708877563477\n",
      "epoch: 68,  batch step: 131, loss: 3.5288076400756836\n",
      "epoch: 68,  batch step: 132, loss: 3.429882526397705\n",
      "epoch: 68,  batch step: 133, loss: 10.074212074279785\n",
      "epoch: 68,  batch step: 134, loss: 8.995884895324707\n",
      "epoch: 68,  batch step: 135, loss: 2.498154401779175\n",
      "epoch: 68,  batch step: 136, loss: 6.073148250579834\n",
      "epoch: 68,  batch step: 137, loss: 4.751627445220947\n",
      "epoch: 68,  batch step: 138, loss: 39.14984893798828\n",
      "epoch: 68,  batch step: 139, loss: 3.866379499435425\n",
      "epoch: 68,  batch step: 140, loss: 58.125755310058594\n",
      "epoch: 68,  batch step: 141, loss: 12.485706329345703\n",
      "epoch: 68,  batch step: 142, loss: 32.609073638916016\n",
      "epoch: 68,  batch step: 143, loss: 3.1016604900360107\n",
      "epoch: 68,  batch step: 144, loss: 3.1260409355163574\n",
      "epoch: 68,  batch step: 145, loss: 17.55837631225586\n",
      "epoch: 68,  batch step: 146, loss: 7.526357650756836\n",
      "epoch: 68,  batch step: 147, loss: 14.792043685913086\n",
      "epoch: 68,  batch step: 148, loss: 3.059091091156006\n",
      "epoch: 68,  batch step: 149, loss: 3.712353467941284\n",
      "epoch: 68,  batch step: 150, loss: 3.685882568359375\n",
      "epoch: 68,  batch step: 151, loss: 16.857654571533203\n",
      "epoch: 68,  batch step: 152, loss: 55.87498092651367\n",
      "epoch: 68,  batch step: 153, loss: 57.274227142333984\n",
      "epoch: 68,  batch step: 154, loss: 13.808364868164062\n",
      "epoch: 68,  batch step: 155, loss: 2.7529096603393555\n",
      "epoch: 68,  batch step: 156, loss: 3.5574748516082764\n",
      "epoch: 68,  batch step: 157, loss: 32.78910827636719\n",
      "epoch: 68,  batch step: 158, loss: 10.733126640319824\n",
      "epoch: 68,  batch step: 159, loss: 24.378599166870117\n",
      "epoch: 68,  batch step: 160, loss: 17.562063217163086\n",
      "epoch: 68,  batch step: 161, loss: 30.02846336364746\n",
      "epoch: 68,  batch step: 162, loss: 4.6332526206970215\n",
      "epoch: 68,  batch step: 163, loss: 2.28147554397583\n",
      "epoch: 68,  batch step: 164, loss: 15.015863418579102\n",
      "epoch: 68,  batch step: 165, loss: 14.098482131958008\n",
      "epoch: 68,  batch step: 166, loss: 15.543895721435547\n",
      "epoch: 68,  batch step: 167, loss: 4.95692777633667\n",
      "epoch: 68,  batch step: 168, loss: 24.945585250854492\n",
      "epoch: 68,  batch step: 169, loss: 3.19769549369812\n",
      "epoch: 68,  batch step: 170, loss: 21.83453941345215\n",
      "epoch: 68,  batch step: 171, loss: 5.772611141204834\n",
      "epoch: 68,  batch step: 172, loss: 25.919570922851562\n",
      "epoch: 68,  batch step: 173, loss: 18.219228744506836\n",
      "epoch: 68,  batch step: 174, loss: 3.3557801246643066\n",
      "epoch: 68,  batch step: 175, loss: 3.608708143234253\n",
      "epoch: 68,  batch step: 176, loss: 25.0264892578125\n",
      "epoch: 68,  batch step: 177, loss: 49.40226745605469\n",
      "epoch: 68,  batch step: 178, loss: 9.490606307983398\n",
      "epoch: 68,  batch step: 179, loss: 11.081777572631836\n",
      "epoch: 68,  batch step: 180, loss: 2.0655159950256348\n",
      "epoch: 68,  batch step: 181, loss: 29.895782470703125\n",
      "epoch: 68,  batch step: 182, loss: 28.4619197845459\n",
      "epoch: 68,  batch step: 183, loss: 9.437118530273438\n",
      "epoch: 68,  batch step: 184, loss: 3.8023014068603516\n",
      "epoch: 68,  batch step: 185, loss: 66.11810302734375\n",
      "epoch: 68,  batch step: 186, loss: 41.767425537109375\n",
      "epoch: 68,  batch step: 187, loss: 12.264142990112305\n",
      "epoch: 68,  batch step: 188, loss: 6.4968767166137695\n",
      "epoch: 68,  batch step: 189, loss: 16.74694061279297\n",
      "epoch: 68,  batch step: 190, loss: 2.6387457847595215\n",
      "epoch: 68,  batch step: 191, loss: 3.8256144523620605\n",
      "epoch: 68,  batch step: 192, loss: 4.345782279968262\n",
      "epoch: 68,  batch step: 193, loss: 3.3314170837402344\n",
      "epoch: 68,  batch step: 194, loss: 60.46892547607422\n",
      "epoch: 68,  batch step: 195, loss: 4.044939041137695\n",
      "epoch: 68,  batch step: 196, loss: 16.26982307434082\n",
      "epoch: 68,  batch step: 197, loss: 19.68245506286621\n",
      "epoch: 68,  batch step: 198, loss: 3.2736403942108154\n",
      "epoch: 68,  batch step: 199, loss: 13.29782485961914\n",
      "epoch: 68,  batch step: 200, loss: 8.230318069458008\n",
      "epoch: 68,  batch step: 201, loss: 5.192431449890137\n",
      "epoch: 68,  batch step: 202, loss: 2.883190155029297\n",
      "epoch: 68,  batch step: 203, loss: 2.720506191253662\n",
      "epoch: 68,  batch step: 204, loss: 3.8012590408325195\n",
      "epoch: 68,  batch step: 205, loss: 5.346762657165527\n",
      "epoch: 68,  batch step: 206, loss: 6.218597412109375\n",
      "epoch: 68,  batch step: 207, loss: 2.570314884185791\n",
      "epoch: 68,  batch step: 208, loss: 66.72405242919922\n",
      "epoch: 68,  batch step: 209, loss: 2.9303665161132812\n",
      "epoch: 68,  batch step: 210, loss: 3.8167006969451904\n",
      "epoch: 68,  batch step: 211, loss: 3.8554577827453613\n",
      "epoch: 68,  batch step: 212, loss: 2.537743091583252\n",
      "epoch: 68,  batch step: 213, loss: 16.806766510009766\n",
      "epoch: 68,  batch step: 214, loss: 31.220542907714844\n",
      "epoch: 68,  batch step: 215, loss: 11.312139511108398\n",
      "epoch: 68,  batch step: 216, loss: 2.4836597442626953\n",
      "epoch: 68,  batch step: 217, loss: 2.1358418464660645\n",
      "epoch: 68,  batch step: 218, loss: 2.44016432762146\n",
      "epoch: 68,  batch step: 219, loss: 2.9718432426452637\n",
      "epoch: 68,  batch step: 220, loss: 4.95064640045166\n",
      "epoch: 68,  batch step: 221, loss: 3.7952399253845215\n",
      "epoch: 68,  batch step: 222, loss: 3.0496621131896973\n",
      "epoch: 68,  batch step: 223, loss: 12.807169914245605\n",
      "epoch: 68,  batch step: 224, loss: 16.15178680419922\n",
      "epoch: 68,  batch step: 225, loss: 10.441076278686523\n",
      "epoch: 68,  batch step: 226, loss: 2.2360172271728516\n",
      "epoch: 68,  batch step: 227, loss: 2.898711681365967\n",
      "epoch: 68,  batch step: 228, loss: 2.182694435119629\n",
      "epoch: 68,  batch step: 229, loss: 3.237513303756714\n",
      "epoch: 68,  batch step: 230, loss: 17.334304809570312\n",
      "epoch: 68,  batch step: 231, loss: 2.619290828704834\n",
      "epoch: 68,  batch step: 232, loss: 40.64570617675781\n",
      "epoch: 68,  batch step: 233, loss: 11.09434700012207\n",
      "epoch: 68,  batch step: 234, loss: 4.829152584075928\n",
      "epoch: 68,  batch step: 235, loss: 2.899033308029175\n",
      "epoch: 68,  batch step: 236, loss: 54.57005310058594\n",
      "epoch: 68,  batch step: 237, loss: 2.0990819931030273\n",
      "epoch: 68,  batch step: 238, loss: 12.305270195007324\n",
      "epoch: 68,  batch step: 239, loss: 16.96665382385254\n",
      "epoch: 68,  batch step: 240, loss: 25.893146514892578\n",
      "epoch: 68,  batch step: 241, loss: 16.184873580932617\n",
      "epoch: 68,  batch step: 242, loss: 27.73011589050293\n",
      "epoch: 68,  batch step: 243, loss: 4.000349521636963\n",
      "epoch: 68,  batch step: 244, loss: 53.78957748413086\n",
      "epoch: 68,  batch step: 245, loss: 16.808456420898438\n",
      "epoch: 68,  batch step: 246, loss: 21.172927856445312\n",
      "epoch: 68,  batch step: 247, loss: 10.732787132263184\n",
      "epoch: 68,  batch step: 248, loss: 13.336503982543945\n",
      "epoch: 68,  batch step: 249, loss: 2.8973844051361084\n",
      "epoch: 68,  batch step: 250, loss: 2.954514503479004\n",
      "epoch: 68,  batch step: 251, loss: 316.75531005859375\n",
      "validation error epoch  68:    tensor(70.1236, device='cuda:0')\n",
      "316\n",
      "epoch: 69,  batch step: 0, loss: 6.3440656661987305\n",
      "epoch: 69,  batch step: 1, loss: 89.21623229980469\n",
      "epoch: 69,  batch step: 2, loss: 25.557636260986328\n",
      "epoch: 69,  batch step: 3, loss: 13.074422836303711\n",
      "epoch: 69,  batch step: 4, loss: 28.32887840270996\n",
      "epoch: 69,  batch step: 5, loss: 33.35970687866211\n",
      "epoch: 69,  batch step: 6, loss: 7.585322380065918\n",
      "epoch: 69,  batch step: 7, loss: 27.958946228027344\n",
      "epoch: 69,  batch step: 8, loss: 7.1231689453125\n",
      "epoch: 69,  batch step: 9, loss: 9.69881820678711\n",
      "epoch: 69,  batch step: 10, loss: 9.54084587097168\n",
      "epoch: 69,  batch step: 11, loss: 6.664470672607422\n",
      "epoch: 69,  batch step: 12, loss: 18.526702880859375\n",
      "epoch: 69,  batch step: 13, loss: 5.172760009765625\n",
      "epoch: 69,  batch step: 14, loss: 10.436654090881348\n",
      "epoch: 69,  batch step: 15, loss: 7.566875457763672\n",
      "epoch: 69,  batch step: 16, loss: 22.48705291748047\n",
      "epoch: 69,  batch step: 17, loss: 19.989017486572266\n",
      "epoch: 69,  batch step: 18, loss: 31.39349937438965\n",
      "epoch: 69,  batch step: 19, loss: 31.94390106201172\n",
      "epoch: 69,  batch step: 20, loss: 19.21296501159668\n",
      "epoch: 69,  batch step: 21, loss: 16.51496124267578\n",
      "epoch: 69,  batch step: 22, loss: 8.29266357421875\n",
      "epoch: 69,  batch step: 23, loss: 9.388105392456055\n",
      "epoch: 69,  batch step: 24, loss: 11.398906707763672\n",
      "epoch: 69,  batch step: 25, loss: 46.5343017578125\n",
      "epoch: 69,  batch step: 26, loss: 5.027369022369385\n",
      "epoch: 69,  batch step: 27, loss: 33.91432189941406\n",
      "epoch: 69,  batch step: 28, loss: 24.274961471557617\n",
      "epoch: 69,  batch step: 29, loss: 20.638839721679688\n",
      "epoch: 69,  batch step: 30, loss: 3.6938321590423584\n",
      "epoch: 69,  batch step: 31, loss: 10.115127563476562\n",
      "epoch: 69,  batch step: 32, loss: 3.673194408416748\n",
      "epoch: 69,  batch step: 33, loss: 23.440725326538086\n",
      "epoch: 69,  batch step: 34, loss: 5.02934455871582\n",
      "epoch: 69,  batch step: 35, loss: 3.5530192852020264\n",
      "epoch: 69,  batch step: 36, loss: 19.346242904663086\n",
      "epoch: 69,  batch step: 37, loss: 103.62551879882812\n",
      "epoch: 69,  batch step: 38, loss: 4.3254828453063965\n",
      "epoch: 69,  batch step: 39, loss: 1.9346988201141357\n",
      "epoch: 69,  batch step: 40, loss: 19.225208282470703\n",
      "epoch: 69,  batch step: 41, loss: 24.485219955444336\n",
      "epoch: 69,  batch step: 42, loss: 5.441371917724609\n",
      "epoch: 69,  batch step: 43, loss: 25.22182846069336\n",
      "epoch: 69,  batch step: 44, loss: 3.808497190475464\n",
      "epoch: 69,  batch step: 45, loss: 5.905412197113037\n",
      "epoch: 69,  batch step: 46, loss: 3.4462738037109375\n",
      "epoch: 69,  batch step: 47, loss: 3.125840902328491\n",
      "epoch: 69,  batch step: 48, loss: 3.239081859588623\n",
      "epoch: 69,  batch step: 49, loss: 3.5473780632019043\n",
      "epoch: 69,  batch step: 50, loss: 4.861325263977051\n",
      "epoch: 69,  batch step: 51, loss: 15.53814697265625\n",
      "epoch: 69,  batch step: 52, loss: 2.9027442932128906\n",
      "epoch: 69,  batch step: 53, loss: 4.008342266082764\n",
      "epoch: 69,  batch step: 54, loss: 21.064199447631836\n",
      "epoch: 69,  batch step: 55, loss: 15.036312103271484\n",
      "epoch: 69,  batch step: 56, loss: 11.205495834350586\n",
      "epoch: 69,  batch step: 57, loss: 20.289718627929688\n",
      "epoch: 69,  batch step: 58, loss: 4.200888156890869\n",
      "epoch: 69,  batch step: 59, loss: 2.919137954711914\n",
      "epoch: 69,  batch step: 60, loss: 16.476755142211914\n",
      "epoch: 69,  batch step: 61, loss: 22.304122924804688\n",
      "epoch: 69,  batch step: 62, loss: 14.36817741394043\n",
      "epoch: 69,  batch step: 63, loss: 7.088927268981934\n",
      "epoch: 69,  batch step: 64, loss: 15.17803955078125\n",
      "epoch: 69,  batch step: 65, loss: 7.371150493621826\n",
      "epoch: 69,  batch step: 66, loss: 4.931734085083008\n",
      "epoch: 69,  batch step: 67, loss: 20.241025924682617\n",
      "epoch: 69,  batch step: 68, loss: 13.597068786621094\n",
      "epoch: 69,  batch step: 69, loss: 65.54314422607422\n",
      "epoch: 69,  batch step: 70, loss: 9.454341888427734\n",
      "epoch: 69,  batch step: 71, loss: 38.179473876953125\n",
      "epoch: 69,  batch step: 72, loss: 17.87480354309082\n",
      "epoch: 69,  batch step: 73, loss: 2.509526014328003\n",
      "epoch: 69,  batch step: 74, loss: 30.258331298828125\n",
      "epoch: 69,  batch step: 75, loss: 3.4438953399658203\n",
      "epoch: 69,  batch step: 76, loss: 5.918172836303711\n",
      "epoch: 69,  batch step: 77, loss: 5.493922710418701\n",
      "epoch: 69,  batch step: 78, loss: 3.7492835521698\n",
      "epoch: 69,  batch step: 79, loss: 2.7292914390563965\n",
      "epoch: 69,  batch step: 80, loss: 14.698673248291016\n",
      "epoch: 69,  batch step: 81, loss: 19.05931854248047\n",
      "epoch: 69,  batch step: 82, loss: 16.037620544433594\n",
      "epoch: 69,  batch step: 83, loss: 18.442638397216797\n",
      "epoch: 69,  batch step: 84, loss: 2.363481283187866\n",
      "epoch: 69,  batch step: 85, loss: 25.25897216796875\n",
      "epoch: 69,  batch step: 86, loss: 15.703661918640137\n",
      "epoch: 69,  batch step: 87, loss: 4.394801139831543\n",
      "epoch: 69,  batch step: 88, loss: 7.035586833953857\n",
      "epoch: 69,  batch step: 89, loss: 3.4465956687927246\n",
      "epoch: 69,  batch step: 90, loss: 5.288766860961914\n",
      "epoch: 69,  batch step: 91, loss: 3.0832736492156982\n",
      "epoch: 69,  batch step: 92, loss: 29.711261749267578\n",
      "epoch: 69,  batch step: 93, loss: 3.75164794921875\n",
      "epoch: 69,  batch step: 94, loss: 28.039588928222656\n",
      "epoch: 69,  batch step: 95, loss: 24.024250030517578\n",
      "epoch: 69,  batch step: 96, loss: 13.405755996704102\n",
      "epoch: 69,  batch step: 97, loss: 3.2913730144500732\n",
      "epoch: 69,  batch step: 98, loss: 59.200252532958984\n",
      "epoch: 69,  batch step: 99, loss: 3.81018328666687\n",
      "epoch: 69,  batch step: 100, loss: 4.389734268188477\n",
      "epoch: 69,  batch step: 101, loss: 13.085184097290039\n",
      "epoch: 69,  batch step: 102, loss: 19.457473754882812\n",
      "epoch: 69,  batch step: 103, loss: 2.40994930267334\n",
      "epoch: 69,  batch step: 104, loss: 49.50133514404297\n",
      "epoch: 69,  batch step: 105, loss: 7.2945990562438965\n",
      "epoch: 69,  batch step: 106, loss: 40.52803039550781\n",
      "epoch: 69,  batch step: 107, loss: 2.4228897094726562\n",
      "epoch: 69,  batch step: 108, loss: 3.4080638885498047\n",
      "epoch: 69,  batch step: 109, loss: 2.6097187995910645\n",
      "epoch: 69,  batch step: 110, loss: 11.5665922164917\n",
      "epoch: 69,  batch step: 111, loss: 11.78208065032959\n",
      "epoch: 69,  batch step: 112, loss: 33.443965911865234\n",
      "epoch: 69,  batch step: 113, loss: 23.481300354003906\n",
      "epoch: 69,  batch step: 114, loss: 6.1294965744018555\n",
      "epoch: 69,  batch step: 115, loss: 44.14860153198242\n",
      "epoch: 69,  batch step: 116, loss: 3.1709494590759277\n",
      "epoch: 69,  batch step: 117, loss: 14.515281677246094\n",
      "epoch: 69,  batch step: 118, loss: 134.8204345703125\n",
      "epoch: 69,  batch step: 119, loss: 31.339885711669922\n",
      "epoch: 69,  batch step: 120, loss: 2.8007724285125732\n",
      "epoch: 69,  batch step: 121, loss: 19.490901947021484\n",
      "epoch: 69,  batch step: 122, loss: 1.5927138328552246\n",
      "epoch: 69,  batch step: 123, loss: 2.4091265201568604\n",
      "epoch: 69,  batch step: 124, loss: 2.313739538192749\n",
      "epoch: 69,  batch step: 125, loss: 3.206860065460205\n",
      "epoch: 69,  batch step: 126, loss: 32.449913024902344\n",
      "epoch: 69,  batch step: 127, loss: 5.351912498474121\n",
      "epoch: 69,  batch step: 128, loss: 2.0825443267822266\n",
      "epoch: 69,  batch step: 129, loss: 10.222223281860352\n",
      "epoch: 69,  batch step: 130, loss: 13.3729248046875\n",
      "epoch: 69,  batch step: 131, loss: 13.37531566619873\n",
      "epoch: 69,  batch step: 132, loss: 17.104629516601562\n",
      "epoch: 69,  batch step: 133, loss: 2.53348445892334\n",
      "epoch: 69,  batch step: 134, loss: 93.6504135131836\n",
      "epoch: 69,  batch step: 135, loss: 13.757638931274414\n",
      "epoch: 69,  batch step: 136, loss: 9.589862823486328\n",
      "epoch: 69,  batch step: 137, loss: 2.3999242782592773\n",
      "epoch: 69,  batch step: 138, loss: 16.517658233642578\n",
      "epoch: 69,  batch step: 139, loss: 52.383033752441406\n",
      "epoch: 69,  batch step: 140, loss: 4.294134140014648\n",
      "epoch: 69,  batch step: 141, loss: 2.523499011993408\n",
      "epoch: 69,  batch step: 142, loss: 9.900293350219727\n",
      "epoch: 69,  batch step: 143, loss: 4.140127182006836\n",
      "epoch: 69,  batch step: 144, loss: 32.88139343261719\n",
      "epoch: 69,  batch step: 145, loss: 3.593247890472412\n",
      "epoch: 69,  batch step: 146, loss: 44.075233459472656\n",
      "epoch: 69,  batch step: 147, loss: 6.587493419647217\n",
      "epoch: 69,  batch step: 148, loss: 4.469775676727295\n",
      "epoch: 69,  batch step: 149, loss: 16.521892547607422\n",
      "epoch: 69,  batch step: 150, loss: 13.108882904052734\n",
      "epoch: 69,  batch step: 151, loss: 2.714480400085449\n",
      "epoch: 69,  batch step: 152, loss: 29.87066650390625\n",
      "epoch: 69,  batch step: 153, loss: 43.64958190917969\n",
      "epoch: 69,  batch step: 154, loss: 2.4254424571990967\n",
      "epoch: 69,  batch step: 155, loss: 9.992820739746094\n",
      "epoch: 69,  batch step: 156, loss: 92.16967010498047\n",
      "epoch: 69,  batch step: 157, loss: 8.158430099487305\n",
      "epoch: 69,  batch step: 158, loss: 8.1536865234375\n",
      "epoch: 69,  batch step: 159, loss: 27.9210262298584\n",
      "epoch: 69,  batch step: 160, loss: 3.3113017082214355\n",
      "epoch: 69,  batch step: 161, loss: 3.5295729637145996\n",
      "epoch: 69,  batch step: 162, loss: 2.4334933757781982\n",
      "epoch: 69,  batch step: 163, loss: 8.418691635131836\n",
      "epoch: 69,  batch step: 164, loss: 39.22630310058594\n",
      "epoch: 69,  batch step: 165, loss: 4.967820167541504\n",
      "epoch: 69,  batch step: 166, loss: 5.967662811279297\n",
      "epoch: 69,  batch step: 167, loss: 6.575831413269043\n",
      "epoch: 69,  batch step: 168, loss: 28.07978057861328\n",
      "epoch: 69,  batch step: 169, loss: 2.986537456512451\n",
      "epoch: 69,  batch step: 170, loss: 19.685176849365234\n",
      "epoch: 69,  batch step: 171, loss: 32.30705261230469\n",
      "epoch: 69,  batch step: 172, loss: 12.480958938598633\n",
      "epoch: 69,  batch step: 173, loss: 25.479270935058594\n",
      "epoch: 69,  batch step: 174, loss: 23.579343795776367\n",
      "epoch: 69,  batch step: 175, loss: 2.2571680545806885\n",
      "epoch: 69,  batch step: 176, loss: 1.935941219329834\n",
      "epoch: 69,  batch step: 177, loss: 2.683441638946533\n",
      "epoch: 69,  batch step: 178, loss: 4.2444281578063965\n",
      "epoch: 69,  batch step: 179, loss: 46.98113250732422\n",
      "epoch: 69,  batch step: 180, loss: 32.87073516845703\n",
      "epoch: 69,  batch step: 181, loss: 1.833383560180664\n",
      "epoch: 69,  batch step: 182, loss: 27.053436279296875\n",
      "epoch: 69,  batch step: 183, loss: 2.703695297241211\n",
      "epoch: 69,  batch step: 184, loss: 2.1038174629211426\n",
      "epoch: 69,  batch step: 185, loss: 25.528675079345703\n",
      "epoch: 69,  batch step: 186, loss: 60.489341735839844\n",
      "epoch: 69,  batch step: 187, loss: 3.971226215362549\n",
      "epoch: 69,  batch step: 188, loss: 15.713846206665039\n",
      "epoch: 69,  batch step: 189, loss: 65.67159271240234\n",
      "epoch: 69,  batch step: 190, loss: 26.177261352539062\n",
      "epoch: 69,  batch step: 191, loss: 2.757214307785034\n",
      "epoch: 69,  batch step: 192, loss: 11.70445442199707\n",
      "epoch: 69,  batch step: 193, loss: 11.97647476196289\n",
      "epoch: 69,  batch step: 194, loss: 3.2809247970581055\n",
      "epoch: 69,  batch step: 195, loss: 4.332157611846924\n",
      "epoch: 69,  batch step: 196, loss: 2.474619150161743\n",
      "epoch: 69,  batch step: 197, loss: 2.9959840774536133\n",
      "epoch: 69,  batch step: 198, loss: 2.8759055137634277\n",
      "epoch: 69,  batch step: 199, loss: 3.594001293182373\n",
      "epoch: 69,  batch step: 200, loss: 21.334556579589844\n",
      "epoch: 69,  batch step: 201, loss: 3.536825656890869\n",
      "epoch: 69,  batch step: 202, loss: 2.0895981788635254\n",
      "epoch: 69,  batch step: 203, loss: 4.027797222137451\n",
      "epoch: 69,  batch step: 204, loss: 42.47439956665039\n",
      "epoch: 69,  batch step: 205, loss: 28.374130249023438\n",
      "epoch: 69,  batch step: 206, loss: 3.620800495147705\n",
      "epoch: 69,  batch step: 207, loss: 14.967734336853027\n",
      "epoch: 69,  batch step: 208, loss: 28.91246795654297\n",
      "epoch: 69,  batch step: 209, loss: 24.296581268310547\n",
      "epoch: 69,  batch step: 210, loss: 10.30350399017334\n",
      "epoch: 69,  batch step: 211, loss: 31.386884689331055\n",
      "epoch: 69,  batch step: 212, loss: 4.906692028045654\n",
      "epoch: 69,  batch step: 213, loss: 28.17147445678711\n",
      "epoch: 69,  batch step: 214, loss: 21.57771110534668\n",
      "epoch: 69,  batch step: 215, loss: 39.435585021972656\n",
      "epoch: 69,  batch step: 216, loss: 19.680130004882812\n",
      "epoch: 69,  batch step: 217, loss: 16.308307647705078\n",
      "epoch: 69,  batch step: 218, loss: 8.763055801391602\n",
      "epoch: 69,  batch step: 219, loss: 20.81613540649414\n",
      "epoch: 69,  batch step: 220, loss: 3.304692029953003\n",
      "epoch: 69,  batch step: 221, loss: 13.509147644042969\n",
      "epoch: 69,  batch step: 222, loss: 16.471351623535156\n",
      "epoch: 69,  batch step: 223, loss: 3.5645320415496826\n",
      "epoch: 69,  batch step: 224, loss: 2.4410147666931152\n",
      "epoch: 69,  batch step: 225, loss: 12.197904586791992\n",
      "epoch: 69,  batch step: 226, loss: 19.890914916992188\n",
      "epoch: 69,  batch step: 227, loss: 4.519978046417236\n",
      "epoch: 69,  batch step: 228, loss: 42.81779479980469\n",
      "epoch: 69,  batch step: 229, loss: 19.935073852539062\n",
      "epoch: 69,  batch step: 230, loss: 45.26479721069336\n",
      "epoch: 69,  batch step: 231, loss: 10.979986190795898\n",
      "epoch: 69,  batch step: 232, loss: 10.059375762939453\n",
      "epoch: 69,  batch step: 233, loss: 9.019073486328125\n",
      "epoch: 69,  batch step: 234, loss: 9.687747955322266\n",
      "epoch: 69,  batch step: 235, loss: 17.150283813476562\n",
      "epoch: 69,  batch step: 236, loss: 18.823925018310547\n",
      "epoch: 69,  batch step: 237, loss: 2.977473258972168\n",
      "epoch: 69,  batch step: 238, loss: 3.780392646789551\n",
      "epoch: 69,  batch step: 239, loss: 3.074305295944214\n",
      "epoch: 69,  batch step: 240, loss: 59.412322998046875\n",
      "epoch: 69,  batch step: 241, loss: 31.583251953125\n",
      "epoch: 69,  batch step: 242, loss: 17.708059310913086\n",
      "epoch: 69,  batch step: 243, loss: 19.19938087463379\n",
      "epoch: 69,  batch step: 244, loss: 1.9722206592559814\n",
      "epoch: 69,  batch step: 245, loss: 5.592218399047852\n",
      "epoch: 69,  batch step: 246, loss: 2.936934232711792\n",
      "epoch: 69,  batch step: 247, loss: 5.185269355773926\n",
      "epoch: 69,  batch step: 248, loss: 12.559749603271484\n",
      "epoch: 69,  batch step: 249, loss: 2.4976134300231934\n",
      "epoch: 69,  batch step: 250, loss: 7.1706862449646\n",
      "epoch: 69,  batch step: 251, loss: 77.8552474975586\n",
      "finished saving checkpoints\n",
      "validation error epoch  69:    tensor(69.8178, device='cuda:0')\n",
      "316\n",
      "epoch: 70,  batch step: 0, loss: 2.0387113094329834\n",
      "epoch: 70,  batch step: 1, loss: 7.157613754272461\n",
      "epoch: 70,  batch step: 2, loss: 13.874320030212402\n",
      "epoch: 70,  batch step: 3, loss: 18.307039260864258\n",
      "epoch: 70,  batch step: 4, loss: 11.062070846557617\n",
      "epoch: 70,  batch step: 5, loss: 2.9442172050476074\n",
      "epoch: 70,  batch step: 6, loss: 19.64398193359375\n",
      "epoch: 70,  batch step: 7, loss: 2.4479665756225586\n",
      "epoch: 70,  batch step: 8, loss: 4.468205451965332\n",
      "epoch: 70,  batch step: 9, loss: 10.039327621459961\n",
      "epoch: 70,  batch step: 10, loss: 71.09268188476562\n",
      "epoch: 70,  batch step: 11, loss: 44.23603820800781\n",
      "epoch: 70,  batch step: 12, loss: 17.152816772460938\n",
      "epoch: 70,  batch step: 13, loss: 14.658353805541992\n",
      "epoch: 70,  batch step: 14, loss: 2.313826322555542\n",
      "epoch: 70,  batch step: 15, loss: 37.993858337402344\n",
      "epoch: 70,  batch step: 16, loss: 4.527481555938721\n",
      "epoch: 70,  batch step: 17, loss: 8.419615745544434\n",
      "epoch: 70,  batch step: 18, loss: 2.39563250541687\n",
      "epoch: 70,  batch step: 19, loss: 47.641109466552734\n",
      "epoch: 70,  batch step: 20, loss: 4.630859375\n",
      "epoch: 70,  batch step: 21, loss: 3.2286946773529053\n",
      "epoch: 70,  batch step: 22, loss: 4.197307109832764\n",
      "epoch: 70,  batch step: 23, loss: 2.848789930343628\n",
      "epoch: 70,  batch step: 24, loss: 32.920772552490234\n",
      "epoch: 70,  batch step: 25, loss: 51.562713623046875\n",
      "epoch: 70,  batch step: 26, loss: 29.335208892822266\n",
      "epoch: 70,  batch step: 27, loss: 3.1158626079559326\n",
      "epoch: 70,  batch step: 28, loss: 20.444787979125977\n",
      "epoch: 70,  batch step: 29, loss: 45.37029266357422\n",
      "epoch: 70,  batch step: 30, loss: 11.489851951599121\n",
      "epoch: 70,  batch step: 31, loss: 50.12151336669922\n",
      "epoch: 70,  batch step: 32, loss: 34.94955825805664\n",
      "epoch: 70,  batch step: 33, loss: 4.0535688400268555\n",
      "epoch: 70,  batch step: 34, loss: 11.510269165039062\n",
      "epoch: 70,  batch step: 35, loss: 44.2734375\n",
      "epoch: 70,  batch step: 36, loss: 2.7883689403533936\n",
      "epoch: 70,  batch step: 37, loss: 8.364304542541504\n",
      "epoch: 70,  batch step: 38, loss: 6.950527191162109\n",
      "epoch: 70,  batch step: 39, loss: 97.54150390625\n",
      "epoch: 70,  batch step: 40, loss: 22.6804256439209\n",
      "epoch: 70,  batch step: 41, loss: 3.4351720809936523\n",
      "epoch: 70,  batch step: 42, loss: 29.104427337646484\n",
      "epoch: 70,  batch step: 43, loss: 11.986799240112305\n",
      "epoch: 70,  batch step: 44, loss: 4.272388935089111\n",
      "epoch: 70,  batch step: 45, loss: 7.821681022644043\n",
      "epoch: 70,  batch step: 46, loss: 11.864256858825684\n",
      "epoch: 70,  batch step: 47, loss: 10.3084716796875\n",
      "epoch: 70,  batch step: 48, loss: 74.78823852539062\n",
      "epoch: 70,  batch step: 49, loss: 3.9790377616882324\n",
      "epoch: 70,  batch step: 50, loss: 56.94867706298828\n",
      "epoch: 70,  batch step: 51, loss: 25.006643295288086\n",
      "epoch: 70,  batch step: 52, loss: 9.320399284362793\n",
      "epoch: 70,  batch step: 53, loss: 7.3465094566345215\n",
      "epoch: 70,  batch step: 54, loss: 5.402896881103516\n",
      "epoch: 70,  batch step: 55, loss: 67.03626251220703\n",
      "epoch: 70,  batch step: 56, loss: 85.89778900146484\n",
      "epoch: 70,  batch step: 57, loss: 23.2932186126709\n",
      "epoch: 70,  batch step: 58, loss: 8.514519691467285\n",
      "epoch: 70,  batch step: 59, loss: 57.245086669921875\n",
      "epoch: 70,  batch step: 60, loss: 41.26182556152344\n",
      "epoch: 70,  batch step: 61, loss: 64.35619354248047\n",
      "epoch: 70,  batch step: 62, loss: 89.25139617919922\n",
      "epoch: 70,  batch step: 63, loss: 5.999874114990234\n",
      "epoch: 70,  batch step: 64, loss: 12.675681114196777\n",
      "epoch: 70,  batch step: 65, loss: 85.52581787109375\n",
      "epoch: 70,  batch step: 66, loss: 83.85469055175781\n",
      "epoch: 70,  batch step: 67, loss: 53.30515670776367\n",
      "epoch: 70,  batch step: 68, loss: 13.24232006072998\n",
      "epoch: 70,  batch step: 69, loss: 21.786685943603516\n",
      "epoch: 70,  batch step: 70, loss: 108.32308959960938\n",
      "epoch: 70,  batch step: 71, loss: 6.535269737243652\n",
      "epoch: 70,  batch step: 72, loss: 7.845203399658203\n",
      "epoch: 70,  batch step: 73, loss: 7.696681976318359\n",
      "epoch: 70,  batch step: 74, loss: 27.015111923217773\n",
      "epoch: 70,  batch step: 75, loss: 22.91937255859375\n",
      "epoch: 70,  batch step: 76, loss: 12.187675476074219\n",
      "epoch: 70,  batch step: 77, loss: 113.98625946044922\n",
      "epoch: 70,  batch step: 78, loss: 141.24386596679688\n",
      "epoch: 70,  batch step: 79, loss: 9.719059944152832\n",
      "epoch: 70,  batch step: 80, loss: 185.40145874023438\n",
      "epoch: 70,  batch step: 81, loss: 31.249448776245117\n",
      "epoch: 70,  batch step: 82, loss: 31.615951538085938\n",
      "epoch: 70,  batch step: 83, loss: 79.38160705566406\n",
      "epoch: 70,  batch step: 84, loss: 26.0477352142334\n",
      "epoch: 70,  batch step: 85, loss: 11.965571403503418\n",
      "epoch: 70,  batch step: 86, loss: 8.049659729003906\n",
      "epoch: 70,  batch step: 87, loss: 81.96501159667969\n",
      "epoch: 70,  batch step: 88, loss: 5.6864118576049805\n",
      "epoch: 70,  batch step: 89, loss: 5.325860500335693\n",
      "epoch: 70,  batch step: 90, loss: 9.087902069091797\n",
      "epoch: 70,  batch step: 91, loss: 13.714350700378418\n",
      "epoch: 70,  batch step: 92, loss: 7.03887939453125\n",
      "epoch: 70,  batch step: 93, loss: 32.35820770263672\n",
      "epoch: 70,  batch step: 94, loss: 135.37237548828125\n",
      "epoch: 70,  batch step: 95, loss: 7.001554012298584\n",
      "epoch: 70,  batch step: 96, loss: 31.951763153076172\n",
      "epoch: 70,  batch step: 97, loss: 19.43060874938965\n",
      "epoch: 70,  batch step: 98, loss: 167.73361206054688\n",
      "epoch: 70,  batch step: 99, loss: 8.298028945922852\n",
      "epoch: 70,  batch step: 100, loss: 85.84344482421875\n",
      "epoch: 70,  batch step: 101, loss: 27.812862396240234\n",
      "epoch: 70,  batch step: 102, loss: 4.257787704467773\n",
      "epoch: 70,  batch step: 103, loss: 6.261076927185059\n",
      "epoch: 70,  batch step: 104, loss: 7.255652904510498\n",
      "epoch: 70,  batch step: 105, loss: 6.474729061126709\n",
      "epoch: 70,  batch step: 106, loss: 25.424657821655273\n",
      "epoch: 70,  batch step: 107, loss: 28.199176788330078\n",
      "epoch: 70,  batch step: 108, loss: 84.72342681884766\n",
      "epoch: 70,  batch step: 109, loss: 55.42061996459961\n",
      "epoch: 70,  batch step: 110, loss: 23.78750991821289\n",
      "epoch: 70,  batch step: 111, loss: 16.751686096191406\n",
      "epoch: 70,  batch step: 112, loss: 86.88884735107422\n",
      "epoch: 70,  batch step: 113, loss: 64.91851806640625\n",
      "epoch: 70,  batch step: 114, loss: 4.3754801750183105\n",
      "epoch: 70,  batch step: 115, loss: 5.627461910247803\n",
      "epoch: 70,  batch step: 116, loss: 8.582809448242188\n",
      "epoch: 70,  batch step: 117, loss: 46.01676559448242\n",
      "epoch: 70,  batch step: 118, loss: 10.59067440032959\n",
      "epoch: 70,  batch step: 119, loss: 5.601928234100342\n",
      "epoch: 70,  batch step: 120, loss: 26.6375789642334\n",
      "epoch: 70,  batch step: 121, loss: 5.833648204803467\n",
      "epoch: 70,  batch step: 122, loss: 26.223134994506836\n",
      "epoch: 70,  batch step: 123, loss: 7.070462226867676\n",
      "epoch: 70,  batch step: 124, loss: 3.759160041809082\n",
      "epoch: 70,  batch step: 125, loss: 4.3041486740112305\n",
      "epoch: 70,  batch step: 126, loss: 7.839707851409912\n",
      "epoch: 70,  batch step: 127, loss: 5.367339134216309\n",
      "epoch: 70,  batch step: 128, loss: 5.514400482177734\n",
      "epoch: 70,  batch step: 129, loss: 17.19135093688965\n",
      "epoch: 70,  batch step: 130, loss: 45.3277587890625\n",
      "epoch: 70,  batch step: 131, loss: 138.74554443359375\n",
      "epoch: 70,  batch step: 132, loss: 3.1932764053344727\n",
      "epoch: 70,  batch step: 133, loss: 5.195148468017578\n",
      "epoch: 70,  batch step: 134, loss: 3.7072484493255615\n",
      "epoch: 70,  batch step: 135, loss: 118.14530944824219\n",
      "epoch: 70,  batch step: 136, loss: 38.81711959838867\n",
      "epoch: 70,  batch step: 137, loss: 201.1790008544922\n",
      "epoch: 70,  batch step: 138, loss: 89.9520263671875\n",
      "epoch: 70,  batch step: 139, loss: 26.375600814819336\n",
      "epoch: 70,  batch step: 140, loss: 48.06856918334961\n",
      "epoch: 70,  batch step: 141, loss: 13.341822624206543\n",
      "epoch: 70,  batch step: 142, loss: 52.74120330810547\n",
      "epoch: 70,  batch step: 143, loss: 35.61264419555664\n",
      "epoch: 70,  batch step: 144, loss: 26.20864486694336\n",
      "epoch: 70,  batch step: 145, loss: 58.65283203125\n",
      "epoch: 70,  batch step: 146, loss: 44.397029876708984\n",
      "epoch: 70,  batch step: 147, loss: 5.74326229095459\n",
      "epoch: 70,  batch step: 148, loss: 49.11863708496094\n",
      "epoch: 70,  batch step: 149, loss: 5.8814544677734375\n",
      "epoch: 70,  batch step: 150, loss: 88.14646911621094\n",
      "epoch: 70,  batch step: 151, loss: 8.353588104248047\n",
      "epoch: 70,  batch step: 152, loss: 79.58311462402344\n",
      "epoch: 70,  batch step: 153, loss: 101.85954284667969\n",
      "epoch: 70,  batch step: 154, loss: 3.896679401397705\n",
      "epoch: 70,  batch step: 155, loss: 6.15097713470459\n",
      "epoch: 70,  batch step: 156, loss: 7.576436519622803\n",
      "epoch: 70,  batch step: 157, loss: 8.858933448791504\n",
      "epoch: 70,  batch step: 158, loss: 43.067012786865234\n",
      "epoch: 70,  batch step: 159, loss: 5.938870906829834\n",
      "epoch: 70,  batch step: 160, loss: 9.895296096801758\n",
      "epoch: 70,  batch step: 161, loss: 56.79209518432617\n",
      "epoch: 70,  batch step: 162, loss: 7.033618927001953\n",
      "epoch: 70,  batch step: 163, loss: 7.4098968505859375\n",
      "epoch: 70,  batch step: 164, loss: 21.147571563720703\n",
      "epoch: 70,  batch step: 165, loss: 8.16102409362793\n",
      "epoch: 70,  batch step: 166, loss: 29.526901245117188\n",
      "epoch: 70,  batch step: 167, loss: 6.109748840332031\n",
      "epoch: 70,  batch step: 168, loss: 198.06768798828125\n",
      "epoch: 70,  batch step: 169, loss: 69.34361267089844\n",
      "epoch: 70,  batch step: 170, loss: 4.968209266662598\n",
      "epoch: 70,  batch step: 171, loss: 5.645009517669678\n",
      "epoch: 70,  batch step: 172, loss: 28.009613037109375\n",
      "epoch: 70,  batch step: 173, loss: 30.8073787689209\n",
      "epoch: 70,  batch step: 174, loss: 4.21524715423584\n",
      "epoch: 70,  batch step: 175, loss: 29.510732650756836\n",
      "epoch: 70,  batch step: 176, loss: 98.62804412841797\n",
      "epoch: 70,  batch step: 177, loss: 18.3240966796875\n",
      "epoch: 70,  batch step: 178, loss: 117.26834106445312\n",
      "epoch: 70,  batch step: 179, loss: 15.969112396240234\n",
      "epoch: 70,  batch step: 180, loss: 16.784503936767578\n",
      "epoch: 70,  batch step: 181, loss: 78.66288757324219\n",
      "epoch: 70,  batch step: 182, loss: 10.607902526855469\n",
      "epoch: 70,  batch step: 183, loss: 5.176098823547363\n",
      "epoch: 70,  batch step: 184, loss: 51.19036865234375\n",
      "epoch: 70,  batch step: 185, loss: 54.65332794189453\n",
      "epoch: 70,  batch step: 186, loss: 19.921476364135742\n",
      "epoch: 70,  batch step: 187, loss: 26.874311447143555\n",
      "epoch: 70,  batch step: 188, loss: 24.997594833374023\n",
      "epoch: 70,  batch step: 189, loss: 56.81450653076172\n",
      "epoch: 70,  batch step: 190, loss: 3.6567044258117676\n",
      "epoch: 70,  batch step: 191, loss: 42.911170959472656\n",
      "epoch: 70,  batch step: 192, loss: 37.252742767333984\n",
      "epoch: 70,  batch step: 193, loss: 6.669467926025391\n",
      "epoch: 70,  batch step: 194, loss: 8.249823570251465\n",
      "epoch: 70,  batch step: 195, loss: 4.341772079467773\n",
      "epoch: 70,  batch step: 196, loss: 18.013015747070312\n",
      "epoch: 70,  batch step: 197, loss: 35.15491485595703\n",
      "epoch: 70,  batch step: 198, loss: 4.239117622375488\n",
      "epoch: 70,  batch step: 199, loss: 31.025619506835938\n",
      "epoch: 70,  batch step: 200, loss: 12.23133659362793\n",
      "epoch: 70,  batch step: 201, loss: 2.7198121547698975\n",
      "epoch: 70,  batch step: 202, loss: 6.667158126831055\n",
      "epoch: 70,  batch step: 203, loss: 56.56178283691406\n",
      "epoch: 70,  batch step: 204, loss: 113.55159759521484\n",
      "epoch: 70,  batch step: 205, loss: 39.30596160888672\n",
      "epoch: 70,  batch step: 206, loss: 3.1941306591033936\n",
      "epoch: 70,  batch step: 207, loss: 88.58953857421875\n",
      "epoch: 70,  batch step: 208, loss: 20.964200973510742\n",
      "epoch: 70,  batch step: 209, loss: 18.726531982421875\n",
      "epoch: 70,  batch step: 210, loss: 5.436385154724121\n",
      "epoch: 70,  batch step: 211, loss: 115.81999969482422\n",
      "epoch: 70,  batch step: 212, loss: 53.40974044799805\n",
      "epoch: 70,  batch step: 213, loss: 82.7358627319336\n",
      "epoch: 70,  batch step: 214, loss: 9.965888977050781\n",
      "epoch: 70,  batch step: 215, loss: 18.149871826171875\n",
      "epoch: 70,  batch step: 216, loss: 6.917684078216553\n",
      "epoch: 70,  batch step: 217, loss: 25.648853302001953\n",
      "epoch: 70,  batch step: 218, loss: 15.100034713745117\n",
      "epoch: 70,  batch step: 219, loss: 52.574493408203125\n",
      "epoch: 70,  batch step: 220, loss: 15.0471830368042\n",
      "epoch: 70,  batch step: 221, loss: 3.436861753463745\n",
      "epoch: 70,  batch step: 222, loss: 4.217601299285889\n",
      "epoch: 70,  batch step: 223, loss: 19.73025131225586\n",
      "epoch: 70,  batch step: 224, loss: 75.52274322509766\n",
      "epoch: 70,  batch step: 225, loss: 3.4663779735565186\n",
      "epoch: 70,  batch step: 226, loss: 3.3282198905944824\n",
      "epoch: 70,  batch step: 227, loss: 8.422330856323242\n",
      "epoch: 70,  batch step: 228, loss: 44.9541015625\n",
      "epoch: 70,  batch step: 229, loss: 5.997397422790527\n",
      "epoch: 70,  batch step: 230, loss: 4.444907188415527\n",
      "epoch: 70,  batch step: 231, loss: 6.172362327575684\n",
      "epoch: 70,  batch step: 232, loss: 40.96454620361328\n",
      "epoch: 70,  batch step: 233, loss: 5.640927791595459\n",
      "epoch: 70,  batch step: 234, loss: 46.917579650878906\n",
      "epoch: 70,  batch step: 235, loss: 4.512014389038086\n",
      "epoch: 70,  batch step: 236, loss: 12.317267417907715\n",
      "epoch: 70,  batch step: 237, loss: 31.826644897460938\n",
      "epoch: 70,  batch step: 238, loss: 7.669286251068115\n",
      "epoch: 70,  batch step: 239, loss: 3.423612117767334\n",
      "epoch: 70,  batch step: 240, loss: 37.539764404296875\n",
      "epoch: 70,  batch step: 241, loss: 5.587541580200195\n",
      "epoch: 70,  batch step: 242, loss: 39.349945068359375\n",
      "epoch: 70,  batch step: 243, loss: 10.634915351867676\n",
      "epoch: 70,  batch step: 244, loss: 3.190124034881592\n",
      "epoch: 70,  batch step: 245, loss: 7.183876991271973\n",
      "epoch: 70,  batch step: 246, loss: 25.180850982666016\n",
      "epoch: 70,  batch step: 247, loss: 6.919708251953125\n",
      "epoch: 70,  batch step: 248, loss: 22.897201538085938\n",
      "epoch: 70,  batch step: 249, loss: 43.96350860595703\n",
      "epoch: 70,  batch step: 250, loss: 4.011772155761719\n",
      "epoch: 70,  batch step: 251, loss: 292.44146728515625\n",
      "validation error epoch  70:    tensor(68.2017, device='cuda:0')\n",
      "316\n",
      "epoch: 71,  batch step: 0, loss: 39.04890823364258\n",
      "epoch: 71,  batch step: 1, loss: 9.931231498718262\n",
      "epoch: 71,  batch step: 2, loss: 10.202474594116211\n",
      "epoch: 71,  batch step: 3, loss: 11.96439266204834\n",
      "epoch: 71,  batch step: 4, loss: 27.005340576171875\n",
      "epoch: 71,  batch step: 5, loss: 9.964770317077637\n",
      "epoch: 71,  batch step: 6, loss: 56.75461959838867\n",
      "epoch: 71,  batch step: 7, loss: 9.861960411071777\n",
      "epoch: 71,  batch step: 8, loss: 28.85292625427246\n",
      "epoch: 71,  batch step: 9, loss: 23.26239013671875\n",
      "epoch: 71,  batch step: 10, loss: 10.594087600708008\n",
      "epoch: 71,  batch step: 11, loss: 8.319580078125\n",
      "epoch: 71,  batch step: 12, loss: 8.830500602722168\n",
      "epoch: 71,  batch step: 13, loss: 11.50805950164795\n",
      "epoch: 71,  batch step: 14, loss: 19.591705322265625\n",
      "epoch: 71,  batch step: 15, loss: 6.675524711608887\n",
      "epoch: 71,  batch step: 16, loss: 57.4130859375\n",
      "epoch: 71,  batch step: 17, loss: 15.617546081542969\n",
      "epoch: 71,  batch step: 18, loss: 33.675506591796875\n",
      "epoch: 71,  batch step: 19, loss: 28.679065704345703\n",
      "epoch: 71,  batch step: 20, loss: 14.330611228942871\n",
      "epoch: 71,  batch step: 21, loss: 62.41657638549805\n",
      "epoch: 71,  batch step: 22, loss: 20.486116409301758\n",
      "epoch: 71,  batch step: 23, loss: 64.60271453857422\n",
      "epoch: 71,  batch step: 24, loss: 19.485313415527344\n",
      "epoch: 71,  batch step: 25, loss: 6.1050124168396\n",
      "epoch: 71,  batch step: 26, loss: 7.079471588134766\n",
      "epoch: 71,  batch step: 27, loss: 9.13537883758545\n",
      "epoch: 71,  batch step: 28, loss: 17.166685104370117\n",
      "epoch: 71,  batch step: 29, loss: 10.195919036865234\n",
      "epoch: 71,  batch step: 30, loss: 5.5337629318237305\n",
      "epoch: 71,  batch step: 31, loss: 5.425085067749023\n",
      "epoch: 71,  batch step: 32, loss: 8.086006164550781\n",
      "epoch: 71,  batch step: 33, loss: 47.78867721557617\n",
      "epoch: 71,  batch step: 34, loss: 13.482706069946289\n",
      "epoch: 71,  batch step: 35, loss: 19.570573806762695\n",
      "epoch: 71,  batch step: 36, loss: 12.848821640014648\n",
      "epoch: 71,  batch step: 37, loss: 6.383580684661865\n",
      "epoch: 71,  batch step: 38, loss: 7.4877166748046875\n",
      "epoch: 71,  batch step: 39, loss: 4.9660844802856445\n",
      "epoch: 71,  batch step: 40, loss: 39.62696075439453\n",
      "epoch: 71,  batch step: 41, loss: 6.962467193603516\n",
      "epoch: 71,  batch step: 42, loss: 101.90484619140625\n",
      "epoch: 71,  batch step: 43, loss: 31.253324508666992\n",
      "epoch: 71,  batch step: 44, loss: 24.78981590270996\n",
      "epoch: 71,  batch step: 45, loss: 153.16294860839844\n",
      "epoch: 71,  batch step: 46, loss: 25.65557861328125\n",
      "epoch: 71,  batch step: 47, loss: 7.1742072105407715\n",
      "epoch: 71,  batch step: 48, loss: 92.99162292480469\n",
      "epoch: 71,  batch step: 49, loss: 5.192127704620361\n",
      "epoch: 71,  batch step: 50, loss: 29.846084594726562\n",
      "epoch: 71,  batch step: 51, loss: 5.224270820617676\n",
      "epoch: 71,  batch step: 52, loss: 3.9110255241394043\n",
      "epoch: 71,  batch step: 53, loss: 19.525348663330078\n",
      "epoch: 71,  batch step: 54, loss: 4.229180335998535\n",
      "epoch: 71,  batch step: 55, loss: 4.212780952453613\n",
      "epoch: 71,  batch step: 56, loss: 5.634444236755371\n",
      "epoch: 71,  batch step: 57, loss: 84.16412353515625\n",
      "epoch: 71,  batch step: 58, loss: 54.03167724609375\n",
      "epoch: 71,  batch step: 59, loss: 14.098413467407227\n",
      "epoch: 71,  batch step: 60, loss: 4.189055442810059\n",
      "epoch: 71,  batch step: 61, loss: 4.235254287719727\n",
      "epoch: 71,  batch step: 62, loss: 16.15654754638672\n",
      "epoch: 71,  batch step: 63, loss: 8.835333824157715\n",
      "epoch: 71,  batch step: 64, loss: 20.968727111816406\n",
      "epoch: 71,  batch step: 65, loss: 69.76521301269531\n",
      "epoch: 71,  batch step: 66, loss: 10.660609245300293\n",
      "epoch: 71,  batch step: 67, loss: 3.8255794048309326\n",
      "epoch: 71,  batch step: 68, loss: 30.020275115966797\n",
      "epoch: 71,  batch step: 69, loss: 3.574079990386963\n",
      "epoch: 71,  batch step: 70, loss: 4.740816116333008\n",
      "epoch: 71,  batch step: 71, loss: 6.959210395812988\n",
      "epoch: 71,  batch step: 72, loss: 22.443798065185547\n",
      "epoch: 71,  batch step: 73, loss: 8.657005310058594\n",
      "epoch: 71,  batch step: 74, loss: 35.03271484375\n",
      "epoch: 71,  batch step: 75, loss: 13.557392120361328\n",
      "epoch: 71,  batch step: 76, loss: 19.951374053955078\n",
      "epoch: 71,  batch step: 77, loss: 9.371105194091797\n",
      "epoch: 71,  batch step: 78, loss: 18.636110305786133\n",
      "epoch: 71,  batch step: 79, loss: 34.784576416015625\n",
      "epoch: 71,  batch step: 80, loss: 3.314718246459961\n",
      "epoch: 71,  batch step: 81, loss: 3.526818037033081\n",
      "epoch: 71,  batch step: 82, loss: 3.894852638244629\n",
      "epoch: 71,  batch step: 83, loss: 30.537639617919922\n",
      "epoch: 71,  batch step: 84, loss: 18.769174575805664\n",
      "epoch: 71,  batch step: 85, loss: 46.10608673095703\n",
      "epoch: 71,  batch step: 86, loss: 3.121248245239258\n",
      "epoch: 71,  batch step: 87, loss: 92.6844711303711\n",
      "epoch: 71,  batch step: 88, loss: 33.786094665527344\n",
      "epoch: 71,  batch step: 89, loss: 43.440277099609375\n",
      "epoch: 71,  batch step: 90, loss: 18.633634567260742\n",
      "epoch: 71,  batch step: 91, loss: 36.37631607055664\n",
      "epoch: 71,  batch step: 92, loss: 6.598781585693359\n",
      "epoch: 71,  batch step: 93, loss: 6.58212423324585\n",
      "epoch: 71,  batch step: 94, loss: 4.951998710632324\n",
      "epoch: 71,  batch step: 95, loss: 4.128303527832031\n",
      "epoch: 71,  batch step: 96, loss: 20.435787200927734\n",
      "epoch: 71,  batch step: 97, loss: 15.572428703308105\n",
      "epoch: 71,  batch step: 98, loss: 4.407043933868408\n",
      "epoch: 71,  batch step: 99, loss: 6.245181560516357\n",
      "epoch: 71,  batch step: 100, loss: 6.227867126464844\n",
      "epoch: 71,  batch step: 101, loss: 3.3676862716674805\n",
      "epoch: 71,  batch step: 102, loss: 47.14771270751953\n",
      "epoch: 71,  batch step: 103, loss: 77.96855163574219\n",
      "epoch: 71,  batch step: 104, loss: 4.652990818023682\n",
      "epoch: 71,  batch step: 105, loss: 2.8984427452087402\n",
      "epoch: 71,  batch step: 106, loss: 28.98800277709961\n",
      "epoch: 71,  batch step: 107, loss: 3.390415668487549\n",
      "epoch: 71,  batch step: 108, loss: 9.683319091796875\n",
      "epoch: 71,  batch step: 109, loss: 11.372035026550293\n",
      "epoch: 71,  batch step: 110, loss: 17.3873291015625\n",
      "epoch: 71,  batch step: 111, loss: 2.720686435699463\n",
      "epoch: 71,  batch step: 112, loss: 39.7305908203125\n",
      "epoch: 71,  batch step: 113, loss: 2.6971731185913086\n",
      "epoch: 71,  batch step: 114, loss: 10.846747398376465\n",
      "epoch: 71,  batch step: 115, loss: 2.52514910697937\n",
      "epoch: 71,  batch step: 116, loss: 19.757741928100586\n",
      "epoch: 71,  batch step: 117, loss: 66.48222351074219\n",
      "epoch: 71,  batch step: 118, loss: 2.5805490016937256\n",
      "epoch: 71,  batch step: 119, loss: 7.0835394859313965\n",
      "epoch: 71,  batch step: 120, loss: 12.00727653503418\n",
      "epoch: 71,  batch step: 121, loss: 3.5567779541015625\n",
      "epoch: 71,  batch step: 122, loss: 5.708096981048584\n",
      "epoch: 71,  batch step: 123, loss: 41.955047607421875\n",
      "epoch: 71,  batch step: 124, loss: 6.572754859924316\n",
      "epoch: 71,  batch step: 125, loss: 6.306630611419678\n",
      "epoch: 71,  batch step: 126, loss: 39.76220703125\n",
      "epoch: 71,  batch step: 127, loss: 47.072628021240234\n",
      "epoch: 71,  batch step: 128, loss: 24.420589447021484\n",
      "epoch: 71,  batch step: 129, loss: 19.84483528137207\n",
      "epoch: 71,  batch step: 130, loss: 21.52040672302246\n",
      "epoch: 71,  batch step: 131, loss: 33.95332336425781\n",
      "epoch: 71,  batch step: 132, loss: 3.6020545959472656\n",
      "epoch: 71,  batch step: 133, loss: 86.31878662109375\n",
      "epoch: 71,  batch step: 134, loss: 3.9373435974121094\n",
      "epoch: 71,  batch step: 135, loss: 47.181758880615234\n",
      "epoch: 71,  batch step: 136, loss: 58.809932708740234\n",
      "epoch: 71,  batch step: 137, loss: 5.35465145111084\n",
      "epoch: 71,  batch step: 138, loss: 28.52191162109375\n",
      "epoch: 71,  batch step: 139, loss: 3.694957733154297\n",
      "epoch: 71,  batch step: 140, loss: 54.702110290527344\n",
      "epoch: 71,  batch step: 141, loss: 38.32148742675781\n",
      "epoch: 71,  batch step: 142, loss: 6.759403228759766\n",
      "epoch: 71,  batch step: 143, loss: 23.92487907409668\n",
      "epoch: 71,  batch step: 144, loss: 2.2287299633026123\n",
      "epoch: 71,  batch step: 145, loss: 4.18213415145874\n",
      "epoch: 71,  batch step: 146, loss: 3.8056349754333496\n",
      "epoch: 71,  batch step: 147, loss: 69.54965209960938\n",
      "epoch: 71,  batch step: 148, loss: 62.29466247558594\n",
      "epoch: 71,  batch step: 149, loss: 17.578548431396484\n",
      "epoch: 71,  batch step: 150, loss: 28.66421890258789\n",
      "epoch: 71,  batch step: 151, loss: 68.63941955566406\n",
      "epoch: 71,  batch step: 152, loss: 2.817103385925293\n",
      "epoch: 71,  batch step: 153, loss: 7.8061652183532715\n",
      "epoch: 71,  batch step: 154, loss: 2.8886642456054688\n",
      "epoch: 71,  batch step: 155, loss: 90.57775115966797\n",
      "epoch: 71,  batch step: 156, loss: 7.630614280700684\n",
      "epoch: 71,  batch step: 157, loss: 2.8222146034240723\n",
      "epoch: 71,  batch step: 158, loss: 35.544620513916016\n",
      "epoch: 71,  batch step: 159, loss: 5.685424327850342\n",
      "epoch: 71,  batch step: 160, loss: 5.234238624572754\n",
      "epoch: 71,  batch step: 161, loss: 3.3193538188934326\n",
      "epoch: 71,  batch step: 162, loss: 11.10245132446289\n",
      "epoch: 71,  batch step: 163, loss: 5.9872565269470215\n",
      "epoch: 71,  batch step: 164, loss: 36.221107482910156\n",
      "epoch: 71,  batch step: 165, loss: 3.5182604789733887\n",
      "epoch: 71,  batch step: 166, loss: 3.7803001403808594\n",
      "epoch: 71,  batch step: 167, loss: 10.6958646774292\n",
      "epoch: 71,  batch step: 168, loss: 51.25141143798828\n",
      "epoch: 71,  batch step: 169, loss: 15.97834587097168\n",
      "epoch: 71,  batch step: 170, loss: 6.499256134033203\n",
      "epoch: 71,  batch step: 171, loss: 5.746206283569336\n",
      "epoch: 71,  batch step: 172, loss: 3.425795078277588\n",
      "epoch: 71,  batch step: 173, loss: 11.794272422790527\n",
      "epoch: 71,  batch step: 174, loss: 3.9331765174865723\n",
      "epoch: 71,  batch step: 175, loss: 3.106294631958008\n",
      "epoch: 71,  batch step: 176, loss: 4.397799968719482\n",
      "epoch: 71,  batch step: 177, loss: 2.747075080871582\n",
      "epoch: 71,  batch step: 178, loss: 70.15188598632812\n",
      "epoch: 71,  batch step: 179, loss: 51.86610412597656\n",
      "epoch: 71,  batch step: 180, loss: 4.531811714172363\n",
      "epoch: 71,  batch step: 181, loss: 68.43761444091797\n",
      "epoch: 71,  batch step: 182, loss: 3.4147377014160156\n",
      "epoch: 71,  batch step: 183, loss: 16.876663208007812\n",
      "epoch: 71,  batch step: 184, loss: 82.83141326904297\n",
      "epoch: 71,  batch step: 185, loss: 15.037928581237793\n",
      "epoch: 71,  batch step: 186, loss: 2.2257277965545654\n",
      "epoch: 71,  batch step: 187, loss: 18.142126083374023\n",
      "epoch: 71,  batch step: 188, loss: 2.629854917526245\n",
      "epoch: 71,  batch step: 189, loss: 4.451774597167969\n",
      "epoch: 71,  batch step: 190, loss: 5.039664268493652\n",
      "epoch: 71,  batch step: 191, loss: 4.3228349685668945\n",
      "epoch: 71,  batch step: 192, loss: 14.10145092010498\n",
      "epoch: 71,  batch step: 193, loss: 2.8101460933685303\n",
      "epoch: 71,  batch step: 194, loss: 26.284595489501953\n",
      "epoch: 71,  batch step: 195, loss: 9.082728385925293\n",
      "epoch: 71,  batch step: 196, loss: 4.696928024291992\n",
      "epoch: 71,  batch step: 197, loss: 59.412696838378906\n",
      "epoch: 71,  batch step: 198, loss: 47.36677551269531\n",
      "epoch: 71,  batch step: 199, loss: 18.46209716796875\n",
      "epoch: 71,  batch step: 200, loss: 7.534396171569824\n",
      "epoch: 71,  batch step: 201, loss: 3.355682134628296\n",
      "epoch: 71,  batch step: 202, loss: 31.344982147216797\n",
      "epoch: 71,  batch step: 203, loss: 25.244888305664062\n",
      "epoch: 71,  batch step: 204, loss: 12.025503158569336\n",
      "epoch: 71,  batch step: 205, loss: 2.877882480621338\n",
      "epoch: 71,  batch step: 206, loss: 7.471467018127441\n",
      "epoch: 71,  batch step: 207, loss: 16.44927978515625\n",
      "epoch: 71,  batch step: 208, loss: 28.613903045654297\n",
      "epoch: 71,  batch step: 209, loss: 3.4211277961730957\n",
      "epoch: 71,  batch step: 210, loss: 4.857247352600098\n",
      "epoch: 71,  batch step: 211, loss: 3.2252936363220215\n",
      "epoch: 71,  batch step: 212, loss: 3.3763625621795654\n",
      "epoch: 71,  batch step: 213, loss: 21.716161727905273\n",
      "epoch: 71,  batch step: 214, loss: 17.57141876220703\n",
      "epoch: 71,  batch step: 215, loss: 3.8133368492126465\n",
      "epoch: 71,  batch step: 216, loss: 17.697261810302734\n",
      "epoch: 71,  batch step: 217, loss: 3.3353219032287598\n",
      "epoch: 71,  batch step: 218, loss: 34.93548583984375\n",
      "epoch: 71,  batch step: 219, loss: 73.27848815917969\n",
      "epoch: 71,  batch step: 220, loss: 9.28430461883545\n",
      "epoch: 71,  batch step: 221, loss: 13.407495498657227\n",
      "epoch: 71,  batch step: 222, loss: 2.949434280395508\n",
      "epoch: 71,  batch step: 223, loss: 2.921164035797119\n",
      "epoch: 71,  batch step: 224, loss: 2.2977092266082764\n",
      "epoch: 71,  batch step: 225, loss: 2.144118309020996\n",
      "epoch: 71,  batch step: 226, loss: 71.58641815185547\n",
      "epoch: 71,  batch step: 227, loss: 17.534730911254883\n",
      "epoch: 71,  batch step: 228, loss: 22.445201873779297\n",
      "epoch: 71,  batch step: 229, loss: 3.5874791145324707\n",
      "epoch: 71,  batch step: 230, loss: 2.81721830368042\n",
      "epoch: 71,  batch step: 231, loss: 13.96821117401123\n",
      "epoch: 71,  batch step: 232, loss: 12.189668655395508\n",
      "epoch: 71,  batch step: 233, loss: 3.246978282928467\n",
      "epoch: 71,  batch step: 234, loss: 8.164920806884766\n",
      "epoch: 71,  batch step: 235, loss: 3.4245853424072266\n",
      "epoch: 71,  batch step: 236, loss: 4.315464019775391\n",
      "epoch: 71,  batch step: 237, loss: 24.784971237182617\n",
      "epoch: 71,  batch step: 238, loss: 2.7462520599365234\n",
      "epoch: 71,  batch step: 239, loss: 2.8121700286865234\n",
      "epoch: 71,  batch step: 240, loss: 4.168091773986816\n",
      "epoch: 71,  batch step: 241, loss: 22.3800048828125\n",
      "epoch: 71,  batch step: 242, loss: 2.943584442138672\n",
      "epoch: 71,  batch step: 243, loss: 2.9681601524353027\n",
      "epoch: 71,  batch step: 244, loss: 96.02557373046875\n",
      "epoch: 71,  batch step: 245, loss: 16.510595321655273\n",
      "epoch: 71,  batch step: 246, loss: 37.124305725097656\n",
      "epoch: 71,  batch step: 247, loss: 50.058895111083984\n",
      "epoch: 71,  batch step: 248, loss: 53.15510177612305\n",
      "epoch: 71,  batch step: 249, loss: 21.12204360961914\n",
      "epoch: 71,  batch step: 250, loss: 78.83495330810547\n",
      "epoch: 71,  batch step: 251, loss: 98.33055114746094\n",
      "validation error epoch  71:    tensor(67.2858, device='cuda:0')\n",
      "316\n",
      "epoch: 72,  batch step: 0, loss: 45.984832763671875\n",
      "epoch: 72,  batch step: 1, loss: 18.6574764251709\n",
      "epoch: 72,  batch step: 2, loss: 10.088701248168945\n",
      "epoch: 72,  batch step: 3, loss: 39.429439544677734\n",
      "epoch: 72,  batch step: 4, loss: 8.983885765075684\n",
      "epoch: 72,  batch step: 5, loss: 7.166329860687256\n",
      "epoch: 72,  batch step: 6, loss: 5.797207832336426\n",
      "epoch: 72,  batch step: 7, loss: 17.010208129882812\n",
      "epoch: 72,  batch step: 8, loss: 25.794464111328125\n",
      "epoch: 72,  batch step: 9, loss: 19.49890899658203\n",
      "epoch: 72,  batch step: 10, loss: 70.17741394042969\n",
      "epoch: 72,  batch step: 11, loss: 12.305688858032227\n",
      "epoch: 72,  batch step: 12, loss: 6.082942962646484\n",
      "epoch: 72,  batch step: 13, loss: 3.644334554672241\n",
      "epoch: 72,  batch step: 14, loss: 5.542596340179443\n",
      "epoch: 72,  batch step: 15, loss: 6.124233245849609\n",
      "epoch: 72,  batch step: 16, loss: 34.38459396362305\n",
      "epoch: 72,  batch step: 17, loss: 3.733821392059326\n",
      "epoch: 72,  batch step: 18, loss: 5.24202823638916\n",
      "epoch: 72,  batch step: 19, loss: 21.93587303161621\n",
      "epoch: 72,  batch step: 20, loss: 17.18859100341797\n",
      "epoch: 72,  batch step: 21, loss: 29.301025390625\n",
      "epoch: 72,  batch step: 22, loss: 35.855079650878906\n",
      "epoch: 72,  batch step: 23, loss: 5.7739386558532715\n",
      "epoch: 72,  batch step: 24, loss: 28.340253829956055\n",
      "epoch: 72,  batch step: 25, loss: 39.53673553466797\n",
      "epoch: 72,  batch step: 26, loss: 89.62030029296875\n",
      "epoch: 72,  batch step: 27, loss: 9.050849914550781\n",
      "epoch: 72,  batch step: 28, loss: 17.378263473510742\n",
      "epoch: 72,  batch step: 29, loss: 5.155288219451904\n",
      "epoch: 72,  batch step: 30, loss: 4.307377815246582\n",
      "epoch: 72,  batch step: 31, loss: 4.7361907958984375\n",
      "epoch: 72,  batch step: 32, loss: 12.810530662536621\n",
      "epoch: 72,  batch step: 33, loss: 3.7318427562713623\n",
      "epoch: 72,  batch step: 34, loss: 5.924123764038086\n",
      "epoch: 72,  batch step: 35, loss: 20.220308303833008\n",
      "epoch: 72,  batch step: 36, loss: 2.9902541637420654\n",
      "epoch: 72,  batch step: 37, loss: 27.297374725341797\n",
      "epoch: 72,  batch step: 38, loss: 5.33442497253418\n",
      "epoch: 72,  batch step: 39, loss: 46.33333969116211\n",
      "epoch: 72,  batch step: 40, loss: 19.020095825195312\n",
      "epoch: 72,  batch step: 41, loss: 4.459712982177734\n",
      "epoch: 72,  batch step: 42, loss: 14.803927421569824\n",
      "epoch: 72,  batch step: 43, loss: 2.818592071533203\n",
      "epoch: 72,  batch step: 44, loss: 40.34772491455078\n",
      "epoch: 72,  batch step: 45, loss: 2.870523452758789\n",
      "epoch: 72,  batch step: 46, loss: 5.8888349533081055\n",
      "epoch: 72,  batch step: 47, loss: 18.72142791748047\n",
      "epoch: 72,  batch step: 48, loss: 5.508995056152344\n",
      "epoch: 72,  batch step: 49, loss: 2.813438892364502\n",
      "epoch: 72,  batch step: 50, loss: 3.5599303245544434\n",
      "epoch: 72,  batch step: 51, loss: 48.88108444213867\n",
      "epoch: 72,  batch step: 52, loss: 55.090484619140625\n",
      "epoch: 72,  batch step: 53, loss: 2.6827943325042725\n",
      "epoch: 72,  batch step: 54, loss: 28.571598052978516\n",
      "epoch: 72,  batch step: 55, loss: 1.9108012914657593\n",
      "epoch: 72,  batch step: 56, loss: 3.2104387283325195\n",
      "epoch: 72,  batch step: 57, loss: 15.641703605651855\n",
      "epoch: 72,  batch step: 58, loss: 15.69420051574707\n",
      "epoch: 72,  batch step: 59, loss: 10.457040786743164\n",
      "epoch: 72,  batch step: 60, loss: 31.361175537109375\n",
      "epoch: 72,  batch step: 61, loss: 20.860889434814453\n",
      "epoch: 72,  batch step: 62, loss: 3.5859975814819336\n",
      "epoch: 72,  batch step: 63, loss: 3.8438282012939453\n",
      "epoch: 72,  batch step: 64, loss: 3.375098943710327\n",
      "epoch: 72,  batch step: 65, loss: 12.077991485595703\n",
      "epoch: 72,  batch step: 66, loss: 2.132655143737793\n",
      "epoch: 72,  batch step: 67, loss: 2.560659885406494\n",
      "epoch: 72,  batch step: 68, loss: 24.989910125732422\n",
      "epoch: 72,  batch step: 69, loss: 18.63274574279785\n",
      "epoch: 72,  batch step: 70, loss: 36.425994873046875\n",
      "epoch: 72,  batch step: 71, loss: 22.32452964782715\n",
      "epoch: 72,  batch step: 72, loss: 13.759281158447266\n",
      "epoch: 72,  batch step: 73, loss: 46.77798080444336\n",
      "epoch: 72,  batch step: 74, loss: 28.430761337280273\n",
      "epoch: 72,  batch step: 75, loss: 5.417599678039551\n",
      "epoch: 72,  batch step: 76, loss: 17.181377410888672\n",
      "epoch: 72,  batch step: 77, loss: 3.0920896530151367\n",
      "epoch: 72,  batch step: 78, loss: 3.9379725456237793\n",
      "epoch: 72,  batch step: 79, loss: 3.0484023094177246\n",
      "epoch: 72,  batch step: 80, loss: 4.2516188621521\n",
      "epoch: 72,  batch step: 81, loss: 19.33016586303711\n",
      "epoch: 72,  batch step: 82, loss: 2.0078649520874023\n",
      "epoch: 72,  batch step: 83, loss: 23.717544555664062\n",
      "epoch: 72,  batch step: 84, loss: 4.235139846801758\n",
      "epoch: 72,  batch step: 85, loss: 2.4584598541259766\n",
      "epoch: 72,  batch step: 86, loss: 4.5137152671813965\n",
      "epoch: 72,  batch step: 87, loss: 26.733707427978516\n",
      "epoch: 72,  batch step: 88, loss: 4.606667995452881\n",
      "epoch: 72,  batch step: 89, loss: 33.54887771606445\n",
      "epoch: 72,  batch step: 90, loss: 13.665266036987305\n",
      "epoch: 72,  batch step: 91, loss: 11.440077781677246\n",
      "epoch: 72,  batch step: 92, loss: 30.612043380737305\n",
      "epoch: 72,  batch step: 93, loss: 10.721418380737305\n",
      "epoch: 72,  batch step: 94, loss: 28.40945816040039\n",
      "epoch: 72,  batch step: 95, loss: 4.417529106140137\n",
      "epoch: 72,  batch step: 96, loss: 4.957538604736328\n",
      "epoch: 72,  batch step: 97, loss: 2.9628658294677734\n",
      "epoch: 72,  batch step: 98, loss: 2.2052860260009766\n",
      "epoch: 72,  batch step: 99, loss: 47.27989959716797\n",
      "epoch: 72,  batch step: 100, loss: 1.8722972869873047\n",
      "epoch: 72,  batch step: 101, loss: 5.527883052825928\n",
      "epoch: 72,  batch step: 102, loss: 2.7804923057556152\n",
      "epoch: 72,  batch step: 103, loss: 5.713375091552734\n",
      "epoch: 72,  batch step: 104, loss: 5.9170684814453125\n",
      "epoch: 72,  batch step: 105, loss: 2.2835004329681396\n",
      "epoch: 72,  batch step: 106, loss: 4.308732032775879\n",
      "epoch: 72,  batch step: 107, loss: 2.6681361198425293\n",
      "epoch: 72,  batch step: 108, loss: 10.236228942871094\n",
      "epoch: 72,  batch step: 109, loss: 2.3973286151885986\n",
      "epoch: 72,  batch step: 110, loss: 2.646327018737793\n",
      "epoch: 72,  batch step: 111, loss: 2.213435649871826\n",
      "epoch: 72,  batch step: 112, loss: 15.656744003295898\n",
      "epoch: 72,  batch step: 113, loss: 18.482295989990234\n",
      "epoch: 72,  batch step: 114, loss: 2.4496660232543945\n",
      "epoch: 72,  batch step: 115, loss: 3.716845989227295\n",
      "epoch: 72,  batch step: 116, loss: 2.197384834289551\n",
      "epoch: 72,  batch step: 117, loss: 27.029109954833984\n",
      "epoch: 72,  batch step: 118, loss: 2.9559998512268066\n",
      "epoch: 72,  batch step: 119, loss: 2.6779284477233887\n",
      "epoch: 72,  batch step: 120, loss: 30.884319305419922\n",
      "epoch: 72,  batch step: 121, loss: 18.719585418701172\n",
      "epoch: 72,  batch step: 122, loss: 15.47488784790039\n",
      "epoch: 72,  batch step: 123, loss: 26.058277130126953\n",
      "epoch: 72,  batch step: 124, loss: 2.77002215385437\n",
      "epoch: 72,  batch step: 125, loss: 3.0280396938323975\n",
      "epoch: 72,  batch step: 126, loss: 5.4983320236206055\n",
      "epoch: 72,  batch step: 127, loss: 2.6306967735290527\n",
      "epoch: 72,  batch step: 128, loss: 2.514918088912964\n",
      "epoch: 72,  batch step: 129, loss: 2.832794189453125\n",
      "epoch: 72,  batch step: 130, loss: 3.10730242729187\n",
      "epoch: 72,  batch step: 131, loss: 3.915050506591797\n",
      "epoch: 72,  batch step: 132, loss: 5.624667167663574\n",
      "epoch: 72,  batch step: 133, loss: 4.300849914550781\n",
      "epoch: 72,  batch step: 134, loss: 4.106380462646484\n",
      "epoch: 72,  batch step: 135, loss: 22.16463279724121\n",
      "epoch: 72,  batch step: 136, loss: 1.962868094444275\n",
      "epoch: 72,  batch step: 137, loss: 26.668806076049805\n",
      "epoch: 72,  batch step: 138, loss: 21.391368865966797\n",
      "epoch: 72,  batch step: 139, loss: 19.211374282836914\n",
      "epoch: 72,  batch step: 140, loss: 62.37434005737305\n",
      "epoch: 72,  batch step: 141, loss: 3.8218798637390137\n",
      "epoch: 72,  batch step: 142, loss: 60.69524383544922\n",
      "epoch: 72,  batch step: 143, loss: 13.7127685546875\n",
      "epoch: 72,  batch step: 144, loss: 11.047552108764648\n",
      "epoch: 72,  batch step: 145, loss: 32.79317855834961\n",
      "epoch: 72,  batch step: 146, loss: 2.765120267868042\n",
      "epoch: 72,  batch step: 147, loss: 7.41331672668457\n",
      "epoch: 72,  batch step: 148, loss: 91.17134094238281\n",
      "epoch: 72,  batch step: 149, loss: 24.350200653076172\n",
      "epoch: 72,  batch step: 150, loss: 6.111673355102539\n",
      "epoch: 72,  batch step: 151, loss: 29.43732261657715\n",
      "epoch: 72,  batch step: 152, loss: 3.9290218353271484\n",
      "epoch: 72,  batch step: 153, loss: 3.90573787689209\n",
      "epoch: 72,  batch step: 154, loss: 13.274772644042969\n",
      "epoch: 72,  batch step: 155, loss: 21.449085235595703\n",
      "epoch: 72,  batch step: 156, loss: 28.666305541992188\n",
      "epoch: 72,  batch step: 157, loss: 3.2025458812713623\n",
      "epoch: 72,  batch step: 158, loss: 3.029503583908081\n",
      "epoch: 72,  batch step: 159, loss: 20.559410095214844\n",
      "epoch: 72,  batch step: 160, loss: 43.513458251953125\n",
      "epoch: 72,  batch step: 161, loss: 3.187237501144409\n",
      "epoch: 72,  batch step: 162, loss: 9.964521408081055\n",
      "epoch: 72,  batch step: 163, loss: 22.571971893310547\n",
      "epoch: 72,  batch step: 164, loss: 52.76073455810547\n",
      "epoch: 72,  batch step: 165, loss: 13.939227104187012\n",
      "epoch: 72,  batch step: 166, loss: 4.77105712890625\n",
      "epoch: 72,  batch step: 167, loss: 24.411977767944336\n",
      "epoch: 72,  batch step: 168, loss: 30.409221649169922\n",
      "epoch: 72,  batch step: 169, loss: 2.9990081787109375\n",
      "epoch: 72,  batch step: 170, loss: 16.440120697021484\n",
      "epoch: 72,  batch step: 171, loss: 3.3848941326141357\n",
      "epoch: 72,  batch step: 172, loss: 24.610565185546875\n",
      "epoch: 72,  batch step: 173, loss: 34.392974853515625\n",
      "epoch: 72,  batch step: 174, loss: 132.7639617919922\n",
      "epoch: 72,  batch step: 175, loss: 7.245209693908691\n",
      "epoch: 72,  batch step: 176, loss: 50.80943298339844\n",
      "epoch: 72,  batch step: 177, loss: 84.53337097167969\n",
      "epoch: 72,  batch step: 178, loss: 4.896353721618652\n",
      "epoch: 72,  batch step: 179, loss: 10.073366165161133\n",
      "epoch: 72,  batch step: 180, loss: 3.8674283027648926\n",
      "epoch: 72,  batch step: 181, loss: 3.6640803813934326\n",
      "epoch: 72,  batch step: 182, loss: 32.646324157714844\n",
      "epoch: 72,  batch step: 183, loss: 18.359115600585938\n",
      "epoch: 72,  batch step: 184, loss: 2.7427079677581787\n",
      "epoch: 72,  batch step: 185, loss: 4.852721214294434\n",
      "epoch: 72,  batch step: 186, loss: 4.909602165222168\n",
      "epoch: 72,  batch step: 187, loss: 31.66596031188965\n",
      "epoch: 72,  batch step: 188, loss: 16.60482406616211\n",
      "epoch: 72,  batch step: 189, loss: 2.8654417991638184\n",
      "epoch: 72,  batch step: 190, loss: 4.297487258911133\n",
      "epoch: 72,  batch step: 191, loss: 27.177494049072266\n",
      "epoch: 72,  batch step: 192, loss: 3.3002099990844727\n",
      "epoch: 72,  batch step: 193, loss: 2.6971282958984375\n",
      "epoch: 72,  batch step: 194, loss: 13.102170944213867\n",
      "epoch: 72,  batch step: 195, loss: 13.107179641723633\n",
      "epoch: 72,  batch step: 196, loss: 17.121952056884766\n",
      "epoch: 72,  batch step: 197, loss: 3.1699976921081543\n",
      "epoch: 72,  batch step: 198, loss: 54.71715545654297\n",
      "epoch: 72,  batch step: 199, loss: 13.956964492797852\n",
      "epoch: 72,  batch step: 200, loss: 3.0411033630371094\n",
      "epoch: 72,  batch step: 201, loss: 3.1941497325897217\n",
      "epoch: 72,  batch step: 202, loss: 3.4540343284606934\n",
      "epoch: 72,  batch step: 203, loss: 30.110321044921875\n",
      "epoch: 72,  batch step: 204, loss: 4.45167875289917\n",
      "epoch: 72,  batch step: 205, loss: 3.436866283416748\n",
      "epoch: 72,  batch step: 206, loss: 16.641399383544922\n",
      "epoch: 72,  batch step: 207, loss: 49.43083190917969\n",
      "epoch: 72,  batch step: 208, loss: 13.2718505859375\n",
      "epoch: 72,  batch step: 209, loss: 35.32939910888672\n",
      "epoch: 72,  batch step: 210, loss: 12.393553733825684\n",
      "epoch: 72,  batch step: 211, loss: 38.35676956176758\n",
      "epoch: 72,  batch step: 212, loss: 18.563135147094727\n",
      "epoch: 72,  batch step: 213, loss: 3.7978837490081787\n",
      "epoch: 72,  batch step: 214, loss: 34.70166778564453\n",
      "epoch: 72,  batch step: 215, loss: 3.6702706813812256\n",
      "epoch: 72,  batch step: 216, loss: 3.0989444255828857\n",
      "epoch: 72,  batch step: 217, loss: 7.133820533752441\n",
      "epoch: 72,  batch step: 218, loss: 2.5572562217712402\n",
      "epoch: 72,  batch step: 219, loss: 3.651454448699951\n",
      "epoch: 72,  batch step: 220, loss: 12.520106315612793\n",
      "epoch: 72,  batch step: 221, loss: 44.069976806640625\n",
      "epoch: 72,  batch step: 222, loss: 76.26775360107422\n",
      "epoch: 72,  batch step: 223, loss: 2.5153064727783203\n",
      "epoch: 72,  batch step: 224, loss: 10.74991512298584\n",
      "epoch: 72,  batch step: 225, loss: 15.10496711730957\n",
      "epoch: 72,  batch step: 226, loss: 23.485485076904297\n",
      "epoch: 72,  batch step: 227, loss: 3.2628111839294434\n",
      "epoch: 72,  batch step: 228, loss: 42.170196533203125\n",
      "epoch: 72,  batch step: 229, loss: 14.771437644958496\n",
      "epoch: 72,  batch step: 230, loss: 22.563888549804688\n",
      "epoch: 72,  batch step: 231, loss: 24.799604415893555\n",
      "epoch: 72,  batch step: 232, loss: 16.455707550048828\n",
      "epoch: 72,  batch step: 233, loss: 3.1781489849090576\n",
      "epoch: 72,  batch step: 234, loss: 14.738504409790039\n",
      "epoch: 72,  batch step: 235, loss: 48.266075134277344\n",
      "epoch: 72,  batch step: 236, loss: 12.56909465789795\n",
      "epoch: 72,  batch step: 237, loss: 3.2120933532714844\n",
      "epoch: 72,  batch step: 238, loss: 48.828426361083984\n",
      "epoch: 72,  batch step: 239, loss: 3.107276678085327\n",
      "epoch: 72,  batch step: 240, loss: 26.960735321044922\n",
      "epoch: 72,  batch step: 241, loss: 3.4760100841522217\n",
      "epoch: 72,  batch step: 242, loss: 3.7922558784484863\n",
      "epoch: 72,  batch step: 243, loss: 2.8992021083831787\n",
      "epoch: 72,  batch step: 244, loss: 24.300765991210938\n",
      "epoch: 72,  batch step: 245, loss: 2.6591567993164062\n",
      "epoch: 72,  batch step: 246, loss: 3.5857315063476562\n",
      "epoch: 72,  batch step: 247, loss: 2.7063770294189453\n",
      "epoch: 72,  batch step: 248, loss: 2.689197301864624\n",
      "epoch: 72,  batch step: 249, loss: 2.741929054260254\n",
      "epoch: 72,  batch step: 250, loss: 10.71406364440918\n",
      "epoch: 72,  batch step: 251, loss: 70.84083557128906\n",
      "validation error epoch  72:    tensor(66.4639, device='cuda:0')\n",
      "316\n",
      "epoch: 73,  batch step: 0, loss: 41.098114013671875\n",
      "epoch: 73,  batch step: 1, loss: 5.776481628417969\n",
      "epoch: 73,  batch step: 2, loss: 48.57826232910156\n",
      "epoch: 73,  batch step: 3, loss: 16.388019561767578\n",
      "epoch: 73,  batch step: 4, loss: 12.516024589538574\n",
      "epoch: 73,  batch step: 5, loss: 17.87682342529297\n",
      "epoch: 73,  batch step: 6, loss: 38.49761962890625\n",
      "epoch: 73,  batch step: 7, loss: 13.552947044372559\n",
      "epoch: 73,  batch step: 8, loss: 3.336582899093628\n",
      "epoch: 73,  batch step: 9, loss: 21.46063232421875\n",
      "epoch: 73,  batch step: 10, loss: 12.232723236083984\n",
      "epoch: 73,  batch step: 11, loss: 23.009456634521484\n",
      "epoch: 73,  batch step: 12, loss: 23.483875274658203\n",
      "epoch: 73,  batch step: 13, loss: 4.02661657333374\n",
      "epoch: 73,  batch step: 14, loss: 35.52020263671875\n",
      "epoch: 73,  batch step: 15, loss: 14.882705688476562\n",
      "epoch: 73,  batch step: 16, loss: 3.7144789695739746\n",
      "epoch: 73,  batch step: 17, loss: 29.400081634521484\n",
      "epoch: 73,  batch step: 18, loss: 16.21869659423828\n",
      "epoch: 73,  batch step: 19, loss: 3.7220141887664795\n",
      "epoch: 73,  batch step: 20, loss: 12.600801467895508\n",
      "epoch: 73,  batch step: 21, loss: 42.44733428955078\n",
      "epoch: 73,  batch step: 22, loss: 2.995621681213379\n",
      "epoch: 73,  batch step: 23, loss: 2.5742197036743164\n",
      "epoch: 73,  batch step: 24, loss: 20.01319122314453\n",
      "epoch: 73,  batch step: 25, loss: 52.32025146484375\n",
      "epoch: 73,  batch step: 26, loss: 2.7865376472473145\n",
      "epoch: 73,  batch step: 27, loss: 3.0095725059509277\n",
      "epoch: 73,  batch step: 28, loss: 3.846738815307617\n",
      "epoch: 73,  batch step: 29, loss: 2.574223041534424\n",
      "epoch: 73,  batch step: 30, loss: 3.5137717723846436\n",
      "epoch: 73,  batch step: 31, loss: 2.2233121395111084\n",
      "epoch: 73,  batch step: 32, loss: 9.90942096710205\n",
      "epoch: 73,  batch step: 33, loss: 5.553241729736328\n",
      "epoch: 73,  batch step: 34, loss: 6.1315436363220215\n",
      "epoch: 73,  batch step: 35, loss: 3.5820999145507812\n",
      "epoch: 73,  batch step: 36, loss: 24.59475326538086\n",
      "epoch: 73,  batch step: 37, loss: 13.059385299682617\n",
      "epoch: 73,  batch step: 38, loss: 22.301715850830078\n",
      "epoch: 73,  batch step: 39, loss: 5.2933878898620605\n",
      "epoch: 73,  batch step: 40, loss: 2.5787858963012695\n",
      "epoch: 73,  batch step: 41, loss: 11.558652877807617\n",
      "epoch: 73,  batch step: 42, loss: 15.747861862182617\n",
      "epoch: 73,  batch step: 43, loss: 23.592147827148438\n",
      "epoch: 73,  batch step: 44, loss: 2.362661838531494\n",
      "epoch: 73,  batch step: 45, loss: 2.877992630004883\n",
      "epoch: 73,  batch step: 46, loss: 6.2014875411987305\n",
      "epoch: 73,  batch step: 47, loss: 8.17774486541748\n",
      "epoch: 73,  batch step: 48, loss: 2.359121322631836\n",
      "epoch: 73,  batch step: 49, loss: 2.6666676998138428\n",
      "epoch: 73,  batch step: 50, loss: 1.7947747707366943\n",
      "epoch: 73,  batch step: 51, loss: 3.865640163421631\n",
      "epoch: 73,  batch step: 52, loss: 11.436385154724121\n",
      "epoch: 73,  batch step: 53, loss: 31.14919090270996\n",
      "epoch: 73,  batch step: 54, loss: 2.3319411277770996\n",
      "epoch: 73,  batch step: 55, loss: 19.936189651489258\n",
      "epoch: 73,  batch step: 56, loss: 15.965571403503418\n",
      "epoch: 73,  batch step: 57, loss: 2.966254711151123\n",
      "epoch: 73,  batch step: 58, loss: 3.458806037902832\n",
      "epoch: 73,  batch step: 59, loss: 2.6637704372406006\n",
      "epoch: 73,  batch step: 60, loss: 2.163792133331299\n",
      "epoch: 73,  batch step: 61, loss: 24.240509033203125\n",
      "epoch: 73,  batch step: 62, loss: 45.274898529052734\n",
      "epoch: 73,  batch step: 63, loss: 14.593039512634277\n",
      "epoch: 73,  batch step: 64, loss: 2.1147520542144775\n",
      "epoch: 73,  batch step: 65, loss: 5.613677501678467\n",
      "epoch: 73,  batch step: 66, loss: 3.9255847930908203\n",
      "epoch: 73,  batch step: 67, loss: 4.151003837585449\n",
      "epoch: 73,  batch step: 68, loss: 10.810821533203125\n",
      "epoch: 73,  batch step: 69, loss: 83.18580627441406\n",
      "epoch: 73,  batch step: 70, loss: 37.521095275878906\n",
      "epoch: 73,  batch step: 71, loss: 28.18219566345215\n",
      "epoch: 73,  batch step: 72, loss: 3.011235237121582\n",
      "epoch: 73,  batch step: 73, loss: 3.597733497619629\n",
      "epoch: 73,  batch step: 74, loss: 4.964109897613525\n",
      "epoch: 73,  batch step: 75, loss: 3.3920364379882812\n",
      "epoch: 73,  batch step: 76, loss: 22.748943328857422\n",
      "epoch: 73,  batch step: 77, loss: 17.793598175048828\n",
      "epoch: 73,  batch step: 78, loss: 14.15405559539795\n",
      "epoch: 73,  batch step: 79, loss: 13.709966659545898\n",
      "epoch: 73,  batch step: 80, loss: 2.2163400650024414\n",
      "epoch: 73,  batch step: 81, loss: 3.3555712699890137\n",
      "epoch: 73,  batch step: 82, loss: 12.618315696716309\n",
      "epoch: 73,  batch step: 83, loss: 3.0019023418426514\n",
      "epoch: 73,  batch step: 84, loss: 49.54315948486328\n",
      "epoch: 73,  batch step: 85, loss: 9.446802139282227\n",
      "epoch: 73,  batch step: 86, loss: 13.257400512695312\n",
      "epoch: 73,  batch step: 87, loss: 14.782828330993652\n",
      "epoch: 73,  batch step: 88, loss: 15.887499809265137\n",
      "epoch: 73,  batch step: 89, loss: 21.921964645385742\n",
      "epoch: 73,  batch step: 90, loss: 36.088722229003906\n",
      "epoch: 73,  batch step: 91, loss: 10.201408386230469\n",
      "epoch: 73,  batch step: 92, loss: 2.7602577209472656\n",
      "epoch: 73,  batch step: 93, loss: 2.4687306880950928\n",
      "epoch: 73,  batch step: 94, loss: 4.853950023651123\n",
      "epoch: 73,  batch step: 95, loss: 2.8718833923339844\n",
      "epoch: 73,  batch step: 96, loss: 3.4110679626464844\n",
      "epoch: 73,  batch step: 97, loss: 2.432053565979004\n",
      "epoch: 73,  batch step: 98, loss: 2.172811508178711\n",
      "epoch: 73,  batch step: 99, loss: 50.050086975097656\n",
      "epoch: 73,  batch step: 100, loss: 3.133814573287964\n",
      "epoch: 73,  batch step: 101, loss: 67.90423583984375\n",
      "epoch: 73,  batch step: 102, loss: 9.47834300994873\n",
      "epoch: 73,  batch step: 103, loss: 3.8578574657440186\n",
      "epoch: 73,  batch step: 104, loss: 3.372786045074463\n",
      "epoch: 73,  batch step: 105, loss: 26.395000457763672\n",
      "epoch: 73,  batch step: 106, loss: 4.927609443664551\n",
      "epoch: 73,  batch step: 107, loss: 43.17402648925781\n",
      "epoch: 73,  batch step: 108, loss: 2.510596752166748\n",
      "epoch: 73,  batch step: 109, loss: 1.7469991445541382\n",
      "epoch: 73,  batch step: 110, loss: 30.880619049072266\n",
      "epoch: 73,  batch step: 111, loss: 1.8530170917510986\n",
      "epoch: 73,  batch step: 112, loss: 4.774050235748291\n",
      "epoch: 73,  batch step: 113, loss: 2.751147747039795\n",
      "epoch: 73,  batch step: 114, loss: 6.3040876388549805\n",
      "epoch: 73,  batch step: 115, loss: 68.51921081542969\n",
      "epoch: 73,  batch step: 116, loss: 12.479684829711914\n",
      "epoch: 73,  batch step: 117, loss: 93.34739685058594\n",
      "epoch: 73,  batch step: 118, loss: 3.106468915939331\n",
      "epoch: 73,  batch step: 119, loss: 18.00153350830078\n",
      "epoch: 73,  batch step: 120, loss: 29.444150924682617\n",
      "epoch: 73,  batch step: 121, loss: 19.971452713012695\n",
      "epoch: 73,  batch step: 122, loss: 4.341800689697266\n",
      "epoch: 73,  batch step: 123, loss: 3.891050338745117\n",
      "epoch: 73,  batch step: 124, loss: 21.993574142456055\n",
      "epoch: 73,  batch step: 125, loss: 24.628942489624023\n",
      "epoch: 73,  batch step: 126, loss: 10.266092300415039\n",
      "epoch: 73,  batch step: 127, loss: 3.1802051067352295\n",
      "epoch: 73,  batch step: 128, loss: 2.2512097358703613\n",
      "epoch: 73,  batch step: 129, loss: 13.33344554901123\n",
      "epoch: 73,  batch step: 130, loss: 17.06486701965332\n",
      "epoch: 73,  batch step: 131, loss: 23.74805450439453\n",
      "epoch: 73,  batch step: 132, loss: 4.52046012878418\n",
      "epoch: 73,  batch step: 133, loss: 4.645987033843994\n",
      "epoch: 73,  batch step: 134, loss: 2.52931809425354\n",
      "epoch: 73,  batch step: 135, loss: 47.23174285888672\n",
      "epoch: 73,  batch step: 136, loss: 3.621525764465332\n",
      "epoch: 73,  batch step: 137, loss: 9.122579574584961\n",
      "epoch: 73,  batch step: 138, loss: 3.0440750122070312\n",
      "epoch: 73,  batch step: 139, loss: 3.514781951904297\n",
      "epoch: 73,  batch step: 140, loss: 15.738222122192383\n",
      "epoch: 73,  batch step: 141, loss: 8.982885360717773\n",
      "epoch: 73,  batch step: 142, loss: 1.9494752883911133\n",
      "epoch: 73,  batch step: 143, loss: 22.791120529174805\n",
      "epoch: 73,  batch step: 144, loss: 3.8377737998962402\n",
      "epoch: 73,  batch step: 145, loss: 1.9938501119613647\n",
      "epoch: 73,  batch step: 146, loss: 24.385074615478516\n",
      "epoch: 73,  batch step: 147, loss: 7.177001953125\n",
      "epoch: 73,  batch step: 148, loss: 5.73654842376709\n",
      "epoch: 73,  batch step: 149, loss: 4.943957328796387\n",
      "epoch: 73,  batch step: 150, loss: 4.663129806518555\n",
      "epoch: 73,  batch step: 151, loss: 51.07853317260742\n",
      "epoch: 73,  batch step: 152, loss: 14.201578140258789\n",
      "epoch: 73,  batch step: 153, loss: 10.72431468963623\n",
      "epoch: 73,  batch step: 154, loss: 15.683173179626465\n",
      "epoch: 73,  batch step: 155, loss: 60.69917297363281\n",
      "epoch: 73,  batch step: 156, loss: 5.8532209396362305\n",
      "epoch: 73,  batch step: 157, loss: 5.081015586853027\n",
      "epoch: 73,  batch step: 158, loss: 15.452609062194824\n",
      "epoch: 73,  batch step: 159, loss: 3.4404189586639404\n",
      "epoch: 73,  batch step: 160, loss: 3.464906692504883\n",
      "epoch: 73,  batch step: 161, loss: 25.140743255615234\n",
      "epoch: 73,  batch step: 162, loss: 26.280059814453125\n",
      "epoch: 73,  batch step: 163, loss: 4.581938743591309\n",
      "epoch: 73,  batch step: 164, loss: 2.9629266262054443\n",
      "epoch: 73,  batch step: 165, loss: 57.29869842529297\n",
      "epoch: 73,  batch step: 166, loss: 18.577924728393555\n",
      "epoch: 73,  batch step: 167, loss: 14.260438919067383\n",
      "epoch: 73,  batch step: 168, loss: 3.86179518699646\n",
      "epoch: 73,  batch step: 169, loss: 5.155440330505371\n",
      "epoch: 73,  batch step: 170, loss: 2.0112576484680176\n",
      "epoch: 73,  batch step: 171, loss: 9.698881149291992\n",
      "epoch: 73,  batch step: 172, loss: 59.42981719970703\n",
      "epoch: 73,  batch step: 173, loss: 11.387832641601562\n",
      "epoch: 73,  batch step: 174, loss: 34.063079833984375\n",
      "epoch: 73,  batch step: 175, loss: 2.3703296184539795\n",
      "epoch: 73,  batch step: 176, loss: 3.62580943107605\n",
      "epoch: 73,  batch step: 177, loss: 25.630245208740234\n",
      "epoch: 73,  batch step: 178, loss: 2.471916437149048\n",
      "epoch: 73,  batch step: 179, loss: 2.3884246349334717\n",
      "epoch: 73,  batch step: 180, loss: 2.368774890899658\n",
      "epoch: 73,  batch step: 181, loss: 3.170691967010498\n",
      "epoch: 73,  batch step: 182, loss: 49.178367614746094\n",
      "epoch: 73,  batch step: 183, loss: 15.706182479858398\n",
      "epoch: 73,  batch step: 184, loss: 50.17533874511719\n",
      "epoch: 73,  batch step: 185, loss: 3.0872340202331543\n",
      "epoch: 73,  batch step: 186, loss: 2.814436912536621\n",
      "epoch: 73,  batch step: 187, loss: 2.1762096881866455\n",
      "epoch: 73,  batch step: 188, loss: 2.3269295692443848\n",
      "epoch: 73,  batch step: 189, loss: 7.274294853210449\n",
      "epoch: 73,  batch step: 190, loss: 1.9514249563217163\n",
      "epoch: 73,  batch step: 191, loss: 2.4804141521453857\n",
      "epoch: 73,  batch step: 192, loss: 22.71735954284668\n",
      "epoch: 73,  batch step: 193, loss: 6.628300666809082\n",
      "epoch: 73,  batch step: 194, loss: 1.896862268447876\n",
      "epoch: 73,  batch step: 195, loss: 2.1687073707580566\n",
      "epoch: 73,  batch step: 196, loss: 20.859785079956055\n",
      "epoch: 73,  batch step: 197, loss: 14.832401275634766\n",
      "epoch: 73,  batch step: 198, loss: 12.885520935058594\n",
      "epoch: 73,  batch step: 199, loss: 4.2435622215271\n",
      "epoch: 73,  batch step: 200, loss: 25.127059936523438\n",
      "epoch: 73,  batch step: 201, loss: 2.4535346031188965\n",
      "epoch: 73,  batch step: 202, loss: 5.444917678833008\n",
      "epoch: 73,  batch step: 203, loss: 35.08046340942383\n",
      "epoch: 73,  batch step: 204, loss: 4.775684356689453\n",
      "epoch: 73,  batch step: 205, loss: 2.1298716068267822\n",
      "epoch: 73,  batch step: 206, loss: 13.208670616149902\n",
      "epoch: 73,  batch step: 207, loss: 12.015539169311523\n",
      "epoch: 73,  batch step: 208, loss: 2.455211877822876\n",
      "epoch: 73,  batch step: 209, loss: 153.58560180664062\n",
      "epoch: 73,  batch step: 210, loss: 10.438136100769043\n",
      "epoch: 73,  batch step: 211, loss: 2.2292754650115967\n",
      "epoch: 73,  batch step: 212, loss: 2.3405299186706543\n",
      "epoch: 73,  batch step: 213, loss: 25.062591552734375\n",
      "epoch: 73,  batch step: 214, loss: 13.684844970703125\n",
      "epoch: 73,  batch step: 215, loss: 3.0797595977783203\n",
      "epoch: 73,  batch step: 216, loss: 2.6789653301239014\n",
      "epoch: 73,  batch step: 217, loss: 2.6134390830993652\n",
      "epoch: 73,  batch step: 218, loss: 9.921346664428711\n",
      "epoch: 73,  batch step: 219, loss: 1.9803130626678467\n",
      "epoch: 73,  batch step: 220, loss: 21.120864868164062\n",
      "epoch: 73,  batch step: 221, loss: 16.839994430541992\n",
      "epoch: 73,  batch step: 222, loss: 24.554256439208984\n",
      "epoch: 73,  batch step: 223, loss: 3.7767293453216553\n",
      "epoch: 73,  batch step: 224, loss: 10.822391510009766\n",
      "epoch: 73,  batch step: 225, loss: 2.0349061489105225\n",
      "epoch: 73,  batch step: 226, loss: 22.610044479370117\n",
      "epoch: 73,  batch step: 227, loss: 2.6218717098236084\n",
      "epoch: 73,  batch step: 228, loss: 44.606285095214844\n",
      "epoch: 73,  batch step: 229, loss: 16.122709274291992\n",
      "epoch: 73,  batch step: 230, loss: 39.474937438964844\n",
      "epoch: 73,  batch step: 231, loss: 2.9280591011047363\n",
      "epoch: 73,  batch step: 232, loss: 3.9347095489501953\n",
      "epoch: 73,  batch step: 233, loss: 2.501640796661377\n",
      "epoch: 73,  batch step: 234, loss: 4.747982978820801\n",
      "epoch: 73,  batch step: 235, loss: 1.7187671661376953\n",
      "epoch: 73,  batch step: 236, loss: 7.762703895568848\n",
      "epoch: 73,  batch step: 237, loss: 7.541200637817383\n",
      "epoch: 73,  batch step: 238, loss: 2.2449183464050293\n",
      "epoch: 73,  batch step: 239, loss: 2.54366397857666\n",
      "epoch: 73,  batch step: 240, loss: 40.76060104370117\n",
      "epoch: 73,  batch step: 241, loss: 30.191059112548828\n",
      "epoch: 73,  batch step: 242, loss: 3.0054690837860107\n",
      "epoch: 73,  batch step: 243, loss: 4.04169225692749\n",
      "epoch: 73,  batch step: 244, loss: 4.377297401428223\n",
      "epoch: 73,  batch step: 245, loss: 12.465034484863281\n",
      "epoch: 73,  batch step: 246, loss: 2.750579833984375\n",
      "epoch: 73,  batch step: 247, loss: 13.642523765563965\n",
      "epoch: 73,  batch step: 248, loss: 4.289534568786621\n",
      "epoch: 73,  batch step: 249, loss: 4.543061256408691\n",
      "epoch: 73,  batch step: 250, loss: 2.500187397003174\n",
      "epoch: 73,  batch step: 251, loss: 24.8265380859375\n",
      "validation error epoch  73:    tensor(67.8400, device='cuda:0')\n",
      "316\n",
      "epoch: 74,  batch step: 0, loss: 14.286406517028809\n",
      "epoch: 74,  batch step: 1, loss: 14.065673828125\n",
      "epoch: 74,  batch step: 2, loss: 15.299276351928711\n",
      "epoch: 74,  batch step: 3, loss: 12.135828971862793\n",
      "epoch: 74,  batch step: 4, loss: 4.412461757659912\n",
      "epoch: 74,  batch step: 5, loss: 12.558469772338867\n",
      "epoch: 74,  batch step: 6, loss: 50.75179672241211\n",
      "epoch: 74,  batch step: 7, loss: 40.344905853271484\n",
      "epoch: 74,  batch step: 8, loss: 3.671741008758545\n",
      "epoch: 74,  batch step: 9, loss: 3.7546441555023193\n",
      "epoch: 74,  batch step: 10, loss: 2.3159642219543457\n",
      "epoch: 74,  batch step: 11, loss: 12.822147369384766\n",
      "epoch: 74,  batch step: 12, loss: 3.0350160598754883\n",
      "epoch: 74,  batch step: 13, loss: 2.8867616653442383\n",
      "epoch: 74,  batch step: 14, loss: 11.603767395019531\n",
      "epoch: 74,  batch step: 15, loss: 2.5389020442962646\n",
      "epoch: 74,  batch step: 16, loss: 3.883493423461914\n",
      "epoch: 74,  batch step: 17, loss: 11.557083129882812\n",
      "epoch: 74,  batch step: 18, loss: 11.708949089050293\n",
      "epoch: 74,  batch step: 19, loss: 7.544981002807617\n",
      "epoch: 74,  batch step: 20, loss: 39.86479568481445\n",
      "epoch: 74,  batch step: 21, loss: 1.9929912090301514\n",
      "epoch: 74,  batch step: 22, loss: 2.3111319541931152\n",
      "epoch: 74,  batch step: 23, loss: 21.37503433227539\n",
      "epoch: 74,  batch step: 24, loss: 4.862326145172119\n",
      "epoch: 74,  batch step: 25, loss: 29.837989807128906\n",
      "epoch: 74,  batch step: 26, loss: 2.4884603023529053\n",
      "epoch: 74,  batch step: 27, loss: 8.510223388671875\n",
      "epoch: 74,  batch step: 28, loss: 3.2618064880371094\n",
      "epoch: 74,  batch step: 29, loss: 2.8483188152313232\n",
      "epoch: 74,  batch step: 30, loss: 2.2615652084350586\n",
      "epoch: 74,  batch step: 31, loss: 3.987992763519287\n",
      "epoch: 74,  batch step: 32, loss: 3.670557975769043\n",
      "epoch: 74,  batch step: 33, loss: 2.487536907196045\n",
      "epoch: 74,  batch step: 34, loss: 37.495704650878906\n",
      "epoch: 74,  batch step: 35, loss: 2.6308176517486572\n",
      "epoch: 74,  batch step: 36, loss: 2.3943209648132324\n",
      "epoch: 74,  batch step: 37, loss: 15.979375839233398\n",
      "epoch: 74,  batch step: 38, loss: 4.819398403167725\n",
      "epoch: 74,  batch step: 39, loss: 29.923274993896484\n",
      "epoch: 74,  batch step: 40, loss: 19.842178344726562\n",
      "epoch: 74,  batch step: 41, loss: 66.14653778076172\n",
      "epoch: 74,  batch step: 42, loss: 34.49998474121094\n",
      "epoch: 74,  batch step: 43, loss: 3.101583480834961\n",
      "epoch: 74,  batch step: 44, loss: 9.987960815429688\n",
      "epoch: 74,  batch step: 45, loss: 56.90745162963867\n",
      "epoch: 74,  batch step: 46, loss: 7.426000118255615\n",
      "epoch: 74,  batch step: 47, loss: 2.8135361671447754\n",
      "epoch: 74,  batch step: 48, loss: 31.984710693359375\n",
      "epoch: 74,  batch step: 49, loss: 5.7926764488220215\n",
      "epoch: 74,  batch step: 50, loss: 29.53803062438965\n",
      "epoch: 74,  batch step: 51, loss: 8.275430679321289\n",
      "epoch: 74,  batch step: 52, loss: 2.984494686126709\n",
      "epoch: 74,  batch step: 53, loss: 3.064687728881836\n",
      "epoch: 74,  batch step: 54, loss: 51.47380065917969\n",
      "epoch: 74,  batch step: 55, loss: 15.743598937988281\n",
      "epoch: 74,  batch step: 56, loss: 2.122452735900879\n",
      "epoch: 74,  batch step: 57, loss: 18.52640151977539\n",
      "epoch: 74,  batch step: 58, loss: 3.141636610031128\n",
      "epoch: 74,  batch step: 59, loss: 3.500730276107788\n",
      "epoch: 74,  batch step: 60, loss: 35.13097381591797\n",
      "epoch: 74,  batch step: 61, loss: 21.401853561401367\n",
      "epoch: 74,  batch step: 62, loss: 30.18142318725586\n",
      "epoch: 74,  batch step: 63, loss: 19.720674514770508\n",
      "epoch: 74,  batch step: 64, loss: 5.645842552185059\n",
      "epoch: 74,  batch step: 65, loss: 3.3037891387939453\n",
      "epoch: 74,  batch step: 66, loss: 4.930220603942871\n",
      "epoch: 74,  batch step: 67, loss: 10.921173095703125\n",
      "epoch: 74,  batch step: 68, loss: 10.177428245544434\n",
      "epoch: 74,  batch step: 69, loss: 4.901620388031006\n",
      "epoch: 74,  batch step: 70, loss: 2.874293327331543\n",
      "epoch: 74,  batch step: 71, loss: 3.1175007820129395\n",
      "epoch: 74,  batch step: 72, loss: 16.87514877319336\n",
      "epoch: 74,  batch step: 73, loss: 36.239383697509766\n",
      "epoch: 74,  batch step: 74, loss: 9.105620384216309\n",
      "epoch: 74,  batch step: 75, loss: 46.493690490722656\n",
      "epoch: 74,  batch step: 76, loss: 2.632357597351074\n",
      "epoch: 74,  batch step: 77, loss: 4.643714427947998\n",
      "epoch: 74,  batch step: 78, loss: 5.1984758377075195\n",
      "epoch: 74,  batch step: 79, loss: 30.51049041748047\n",
      "epoch: 74,  batch step: 80, loss: 52.580833435058594\n",
      "epoch: 74,  batch step: 81, loss: 45.39195251464844\n",
      "epoch: 74,  batch step: 82, loss: 10.538996696472168\n",
      "epoch: 74,  batch step: 83, loss: 24.9013671875\n",
      "epoch: 74,  batch step: 84, loss: 3.6611318588256836\n",
      "epoch: 74,  batch step: 85, loss: 39.28696823120117\n",
      "epoch: 74,  batch step: 86, loss: 57.82195281982422\n",
      "epoch: 74,  batch step: 87, loss: 26.34675407409668\n",
      "epoch: 74,  batch step: 88, loss: 16.901290893554688\n",
      "epoch: 74,  batch step: 89, loss: 3.1954073905944824\n",
      "epoch: 74,  batch step: 90, loss: 8.931575775146484\n",
      "epoch: 74,  batch step: 91, loss: 11.496000289916992\n",
      "epoch: 74,  batch step: 92, loss: 4.966294288635254\n",
      "epoch: 74,  batch step: 93, loss: 4.584527015686035\n",
      "epoch: 74,  batch step: 94, loss: 2.8641791343688965\n",
      "epoch: 74,  batch step: 95, loss: 2.1965084075927734\n",
      "epoch: 74,  batch step: 96, loss: 2.926208257675171\n",
      "epoch: 74,  batch step: 97, loss: 12.269266128540039\n",
      "epoch: 74,  batch step: 98, loss: 9.945021629333496\n",
      "epoch: 74,  batch step: 99, loss: 5.381234645843506\n",
      "epoch: 74,  batch step: 100, loss: 11.293413162231445\n",
      "epoch: 74,  batch step: 101, loss: 6.646329402923584\n",
      "epoch: 74,  batch step: 102, loss: 8.003107070922852\n",
      "epoch: 74,  batch step: 103, loss: 2.768899917602539\n",
      "epoch: 74,  batch step: 104, loss: 27.134666442871094\n",
      "epoch: 74,  batch step: 105, loss: 10.149107933044434\n",
      "epoch: 74,  batch step: 106, loss: 22.226852416992188\n",
      "epoch: 74,  batch step: 107, loss: 2.145482063293457\n",
      "epoch: 74,  batch step: 108, loss: 3.649935722351074\n",
      "epoch: 74,  batch step: 109, loss: 10.210643768310547\n",
      "epoch: 74,  batch step: 110, loss: 12.233865737915039\n",
      "epoch: 74,  batch step: 111, loss: 7.247308254241943\n",
      "epoch: 74,  batch step: 112, loss: 34.93052291870117\n",
      "epoch: 74,  batch step: 113, loss: 2.4114346504211426\n",
      "epoch: 74,  batch step: 114, loss: 4.417093753814697\n",
      "epoch: 74,  batch step: 115, loss: 9.470267295837402\n",
      "epoch: 74,  batch step: 116, loss: 15.035632133483887\n",
      "epoch: 74,  batch step: 117, loss: 8.60274887084961\n",
      "epoch: 74,  batch step: 118, loss: 28.4530029296875\n",
      "epoch: 74,  batch step: 119, loss: 32.05885314941406\n",
      "epoch: 74,  batch step: 120, loss: 38.40031433105469\n",
      "epoch: 74,  batch step: 121, loss: 5.121349811553955\n",
      "epoch: 74,  batch step: 122, loss: 2.417247772216797\n",
      "epoch: 74,  batch step: 123, loss: 2.8077683448791504\n",
      "epoch: 74,  batch step: 124, loss: 3.9471852779388428\n",
      "epoch: 74,  batch step: 125, loss: 2.4778828620910645\n",
      "epoch: 74,  batch step: 126, loss: 3.5426387786865234\n",
      "epoch: 74,  batch step: 127, loss: 53.23531723022461\n",
      "epoch: 74,  batch step: 128, loss: 13.109231948852539\n",
      "epoch: 74,  batch step: 129, loss: 2.869771957397461\n",
      "epoch: 74,  batch step: 130, loss: 2.3219151496887207\n",
      "epoch: 74,  batch step: 131, loss: 25.888851165771484\n",
      "epoch: 74,  batch step: 132, loss: 37.08411407470703\n",
      "epoch: 74,  batch step: 133, loss: 2.2324657440185547\n",
      "epoch: 74,  batch step: 134, loss: 29.487926483154297\n",
      "epoch: 74,  batch step: 135, loss: 2.281651258468628\n",
      "epoch: 74,  batch step: 136, loss: 61.07081604003906\n",
      "epoch: 74,  batch step: 137, loss: 2.6425061225891113\n",
      "epoch: 74,  batch step: 138, loss: 18.68869400024414\n",
      "epoch: 74,  batch step: 139, loss: 2.675518751144409\n",
      "epoch: 74,  batch step: 140, loss: 4.191802978515625\n",
      "epoch: 74,  batch step: 141, loss: 36.99214553833008\n",
      "epoch: 74,  batch step: 142, loss: 1.7808105945587158\n",
      "epoch: 74,  batch step: 143, loss: 19.196548461914062\n",
      "epoch: 74,  batch step: 144, loss: 9.778810501098633\n",
      "epoch: 74,  batch step: 145, loss: 2.252572536468506\n",
      "epoch: 74,  batch step: 146, loss: 4.579995155334473\n",
      "epoch: 74,  batch step: 147, loss: 14.627994537353516\n",
      "epoch: 74,  batch step: 148, loss: 18.79073715209961\n",
      "epoch: 74,  batch step: 149, loss: 11.696871757507324\n",
      "epoch: 74,  batch step: 150, loss: 23.39236068725586\n",
      "epoch: 74,  batch step: 151, loss: 47.27355194091797\n",
      "epoch: 74,  batch step: 152, loss: 20.234745025634766\n",
      "epoch: 74,  batch step: 153, loss: 17.12896156311035\n",
      "epoch: 74,  batch step: 154, loss: 18.851003646850586\n",
      "epoch: 74,  batch step: 155, loss: 9.50487232208252\n",
      "epoch: 74,  batch step: 156, loss: 28.47247314453125\n",
      "epoch: 74,  batch step: 157, loss: 3.889655113220215\n",
      "epoch: 74,  batch step: 158, loss: 3.5226669311523438\n",
      "epoch: 74,  batch step: 159, loss: 9.61897087097168\n",
      "epoch: 74,  batch step: 160, loss: 3.3206329345703125\n",
      "epoch: 74,  batch step: 161, loss: 1.8379316329956055\n",
      "epoch: 74,  batch step: 162, loss: 11.627245903015137\n",
      "epoch: 74,  batch step: 163, loss: 2.283846855163574\n",
      "epoch: 74,  batch step: 164, loss: 3.7404122352600098\n",
      "epoch: 74,  batch step: 165, loss: 1.9496278762817383\n",
      "epoch: 74,  batch step: 166, loss: 13.368963241577148\n",
      "epoch: 74,  batch step: 167, loss: 2.5958685874938965\n",
      "epoch: 74,  batch step: 168, loss: 3.186383008956909\n",
      "epoch: 74,  batch step: 169, loss: 11.780180931091309\n",
      "epoch: 74,  batch step: 170, loss: 4.321849822998047\n",
      "epoch: 74,  batch step: 171, loss: 3.569197416305542\n",
      "epoch: 74,  batch step: 172, loss: 1.959542155265808\n",
      "epoch: 74,  batch step: 173, loss: 10.088186264038086\n",
      "epoch: 74,  batch step: 174, loss: 2.9699459075927734\n",
      "epoch: 74,  batch step: 175, loss: 1.921189785003662\n",
      "epoch: 74,  batch step: 176, loss: 2.6986732482910156\n",
      "epoch: 74,  batch step: 177, loss: 2.6794631481170654\n",
      "epoch: 74,  batch step: 178, loss: 8.260334968566895\n",
      "epoch: 74,  batch step: 179, loss: 18.547687530517578\n",
      "epoch: 74,  batch step: 180, loss: 2.2170960903167725\n",
      "epoch: 74,  batch step: 181, loss: 2.6205756664276123\n",
      "epoch: 74,  batch step: 182, loss: 4.010302543640137\n",
      "epoch: 74,  batch step: 183, loss: 10.553766250610352\n",
      "epoch: 74,  batch step: 184, loss: 3.1610560417175293\n",
      "epoch: 74,  batch step: 185, loss: 3.6665217876434326\n",
      "epoch: 74,  batch step: 186, loss: 2.605502128601074\n",
      "epoch: 74,  batch step: 187, loss: 12.123565673828125\n",
      "epoch: 74,  batch step: 188, loss: 12.131902694702148\n",
      "epoch: 74,  batch step: 189, loss: 10.856466293334961\n",
      "epoch: 74,  batch step: 190, loss: 19.534286499023438\n",
      "epoch: 74,  batch step: 191, loss: 1.5096077919006348\n",
      "epoch: 74,  batch step: 192, loss: 3.2123517990112305\n",
      "epoch: 74,  batch step: 193, loss: 2.443434476852417\n",
      "epoch: 74,  batch step: 194, loss: 13.231164932250977\n",
      "epoch: 74,  batch step: 195, loss: 2.6575231552124023\n",
      "epoch: 74,  batch step: 196, loss: 2.410416603088379\n",
      "epoch: 74,  batch step: 197, loss: 35.9396858215332\n",
      "epoch: 74,  batch step: 198, loss: 41.20951843261719\n",
      "epoch: 74,  batch step: 199, loss: 2.618274211883545\n",
      "epoch: 74,  batch step: 200, loss: 13.048219680786133\n",
      "epoch: 74,  batch step: 201, loss: 10.412656784057617\n",
      "epoch: 74,  batch step: 202, loss: 4.146846771240234\n",
      "epoch: 74,  batch step: 203, loss: 2.9972474575042725\n",
      "epoch: 74,  batch step: 204, loss: 1.9955549240112305\n",
      "epoch: 74,  batch step: 205, loss: 9.52839469909668\n",
      "epoch: 74,  batch step: 206, loss: 7.626868724822998\n",
      "epoch: 74,  batch step: 207, loss: 3.8781487941741943\n",
      "epoch: 74,  batch step: 208, loss: 10.515135765075684\n",
      "epoch: 74,  batch step: 209, loss: 51.5664176940918\n",
      "epoch: 74,  batch step: 210, loss: 3.4979453086853027\n",
      "epoch: 74,  batch step: 211, loss: 4.587948799133301\n",
      "epoch: 74,  batch step: 212, loss: 31.064151763916016\n",
      "epoch: 74,  batch step: 213, loss: 9.344667434692383\n",
      "epoch: 74,  batch step: 214, loss: 31.584136962890625\n",
      "epoch: 74,  batch step: 215, loss: 15.184551239013672\n",
      "epoch: 74,  batch step: 216, loss: 24.11531639099121\n",
      "epoch: 74,  batch step: 217, loss: 6.12927770614624\n",
      "epoch: 74,  batch step: 218, loss: 79.50555419921875\n",
      "epoch: 74,  batch step: 219, loss: 14.6995267868042\n",
      "epoch: 74,  batch step: 220, loss: 2.3748226165771484\n",
      "epoch: 74,  batch step: 221, loss: 3.021139144897461\n",
      "epoch: 74,  batch step: 222, loss: 2.018402338027954\n",
      "epoch: 74,  batch step: 223, loss: 5.723113059997559\n",
      "epoch: 74,  batch step: 224, loss: 11.823270797729492\n",
      "epoch: 74,  batch step: 225, loss: 2.551621675491333\n",
      "epoch: 74,  batch step: 226, loss: 4.250234127044678\n",
      "epoch: 74,  batch step: 227, loss: 2.3952059745788574\n",
      "epoch: 74,  batch step: 228, loss: 2.0873169898986816\n",
      "epoch: 74,  batch step: 229, loss: 11.697948455810547\n",
      "epoch: 74,  batch step: 230, loss: 3.667112350463867\n",
      "epoch: 74,  batch step: 231, loss: 4.085432052612305\n",
      "epoch: 74,  batch step: 232, loss: 2.8603954315185547\n",
      "epoch: 74,  batch step: 233, loss: 2.6162216663360596\n",
      "epoch: 74,  batch step: 234, loss: 2.880121946334839\n",
      "epoch: 74,  batch step: 235, loss: 3.1426942348480225\n",
      "epoch: 74,  batch step: 236, loss: 15.26083755493164\n",
      "epoch: 74,  batch step: 237, loss: 2.0438072681427\n",
      "epoch: 74,  batch step: 238, loss: 2.588779926300049\n",
      "epoch: 74,  batch step: 239, loss: 6.353230953216553\n",
      "epoch: 74,  batch step: 240, loss: 2.1937999725341797\n",
      "epoch: 74,  batch step: 241, loss: 141.73056030273438\n",
      "epoch: 74,  batch step: 242, loss: 4.7390289306640625\n",
      "epoch: 74,  batch step: 243, loss: 1.8444904088974\n",
      "epoch: 74,  batch step: 244, loss: 67.87873840332031\n",
      "epoch: 74,  batch step: 245, loss: 25.12154197692871\n",
      "epoch: 74,  batch step: 246, loss: 3.631376266479492\n",
      "epoch: 74,  batch step: 247, loss: 9.091543197631836\n",
      "epoch: 74,  batch step: 248, loss: 3.696300506591797\n",
      "epoch: 74,  batch step: 249, loss: 11.631175994873047\n",
      "epoch: 74,  batch step: 250, loss: 22.386402130126953\n",
      "epoch: 74,  batch step: 251, loss: 3.1974496841430664\n",
      "validation error epoch  74:    tensor(66.5472, device='cuda:0')\n",
      "316\n",
      "epoch: 75,  batch step: 0, loss: 40.385406494140625\n",
      "epoch: 75,  batch step: 1, loss: 2.9447522163391113\n",
      "epoch: 75,  batch step: 2, loss: 1.7510669231414795\n",
      "epoch: 75,  batch step: 3, loss: 2.7880191802978516\n",
      "epoch: 75,  batch step: 4, loss: 1.668792486190796\n",
      "epoch: 75,  batch step: 5, loss: 4.334544658660889\n",
      "epoch: 75,  batch step: 6, loss: 10.168704986572266\n",
      "epoch: 75,  batch step: 7, loss: 4.221744537353516\n",
      "epoch: 75,  batch step: 8, loss: 11.672021865844727\n",
      "epoch: 75,  batch step: 9, loss: 8.382912635803223\n",
      "epoch: 75,  batch step: 10, loss: 12.937946319580078\n",
      "epoch: 75,  batch step: 11, loss: 3.4213767051696777\n",
      "epoch: 75,  batch step: 12, loss: 2.4060921669006348\n",
      "epoch: 75,  batch step: 13, loss: 2.1885688304901123\n",
      "epoch: 75,  batch step: 14, loss: 76.8190689086914\n",
      "epoch: 75,  batch step: 15, loss: 10.486184120178223\n",
      "epoch: 75,  batch step: 16, loss: 1.874511480331421\n",
      "epoch: 75,  batch step: 17, loss: 12.151476860046387\n",
      "epoch: 75,  batch step: 18, loss: 3.1475493907928467\n",
      "epoch: 75,  batch step: 19, loss: 5.1747541427612305\n",
      "epoch: 75,  batch step: 20, loss: 2.1985483169555664\n",
      "epoch: 75,  batch step: 21, loss: 9.016585350036621\n",
      "epoch: 75,  batch step: 22, loss: 2.6649904251098633\n",
      "epoch: 75,  batch step: 23, loss: 30.75038719177246\n",
      "epoch: 75,  batch step: 24, loss: 21.62567710876465\n",
      "epoch: 75,  batch step: 25, loss: 1.792838454246521\n",
      "epoch: 75,  batch step: 26, loss: 5.988627910614014\n",
      "epoch: 75,  batch step: 27, loss: 2.001368522644043\n",
      "epoch: 75,  batch step: 28, loss: 9.826460838317871\n",
      "epoch: 75,  batch step: 29, loss: 6.216351509094238\n",
      "epoch: 75,  batch step: 30, loss: 1.978370189666748\n",
      "epoch: 75,  batch step: 31, loss: 1.7166194915771484\n",
      "epoch: 75,  batch step: 32, loss: 2.816317081451416\n",
      "epoch: 75,  batch step: 33, loss: 3.3259716033935547\n",
      "epoch: 75,  batch step: 34, loss: 2.026630401611328\n",
      "epoch: 75,  batch step: 35, loss: 2.837690830230713\n",
      "epoch: 75,  batch step: 36, loss: 2.626246929168701\n",
      "epoch: 75,  batch step: 37, loss: 68.57178497314453\n",
      "epoch: 75,  batch step: 38, loss: 2.1476807594299316\n",
      "epoch: 75,  batch step: 39, loss: 2.154632806777954\n",
      "epoch: 75,  batch step: 40, loss: 4.618958473205566\n",
      "epoch: 75,  batch step: 41, loss: 4.458852291107178\n",
      "epoch: 75,  batch step: 42, loss: 18.021753311157227\n",
      "epoch: 75,  batch step: 43, loss: 11.351499557495117\n",
      "epoch: 75,  batch step: 44, loss: 19.565353393554688\n",
      "epoch: 75,  batch step: 45, loss: 4.646900653839111\n",
      "epoch: 75,  batch step: 46, loss: 10.17561149597168\n",
      "epoch: 75,  batch step: 47, loss: 1.655809760093689\n",
      "epoch: 75,  batch step: 48, loss: 2.199404716491699\n",
      "epoch: 75,  batch step: 49, loss: 6.292393684387207\n",
      "epoch: 75,  batch step: 50, loss: 8.230960845947266\n",
      "epoch: 75,  batch step: 51, loss: 3.3019142150878906\n",
      "epoch: 75,  batch step: 52, loss: 12.905795097351074\n",
      "epoch: 75,  batch step: 53, loss: 25.486785888671875\n",
      "epoch: 75,  batch step: 54, loss: 12.253154754638672\n",
      "epoch: 75,  batch step: 55, loss: 8.900312423706055\n",
      "epoch: 75,  batch step: 56, loss: 2.0935819149017334\n",
      "epoch: 75,  batch step: 57, loss: 2.482876777648926\n",
      "epoch: 75,  batch step: 58, loss: 23.25811004638672\n",
      "epoch: 75,  batch step: 59, loss: 2.2160274982452393\n",
      "epoch: 75,  batch step: 60, loss: 2.397916078567505\n",
      "epoch: 75,  batch step: 61, loss: 2.6434922218322754\n",
      "epoch: 75,  batch step: 62, loss: 133.96145629882812\n",
      "epoch: 75,  batch step: 63, loss: 2.0614843368530273\n",
      "epoch: 75,  batch step: 64, loss: 3.1904430389404297\n",
      "epoch: 75,  batch step: 65, loss: 5.290706634521484\n",
      "epoch: 75,  batch step: 66, loss: 34.60764694213867\n",
      "epoch: 75,  batch step: 67, loss: 42.586090087890625\n",
      "epoch: 75,  batch step: 68, loss: 2.9284026622772217\n",
      "epoch: 75,  batch step: 69, loss: 52.5688591003418\n",
      "epoch: 75,  batch step: 70, loss: 23.25394058227539\n",
      "epoch: 75,  batch step: 71, loss: 35.6341438293457\n",
      "epoch: 75,  batch step: 72, loss: 7.1084113121032715\n",
      "epoch: 75,  batch step: 73, loss: 32.5953254699707\n",
      "epoch: 75,  batch step: 74, loss: 2.5797314643859863\n",
      "epoch: 75,  batch step: 75, loss: 3.367356777191162\n",
      "epoch: 75,  batch step: 76, loss: 32.81514358520508\n",
      "epoch: 75,  batch step: 77, loss: 3.157100200653076\n",
      "epoch: 75,  batch step: 78, loss: 3.857433319091797\n",
      "epoch: 75,  batch step: 79, loss: 22.264835357666016\n",
      "epoch: 75,  batch step: 80, loss: 58.08129119873047\n",
      "epoch: 75,  batch step: 81, loss: 17.84259033203125\n",
      "epoch: 75,  batch step: 82, loss: 36.12560272216797\n",
      "epoch: 75,  batch step: 83, loss: 67.82561492919922\n",
      "epoch: 75,  batch step: 84, loss: 27.3264217376709\n",
      "epoch: 75,  batch step: 85, loss: 5.890193939208984\n",
      "epoch: 75,  batch step: 86, loss: 14.34620189666748\n",
      "epoch: 75,  batch step: 87, loss: 14.727676391601562\n",
      "epoch: 75,  batch step: 88, loss: 4.319652557373047\n",
      "epoch: 75,  batch step: 89, loss: 4.59991455078125\n",
      "epoch: 75,  batch step: 90, loss: 4.317526817321777\n",
      "epoch: 75,  batch step: 91, loss: 4.206525802612305\n",
      "epoch: 75,  batch step: 92, loss: 17.09968376159668\n",
      "epoch: 75,  batch step: 93, loss: 72.24861145019531\n",
      "epoch: 75,  batch step: 94, loss: 35.207279205322266\n",
      "epoch: 75,  batch step: 95, loss: 23.73546600341797\n",
      "epoch: 75,  batch step: 96, loss: 4.2521467208862305\n",
      "epoch: 75,  batch step: 97, loss: 11.333728790283203\n",
      "epoch: 75,  batch step: 98, loss: 4.458017349243164\n",
      "epoch: 75,  batch step: 99, loss: 66.2412109375\n",
      "epoch: 75,  batch step: 100, loss: 4.461987495422363\n",
      "epoch: 75,  batch step: 101, loss: 56.72077178955078\n",
      "epoch: 75,  batch step: 102, loss: 2.9824070930480957\n",
      "epoch: 75,  batch step: 103, loss: 4.788835525512695\n",
      "epoch: 75,  batch step: 104, loss: 5.538051605224609\n",
      "epoch: 75,  batch step: 105, loss: 79.49513244628906\n",
      "epoch: 75,  batch step: 106, loss: 12.583425521850586\n",
      "epoch: 75,  batch step: 107, loss: 16.59360122680664\n",
      "epoch: 75,  batch step: 108, loss: 5.802349090576172\n",
      "epoch: 75,  batch step: 109, loss: 64.95137786865234\n",
      "epoch: 75,  batch step: 110, loss: 18.29157829284668\n",
      "epoch: 75,  batch step: 111, loss: 9.107843399047852\n",
      "epoch: 75,  batch step: 112, loss: 27.756778717041016\n",
      "epoch: 75,  batch step: 113, loss: 4.415637016296387\n",
      "epoch: 75,  batch step: 114, loss: 39.057430267333984\n",
      "epoch: 75,  batch step: 115, loss: 14.424491882324219\n",
      "epoch: 75,  batch step: 116, loss: 3.312643051147461\n",
      "epoch: 75,  batch step: 117, loss: 10.61633586883545\n",
      "epoch: 75,  batch step: 118, loss: 75.03931427001953\n",
      "epoch: 75,  batch step: 119, loss: 6.250859260559082\n",
      "epoch: 75,  batch step: 120, loss: 16.623565673828125\n",
      "epoch: 75,  batch step: 121, loss: 20.484792709350586\n",
      "epoch: 75,  batch step: 122, loss: 5.248034477233887\n",
      "epoch: 75,  batch step: 123, loss: 38.26648712158203\n",
      "epoch: 75,  batch step: 124, loss: 5.9059576988220215\n",
      "epoch: 75,  batch step: 125, loss: 7.4390387535095215\n",
      "epoch: 75,  batch step: 126, loss: 8.704612731933594\n",
      "epoch: 75,  batch step: 127, loss: 74.6771240234375\n",
      "epoch: 75,  batch step: 128, loss: 45.318382263183594\n",
      "epoch: 75,  batch step: 129, loss: 5.368730068206787\n",
      "epoch: 75,  batch step: 130, loss: 4.558700084686279\n",
      "epoch: 75,  batch step: 131, loss: 3.7488627433776855\n",
      "epoch: 75,  batch step: 132, loss: 73.27793884277344\n",
      "epoch: 75,  batch step: 133, loss: 33.3541374206543\n",
      "epoch: 75,  batch step: 134, loss: 6.339969158172607\n",
      "epoch: 75,  batch step: 135, loss: 5.0548624992370605\n",
      "epoch: 75,  batch step: 136, loss: 10.926109313964844\n",
      "epoch: 75,  batch step: 137, loss: 149.26097106933594\n",
      "epoch: 75,  batch step: 138, loss: 19.152385711669922\n",
      "epoch: 75,  batch step: 139, loss: 12.360182762145996\n",
      "epoch: 75,  batch step: 140, loss: 72.08255004882812\n",
      "epoch: 75,  batch step: 141, loss: 54.544830322265625\n",
      "epoch: 75,  batch step: 142, loss: 26.757282257080078\n",
      "epoch: 75,  batch step: 143, loss: 21.242929458618164\n",
      "epoch: 75,  batch step: 144, loss: 4.683338165283203\n",
      "epoch: 75,  batch step: 145, loss: 20.759113311767578\n",
      "epoch: 75,  batch step: 146, loss: 8.876133918762207\n",
      "epoch: 75,  batch step: 147, loss: 32.271644592285156\n",
      "epoch: 75,  batch step: 148, loss: 19.228553771972656\n",
      "epoch: 75,  batch step: 149, loss: 55.00663375854492\n",
      "epoch: 75,  batch step: 150, loss: 3.995056629180908\n",
      "epoch: 75,  batch step: 151, loss: 16.727802276611328\n",
      "epoch: 75,  batch step: 152, loss: 51.31600570678711\n",
      "epoch: 75,  batch step: 153, loss: 15.789325714111328\n",
      "epoch: 75,  batch step: 154, loss: 15.8179349899292\n",
      "epoch: 75,  batch step: 155, loss: 8.281332015991211\n",
      "epoch: 75,  batch step: 156, loss: 3.338505744934082\n",
      "epoch: 75,  batch step: 157, loss: 4.377958297729492\n",
      "epoch: 75,  batch step: 158, loss: 28.39834976196289\n",
      "epoch: 75,  batch step: 159, loss: 14.804269790649414\n",
      "epoch: 75,  batch step: 160, loss: 4.162877559661865\n",
      "epoch: 75,  batch step: 161, loss: 21.704627990722656\n",
      "epoch: 75,  batch step: 162, loss: 5.786046504974365\n",
      "epoch: 75,  batch step: 163, loss: 5.537065029144287\n",
      "epoch: 75,  batch step: 164, loss: 6.425472259521484\n",
      "epoch: 75,  batch step: 165, loss: 32.17148971557617\n",
      "epoch: 75,  batch step: 166, loss: 4.073075294494629\n",
      "epoch: 75,  batch step: 167, loss: 18.977569580078125\n",
      "epoch: 75,  batch step: 168, loss: 6.817088603973389\n",
      "epoch: 75,  batch step: 169, loss: 3.739262342453003\n",
      "epoch: 75,  batch step: 170, loss: 4.442480564117432\n",
      "epoch: 75,  batch step: 171, loss: 41.504066467285156\n",
      "epoch: 75,  batch step: 172, loss: 33.5534553527832\n",
      "epoch: 75,  batch step: 173, loss: 6.515623092651367\n",
      "epoch: 75,  batch step: 174, loss: 5.363936901092529\n",
      "epoch: 75,  batch step: 175, loss: 31.895612716674805\n",
      "epoch: 75,  batch step: 176, loss: 8.343866348266602\n",
      "epoch: 75,  batch step: 177, loss: 67.2633056640625\n",
      "epoch: 75,  batch step: 178, loss: 2.7172012329101562\n",
      "epoch: 75,  batch step: 179, loss: 18.457660675048828\n",
      "epoch: 75,  batch step: 180, loss: 3.9187703132629395\n",
      "epoch: 75,  batch step: 181, loss: 13.009963989257812\n",
      "epoch: 75,  batch step: 182, loss: 2.8974483013153076\n",
      "epoch: 75,  batch step: 183, loss: 10.731101989746094\n",
      "epoch: 75,  batch step: 184, loss: 31.394638061523438\n",
      "epoch: 75,  batch step: 185, loss: 51.485328674316406\n",
      "epoch: 75,  batch step: 186, loss: 4.160698890686035\n",
      "epoch: 75,  batch step: 187, loss: 7.790946006774902\n",
      "epoch: 75,  batch step: 188, loss: 64.70614624023438\n",
      "epoch: 75,  batch step: 189, loss: 20.255016326904297\n",
      "epoch: 75,  batch step: 190, loss: 2.358642101287842\n",
      "epoch: 75,  batch step: 191, loss: 4.401855945587158\n",
      "epoch: 75,  batch step: 192, loss: 3.223580837249756\n",
      "epoch: 75,  batch step: 193, loss: 71.07437896728516\n",
      "epoch: 75,  batch step: 194, loss: 54.098663330078125\n",
      "epoch: 75,  batch step: 195, loss: 4.278449535369873\n",
      "epoch: 75,  batch step: 196, loss: 2.9897818565368652\n",
      "epoch: 75,  batch step: 197, loss: 8.606866836547852\n",
      "epoch: 75,  batch step: 198, loss: 4.052248477935791\n",
      "epoch: 75,  batch step: 199, loss: 32.609519958496094\n",
      "epoch: 75,  batch step: 200, loss: 14.07124137878418\n",
      "epoch: 75,  batch step: 201, loss: 22.124011993408203\n",
      "epoch: 75,  batch step: 202, loss: 4.3708319664001465\n",
      "epoch: 75,  batch step: 203, loss: 34.727325439453125\n",
      "epoch: 75,  batch step: 204, loss: 29.879117965698242\n",
      "epoch: 75,  batch step: 205, loss: 39.047855377197266\n",
      "epoch: 75,  batch step: 206, loss: 3.7306485176086426\n",
      "epoch: 75,  batch step: 207, loss: 4.061221122741699\n",
      "epoch: 75,  batch step: 208, loss: 18.938756942749023\n",
      "epoch: 75,  batch step: 209, loss: 2.5566463470458984\n",
      "epoch: 75,  batch step: 210, loss: 2.5466277599334717\n",
      "epoch: 75,  batch step: 211, loss: 66.1182861328125\n",
      "epoch: 75,  batch step: 212, loss: 3.2843003273010254\n",
      "epoch: 75,  batch step: 213, loss: 31.92776107788086\n",
      "epoch: 75,  batch step: 214, loss: 40.61860275268555\n",
      "epoch: 75,  batch step: 215, loss: 19.745269775390625\n",
      "epoch: 75,  batch step: 216, loss: 3.0798964500427246\n",
      "epoch: 75,  batch step: 217, loss: 51.830039978027344\n",
      "epoch: 75,  batch step: 218, loss: 47.80763244628906\n",
      "epoch: 75,  batch step: 219, loss: 5.072715759277344\n",
      "epoch: 75,  batch step: 220, loss: 3.93678617477417\n",
      "epoch: 75,  batch step: 221, loss: 20.515085220336914\n",
      "epoch: 75,  batch step: 222, loss: 12.360151290893555\n",
      "epoch: 75,  batch step: 223, loss: 25.46673583984375\n",
      "epoch: 75,  batch step: 224, loss: 19.240203857421875\n",
      "epoch: 75,  batch step: 225, loss: 46.91838073730469\n",
      "epoch: 75,  batch step: 226, loss: 5.371870040893555\n",
      "epoch: 75,  batch step: 227, loss: 16.351533889770508\n",
      "epoch: 75,  batch step: 228, loss: 18.980791091918945\n",
      "epoch: 75,  batch step: 229, loss: 31.87842559814453\n",
      "epoch: 75,  batch step: 230, loss: 4.226180553436279\n",
      "epoch: 75,  batch step: 231, loss: 7.036508083343506\n",
      "epoch: 75,  batch step: 232, loss: 3.5048351287841797\n",
      "epoch: 75,  batch step: 233, loss: 12.61850357055664\n",
      "epoch: 75,  batch step: 234, loss: 2.1467459201812744\n",
      "epoch: 75,  batch step: 235, loss: 2.213352918624878\n",
      "epoch: 75,  batch step: 236, loss: 15.70302963256836\n",
      "epoch: 75,  batch step: 237, loss: 2.8035287857055664\n",
      "epoch: 75,  batch step: 238, loss: 27.048259735107422\n",
      "epoch: 75,  batch step: 239, loss: 26.54625701904297\n",
      "epoch: 75,  batch step: 240, loss: 3.0389881134033203\n",
      "epoch: 75,  batch step: 241, loss: 5.078822612762451\n",
      "epoch: 75,  batch step: 242, loss: 50.56919479370117\n",
      "epoch: 75,  batch step: 243, loss: 2.466248035430908\n",
      "epoch: 75,  batch step: 244, loss: 20.31402587890625\n",
      "epoch: 75,  batch step: 245, loss: 2.905251979827881\n",
      "epoch: 75,  batch step: 246, loss: 84.79749298095703\n",
      "epoch: 75,  batch step: 247, loss: 1.9061143398284912\n",
      "epoch: 75,  batch step: 248, loss: 20.604175567626953\n",
      "epoch: 75,  batch step: 249, loss: 2.640432834625244\n",
      "epoch: 75,  batch step: 250, loss: 3.4512643814086914\n",
      "epoch: 75,  batch step: 251, loss: 30.451997756958008\n",
      "validation error epoch  75:    tensor(67.0857, device='cuda:0')\n",
      "316\n",
      "epoch: 76,  batch step: 0, loss: 5.668642044067383\n",
      "epoch: 76,  batch step: 1, loss: 10.608652114868164\n",
      "epoch: 76,  batch step: 2, loss: 4.898383617401123\n",
      "epoch: 76,  batch step: 3, loss: 18.718563079833984\n",
      "epoch: 76,  batch step: 4, loss: 43.19847106933594\n",
      "epoch: 76,  batch step: 5, loss: 7.831925868988037\n",
      "epoch: 76,  batch step: 6, loss: 30.10275650024414\n",
      "epoch: 76,  batch step: 7, loss: 52.18000411987305\n",
      "epoch: 76,  batch step: 8, loss: 2.957637310028076\n",
      "epoch: 76,  batch step: 9, loss: 6.161563396453857\n",
      "epoch: 76,  batch step: 10, loss: 12.092966079711914\n",
      "epoch: 76,  batch step: 11, loss: 8.220002174377441\n",
      "epoch: 76,  batch step: 12, loss: 21.578868865966797\n",
      "epoch: 76,  batch step: 13, loss: 3.9469475746154785\n",
      "epoch: 76,  batch step: 14, loss: 34.288818359375\n",
      "epoch: 76,  batch step: 15, loss: 26.570964813232422\n",
      "epoch: 76,  batch step: 16, loss: 24.339155197143555\n",
      "epoch: 76,  batch step: 17, loss: 26.85181999206543\n",
      "epoch: 76,  batch step: 18, loss: 2.8594183921813965\n",
      "epoch: 76,  batch step: 19, loss: 2.035928249359131\n",
      "epoch: 76,  batch step: 20, loss: 3.722008466720581\n",
      "epoch: 76,  batch step: 21, loss: 2.256584882736206\n",
      "epoch: 76,  batch step: 22, loss: 3.287402629852295\n",
      "epoch: 76,  batch step: 23, loss: 3.897390842437744\n",
      "epoch: 76,  batch step: 24, loss: 43.40422821044922\n",
      "epoch: 76,  batch step: 25, loss: 2.2456705570220947\n",
      "epoch: 76,  batch step: 26, loss: 2.341735601425171\n",
      "epoch: 76,  batch step: 27, loss: 100.62506103515625\n",
      "epoch: 76,  batch step: 28, loss: 48.84829330444336\n",
      "epoch: 76,  batch step: 29, loss: 4.123640060424805\n",
      "epoch: 76,  batch step: 30, loss: 5.777774810791016\n",
      "epoch: 76,  batch step: 31, loss: 4.268501281738281\n",
      "epoch: 76,  batch step: 32, loss: 28.15074920654297\n",
      "epoch: 76,  batch step: 33, loss: 3.5035064220428467\n",
      "epoch: 76,  batch step: 34, loss: 13.363726615905762\n",
      "epoch: 76,  batch step: 35, loss: 3.4888124465942383\n",
      "epoch: 76,  batch step: 36, loss: 3.6558475494384766\n",
      "epoch: 76,  batch step: 37, loss: 8.845787048339844\n",
      "epoch: 76,  batch step: 38, loss: 5.383464336395264\n",
      "epoch: 76,  batch step: 39, loss: 14.785072326660156\n",
      "epoch: 76,  batch step: 40, loss: 27.70053482055664\n",
      "epoch: 76,  batch step: 41, loss: 7.004392147064209\n",
      "epoch: 76,  batch step: 42, loss: 101.05279541015625\n",
      "epoch: 76,  batch step: 43, loss: 29.55202293395996\n",
      "epoch: 76,  batch step: 44, loss: 39.47514343261719\n",
      "epoch: 76,  batch step: 45, loss: 20.431045532226562\n",
      "epoch: 76,  batch step: 46, loss: 26.277023315429688\n",
      "epoch: 76,  batch step: 47, loss: 47.66004943847656\n",
      "epoch: 76,  batch step: 48, loss: 37.69021224975586\n",
      "epoch: 76,  batch step: 49, loss: 10.209294319152832\n",
      "epoch: 76,  batch step: 50, loss: 3.538609027862549\n",
      "epoch: 76,  batch step: 51, loss: 8.129865646362305\n",
      "epoch: 76,  batch step: 52, loss: 52.662864685058594\n",
      "epoch: 76,  batch step: 53, loss: 20.94137191772461\n",
      "epoch: 76,  batch step: 54, loss: 7.072310447692871\n",
      "epoch: 76,  batch step: 55, loss: 13.093185424804688\n",
      "epoch: 76,  batch step: 56, loss: 52.809322357177734\n",
      "epoch: 76,  batch step: 57, loss: 11.677976608276367\n",
      "epoch: 76,  batch step: 58, loss: 42.73348617553711\n",
      "epoch: 76,  batch step: 59, loss: 107.21910095214844\n",
      "epoch: 76,  batch step: 60, loss: 12.650869369506836\n",
      "epoch: 76,  batch step: 61, loss: 24.34676170349121\n",
      "epoch: 76,  batch step: 62, loss: 5.035627365112305\n",
      "epoch: 76,  batch step: 63, loss: 68.66242980957031\n",
      "epoch: 76,  batch step: 64, loss: 7.203948974609375\n",
      "epoch: 76,  batch step: 65, loss: 15.654073715209961\n",
      "epoch: 76,  batch step: 66, loss: 6.809926509857178\n",
      "epoch: 76,  batch step: 67, loss: 115.94969177246094\n",
      "epoch: 76,  batch step: 68, loss: 33.284263610839844\n",
      "epoch: 76,  batch step: 69, loss: 18.61357879638672\n",
      "epoch: 76,  batch step: 70, loss: 4.205965995788574\n",
      "epoch: 76,  batch step: 71, loss: 10.92405891418457\n",
      "epoch: 76,  batch step: 72, loss: 9.688807487487793\n",
      "epoch: 76,  batch step: 73, loss: 78.48759460449219\n",
      "epoch: 76,  batch step: 74, loss: 85.90798950195312\n",
      "epoch: 76,  batch step: 75, loss: 24.494665145874023\n",
      "epoch: 76,  batch step: 76, loss: 6.365090370178223\n",
      "epoch: 76,  batch step: 77, loss: 5.937426567077637\n",
      "epoch: 76,  batch step: 78, loss: 49.93650817871094\n",
      "epoch: 76,  batch step: 79, loss: 23.63404083251953\n",
      "epoch: 76,  batch step: 80, loss: 6.759746074676514\n",
      "epoch: 76,  batch step: 81, loss: 4.342536449432373\n",
      "epoch: 76,  batch step: 82, loss: 11.460585594177246\n",
      "epoch: 76,  batch step: 83, loss: 3.990788221359253\n",
      "epoch: 76,  batch step: 84, loss: 28.79938507080078\n",
      "epoch: 76,  batch step: 85, loss: 11.314140319824219\n",
      "epoch: 76,  batch step: 86, loss: 21.94350242614746\n",
      "epoch: 76,  batch step: 87, loss: 29.259166717529297\n",
      "epoch: 76,  batch step: 88, loss: 38.48493957519531\n",
      "epoch: 76,  batch step: 89, loss: 6.0619215965271\n",
      "epoch: 76,  batch step: 90, loss: 57.59305953979492\n",
      "epoch: 76,  batch step: 91, loss: 7.603813171386719\n",
      "epoch: 76,  batch step: 92, loss: 25.430524826049805\n",
      "epoch: 76,  batch step: 93, loss: 4.023974895477295\n",
      "epoch: 76,  batch step: 94, loss: 19.231773376464844\n",
      "epoch: 76,  batch step: 95, loss: 9.213516235351562\n",
      "epoch: 76,  batch step: 96, loss: 2.6993846893310547\n",
      "epoch: 76,  batch step: 97, loss: 3.5208888053894043\n",
      "epoch: 76,  batch step: 98, loss: 31.89742088317871\n",
      "epoch: 76,  batch step: 99, loss: 3.876246213912964\n",
      "epoch: 76,  batch step: 100, loss: 3.7136402130126953\n",
      "epoch: 76,  batch step: 101, loss: 16.89366340637207\n",
      "epoch: 76,  batch step: 102, loss: 3.877362012863159\n",
      "epoch: 76,  batch step: 103, loss: 20.451602935791016\n",
      "epoch: 76,  batch step: 104, loss: 43.13184356689453\n",
      "epoch: 76,  batch step: 105, loss: 3.379094123840332\n",
      "epoch: 76,  batch step: 106, loss: 4.5994696617126465\n",
      "epoch: 76,  batch step: 107, loss: 9.761520385742188\n",
      "epoch: 76,  batch step: 108, loss: 39.77181625366211\n",
      "epoch: 76,  batch step: 109, loss: 4.4813737869262695\n",
      "epoch: 76,  batch step: 110, loss: 33.72534942626953\n",
      "epoch: 76,  batch step: 111, loss: 6.613697052001953\n",
      "epoch: 76,  batch step: 112, loss: 4.517571926116943\n",
      "epoch: 76,  batch step: 113, loss: 28.28072166442871\n",
      "epoch: 76,  batch step: 114, loss: 31.32480239868164\n",
      "epoch: 76,  batch step: 115, loss: 36.06563949584961\n",
      "epoch: 76,  batch step: 116, loss: 3.2830963134765625\n",
      "epoch: 76,  batch step: 117, loss: 2.996767044067383\n",
      "epoch: 76,  batch step: 118, loss: 4.301454067230225\n",
      "epoch: 76,  batch step: 119, loss: 5.177054405212402\n",
      "epoch: 76,  batch step: 120, loss: 36.81108093261719\n",
      "epoch: 76,  batch step: 121, loss: 29.993019104003906\n",
      "epoch: 76,  batch step: 122, loss: 54.93452453613281\n",
      "epoch: 76,  batch step: 123, loss: 34.304779052734375\n",
      "epoch: 76,  batch step: 124, loss: 4.195307731628418\n",
      "epoch: 76,  batch step: 125, loss: 24.598491668701172\n",
      "epoch: 76,  batch step: 126, loss: 4.484583854675293\n",
      "epoch: 76,  batch step: 127, loss: 3.051405429840088\n",
      "epoch: 76,  batch step: 128, loss: 17.460514068603516\n",
      "epoch: 76,  batch step: 129, loss: 29.850521087646484\n",
      "epoch: 76,  batch step: 130, loss: 2.6981258392333984\n",
      "epoch: 76,  batch step: 131, loss: 14.720137596130371\n",
      "epoch: 76,  batch step: 132, loss: 40.062984466552734\n",
      "epoch: 76,  batch step: 133, loss: 34.03276824951172\n",
      "epoch: 76,  batch step: 134, loss: 3.054394245147705\n",
      "epoch: 76,  batch step: 135, loss: 4.00591516494751\n",
      "epoch: 76,  batch step: 136, loss: 19.617053985595703\n",
      "epoch: 76,  batch step: 137, loss: 15.313201904296875\n",
      "epoch: 76,  batch step: 138, loss: 32.980316162109375\n",
      "epoch: 76,  batch step: 139, loss: 3.969494581222534\n",
      "epoch: 76,  batch step: 140, loss: 24.424476623535156\n",
      "epoch: 76,  batch step: 141, loss: 30.591175079345703\n",
      "epoch: 76,  batch step: 142, loss: 29.996173858642578\n",
      "epoch: 76,  batch step: 143, loss: 6.140292167663574\n",
      "epoch: 76,  batch step: 144, loss: 8.363280296325684\n",
      "epoch: 76,  batch step: 145, loss: 23.586950302124023\n",
      "epoch: 76,  batch step: 146, loss: 2.596323251724243\n",
      "epoch: 76,  batch step: 147, loss: 12.31228256225586\n",
      "epoch: 76,  batch step: 148, loss: 2.6419575214385986\n",
      "epoch: 76,  batch step: 149, loss: 3.350236654281616\n",
      "epoch: 76,  batch step: 150, loss: 4.131134986877441\n",
      "epoch: 76,  batch step: 151, loss: 3.7172019481658936\n",
      "epoch: 76,  batch step: 152, loss: 2.467968225479126\n",
      "epoch: 76,  batch step: 153, loss: 33.51504135131836\n",
      "epoch: 76,  batch step: 154, loss: 40.52914810180664\n",
      "epoch: 76,  batch step: 155, loss: 5.4377031326293945\n",
      "epoch: 76,  batch step: 156, loss: 33.49415588378906\n",
      "epoch: 76,  batch step: 157, loss: 5.215611934661865\n",
      "epoch: 76,  batch step: 158, loss: 3.6995177268981934\n",
      "epoch: 76,  batch step: 159, loss: 3.280332565307617\n",
      "epoch: 76,  batch step: 160, loss: 27.14581871032715\n",
      "epoch: 76,  batch step: 161, loss: 59.44413757324219\n",
      "epoch: 76,  batch step: 162, loss: 4.961585998535156\n",
      "epoch: 76,  batch step: 163, loss: 23.50273895263672\n",
      "epoch: 76,  batch step: 164, loss: 7.054064750671387\n",
      "epoch: 76,  batch step: 165, loss: 4.948117256164551\n",
      "epoch: 76,  batch step: 166, loss: 27.782102584838867\n",
      "epoch: 76,  batch step: 167, loss: 33.734188079833984\n",
      "epoch: 76,  batch step: 168, loss: 5.97269344329834\n",
      "epoch: 76,  batch step: 169, loss: 20.59469985961914\n",
      "epoch: 76,  batch step: 170, loss: 3.944199562072754\n",
      "epoch: 76,  batch step: 171, loss: 12.972575187683105\n",
      "epoch: 76,  batch step: 172, loss: 15.089789390563965\n",
      "epoch: 76,  batch step: 173, loss: 3.544830083847046\n",
      "epoch: 76,  batch step: 174, loss: 3.8879356384277344\n",
      "epoch: 76,  batch step: 175, loss: 3.865262985229492\n",
      "epoch: 76,  batch step: 176, loss: 2.3764493465423584\n",
      "epoch: 76,  batch step: 177, loss: 141.12896728515625\n",
      "epoch: 76,  batch step: 178, loss: 10.23997688293457\n",
      "epoch: 76,  batch step: 179, loss: 48.352561950683594\n",
      "epoch: 76,  batch step: 180, loss: 2.7412478923797607\n",
      "epoch: 76,  batch step: 181, loss: 3.484208583831787\n",
      "epoch: 76,  batch step: 182, loss: 17.711496353149414\n",
      "epoch: 76,  batch step: 183, loss: 3.5444045066833496\n",
      "epoch: 76,  batch step: 184, loss: 2.884920597076416\n",
      "epoch: 76,  batch step: 185, loss: 54.59608459472656\n",
      "epoch: 76,  batch step: 186, loss: 3.3991429805755615\n",
      "epoch: 76,  batch step: 187, loss: 51.06202697753906\n",
      "epoch: 76,  batch step: 188, loss: 17.237577438354492\n",
      "epoch: 76,  batch step: 189, loss: 26.455123901367188\n",
      "epoch: 76,  batch step: 190, loss: 24.76997184753418\n",
      "epoch: 76,  batch step: 191, loss: 2.681980609893799\n",
      "epoch: 76,  batch step: 192, loss: 26.980289459228516\n",
      "epoch: 76,  batch step: 193, loss: 15.978625297546387\n",
      "epoch: 76,  batch step: 194, loss: 18.83611297607422\n",
      "epoch: 76,  batch step: 195, loss: 24.090551376342773\n",
      "epoch: 76,  batch step: 196, loss: 4.819772720336914\n",
      "epoch: 76,  batch step: 197, loss: 4.548122406005859\n",
      "epoch: 76,  batch step: 198, loss: 2.894953489303589\n",
      "epoch: 76,  batch step: 199, loss: 5.671200752258301\n",
      "epoch: 76,  batch step: 200, loss: 17.2418270111084\n",
      "epoch: 76,  batch step: 201, loss: 28.524213790893555\n",
      "epoch: 76,  batch step: 202, loss: 21.211395263671875\n",
      "epoch: 76,  batch step: 203, loss: 4.830441474914551\n",
      "epoch: 76,  batch step: 204, loss: 4.784947395324707\n",
      "epoch: 76,  batch step: 205, loss: 3.9417967796325684\n",
      "epoch: 76,  batch step: 206, loss: 13.734739303588867\n",
      "epoch: 76,  batch step: 207, loss: 3.350064992904663\n",
      "epoch: 76,  batch step: 208, loss: 3.317070722579956\n",
      "epoch: 76,  batch step: 209, loss: 43.570045471191406\n",
      "epoch: 76,  batch step: 210, loss: 4.654674530029297\n",
      "epoch: 76,  batch step: 211, loss: 19.62110710144043\n",
      "epoch: 76,  batch step: 212, loss: 12.712860107421875\n",
      "epoch: 76,  batch step: 213, loss: 5.253153324127197\n",
      "epoch: 76,  batch step: 214, loss: 5.359962463378906\n",
      "epoch: 76,  batch step: 215, loss: 8.550237655639648\n",
      "epoch: 76,  batch step: 216, loss: 2.078242063522339\n",
      "epoch: 76,  batch step: 217, loss: 3.3383309841156006\n",
      "epoch: 76,  batch step: 218, loss: 3.2807185649871826\n",
      "epoch: 76,  batch step: 219, loss: 2.516498565673828\n",
      "epoch: 76,  batch step: 220, loss: 79.73624420166016\n",
      "epoch: 76,  batch step: 221, loss: 22.194791793823242\n",
      "epoch: 76,  batch step: 222, loss: 63.61205291748047\n",
      "epoch: 76,  batch step: 223, loss: 3.447274684906006\n",
      "epoch: 76,  batch step: 224, loss: 3.4193246364593506\n",
      "epoch: 76,  batch step: 225, loss: 8.324657440185547\n",
      "epoch: 76,  batch step: 226, loss: 2.51004958152771\n",
      "epoch: 76,  batch step: 227, loss: 15.357501983642578\n",
      "epoch: 76,  batch step: 228, loss: 2.900697708129883\n",
      "epoch: 76,  batch step: 229, loss: 2.0096559524536133\n",
      "epoch: 76,  batch step: 230, loss: 2.430037021636963\n",
      "epoch: 76,  batch step: 231, loss: 13.472604751586914\n",
      "epoch: 76,  batch step: 232, loss: 4.194885730743408\n",
      "epoch: 76,  batch step: 233, loss: 14.009757995605469\n",
      "epoch: 76,  batch step: 234, loss: 1.9660289287567139\n",
      "epoch: 76,  batch step: 235, loss: 14.902769088745117\n",
      "epoch: 76,  batch step: 236, loss: 41.11832046508789\n",
      "epoch: 76,  batch step: 237, loss: 44.138938903808594\n",
      "epoch: 76,  batch step: 238, loss: 11.754176139831543\n",
      "epoch: 76,  batch step: 239, loss: 4.605472564697266\n",
      "epoch: 76,  batch step: 240, loss: 3.0440478324890137\n",
      "epoch: 76,  batch step: 241, loss: 17.490123748779297\n",
      "epoch: 76,  batch step: 242, loss: 25.903411865234375\n",
      "epoch: 76,  batch step: 243, loss: 28.570205688476562\n",
      "epoch: 76,  batch step: 244, loss: 1.9869661331176758\n",
      "epoch: 76,  batch step: 245, loss: 2.804328441619873\n",
      "epoch: 76,  batch step: 246, loss: 4.305177211761475\n",
      "epoch: 76,  batch step: 247, loss: 37.1680908203125\n",
      "epoch: 76,  batch step: 248, loss: 9.36064338684082\n",
      "epoch: 76,  batch step: 249, loss: 22.323741912841797\n",
      "epoch: 76,  batch step: 250, loss: 83.28765869140625\n",
      "epoch: 76,  batch step: 251, loss: 31.690059661865234\n",
      "validation error epoch  76:    tensor(66.2001, device='cuda:0')\n",
      "316\n",
      "epoch: 77,  batch step: 0, loss: 12.551249504089355\n",
      "epoch: 77,  batch step: 1, loss: 19.669757843017578\n",
      "epoch: 77,  batch step: 2, loss: 2.5407466888427734\n",
      "epoch: 77,  batch step: 3, loss: 6.066758632659912\n",
      "epoch: 77,  batch step: 4, loss: 6.105417251586914\n",
      "epoch: 77,  batch step: 5, loss: 12.7742338180542\n",
      "epoch: 77,  batch step: 6, loss: 25.519283294677734\n",
      "epoch: 77,  batch step: 7, loss: 3.6013550758361816\n",
      "epoch: 77,  batch step: 8, loss: 35.580387115478516\n",
      "epoch: 77,  batch step: 9, loss: 16.88690948486328\n",
      "epoch: 77,  batch step: 10, loss: 8.982439041137695\n",
      "epoch: 77,  batch step: 11, loss: 6.886979579925537\n",
      "epoch: 77,  batch step: 12, loss: 4.739047527313232\n",
      "epoch: 77,  batch step: 13, loss: 15.833270072937012\n",
      "epoch: 77,  batch step: 14, loss: 17.263662338256836\n",
      "epoch: 77,  batch step: 15, loss: 9.099157333374023\n",
      "epoch: 77,  batch step: 16, loss: 21.415937423706055\n",
      "epoch: 77,  batch step: 17, loss: 20.80706024169922\n",
      "epoch: 77,  batch step: 18, loss: 3.0407190322875977\n",
      "epoch: 77,  batch step: 19, loss: 45.0155143737793\n",
      "epoch: 77,  batch step: 20, loss: 1.7669258117675781\n",
      "epoch: 77,  batch step: 21, loss: 6.564456939697266\n",
      "epoch: 77,  batch step: 22, loss: 3.0132319927215576\n",
      "epoch: 77,  batch step: 23, loss: 34.56764221191406\n",
      "epoch: 77,  batch step: 24, loss: 11.29508113861084\n",
      "epoch: 77,  batch step: 25, loss: 4.128171443939209\n",
      "epoch: 77,  batch step: 26, loss: 33.811336517333984\n",
      "epoch: 77,  batch step: 27, loss: 5.168231964111328\n",
      "epoch: 77,  batch step: 28, loss: 3.2085864543914795\n",
      "epoch: 77,  batch step: 29, loss: 139.8965301513672\n",
      "epoch: 77,  batch step: 30, loss: 7.5585761070251465\n",
      "epoch: 77,  batch step: 31, loss: 3.428401470184326\n",
      "epoch: 77,  batch step: 32, loss: 4.678314208984375\n",
      "epoch: 77,  batch step: 33, loss: 14.477664947509766\n",
      "epoch: 77,  batch step: 34, loss: 3.6913650035858154\n",
      "epoch: 77,  batch step: 35, loss: 1.6909096240997314\n",
      "epoch: 77,  batch step: 36, loss: 3.3575682640075684\n",
      "epoch: 77,  batch step: 37, loss: 2.634547710418701\n",
      "epoch: 77,  batch step: 38, loss: 2.2714591026306152\n",
      "epoch: 77,  batch step: 39, loss: 2.5132412910461426\n",
      "epoch: 77,  batch step: 40, loss: 10.900711059570312\n",
      "epoch: 77,  batch step: 41, loss: 38.773101806640625\n",
      "epoch: 77,  batch step: 42, loss: 11.290172576904297\n",
      "epoch: 77,  batch step: 43, loss: 4.679191589355469\n",
      "epoch: 77,  batch step: 44, loss: 3.352165937423706\n",
      "epoch: 77,  batch step: 45, loss: 16.0116024017334\n",
      "epoch: 77,  batch step: 46, loss: 3.1320061683654785\n",
      "epoch: 77,  batch step: 47, loss: 10.986207962036133\n",
      "epoch: 77,  batch step: 48, loss: 2.6705493927001953\n",
      "epoch: 77,  batch step: 49, loss: 22.356321334838867\n",
      "epoch: 77,  batch step: 50, loss: 2.904204845428467\n",
      "epoch: 77,  batch step: 51, loss: 19.287097930908203\n",
      "epoch: 77,  batch step: 52, loss: 5.9987993240356445\n",
      "epoch: 77,  batch step: 53, loss: 2.77305269241333\n",
      "epoch: 77,  batch step: 54, loss: 14.87149429321289\n",
      "epoch: 77,  batch step: 55, loss: 22.65956687927246\n",
      "epoch: 77,  batch step: 56, loss: 3.3298416137695312\n",
      "epoch: 77,  batch step: 57, loss: 33.13756561279297\n",
      "epoch: 77,  batch step: 58, loss: 1.9043300151824951\n",
      "epoch: 77,  batch step: 59, loss: 6.557239532470703\n",
      "epoch: 77,  batch step: 60, loss: 10.739047050476074\n",
      "epoch: 77,  batch step: 61, loss: 2.735060453414917\n",
      "epoch: 77,  batch step: 62, loss: 2.9238595962524414\n",
      "epoch: 77,  batch step: 63, loss: 17.47578239440918\n",
      "epoch: 77,  batch step: 64, loss: 31.677885055541992\n",
      "epoch: 77,  batch step: 65, loss: 9.820276260375977\n",
      "epoch: 77,  batch step: 66, loss: 1.779207706451416\n",
      "epoch: 77,  batch step: 67, loss: 2.2785634994506836\n",
      "epoch: 77,  batch step: 68, loss: 16.222089767456055\n",
      "epoch: 77,  batch step: 69, loss: 31.854116439819336\n",
      "epoch: 77,  batch step: 70, loss: 4.663187026977539\n",
      "epoch: 77,  batch step: 71, loss: 32.64680099487305\n",
      "epoch: 77,  batch step: 72, loss: 8.543825149536133\n",
      "epoch: 77,  batch step: 73, loss: 11.838744163513184\n",
      "epoch: 77,  batch step: 74, loss: 2.8977572917938232\n",
      "epoch: 77,  batch step: 75, loss: 3.6457767486572266\n",
      "epoch: 77,  batch step: 76, loss: 10.801939010620117\n",
      "epoch: 77,  batch step: 77, loss: 4.089814186096191\n",
      "epoch: 77,  batch step: 78, loss: 1.9590833187103271\n",
      "epoch: 77,  batch step: 79, loss: 17.236656188964844\n",
      "epoch: 77,  batch step: 80, loss: 8.85621166229248\n",
      "epoch: 77,  batch step: 81, loss: 4.840126991271973\n",
      "epoch: 77,  batch step: 82, loss: 14.190303802490234\n",
      "epoch: 77,  batch step: 83, loss: 10.011402130126953\n",
      "epoch: 77,  batch step: 84, loss: 16.368337631225586\n",
      "epoch: 77,  batch step: 85, loss: 4.289280414581299\n",
      "epoch: 77,  batch step: 86, loss: 2.258117437362671\n",
      "epoch: 77,  batch step: 87, loss: 33.298980712890625\n",
      "epoch: 77,  batch step: 88, loss: 19.59234619140625\n",
      "epoch: 77,  batch step: 89, loss: 7.5009989738464355\n",
      "epoch: 77,  batch step: 90, loss: 4.47117805480957\n",
      "epoch: 77,  batch step: 91, loss: 17.762004852294922\n",
      "epoch: 77,  batch step: 92, loss: 52.136627197265625\n",
      "epoch: 77,  batch step: 93, loss: 10.779106140136719\n",
      "epoch: 77,  batch step: 94, loss: 3.4429070949554443\n",
      "epoch: 77,  batch step: 95, loss: 5.60623836517334\n",
      "epoch: 77,  batch step: 96, loss: 5.766890525817871\n",
      "epoch: 77,  batch step: 97, loss: 43.30846405029297\n",
      "epoch: 77,  batch step: 98, loss: 17.912744522094727\n",
      "epoch: 77,  batch step: 99, loss: 12.991641998291016\n",
      "epoch: 77,  batch step: 100, loss: 13.167150497436523\n",
      "epoch: 77,  batch step: 101, loss: 13.086553573608398\n",
      "epoch: 77,  batch step: 102, loss: 14.329187393188477\n",
      "epoch: 77,  batch step: 103, loss: 3.2302322387695312\n",
      "epoch: 77,  batch step: 104, loss: 42.36614227294922\n",
      "epoch: 77,  batch step: 105, loss: 17.45290184020996\n",
      "epoch: 77,  batch step: 106, loss: 17.06868553161621\n",
      "epoch: 77,  batch step: 107, loss: 19.180816650390625\n",
      "epoch: 77,  batch step: 108, loss: 16.877899169921875\n",
      "epoch: 77,  batch step: 109, loss: 15.074499130249023\n",
      "epoch: 77,  batch step: 110, loss: 53.991634368896484\n",
      "epoch: 77,  batch step: 111, loss: 2.9778518676757812\n",
      "epoch: 77,  batch step: 112, loss: 30.26894187927246\n",
      "epoch: 77,  batch step: 113, loss: 2.6559898853302\n",
      "epoch: 77,  batch step: 114, loss: 6.465807914733887\n",
      "epoch: 77,  batch step: 115, loss: 2.4277405738830566\n",
      "epoch: 77,  batch step: 116, loss: 6.341712951660156\n",
      "epoch: 77,  batch step: 117, loss: 3.584791898727417\n",
      "epoch: 77,  batch step: 118, loss: 3.195434093475342\n",
      "epoch: 77,  batch step: 119, loss: 4.81318473815918\n",
      "epoch: 77,  batch step: 120, loss: 18.40972900390625\n",
      "epoch: 77,  batch step: 121, loss: 2.6347768306732178\n",
      "epoch: 77,  batch step: 122, loss: 3.3784942626953125\n",
      "epoch: 77,  batch step: 123, loss: 17.860095977783203\n",
      "epoch: 77,  batch step: 124, loss: 6.368181228637695\n",
      "epoch: 77,  batch step: 125, loss: 25.677989959716797\n",
      "epoch: 77,  batch step: 126, loss: 60.774559020996094\n",
      "epoch: 77,  batch step: 127, loss: 2.1967921257019043\n",
      "epoch: 77,  batch step: 128, loss: 11.478845596313477\n",
      "epoch: 77,  batch step: 129, loss: 62.04448699951172\n",
      "epoch: 77,  batch step: 130, loss: 50.074867248535156\n",
      "epoch: 77,  batch step: 131, loss: 27.897380828857422\n",
      "epoch: 77,  batch step: 132, loss: 1.8572534322738647\n",
      "epoch: 77,  batch step: 133, loss: 5.130122661590576\n",
      "epoch: 77,  batch step: 134, loss: 2.111001968383789\n",
      "epoch: 77,  batch step: 135, loss: 12.015508651733398\n",
      "epoch: 77,  batch step: 136, loss: 15.533218383789062\n",
      "epoch: 77,  batch step: 137, loss: 41.217933654785156\n",
      "epoch: 77,  batch step: 138, loss: 85.26190948486328\n",
      "epoch: 77,  batch step: 139, loss: 33.057151794433594\n",
      "epoch: 77,  batch step: 140, loss: 2.763619899749756\n",
      "epoch: 77,  batch step: 141, loss: 12.122180938720703\n",
      "epoch: 77,  batch step: 142, loss: 12.525561332702637\n",
      "epoch: 77,  batch step: 143, loss: 3.198676586151123\n",
      "epoch: 77,  batch step: 144, loss: 51.174964904785156\n",
      "epoch: 77,  batch step: 145, loss: 15.638360977172852\n",
      "epoch: 77,  batch step: 146, loss: 22.587553024291992\n",
      "epoch: 77,  batch step: 147, loss: 16.596153259277344\n",
      "epoch: 77,  batch step: 148, loss: 4.93076229095459\n",
      "epoch: 77,  batch step: 149, loss: 14.158846855163574\n",
      "epoch: 77,  batch step: 150, loss: 1.9667210578918457\n",
      "epoch: 77,  batch step: 151, loss: 3.4273548126220703\n",
      "epoch: 77,  batch step: 152, loss: 17.548234939575195\n",
      "epoch: 77,  batch step: 153, loss: 4.1050310134887695\n",
      "epoch: 77,  batch step: 154, loss: 30.717092514038086\n",
      "epoch: 77,  batch step: 155, loss: 4.861995697021484\n",
      "epoch: 77,  batch step: 156, loss: 3.291003704071045\n",
      "epoch: 77,  batch step: 157, loss: 7.219470500946045\n",
      "epoch: 77,  batch step: 158, loss: 7.374364376068115\n",
      "epoch: 77,  batch step: 159, loss: 2.3625760078430176\n",
      "epoch: 77,  batch step: 160, loss: 11.371429443359375\n",
      "epoch: 77,  batch step: 161, loss: 2.8850913047790527\n",
      "epoch: 77,  batch step: 162, loss: 1.8733916282653809\n",
      "epoch: 77,  batch step: 163, loss: 37.27106475830078\n",
      "epoch: 77,  batch step: 164, loss: 22.7254638671875\n",
      "epoch: 77,  batch step: 165, loss: 43.00813293457031\n",
      "epoch: 77,  batch step: 166, loss: 8.924335479736328\n",
      "epoch: 77,  batch step: 167, loss: 2.5873546600341797\n",
      "epoch: 77,  batch step: 168, loss: 61.73188018798828\n",
      "epoch: 77,  batch step: 169, loss: 67.78421020507812\n",
      "epoch: 77,  batch step: 170, loss: 5.341174125671387\n",
      "epoch: 77,  batch step: 171, loss: 2.958500623703003\n",
      "epoch: 77,  batch step: 172, loss: 2.6327786445617676\n",
      "epoch: 77,  batch step: 173, loss: 25.947345733642578\n",
      "epoch: 77,  batch step: 174, loss: 16.75497055053711\n",
      "epoch: 77,  batch step: 175, loss: 3.228889226913452\n",
      "epoch: 77,  batch step: 176, loss: 18.718589782714844\n",
      "epoch: 77,  batch step: 177, loss: 34.01133728027344\n",
      "epoch: 77,  batch step: 178, loss: 18.316913604736328\n",
      "epoch: 77,  batch step: 179, loss: 3.052407741546631\n",
      "epoch: 77,  batch step: 180, loss: 3.123142719268799\n",
      "epoch: 77,  batch step: 181, loss: 10.168478012084961\n",
      "epoch: 77,  batch step: 182, loss: 15.651751518249512\n",
      "epoch: 77,  batch step: 183, loss: 29.143169403076172\n",
      "epoch: 77,  batch step: 184, loss: 16.142375946044922\n",
      "epoch: 77,  batch step: 185, loss: 18.260578155517578\n",
      "epoch: 77,  batch step: 186, loss: 2.619163990020752\n",
      "epoch: 77,  batch step: 187, loss: 30.998863220214844\n",
      "epoch: 77,  batch step: 188, loss: 2.2289621829986572\n",
      "epoch: 77,  batch step: 189, loss: 1.9262783527374268\n",
      "epoch: 77,  batch step: 190, loss: 3.5168120861053467\n",
      "epoch: 77,  batch step: 191, loss: 5.588542938232422\n",
      "epoch: 77,  batch step: 192, loss: 3.159149169921875\n",
      "epoch: 77,  batch step: 193, loss: 3.0252485275268555\n",
      "epoch: 77,  batch step: 194, loss: 9.653362274169922\n",
      "epoch: 77,  batch step: 195, loss: 37.17256164550781\n",
      "epoch: 77,  batch step: 196, loss: 18.93832015991211\n",
      "epoch: 77,  batch step: 197, loss: 2.0247180461883545\n",
      "epoch: 77,  batch step: 198, loss: 3.9739959239959717\n",
      "epoch: 77,  batch step: 199, loss: 15.284674644470215\n",
      "epoch: 77,  batch step: 200, loss: 29.731306076049805\n",
      "epoch: 77,  batch step: 201, loss: 15.003137588500977\n",
      "epoch: 77,  batch step: 202, loss: 2.3885512351989746\n",
      "epoch: 77,  batch step: 203, loss: 26.121788024902344\n",
      "epoch: 77,  batch step: 204, loss: 2.292926549911499\n",
      "epoch: 77,  batch step: 205, loss: 18.199073791503906\n",
      "epoch: 77,  batch step: 206, loss: 15.72266674041748\n",
      "epoch: 77,  batch step: 207, loss: 13.591466903686523\n",
      "epoch: 77,  batch step: 208, loss: 4.8469743728637695\n",
      "epoch: 77,  batch step: 209, loss: 3.5088558197021484\n",
      "epoch: 77,  batch step: 210, loss: 16.816072463989258\n",
      "epoch: 77,  batch step: 211, loss: 4.292717456817627\n",
      "epoch: 77,  batch step: 212, loss: 16.824827194213867\n",
      "epoch: 77,  batch step: 213, loss: 3.4887824058532715\n",
      "epoch: 77,  batch step: 214, loss: 2.5440847873687744\n",
      "epoch: 77,  batch step: 215, loss: 3.793891429901123\n",
      "epoch: 77,  batch step: 216, loss: 2.4716598987579346\n",
      "epoch: 77,  batch step: 217, loss: 1.7036967277526855\n",
      "epoch: 77,  batch step: 218, loss: 2.6897144317626953\n",
      "epoch: 77,  batch step: 219, loss: 16.220760345458984\n",
      "epoch: 77,  batch step: 220, loss: 43.658729553222656\n",
      "epoch: 77,  batch step: 221, loss: 14.683671951293945\n",
      "epoch: 77,  batch step: 222, loss: 2.763981580734253\n",
      "epoch: 77,  batch step: 223, loss: 54.94083786010742\n",
      "epoch: 77,  batch step: 224, loss: 3.233769178390503\n",
      "epoch: 77,  batch step: 225, loss: 2.698915481567383\n",
      "epoch: 77,  batch step: 226, loss: 14.289080619812012\n",
      "epoch: 77,  batch step: 227, loss: 2.8133482933044434\n",
      "epoch: 77,  batch step: 228, loss: 13.649465560913086\n",
      "epoch: 77,  batch step: 229, loss: 5.472847938537598\n",
      "epoch: 77,  batch step: 230, loss: 2.7488696575164795\n",
      "epoch: 77,  batch step: 231, loss: 3.965322971343994\n",
      "epoch: 77,  batch step: 232, loss: 2.7345588207244873\n",
      "epoch: 77,  batch step: 233, loss: 2.5876967906951904\n",
      "epoch: 77,  batch step: 234, loss: 10.400754928588867\n",
      "epoch: 77,  batch step: 235, loss: 3.928354263305664\n",
      "epoch: 77,  batch step: 236, loss: 3.180508613586426\n",
      "epoch: 77,  batch step: 237, loss: 16.93329620361328\n",
      "epoch: 77,  batch step: 238, loss: 19.0643310546875\n",
      "epoch: 77,  batch step: 239, loss: 4.441183090209961\n",
      "epoch: 77,  batch step: 240, loss: 32.308380126953125\n",
      "epoch: 77,  batch step: 241, loss: 7.608919143676758\n",
      "epoch: 77,  batch step: 242, loss: 2.5893197059631348\n",
      "epoch: 77,  batch step: 243, loss: 1.7121680974960327\n",
      "epoch: 77,  batch step: 244, loss: 13.582125663757324\n",
      "epoch: 77,  batch step: 245, loss: 7.914243221282959\n",
      "epoch: 77,  batch step: 246, loss: 3.7703330516815186\n",
      "epoch: 77,  batch step: 247, loss: 36.97732162475586\n",
      "epoch: 77,  batch step: 248, loss: 14.556071281433105\n",
      "epoch: 77,  batch step: 249, loss: 9.085674285888672\n",
      "epoch: 77,  batch step: 250, loss: 2.069070339202881\n",
      "epoch: 77,  batch step: 251, loss: 161.15655517578125\n",
      "validation error epoch  77:    tensor(67.0127, device='cuda:0')\n",
      "316\n",
      "epoch: 78,  batch step: 0, loss: 10.70761775970459\n",
      "epoch: 78,  batch step: 1, loss: 16.006839752197266\n",
      "epoch: 78,  batch step: 2, loss: 32.56452178955078\n",
      "epoch: 78,  batch step: 3, loss: 1.6021146774291992\n",
      "epoch: 78,  batch step: 4, loss: 3.062268018722534\n",
      "epoch: 78,  batch step: 5, loss: 25.122373580932617\n",
      "epoch: 78,  batch step: 6, loss: 3.998626708984375\n",
      "epoch: 78,  batch step: 7, loss: 3.0573325157165527\n",
      "epoch: 78,  batch step: 8, loss: 2.831357002258301\n",
      "epoch: 78,  batch step: 9, loss: 2.8865814208984375\n",
      "epoch: 78,  batch step: 10, loss: 5.6101484298706055\n",
      "epoch: 78,  batch step: 11, loss: 2.850130081176758\n",
      "epoch: 78,  batch step: 12, loss: 4.352251052856445\n",
      "epoch: 78,  batch step: 13, loss: 4.995094299316406\n",
      "epoch: 78,  batch step: 14, loss: 80.73709106445312\n",
      "epoch: 78,  batch step: 15, loss: 9.895339965820312\n",
      "epoch: 78,  batch step: 16, loss: 140.04632568359375\n",
      "epoch: 78,  batch step: 17, loss: 2.919886589050293\n",
      "epoch: 78,  batch step: 18, loss: 2.615676164627075\n",
      "epoch: 78,  batch step: 19, loss: 22.528575897216797\n",
      "epoch: 78,  batch step: 20, loss: 44.77143859863281\n",
      "epoch: 78,  batch step: 21, loss: 2.8389575481414795\n",
      "epoch: 78,  batch step: 22, loss: 2.7086234092712402\n",
      "epoch: 78,  batch step: 23, loss: 10.262857437133789\n",
      "epoch: 78,  batch step: 24, loss: 6.14324951171875\n",
      "epoch: 78,  batch step: 25, loss: 4.667379379272461\n",
      "epoch: 78,  batch step: 26, loss: 4.170117378234863\n",
      "epoch: 78,  batch step: 27, loss: 2.5567643642425537\n",
      "epoch: 78,  batch step: 28, loss: 2.5910563468933105\n",
      "epoch: 78,  batch step: 29, loss: 7.59785795211792\n",
      "epoch: 78,  batch step: 30, loss: 10.420042991638184\n",
      "epoch: 78,  batch step: 31, loss: 24.063091278076172\n",
      "epoch: 78,  batch step: 32, loss: 3.7309703826904297\n",
      "epoch: 78,  batch step: 33, loss: 17.73426628112793\n",
      "epoch: 78,  batch step: 34, loss: 28.767147064208984\n",
      "epoch: 78,  batch step: 35, loss: 3.7623233795166016\n",
      "epoch: 78,  batch step: 36, loss: 2.7268009185791016\n",
      "epoch: 78,  batch step: 37, loss: 47.14961242675781\n",
      "epoch: 78,  batch step: 38, loss: 16.02293586730957\n",
      "epoch: 78,  batch step: 39, loss: 3.12666392326355\n",
      "epoch: 78,  batch step: 40, loss: 2.6481800079345703\n",
      "epoch: 78,  batch step: 41, loss: 18.890668869018555\n",
      "epoch: 78,  batch step: 42, loss: 4.3143415451049805\n",
      "epoch: 78,  batch step: 43, loss: 13.731298446655273\n",
      "epoch: 78,  batch step: 44, loss: 9.237577438354492\n",
      "epoch: 78,  batch step: 45, loss: 13.235464096069336\n",
      "epoch: 78,  batch step: 46, loss: 24.37314224243164\n",
      "epoch: 78,  batch step: 47, loss: 6.418536186218262\n",
      "epoch: 78,  batch step: 48, loss: 25.543739318847656\n",
      "epoch: 78,  batch step: 49, loss: 3.518112897872925\n",
      "epoch: 78,  batch step: 50, loss: 8.442827224731445\n",
      "epoch: 78,  batch step: 51, loss: 30.52865982055664\n",
      "epoch: 78,  batch step: 52, loss: 14.97224235534668\n",
      "epoch: 78,  batch step: 53, loss: 2.4544787406921387\n",
      "epoch: 78,  batch step: 54, loss: 2.4786572456359863\n",
      "epoch: 78,  batch step: 55, loss: 10.135194778442383\n",
      "epoch: 78,  batch step: 56, loss: 2.299833297729492\n",
      "epoch: 78,  batch step: 57, loss: 8.420892715454102\n",
      "epoch: 78,  batch step: 58, loss: 3.8568148612976074\n",
      "epoch: 78,  batch step: 59, loss: 13.751834869384766\n",
      "epoch: 78,  batch step: 60, loss: 1.9511317014694214\n",
      "epoch: 78,  batch step: 61, loss: 5.376927852630615\n",
      "epoch: 78,  batch step: 62, loss: 3.266300916671753\n",
      "epoch: 78,  batch step: 63, loss: 25.642578125\n",
      "epoch: 78,  batch step: 64, loss: 11.707381248474121\n",
      "epoch: 78,  batch step: 65, loss: 2.169665813446045\n",
      "epoch: 78,  batch step: 66, loss: 15.007673263549805\n",
      "epoch: 78,  batch step: 67, loss: 1.6891422271728516\n",
      "epoch: 78,  batch step: 68, loss: 7.738064765930176\n",
      "epoch: 78,  batch step: 69, loss: 3.676320791244507\n",
      "epoch: 78,  batch step: 70, loss: 7.741553783416748\n",
      "epoch: 78,  batch step: 71, loss: 2.831782102584839\n",
      "epoch: 78,  batch step: 72, loss: 10.159910202026367\n",
      "epoch: 78,  batch step: 73, loss: 17.638504028320312\n",
      "epoch: 78,  batch step: 74, loss: 2.325835943222046\n",
      "epoch: 78,  batch step: 75, loss: 49.530982971191406\n",
      "epoch: 78,  batch step: 76, loss: 11.629400253295898\n",
      "epoch: 78,  batch step: 77, loss: 2.4167232513427734\n",
      "epoch: 78,  batch step: 78, loss: 2.5434741973876953\n",
      "epoch: 78,  batch step: 79, loss: 11.553905487060547\n",
      "epoch: 78,  batch step: 80, loss: 5.0623626708984375\n",
      "epoch: 78,  batch step: 81, loss: 13.296378135681152\n",
      "epoch: 78,  batch step: 82, loss: 2.102497100830078\n",
      "epoch: 78,  batch step: 83, loss: 12.519121170043945\n",
      "epoch: 78,  batch step: 84, loss: 42.5924072265625\n",
      "epoch: 78,  batch step: 85, loss: 12.467241287231445\n",
      "epoch: 78,  batch step: 86, loss: 51.83458709716797\n",
      "epoch: 78,  batch step: 87, loss: 11.151796340942383\n",
      "epoch: 78,  batch step: 88, loss: 11.142524719238281\n",
      "epoch: 78,  batch step: 89, loss: 5.9116411209106445\n",
      "epoch: 78,  batch step: 90, loss: 6.292016506195068\n",
      "epoch: 78,  batch step: 91, loss: 18.43914794921875\n",
      "epoch: 78,  batch step: 92, loss: 16.512062072753906\n",
      "epoch: 78,  batch step: 93, loss: 2.7699241638183594\n",
      "epoch: 78,  batch step: 94, loss: 3.480281352996826\n",
      "epoch: 78,  batch step: 95, loss: 18.89246368408203\n",
      "epoch: 78,  batch step: 96, loss: 3.7651193141937256\n",
      "epoch: 78,  batch step: 97, loss: 7.18356990814209\n",
      "epoch: 78,  batch step: 98, loss: 23.213787078857422\n",
      "epoch: 78,  batch step: 99, loss: 2.5938594341278076\n",
      "epoch: 78,  batch step: 100, loss: 2.5386645793914795\n",
      "epoch: 78,  batch step: 101, loss: 2.997465133666992\n",
      "epoch: 78,  batch step: 102, loss: 12.206831932067871\n",
      "epoch: 78,  batch step: 103, loss: 16.45808982849121\n",
      "epoch: 78,  batch step: 104, loss: 10.641535758972168\n",
      "epoch: 78,  batch step: 105, loss: 7.854960918426514\n",
      "epoch: 78,  batch step: 106, loss: 43.236717224121094\n",
      "epoch: 78,  batch step: 107, loss: 2.550978422164917\n",
      "epoch: 78,  batch step: 108, loss: 2.303954601287842\n",
      "epoch: 78,  batch step: 109, loss: 24.969890594482422\n",
      "epoch: 78,  batch step: 110, loss: 10.367881774902344\n",
      "epoch: 78,  batch step: 111, loss: 12.52757453918457\n",
      "epoch: 78,  batch step: 112, loss: 3.7231879234313965\n",
      "epoch: 78,  batch step: 113, loss: 14.578146934509277\n",
      "epoch: 78,  batch step: 114, loss: 4.298967361450195\n",
      "epoch: 78,  batch step: 115, loss: 24.068572998046875\n",
      "epoch: 78,  batch step: 116, loss: 11.922182083129883\n",
      "epoch: 78,  batch step: 117, loss: 6.9488844871521\n",
      "epoch: 78,  batch step: 118, loss: 28.665210723876953\n",
      "epoch: 78,  batch step: 119, loss: 8.527475357055664\n",
      "epoch: 78,  batch step: 120, loss: 3.976261854171753\n",
      "epoch: 78,  batch step: 121, loss: 2.3712496757507324\n",
      "epoch: 78,  batch step: 122, loss: 21.28396987915039\n",
      "epoch: 78,  batch step: 123, loss: 84.83297729492188\n",
      "epoch: 78,  batch step: 124, loss: 57.639495849609375\n",
      "epoch: 78,  batch step: 125, loss: 1.6725952625274658\n",
      "epoch: 78,  batch step: 126, loss: 2.1606016159057617\n",
      "epoch: 78,  batch step: 127, loss: 13.452178955078125\n",
      "epoch: 78,  batch step: 128, loss: 2.7150371074676514\n",
      "epoch: 78,  batch step: 129, loss: 1.8890060186386108\n",
      "epoch: 78,  batch step: 130, loss: 3.6615729331970215\n",
      "epoch: 78,  batch step: 131, loss: 3.107952117919922\n",
      "epoch: 78,  batch step: 132, loss: 2.9497790336608887\n",
      "epoch: 78,  batch step: 133, loss: 16.232608795166016\n",
      "epoch: 78,  batch step: 134, loss: 2.70501971244812\n",
      "epoch: 78,  batch step: 135, loss: 2.3842082023620605\n",
      "epoch: 78,  batch step: 136, loss: 17.570859909057617\n",
      "epoch: 78,  batch step: 137, loss: 59.5169677734375\n",
      "epoch: 78,  batch step: 138, loss: 27.83294677734375\n",
      "epoch: 78,  batch step: 139, loss: 3.120574474334717\n",
      "epoch: 78,  batch step: 140, loss: 9.397296905517578\n",
      "epoch: 78,  batch step: 141, loss: 6.4258880615234375\n",
      "epoch: 78,  batch step: 142, loss: 11.52457046508789\n",
      "epoch: 78,  batch step: 143, loss: 2.5110270977020264\n",
      "epoch: 78,  batch step: 144, loss: 5.98933219909668\n",
      "epoch: 78,  batch step: 145, loss: 4.035425662994385\n",
      "epoch: 78,  batch step: 146, loss: 7.301991939544678\n",
      "epoch: 78,  batch step: 147, loss: 3.0936312675476074\n",
      "epoch: 78,  batch step: 148, loss: 19.37165069580078\n",
      "epoch: 78,  batch step: 149, loss: 3.0290002822875977\n",
      "epoch: 78,  batch step: 150, loss: 3.8742215633392334\n",
      "epoch: 78,  batch step: 151, loss: 2.5153279304504395\n",
      "epoch: 78,  batch step: 152, loss: 2.8694798946380615\n",
      "epoch: 78,  batch step: 153, loss: 1.8803460597991943\n",
      "epoch: 78,  batch step: 154, loss: 2.305241584777832\n",
      "epoch: 78,  batch step: 155, loss: 2.0214993953704834\n",
      "epoch: 78,  batch step: 156, loss: 11.86121940612793\n",
      "epoch: 78,  batch step: 157, loss: 9.398412704467773\n",
      "epoch: 78,  batch step: 158, loss: 29.149028778076172\n",
      "epoch: 78,  batch step: 159, loss: 2.300201892852783\n",
      "epoch: 78,  batch step: 160, loss: 7.015275478363037\n",
      "epoch: 78,  batch step: 161, loss: 11.858068466186523\n",
      "epoch: 78,  batch step: 162, loss: 3.0169811248779297\n",
      "epoch: 78,  batch step: 163, loss: 6.010337829589844\n",
      "epoch: 78,  batch step: 164, loss: 13.529468536376953\n",
      "epoch: 78,  batch step: 165, loss: 2.072422504425049\n",
      "epoch: 78,  batch step: 166, loss: 1.8394426107406616\n",
      "epoch: 78,  batch step: 167, loss: 36.71837615966797\n",
      "epoch: 78,  batch step: 168, loss: 2.005952835083008\n",
      "epoch: 78,  batch step: 169, loss: 15.801050186157227\n",
      "epoch: 78,  batch step: 170, loss: 12.533166885375977\n",
      "epoch: 78,  batch step: 171, loss: 2.1781814098358154\n",
      "epoch: 78,  batch step: 172, loss: 3.081757068634033\n",
      "epoch: 78,  batch step: 173, loss: 2.504462957382202\n",
      "epoch: 78,  batch step: 174, loss: 3.879119396209717\n",
      "epoch: 78,  batch step: 175, loss: 55.103248596191406\n",
      "epoch: 78,  batch step: 176, loss: 2.193791151046753\n",
      "epoch: 78,  batch step: 177, loss: 1.984916090965271\n",
      "epoch: 78,  batch step: 178, loss: 16.09387969970703\n",
      "epoch: 78,  batch step: 179, loss: 1.9885447025299072\n",
      "epoch: 78,  batch step: 180, loss: 2.6816787719726562\n",
      "epoch: 78,  batch step: 181, loss: 1.7896356582641602\n",
      "epoch: 78,  batch step: 182, loss: 11.320757865905762\n",
      "epoch: 78,  batch step: 183, loss: 10.958852767944336\n",
      "epoch: 78,  batch step: 184, loss: 1.365185022354126\n",
      "epoch: 78,  batch step: 185, loss: 3.015176773071289\n",
      "epoch: 78,  batch step: 186, loss: 33.223915100097656\n",
      "epoch: 78,  batch step: 187, loss: 2.0785574913024902\n",
      "epoch: 78,  batch step: 188, loss: 1.749464750289917\n",
      "epoch: 78,  batch step: 189, loss: 4.590909004211426\n",
      "epoch: 78,  batch step: 190, loss: 18.323501586914062\n",
      "epoch: 78,  batch step: 191, loss: 2.8972785472869873\n",
      "epoch: 78,  batch step: 192, loss: 21.335311889648438\n",
      "epoch: 78,  batch step: 193, loss: 3.3724775314331055\n",
      "epoch: 78,  batch step: 194, loss: 8.929643630981445\n",
      "epoch: 78,  batch step: 195, loss: 27.203397750854492\n",
      "epoch: 78,  batch step: 196, loss: 4.3668012619018555\n",
      "epoch: 78,  batch step: 197, loss: 50.84577178955078\n",
      "epoch: 78,  batch step: 198, loss: 2.9898810386657715\n",
      "epoch: 78,  batch step: 199, loss: 4.439610481262207\n",
      "epoch: 78,  batch step: 200, loss: 5.0038323402404785\n",
      "epoch: 78,  batch step: 201, loss: 9.195082664489746\n",
      "epoch: 78,  batch step: 202, loss: 13.093887329101562\n",
      "epoch: 78,  batch step: 203, loss: 57.7932243347168\n",
      "epoch: 78,  batch step: 204, loss: 19.793949127197266\n",
      "epoch: 78,  batch step: 205, loss: 16.946924209594727\n",
      "epoch: 78,  batch step: 206, loss: 21.004100799560547\n",
      "epoch: 78,  batch step: 207, loss: 4.28607702255249\n",
      "epoch: 78,  batch step: 208, loss: 11.932249069213867\n",
      "epoch: 78,  batch step: 209, loss: 14.057382583618164\n",
      "epoch: 78,  batch step: 210, loss: 81.29365539550781\n",
      "epoch: 78,  batch step: 211, loss: 37.026432037353516\n",
      "epoch: 78,  batch step: 212, loss: 3.4552524089813232\n",
      "epoch: 78,  batch step: 213, loss: 1.8277534246444702\n",
      "epoch: 78,  batch step: 214, loss: 2.7419209480285645\n",
      "epoch: 78,  batch step: 215, loss: 2.893662929534912\n",
      "epoch: 78,  batch step: 216, loss: 17.09560775756836\n",
      "epoch: 78,  batch step: 217, loss: 36.081146240234375\n",
      "epoch: 78,  batch step: 218, loss: 4.086462020874023\n",
      "epoch: 78,  batch step: 219, loss: 1.8971984386444092\n",
      "epoch: 78,  batch step: 220, loss: 5.3031768798828125\n",
      "epoch: 78,  batch step: 221, loss: 2.801889419555664\n",
      "epoch: 78,  batch step: 222, loss: 3.6888911724090576\n",
      "epoch: 78,  batch step: 223, loss: 1.9117844104766846\n",
      "epoch: 78,  batch step: 224, loss: 23.423534393310547\n",
      "epoch: 78,  batch step: 225, loss: 3.788701057434082\n",
      "epoch: 78,  batch step: 226, loss: 21.97760009765625\n",
      "epoch: 78,  batch step: 227, loss: 43.93259811401367\n",
      "epoch: 78,  batch step: 228, loss: 3.6007437705993652\n",
      "epoch: 78,  batch step: 229, loss: 4.23834228515625\n",
      "epoch: 78,  batch step: 230, loss: 16.659412384033203\n",
      "epoch: 78,  batch step: 231, loss: 14.168350219726562\n",
      "epoch: 78,  batch step: 232, loss: 5.765105724334717\n",
      "epoch: 78,  batch step: 233, loss: 2.1306495666503906\n",
      "epoch: 78,  batch step: 234, loss: 16.712692260742188\n",
      "epoch: 78,  batch step: 235, loss: 3.254514694213867\n",
      "epoch: 78,  batch step: 236, loss: 43.619102478027344\n",
      "epoch: 78,  batch step: 237, loss: 28.24584197998047\n",
      "epoch: 78,  batch step: 238, loss: 2.686365842819214\n",
      "epoch: 78,  batch step: 239, loss: 2.7801527976989746\n",
      "epoch: 78,  batch step: 240, loss: 2.3044042587280273\n",
      "epoch: 78,  batch step: 241, loss: 13.805416107177734\n",
      "epoch: 78,  batch step: 242, loss: 4.423722743988037\n",
      "epoch: 78,  batch step: 243, loss: 3.580066442489624\n",
      "epoch: 78,  batch step: 244, loss: 3.5070595741271973\n",
      "epoch: 78,  batch step: 245, loss: 24.16046714782715\n",
      "epoch: 78,  batch step: 246, loss: 38.80401611328125\n",
      "epoch: 78,  batch step: 247, loss: 3.5889270305633545\n",
      "epoch: 78,  batch step: 248, loss: 43.08906173706055\n",
      "epoch: 78,  batch step: 249, loss: 5.326303005218506\n",
      "epoch: 78,  batch step: 250, loss: 4.059469223022461\n",
      "epoch: 78,  batch step: 251, loss: 80.392822265625\n",
      "validation error epoch  78:    tensor(70.8554, device='cuda:0')\n",
      "316\n",
      "epoch: 79,  batch step: 0, loss: 30.930965423583984\n",
      "epoch: 79,  batch step: 1, loss: 16.198654174804688\n",
      "epoch: 79,  batch step: 2, loss: 26.44879150390625\n",
      "epoch: 79,  batch step: 3, loss: 23.59400749206543\n",
      "epoch: 79,  batch step: 4, loss: 40.090431213378906\n",
      "epoch: 79,  batch step: 5, loss: 55.970436096191406\n",
      "epoch: 79,  batch step: 6, loss: 10.937006950378418\n",
      "epoch: 79,  batch step: 7, loss: 2.8713433742523193\n",
      "epoch: 79,  batch step: 8, loss: 10.04494571685791\n",
      "epoch: 79,  batch step: 9, loss: 21.777957916259766\n",
      "epoch: 79,  batch step: 10, loss: 18.575347900390625\n",
      "epoch: 79,  batch step: 11, loss: 5.297153472900391\n",
      "epoch: 79,  batch step: 12, loss: 5.509570121765137\n",
      "epoch: 79,  batch step: 13, loss: 20.341590881347656\n",
      "epoch: 79,  batch step: 14, loss: 13.541023254394531\n",
      "epoch: 79,  batch step: 15, loss: 3.3831629753112793\n",
      "epoch: 79,  batch step: 16, loss: 6.078159332275391\n",
      "epoch: 79,  batch step: 17, loss: 3.4774584770202637\n",
      "epoch: 79,  batch step: 18, loss: 3.4624743461608887\n",
      "epoch: 79,  batch step: 19, loss: 5.256772041320801\n",
      "epoch: 79,  batch step: 20, loss: 5.833671569824219\n",
      "epoch: 79,  batch step: 21, loss: 3.489893674850464\n",
      "epoch: 79,  batch step: 22, loss: 3.0558841228485107\n",
      "epoch: 79,  batch step: 23, loss: 3.017949104309082\n",
      "epoch: 79,  batch step: 24, loss: 4.5706467628479\n",
      "epoch: 79,  batch step: 25, loss: 23.17267417907715\n",
      "epoch: 79,  batch step: 26, loss: 3.592440605163574\n",
      "epoch: 79,  batch step: 27, loss: 2.4174084663391113\n",
      "epoch: 79,  batch step: 28, loss: 3.801166534423828\n",
      "epoch: 79,  batch step: 29, loss: 20.320266723632812\n",
      "epoch: 79,  batch step: 30, loss: 37.940528869628906\n",
      "epoch: 79,  batch step: 31, loss: 26.365066528320312\n",
      "epoch: 79,  batch step: 32, loss: 7.894938945770264\n",
      "epoch: 79,  batch step: 33, loss: 15.90515422821045\n",
      "epoch: 79,  batch step: 34, loss: 4.0989274978637695\n",
      "epoch: 79,  batch step: 35, loss: 23.114810943603516\n",
      "epoch: 79,  batch step: 36, loss: 2.256479024887085\n",
      "epoch: 79,  batch step: 37, loss: 3.3220677375793457\n",
      "epoch: 79,  batch step: 38, loss: 7.830352783203125\n",
      "epoch: 79,  batch step: 39, loss: 30.11896514892578\n",
      "epoch: 79,  batch step: 40, loss: 133.62583923339844\n",
      "epoch: 79,  batch step: 41, loss: 46.59129333496094\n",
      "epoch: 79,  batch step: 42, loss: 1.7276113033294678\n",
      "epoch: 79,  batch step: 43, loss: 53.949462890625\n",
      "epoch: 79,  batch step: 44, loss: 28.270328521728516\n",
      "epoch: 79,  batch step: 45, loss: 17.882938385009766\n",
      "epoch: 79,  batch step: 46, loss: 30.621694564819336\n",
      "epoch: 79,  batch step: 47, loss: 2.7565078735351562\n",
      "epoch: 79,  batch step: 48, loss: 30.274009704589844\n",
      "epoch: 79,  batch step: 49, loss: 27.22464370727539\n",
      "epoch: 79,  batch step: 50, loss: 12.665980339050293\n",
      "epoch: 79,  batch step: 51, loss: 2.690281629562378\n",
      "epoch: 79,  batch step: 52, loss: 15.564804077148438\n",
      "epoch: 79,  batch step: 53, loss: 30.857526779174805\n",
      "epoch: 79,  batch step: 54, loss: 2.9843196868896484\n",
      "epoch: 79,  batch step: 55, loss: 15.357454299926758\n",
      "epoch: 79,  batch step: 56, loss: 4.6561970710754395\n",
      "epoch: 79,  batch step: 57, loss: 3.3508334159851074\n",
      "epoch: 79,  batch step: 58, loss: 8.134613990783691\n",
      "epoch: 79,  batch step: 59, loss: 12.799871444702148\n",
      "epoch: 79,  batch step: 60, loss: 31.642791748046875\n",
      "epoch: 79,  batch step: 61, loss: 3.9783294200897217\n",
      "epoch: 79,  batch step: 62, loss: 15.4658842086792\n",
      "epoch: 79,  batch step: 63, loss: 16.285781860351562\n",
      "epoch: 79,  batch step: 64, loss: 1.8605003356933594\n",
      "epoch: 79,  batch step: 65, loss: 57.0904655456543\n",
      "epoch: 79,  batch step: 66, loss: 2.9713029861450195\n",
      "epoch: 79,  batch step: 67, loss: 3.697068214416504\n",
      "epoch: 79,  batch step: 68, loss: 6.538529396057129\n",
      "epoch: 79,  batch step: 69, loss: 2.4961180686950684\n",
      "epoch: 79,  batch step: 70, loss: 2.1354379653930664\n",
      "epoch: 79,  batch step: 71, loss: 2.14605450630188\n",
      "epoch: 79,  batch step: 72, loss: 5.2890753746032715\n",
      "epoch: 79,  batch step: 73, loss: 1.892403483390808\n",
      "epoch: 79,  batch step: 74, loss: 3.41542911529541\n",
      "epoch: 79,  batch step: 75, loss: 18.23538589477539\n",
      "epoch: 79,  batch step: 76, loss: 22.4312744140625\n",
      "epoch: 79,  batch step: 77, loss: 20.363004684448242\n",
      "epoch: 79,  batch step: 78, loss: 3.476713180541992\n",
      "epoch: 79,  batch step: 79, loss: 19.971614837646484\n",
      "epoch: 79,  batch step: 80, loss: 7.450980186462402\n",
      "epoch: 79,  batch step: 81, loss: 30.509471893310547\n",
      "epoch: 79,  batch step: 82, loss: 14.851116180419922\n",
      "epoch: 79,  batch step: 83, loss: 42.92434310913086\n",
      "epoch: 79,  batch step: 84, loss: 1.5637868642807007\n",
      "epoch: 79,  batch step: 85, loss: 2.1846699714660645\n",
      "epoch: 79,  batch step: 86, loss: 17.538074493408203\n",
      "epoch: 79,  batch step: 87, loss: 53.721832275390625\n",
      "epoch: 79,  batch step: 88, loss: 8.082138061523438\n",
      "epoch: 79,  batch step: 89, loss: 1.9303503036499023\n",
      "epoch: 79,  batch step: 90, loss: 29.00121307373047\n",
      "epoch: 79,  batch step: 91, loss: 9.91482162475586\n",
      "epoch: 79,  batch step: 92, loss: 67.4597396850586\n",
      "epoch: 79,  batch step: 93, loss: 44.780330657958984\n",
      "epoch: 79,  batch step: 94, loss: 2.290907859802246\n",
      "epoch: 79,  batch step: 95, loss: 2.5096614360809326\n",
      "epoch: 79,  batch step: 96, loss: 10.04782772064209\n",
      "epoch: 79,  batch step: 97, loss: 1.5404765605926514\n",
      "epoch: 79,  batch step: 98, loss: 7.1882123947143555\n",
      "epoch: 79,  batch step: 99, loss: 17.86754608154297\n",
      "epoch: 79,  batch step: 100, loss: 11.813933372497559\n",
      "epoch: 79,  batch step: 101, loss: 1.5323731899261475\n",
      "epoch: 79,  batch step: 102, loss: 5.6450629234313965\n",
      "epoch: 79,  batch step: 103, loss: 3.17923641204834\n",
      "epoch: 79,  batch step: 104, loss: 14.687281608581543\n",
      "epoch: 79,  batch step: 105, loss: 57.703514099121094\n",
      "epoch: 79,  batch step: 106, loss: 5.73366641998291\n",
      "epoch: 79,  batch step: 107, loss: 14.884142875671387\n",
      "epoch: 79,  batch step: 108, loss: 17.917667388916016\n",
      "epoch: 79,  batch step: 109, loss: 3.384821653366089\n",
      "epoch: 79,  batch step: 110, loss: 15.46940803527832\n",
      "epoch: 79,  batch step: 111, loss: 2.6735172271728516\n",
      "epoch: 79,  batch step: 112, loss: 30.54680633544922\n",
      "epoch: 79,  batch step: 113, loss: 3.513732671737671\n",
      "epoch: 79,  batch step: 114, loss: 2.190549612045288\n",
      "epoch: 79,  batch step: 115, loss: 13.261075973510742\n",
      "epoch: 79,  batch step: 116, loss: 3.234867811203003\n",
      "epoch: 79,  batch step: 117, loss: 3.398740768432617\n",
      "epoch: 79,  batch step: 118, loss: 36.186500549316406\n",
      "epoch: 79,  batch step: 119, loss: 5.912286758422852\n",
      "epoch: 79,  batch step: 120, loss: 33.725975036621094\n",
      "epoch: 79,  batch step: 121, loss: 2.5397415161132812\n",
      "epoch: 79,  batch step: 122, loss: 17.88405990600586\n",
      "epoch: 79,  batch step: 123, loss: 3.661526679992676\n",
      "epoch: 79,  batch step: 124, loss: 2.680540084838867\n",
      "epoch: 79,  batch step: 125, loss: 3.345782995223999\n",
      "epoch: 79,  batch step: 126, loss: 37.486690521240234\n",
      "epoch: 79,  batch step: 127, loss: 5.211114883422852\n",
      "epoch: 79,  batch step: 128, loss: 3.2517995834350586\n",
      "epoch: 79,  batch step: 129, loss: 3.7632365226745605\n",
      "epoch: 79,  batch step: 130, loss: 3.529050588607788\n",
      "epoch: 79,  batch step: 131, loss: 4.73262357711792\n",
      "epoch: 79,  batch step: 132, loss: 17.727388381958008\n",
      "epoch: 79,  batch step: 133, loss: 13.983972549438477\n",
      "epoch: 79,  batch step: 134, loss: 6.723683834075928\n",
      "epoch: 79,  batch step: 135, loss: 2.074134349822998\n",
      "epoch: 79,  batch step: 136, loss: 9.460310935974121\n",
      "epoch: 79,  batch step: 137, loss: 1.5729608535766602\n",
      "epoch: 79,  batch step: 138, loss: 3.4265942573547363\n",
      "epoch: 79,  batch step: 139, loss: 2.1472489833831787\n",
      "epoch: 79,  batch step: 140, loss: 2.078662395477295\n",
      "epoch: 79,  batch step: 141, loss: 3.9713973999023438\n",
      "epoch: 79,  batch step: 142, loss: 17.210968017578125\n",
      "epoch: 79,  batch step: 143, loss: 21.63764762878418\n",
      "epoch: 79,  batch step: 144, loss: 17.80738067626953\n",
      "epoch: 79,  batch step: 145, loss: 3.328150749206543\n",
      "epoch: 79,  batch step: 146, loss: 3.7215819358825684\n",
      "epoch: 79,  batch step: 147, loss: 13.126875877380371\n",
      "epoch: 79,  batch step: 148, loss: 12.392427444458008\n",
      "epoch: 79,  batch step: 149, loss: 15.7321195602417\n",
      "epoch: 79,  batch step: 150, loss: 2.52436900138855\n",
      "epoch: 79,  batch step: 151, loss: 17.765031814575195\n",
      "epoch: 79,  batch step: 152, loss: 14.546805381774902\n",
      "epoch: 79,  batch step: 153, loss: 7.5140814781188965\n",
      "epoch: 79,  batch step: 154, loss: 5.050646781921387\n",
      "epoch: 79,  batch step: 155, loss: 14.859061241149902\n",
      "epoch: 79,  batch step: 156, loss: 4.869696617126465\n",
      "epoch: 79,  batch step: 157, loss: 2.643303871154785\n",
      "epoch: 79,  batch step: 158, loss: 9.980299949645996\n",
      "epoch: 79,  batch step: 159, loss: 3.6140246391296387\n",
      "epoch: 79,  batch step: 160, loss: 1.6875617504119873\n",
      "epoch: 79,  batch step: 161, loss: 24.191831588745117\n",
      "epoch: 79,  batch step: 162, loss: 43.84291076660156\n",
      "epoch: 79,  batch step: 163, loss: 10.669330596923828\n",
      "epoch: 79,  batch step: 164, loss: 8.667816162109375\n",
      "epoch: 79,  batch step: 165, loss: 3.2797322273254395\n",
      "epoch: 79,  batch step: 166, loss: 4.3536882400512695\n",
      "epoch: 79,  batch step: 167, loss: 4.001137733459473\n",
      "epoch: 79,  batch step: 168, loss: 1.913499355316162\n",
      "epoch: 79,  batch step: 169, loss: 14.208200454711914\n",
      "epoch: 79,  batch step: 170, loss: 1.7234971523284912\n",
      "epoch: 79,  batch step: 171, loss: 9.805898666381836\n",
      "epoch: 79,  batch step: 172, loss: 7.17123556137085\n",
      "epoch: 79,  batch step: 173, loss: 17.933279037475586\n",
      "epoch: 79,  batch step: 174, loss: 2.290456533432007\n",
      "epoch: 79,  batch step: 175, loss: 2.466742515563965\n",
      "epoch: 79,  batch step: 176, loss: 2.439026355743408\n",
      "epoch: 79,  batch step: 177, loss: 5.374794960021973\n",
      "epoch: 79,  batch step: 178, loss: 21.308456420898438\n",
      "epoch: 79,  batch step: 179, loss: 10.411069869995117\n",
      "epoch: 79,  batch step: 180, loss: 4.106342792510986\n",
      "epoch: 79,  batch step: 181, loss: 8.303611755371094\n",
      "epoch: 79,  batch step: 182, loss: 3.3497347831726074\n",
      "epoch: 79,  batch step: 183, loss: 8.706803321838379\n",
      "epoch: 79,  batch step: 184, loss: 2.8515419960021973\n",
      "epoch: 79,  batch step: 185, loss: 16.917232513427734\n",
      "epoch: 79,  batch step: 186, loss: 9.286685943603516\n",
      "epoch: 79,  batch step: 187, loss: 9.558899879455566\n",
      "epoch: 79,  batch step: 188, loss: 3.6148643493652344\n",
      "epoch: 79,  batch step: 189, loss: 8.464751243591309\n",
      "epoch: 79,  batch step: 190, loss: 60.51933288574219\n",
      "epoch: 79,  batch step: 191, loss: 6.202254772186279\n",
      "epoch: 79,  batch step: 192, loss: 42.89994812011719\n",
      "epoch: 79,  batch step: 193, loss: 3.1626052856445312\n",
      "epoch: 79,  batch step: 194, loss: 7.693564414978027\n",
      "epoch: 79,  batch step: 195, loss: 13.967613220214844\n",
      "epoch: 79,  batch step: 196, loss: 2.353903293609619\n",
      "epoch: 79,  batch step: 197, loss: 2.3343071937561035\n",
      "epoch: 79,  batch step: 198, loss: 1.8384826183319092\n",
      "epoch: 79,  batch step: 199, loss: 17.56903076171875\n",
      "epoch: 79,  batch step: 200, loss: 38.35157775878906\n",
      "epoch: 79,  batch step: 201, loss: 2.8301634788513184\n",
      "epoch: 79,  batch step: 202, loss: 4.38542366027832\n",
      "epoch: 79,  batch step: 203, loss: 3.1497867107391357\n",
      "epoch: 79,  batch step: 204, loss: 10.753619194030762\n",
      "epoch: 79,  batch step: 205, loss: 3.626756191253662\n",
      "epoch: 79,  batch step: 206, loss: 46.655967712402344\n",
      "epoch: 79,  batch step: 207, loss: 14.511674880981445\n",
      "epoch: 79,  batch step: 208, loss: 27.207670211791992\n",
      "epoch: 79,  batch step: 209, loss: 2.5270562171936035\n",
      "epoch: 79,  batch step: 210, loss: 1.7522516250610352\n",
      "epoch: 79,  batch step: 211, loss: 3.0931921005249023\n",
      "epoch: 79,  batch step: 212, loss: 2.470353126525879\n",
      "epoch: 79,  batch step: 213, loss: 2.649367332458496\n",
      "epoch: 79,  batch step: 214, loss: 6.388319969177246\n",
      "epoch: 79,  batch step: 215, loss: 2.520944595336914\n",
      "epoch: 79,  batch step: 216, loss: 15.14895248413086\n",
      "epoch: 79,  batch step: 217, loss: 1.9298865795135498\n",
      "epoch: 79,  batch step: 218, loss: 1.8065071105957031\n",
      "epoch: 79,  batch step: 219, loss: 12.089011192321777\n",
      "epoch: 79,  batch step: 220, loss: 3.7823646068573\n",
      "epoch: 79,  batch step: 221, loss: 23.74734115600586\n",
      "epoch: 79,  batch step: 222, loss: 9.331106185913086\n",
      "epoch: 79,  batch step: 223, loss: 27.479293823242188\n",
      "epoch: 79,  batch step: 224, loss: 17.210243225097656\n",
      "epoch: 79,  batch step: 225, loss: 1.5263385772705078\n",
      "epoch: 79,  batch step: 226, loss: 2.3635213375091553\n",
      "epoch: 79,  batch step: 227, loss: 48.372100830078125\n",
      "epoch: 79,  batch step: 228, loss: 3.7482242584228516\n",
      "epoch: 79,  batch step: 229, loss: 2.249674081802368\n",
      "epoch: 79,  batch step: 230, loss: 3.1760783195495605\n",
      "epoch: 79,  batch step: 231, loss: 3.00628662109375\n",
      "epoch: 79,  batch step: 232, loss: 30.648731231689453\n",
      "epoch: 79,  batch step: 233, loss: 3.288107395172119\n",
      "epoch: 79,  batch step: 234, loss: 17.69865608215332\n",
      "epoch: 79,  batch step: 235, loss: 9.968332290649414\n",
      "epoch: 79,  batch step: 236, loss: 31.87786102294922\n",
      "epoch: 79,  batch step: 237, loss: 5.041730880737305\n",
      "epoch: 79,  batch step: 238, loss: 3.2857606410980225\n",
      "epoch: 79,  batch step: 239, loss: 2.8601319789886475\n",
      "epoch: 79,  batch step: 240, loss: 3.6409811973571777\n",
      "epoch: 79,  batch step: 241, loss: 1.9349464178085327\n",
      "epoch: 79,  batch step: 242, loss: 15.597627639770508\n",
      "epoch: 79,  batch step: 243, loss: 12.341510772705078\n",
      "epoch: 79,  batch step: 244, loss: 17.39444351196289\n",
      "epoch: 79,  batch step: 245, loss: 3.341916561126709\n",
      "epoch: 79,  batch step: 246, loss: 3.661271572113037\n",
      "epoch: 79,  batch step: 247, loss: 17.441940307617188\n",
      "epoch: 79,  batch step: 248, loss: 3.462043523788452\n",
      "epoch: 79,  batch step: 249, loss: 3.344731092453003\n",
      "epoch: 79,  batch step: 250, loss: 23.536041259765625\n",
      "epoch: 79,  batch step: 251, loss: 19.958297729492188\n",
      "finished saving checkpoints\n",
      "validation error epoch  79:    tensor(67.8025, device='cuda:0')\n",
      "316\n",
      "epoch: 80,  batch step: 0, loss: 2.604923725128174\n",
      "epoch: 80,  batch step: 1, loss: 9.680150032043457\n",
      "epoch: 80,  batch step: 2, loss: 18.39742660522461\n",
      "epoch: 80,  batch step: 3, loss: 11.64321517944336\n",
      "epoch: 80,  batch step: 4, loss: 20.398744583129883\n",
      "epoch: 80,  batch step: 5, loss: 3.5992183685302734\n",
      "epoch: 80,  batch step: 6, loss: 8.930726051330566\n",
      "epoch: 80,  batch step: 7, loss: 13.537818908691406\n",
      "epoch: 80,  batch step: 8, loss: 2.445995807647705\n",
      "epoch: 80,  batch step: 9, loss: 18.399581909179688\n",
      "epoch: 80,  batch step: 10, loss: 23.75777816772461\n",
      "epoch: 80,  batch step: 11, loss: 16.54370880126953\n",
      "epoch: 80,  batch step: 12, loss: 40.746315002441406\n",
      "epoch: 80,  batch step: 13, loss: 9.268354415893555\n",
      "epoch: 80,  batch step: 14, loss: 3.1175830364227295\n",
      "epoch: 80,  batch step: 15, loss: 58.918495178222656\n",
      "epoch: 80,  batch step: 16, loss: 3.4166553020477295\n",
      "epoch: 80,  batch step: 17, loss: 3.518800735473633\n",
      "epoch: 80,  batch step: 18, loss: 1.9802203178405762\n",
      "epoch: 80,  batch step: 19, loss: 14.478569030761719\n",
      "epoch: 80,  batch step: 20, loss: 9.026251792907715\n",
      "epoch: 80,  batch step: 21, loss: 11.211366653442383\n",
      "epoch: 80,  batch step: 22, loss: 1.794346570968628\n",
      "epoch: 80,  batch step: 23, loss: 2.529331684112549\n",
      "epoch: 80,  batch step: 24, loss: 8.137860298156738\n",
      "epoch: 80,  batch step: 25, loss: 48.80622100830078\n",
      "epoch: 80,  batch step: 26, loss: 131.57290649414062\n",
      "epoch: 80,  batch step: 27, loss: 12.572488784790039\n",
      "epoch: 80,  batch step: 28, loss: 17.096832275390625\n",
      "epoch: 80,  batch step: 29, loss: 14.07858943939209\n",
      "epoch: 80,  batch step: 30, loss: 5.520442008972168\n",
      "epoch: 80,  batch step: 31, loss: 5.651528835296631\n",
      "epoch: 80,  batch step: 32, loss: 2.652333974838257\n",
      "epoch: 80,  batch step: 33, loss: 11.036941528320312\n",
      "epoch: 80,  batch step: 34, loss: 12.033461570739746\n",
      "epoch: 80,  batch step: 35, loss: 4.699382781982422\n",
      "epoch: 80,  batch step: 36, loss: 9.815879821777344\n",
      "epoch: 80,  batch step: 37, loss: 2.4854917526245117\n",
      "epoch: 80,  batch step: 38, loss: 2.5196733474731445\n",
      "epoch: 80,  batch step: 39, loss: 16.257543563842773\n",
      "epoch: 80,  batch step: 40, loss: 4.625438690185547\n",
      "epoch: 80,  batch step: 41, loss: 3.583102226257324\n",
      "epoch: 80,  batch step: 42, loss: 3.170949697494507\n",
      "epoch: 80,  batch step: 43, loss: 3.2502007484436035\n",
      "epoch: 80,  batch step: 44, loss: 5.341919898986816\n",
      "epoch: 80,  batch step: 45, loss: 3.3170065879821777\n",
      "epoch: 80,  batch step: 46, loss: 1.9180381298065186\n",
      "epoch: 80,  batch step: 47, loss: 2.5214648246765137\n",
      "epoch: 80,  batch step: 48, loss: 16.034900665283203\n",
      "epoch: 80,  batch step: 49, loss: 4.380825042724609\n",
      "epoch: 80,  batch step: 50, loss: 17.497451782226562\n",
      "epoch: 80,  batch step: 51, loss: 2.4075536727905273\n",
      "epoch: 80,  batch step: 52, loss: 11.660436630249023\n",
      "epoch: 80,  batch step: 53, loss: 16.699405670166016\n",
      "epoch: 80,  batch step: 54, loss: 2.6691653728485107\n",
      "epoch: 80,  batch step: 55, loss: 1.8845446109771729\n",
      "epoch: 80,  batch step: 56, loss: 4.824953079223633\n",
      "epoch: 80,  batch step: 57, loss: 3.8694515228271484\n",
      "epoch: 80,  batch step: 58, loss: 10.407369613647461\n",
      "epoch: 80,  batch step: 59, loss: 15.316930770874023\n",
      "epoch: 80,  batch step: 60, loss: 3.6444032192230225\n",
      "epoch: 80,  batch step: 61, loss: 31.406814575195312\n",
      "epoch: 80,  batch step: 62, loss: 12.599303245544434\n",
      "epoch: 80,  batch step: 63, loss: 2.306335926055908\n",
      "epoch: 80,  batch step: 64, loss: 2.250770092010498\n",
      "epoch: 80,  batch step: 65, loss: 2.409876585006714\n",
      "epoch: 80,  batch step: 66, loss: 11.58576774597168\n",
      "epoch: 80,  batch step: 67, loss: 32.926597595214844\n",
      "epoch: 80,  batch step: 68, loss: 4.389012813568115\n",
      "epoch: 80,  batch step: 69, loss: 3.2900643348693848\n",
      "epoch: 80,  batch step: 70, loss: 1.8775619268417358\n",
      "epoch: 80,  batch step: 71, loss: 77.5228271484375\n",
      "epoch: 80,  batch step: 72, loss: 2.77626895904541\n",
      "epoch: 80,  batch step: 73, loss: 1.6355125904083252\n",
      "epoch: 80,  batch step: 74, loss: 6.106235504150391\n",
      "epoch: 80,  batch step: 75, loss: 2.685946464538574\n",
      "epoch: 80,  batch step: 76, loss: 14.505914688110352\n",
      "epoch: 80,  batch step: 77, loss: 2.1065890789031982\n",
      "epoch: 80,  batch step: 78, loss: 2.0402050018310547\n",
      "epoch: 80,  batch step: 79, loss: 2.9874658584594727\n",
      "epoch: 80,  batch step: 80, loss: 2.793720245361328\n",
      "epoch: 80,  batch step: 81, loss: 10.878230094909668\n",
      "epoch: 80,  batch step: 82, loss: 3.206345319747925\n",
      "epoch: 80,  batch step: 83, loss: 10.776708602905273\n",
      "epoch: 80,  batch step: 84, loss: 4.369000434875488\n",
      "epoch: 80,  batch step: 85, loss: 2.1329782009124756\n",
      "epoch: 80,  batch step: 86, loss: 24.20342254638672\n",
      "epoch: 80,  batch step: 87, loss: 1.701786994934082\n",
      "epoch: 80,  batch step: 88, loss: 14.594317436218262\n",
      "epoch: 80,  batch step: 89, loss: 14.279437065124512\n",
      "epoch: 80,  batch step: 90, loss: 58.72930145263672\n",
      "epoch: 80,  batch step: 91, loss: 2.3805460929870605\n",
      "epoch: 80,  batch step: 92, loss: 3.281007766723633\n",
      "epoch: 80,  batch step: 93, loss: 2.2682909965515137\n",
      "epoch: 80,  batch step: 94, loss: 1.7385060787200928\n",
      "epoch: 80,  batch step: 95, loss: 16.242258071899414\n",
      "epoch: 80,  batch step: 96, loss: 10.714859008789062\n",
      "epoch: 80,  batch step: 97, loss: 51.6715202331543\n",
      "epoch: 80,  batch step: 98, loss: 20.599828720092773\n",
      "epoch: 80,  batch step: 99, loss: 2.5252203941345215\n",
      "epoch: 80,  batch step: 100, loss: 6.377586364746094\n",
      "epoch: 80,  batch step: 101, loss: 1.8530693054199219\n",
      "epoch: 80,  batch step: 102, loss: 8.837322235107422\n",
      "epoch: 80,  batch step: 103, loss: 3.4352545738220215\n",
      "epoch: 80,  batch step: 104, loss: 5.421182632446289\n",
      "epoch: 80,  batch step: 105, loss: 47.999732971191406\n",
      "epoch: 80,  batch step: 106, loss: 3.557615041732788\n",
      "epoch: 80,  batch step: 107, loss: 8.213504791259766\n",
      "epoch: 80,  batch step: 108, loss: 11.67859172821045\n",
      "epoch: 80,  batch step: 109, loss: 2.7212836742401123\n",
      "epoch: 80,  batch step: 110, loss: 9.553583145141602\n",
      "epoch: 80,  batch step: 111, loss: 4.339588165283203\n",
      "epoch: 80,  batch step: 112, loss: 31.832807540893555\n",
      "epoch: 80,  batch step: 113, loss: 7.0529890060424805\n",
      "epoch: 80,  batch step: 114, loss: 29.757131576538086\n",
      "epoch: 80,  batch step: 115, loss: 24.41118621826172\n",
      "epoch: 80,  batch step: 116, loss: 13.733345031738281\n",
      "epoch: 80,  batch step: 117, loss: 18.489089965820312\n",
      "epoch: 80,  batch step: 118, loss: 2.5123889446258545\n",
      "epoch: 80,  batch step: 119, loss: 5.29833984375\n",
      "epoch: 80,  batch step: 120, loss: 2.3406944274902344\n",
      "epoch: 80,  batch step: 121, loss: 2.3274576663970947\n",
      "epoch: 80,  batch step: 122, loss: 2.016087055206299\n",
      "epoch: 80,  batch step: 123, loss: 10.188152313232422\n",
      "epoch: 80,  batch step: 124, loss: 6.94236946105957\n",
      "epoch: 80,  batch step: 125, loss: 17.20701026916504\n",
      "epoch: 80,  batch step: 126, loss: 10.074870109558105\n",
      "epoch: 80,  batch step: 127, loss: 3.06691312789917\n",
      "epoch: 80,  batch step: 128, loss: 3.3273792266845703\n",
      "epoch: 80,  batch step: 129, loss: 2.1129536628723145\n",
      "epoch: 80,  batch step: 130, loss: 27.012868881225586\n",
      "epoch: 80,  batch step: 131, loss: 59.31895065307617\n",
      "epoch: 80,  batch step: 132, loss: 11.712905883789062\n",
      "epoch: 80,  batch step: 133, loss: 2.713362455368042\n",
      "epoch: 80,  batch step: 134, loss: 54.444332122802734\n",
      "epoch: 80,  batch step: 135, loss: 11.303115844726562\n",
      "epoch: 80,  batch step: 136, loss: 2.5029797554016113\n",
      "epoch: 80,  batch step: 137, loss: 14.372913360595703\n",
      "epoch: 80,  batch step: 138, loss: 16.394826889038086\n",
      "epoch: 80,  batch step: 139, loss: 5.721526622772217\n",
      "epoch: 80,  batch step: 140, loss: 1.6409668922424316\n",
      "epoch: 80,  batch step: 141, loss: 40.0634765625\n",
      "epoch: 80,  batch step: 142, loss: 10.642889022827148\n",
      "epoch: 80,  batch step: 143, loss: 10.469327926635742\n",
      "epoch: 80,  batch step: 144, loss: 2.668246269226074\n",
      "epoch: 80,  batch step: 145, loss: 20.069046020507812\n",
      "epoch: 80,  batch step: 146, loss: 10.682493209838867\n",
      "epoch: 80,  batch step: 147, loss: 1.6621350049972534\n",
      "epoch: 80,  batch step: 148, loss: 2.993300199508667\n",
      "epoch: 80,  batch step: 149, loss: 39.58451843261719\n",
      "epoch: 80,  batch step: 150, loss: 12.098448753356934\n",
      "epoch: 80,  batch step: 151, loss: 2.102973461151123\n",
      "epoch: 80,  batch step: 152, loss: 14.941771507263184\n",
      "epoch: 80,  batch step: 153, loss: 3.0252437591552734\n",
      "epoch: 80,  batch step: 154, loss: 2.3476691246032715\n",
      "epoch: 80,  batch step: 155, loss: 2.393188953399658\n",
      "epoch: 80,  batch step: 156, loss: 1.9517192840576172\n",
      "epoch: 80,  batch step: 157, loss: 48.2913818359375\n",
      "epoch: 80,  batch step: 158, loss: 76.72434997558594\n",
      "epoch: 80,  batch step: 159, loss: 12.095592498779297\n",
      "epoch: 80,  batch step: 160, loss: 3.3660507202148438\n",
      "epoch: 80,  batch step: 161, loss: 35.54049301147461\n",
      "epoch: 80,  batch step: 162, loss: 12.107028007507324\n",
      "epoch: 80,  batch step: 163, loss: 2.371312141418457\n",
      "epoch: 80,  batch step: 164, loss: 10.797362327575684\n",
      "epoch: 80,  batch step: 165, loss: 10.63411808013916\n",
      "epoch: 80,  batch step: 166, loss: 2.145382881164551\n",
      "epoch: 80,  batch step: 167, loss: 23.281909942626953\n",
      "epoch: 80,  batch step: 168, loss: 28.97174835205078\n",
      "epoch: 80,  batch step: 169, loss: 3.38718581199646\n",
      "epoch: 80,  batch step: 170, loss: 3.213158130645752\n",
      "epoch: 80,  batch step: 171, loss: 36.4193229675293\n",
      "epoch: 80,  batch step: 172, loss: 2.515570640563965\n",
      "epoch: 80,  batch step: 173, loss: 2.1732280254364014\n",
      "epoch: 80,  batch step: 174, loss: 92.45608520507812\n",
      "epoch: 80,  batch step: 175, loss: 12.808027267456055\n",
      "epoch: 80,  batch step: 176, loss: 12.64205551147461\n",
      "epoch: 80,  batch step: 177, loss: 2.94376277923584\n",
      "epoch: 80,  batch step: 178, loss: 14.682252883911133\n",
      "epoch: 80,  batch step: 179, loss: 3.092952251434326\n",
      "epoch: 80,  batch step: 180, loss: 2.426265239715576\n",
      "epoch: 80,  batch step: 181, loss: 54.76768112182617\n",
      "epoch: 80,  batch step: 182, loss: 6.0220842361450195\n",
      "epoch: 80,  batch step: 183, loss: 3.861961841583252\n",
      "epoch: 80,  batch step: 184, loss: 11.969264030456543\n",
      "epoch: 80,  batch step: 185, loss: 2.640012264251709\n",
      "epoch: 80,  batch step: 186, loss: 4.680768966674805\n",
      "epoch: 80,  batch step: 187, loss: 9.743734359741211\n",
      "epoch: 80,  batch step: 188, loss: 8.5933198928833\n",
      "epoch: 80,  batch step: 189, loss: 2.2330005168914795\n",
      "epoch: 80,  batch step: 190, loss: 21.521251678466797\n",
      "epoch: 80,  batch step: 191, loss: 50.22523498535156\n",
      "epoch: 80,  batch step: 192, loss: 4.122579574584961\n",
      "epoch: 80,  batch step: 193, loss: 31.711170196533203\n",
      "epoch: 80,  batch step: 194, loss: 40.46623229980469\n",
      "epoch: 80,  batch step: 195, loss: 2.038083791732788\n",
      "epoch: 80,  batch step: 196, loss: 27.884967803955078\n",
      "epoch: 80,  batch step: 197, loss: 3.0060174465179443\n",
      "epoch: 80,  batch step: 198, loss: 8.962632179260254\n",
      "epoch: 80,  batch step: 199, loss: 2.544537305831909\n",
      "epoch: 80,  batch step: 200, loss: 20.19036865234375\n",
      "epoch: 80,  batch step: 201, loss: 10.178668022155762\n",
      "epoch: 80,  batch step: 202, loss: 3.3196215629577637\n",
      "epoch: 80,  batch step: 203, loss: 14.914648056030273\n",
      "epoch: 80,  batch step: 204, loss: 28.839488983154297\n",
      "epoch: 80,  batch step: 205, loss: 2.436885356903076\n",
      "epoch: 80,  batch step: 206, loss: 5.831101894378662\n",
      "epoch: 80,  batch step: 207, loss: 14.560882568359375\n",
      "epoch: 80,  batch step: 208, loss: 23.047910690307617\n",
      "epoch: 80,  batch step: 209, loss: 20.65180778503418\n",
      "epoch: 80,  batch step: 210, loss: 2.357635498046875\n",
      "epoch: 80,  batch step: 211, loss: 13.844280242919922\n",
      "epoch: 80,  batch step: 212, loss: 1.9403009414672852\n",
      "epoch: 80,  batch step: 213, loss: 31.223609924316406\n",
      "epoch: 80,  batch step: 214, loss: 62.24772262573242\n",
      "epoch: 80,  batch step: 215, loss: 11.745996475219727\n",
      "epoch: 80,  batch step: 216, loss: 2.2134556770324707\n",
      "epoch: 80,  batch step: 217, loss: 12.88241958618164\n",
      "epoch: 80,  batch step: 218, loss: 2.3267569541931152\n",
      "epoch: 80,  batch step: 219, loss: 2.9689652919769287\n",
      "epoch: 80,  batch step: 220, loss: 5.285174369812012\n",
      "epoch: 80,  batch step: 221, loss: 1.7852873802185059\n",
      "epoch: 80,  batch step: 222, loss: 21.593582153320312\n",
      "epoch: 80,  batch step: 223, loss: 2.1651675701141357\n",
      "epoch: 80,  batch step: 224, loss: 17.12102699279785\n",
      "epoch: 80,  batch step: 225, loss: 43.41367721557617\n",
      "epoch: 80,  batch step: 226, loss: 11.366809844970703\n",
      "epoch: 80,  batch step: 227, loss: 3.3418192863464355\n",
      "epoch: 80,  batch step: 228, loss: 11.991774559020996\n",
      "epoch: 80,  batch step: 229, loss: 13.740965843200684\n",
      "epoch: 80,  batch step: 230, loss: 10.482053756713867\n",
      "epoch: 80,  batch step: 231, loss: 19.393638610839844\n",
      "epoch: 80,  batch step: 232, loss: 2.568790912628174\n",
      "epoch: 80,  batch step: 233, loss: 12.347091674804688\n",
      "epoch: 80,  batch step: 234, loss: 10.991128921508789\n",
      "epoch: 80,  batch step: 235, loss: 19.423076629638672\n",
      "epoch: 80,  batch step: 236, loss: 22.892467498779297\n",
      "epoch: 80,  batch step: 237, loss: 12.30906867980957\n",
      "epoch: 80,  batch step: 238, loss: 2.7773842811584473\n",
      "epoch: 80,  batch step: 239, loss: 3.6134414672851562\n",
      "epoch: 80,  batch step: 240, loss: 6.234039306640625\n",
      "epoch: 80,  batch step: 241, loss: 3.8893814086914062\n",
      "epoch: 80,  batch step: 242, loss: 21.600872039794922\n",
      "epoch: 80,  batch step: 243, loss: 13.483579635620117\n",
      "epoch: 80,  batch step: 244, loss: 1.5407190322875977\n",
      "epoch: 80,  batch step: 245, loss: 13.443930625915527\n",
      "epoch: 80,  batch step: 246, loss: 12.733814239501953\n",
      "epoch: 80,  batch step: 247, loss: 2.279376268386841\n",
      "epoch: 80,  batch step: 248, loss: 1.877558946609497\n",
      "epoch: 80,  batch step: 249, loss: 1.6742675304412842\n",
      "epoch: 80,  batch step: 250, loss: 1.9335784912109375\n",
      "epoch: 80,  batch step: 251, loss: 89.28630828857422\n",
      "validation error epoch  80:    tensor(67.0213, device='cuda:0')\n",
      "316\n",
      "epoch: 81,  batch step: 0, loss: 31.55062484741211\n",
      "epoch: 81,  batch step: 1, loss: 5.198609352111816\n",
      "epoch: 81,  batch step: 2, loss: 28.27284812927246\n",
      "epoch: 81,  batch step: 3, loss: 15.345602035522461\n",
      "epoch: 81,  batch step: 4, loss: 4.823890209197998\n",
      "epoch: 81,  batch step: 5, loss: 13.236361503601074\n",
      "epoch: 81,  batch step: 6, loss: 13.905479431152344\n",
      "epoch: 81,  batch step: 7, loss: 19.904708862304688\n",
      "epoch: 81,  batch step: 8, loss: 15.806657791137695\n",
      "epoch: 81,  batch step: 9, loss: 4.292534828186035\n",
      "epoch: 81,  batch step: 10, loss: 4.63979959487915\n",
      "epoch: 81,  batch step: 11, loss: 35.63922119140625\n",
      "epoch: 81,  batch step: 12, loss: 53.0329704284668\n",
      "epoch: 81,  batch step: 13, loss: 4.210930824279785\n",
      "epoch: 81,  batch step: 14, loss: 5.469103813171387\n",
      "epoch: 81,  batch step: 15, loss: 15.609343528747559\n",
      "epoch: 81,  batch step: 16, loss: 2.972932815551758\n",
      "epoch: 81,  batch step: 17, loss: 4.373574733734131\n",
      "epoch: 81,  batch step: 18, loss: 16.730222702026367\n",
      "epoch: 81,  batch step: 19, loss: 2.585447311401367\n",
      "epoch: 81,  batch step: 20, loss: 3.644885540008545\n",
      "epoch: 81,  batch step: 21, loss: 26.048873901367188\n",
      "epoch: 81,  batch step: 22, loss: 17.430980682373047\n",
      "epoch: 81,  batch step: 23, loss: 6.488544464111328\n",
      "epoch: 81,  batch step: 24, loss: 58.4907341003418\n",
      "epoch: 81,  batch step: 25, loss: 3.256854772567749\n",
      "epoch: 81,  batch step: 26, loss: 23.946117401123047\n",
      "epoch: 81,  batch step: 27, loss: 12.104262351989746\n",
      "epoch: 81,  batch step: 28, loss: 12.751086235046387\n",
      "epoch: 81,  batch step: 29, loss: 21.931011199951172\n",
      "epoch: 81,  batch step: 30, loss: 13.024101257324219\n",
      "epoch: 81,  batch step: 31, loss: 2.9585723876953125\n",
      "epoch: 81,  batch step: 32, loss: 4.652149677276611\n",
      "epoch: 81,  batch step: 33, loss: 5.067276954650879\n",
      "epoch: 81,  batch step: 34, loss: 17.14234161376953\n",
      "epoch: 81,  batch step: 35, loss: 16.278850555419922\n",
      "epoch: 81,  batch step: 36, loss: 13.150336265563965\n",
      "epoch: 81,  batch step: 37, loss: 18.28094482421875\n",
      "epoch: 81,  batch step: 38, loss: 30.063528060913086\n",
      "epoch: 81,  batch step: 39, loss: 58.385921478271484\n",
      "epoch: 81,  batch step: 40, loss: 43.7971305847168\n",
      "epoch: 81,  batch step: 41, loss: 18.349061965942383\n",
      "epoch: 81,  batch step: 42, loss: 2.971860408782959\n",
      "epoch: 81,  batch step: 43, loss: 2.4882383346557617\n",
      "epoch: 81,  batch step: 44, loss: 34.36573791503906\n",
      "epoch: 81,  batch step: 45, loss: 30.1967830657959\n",
      "epoch: 81,  batch step: 46, loss: 3.8705146312713623\n",
      "epoch: 81,  batch step: 47, loss: 3.1226463317871094\n",
      "epoch: 81,  batch step: 48, loss: 23.295988082885742\n",
      "epoch: 81,  batch step: 49, loss: 16.804861068725586\n",
      "epoch: 81,  batch step: 50, loss: 13.352643966674805\n",
      "epoch: 81,  batch step: 51, loss: 71.46092224121094\n",
      "epoch: 81,  batch step: 52, loss: 48.853721618652344\n",
      "epoch: 81,  batch step: 53, loss: 3.075064182281494\n",
      "epoch: 81,  batch step: 54, loss: 6.975914001464844\n",
      "epoch: 81,  batch step: 55, loss: 4.8407158851623535\n",
      "epoch: 81,  batch step: 56, loss: 22.30451774597168\n",
      "epoch: 81,  batch step: 57, loss: 2.578991413116455\n",
      "epoch: 81,  batch step: 58, loss: 13.839231491088867\n",
      "epoch: 81,  batch step: 59, loss: 18.421545028686523\n",
      "epoch: 81,  batch step: 60, loss: 5.124489784240723\n",
      "epoch: 81,  batch step: 61, loss: 14.75477123260498\n",
      "epoch: 81,  batch step: 62, loss: 2.32777738571167\n",
      "epoch: 81,  batch step: 63, loss: 3.8857364654541016\n",
      "epoch: 81,  batch step: 64, loss: 15.345904350280762\n",
      "epoch: 81,  batch step: 65, loss: 2.736140727996826\n",
      "epoch: 81,  batch step: 66, loss: 16.078018188476562\n",
      "epoch: 81,  batch step: 67, loss: 2.978545904159546\n",
      "epoch: 81,  batch step: 68, loss: 2.2109532356262207\n",
      "epoch: 81,  batch step: 69, loss: 5.683799743652344\n",
      "epoch: 81,  batch step: 70, loss: 2.7551932334899902\n",
      "epoch: 81,  batch step: 71, loss: 6.545495510101318\n",
      "epoch: 81,  batch step: 72, loss: 104.25029754638672\n",
      "epoch: 81,  batch step: 73, loss: 3.8112151622772217\n",
      "epoch: 81,  batch step: 74, loss: 20.055217742919922\n",
      "epoch: 81,  batch step: 75, loss: 2.474752902984619\n",
      "epoch: 81,  batch step: 76, loss: 3.694065809249878\n",
      "epoch: 81,  batch step: 77, loss: 16.43069076538086\n",
      "epoch: 81,  batch step: 78, loss: 17.61260986328125\n",
      "epoch: 81,  batch step: 79, loss: 2.3865318298339844\n",
      "epoch: 81,  batch step: 80, loss: 14.788122177124023\n",
      "epoch: 81,  batch step: 81, loss: 17.724700927734375\n",
      "epoch: 81,  batch step: 82, loss: 3.3409366607666016\n",
      "epoch: 81,  batch step: 83, loss: 15.67724609375\n",
      "epoch: 81,  batch step: 84, loss: 35.98362350463867\n",
      "epoch: 81,  batch step: 85, loss: 3.088090658187866\n",
      "epoch: 81,  batch step: 86, loss: 2.693061351776123\n",
      "epoch: 81,  batch step: 87, loss: 132.38876342773438\n",
      "epoch: 81,  batch step: 88, loss: 9.496679306030273\n",
      "epoch: 81,  batch step: 89, loss: 20.404705047607422\n",
      "epoch: 81,  batch step: 90, loss: 8.146319389343262\n",
      "epoch: 81,  batch step: 91, loss: 30.31399917602539\n",
      "epoch: 81,  batch step: 92, loss: 2.3322720527648926\n",
      "epoch: 81,  batch step: 93, loss: 4.210239410400391\n",
      "epoch: 81,  batch step: 94, loss: 2.626089572906494\n",
      "epoch: 81,  batch step: 95, loss: 7.895750045776367\n",
      "epoch: 81,  batch step: 96, loss: 7.349389553070068\n",
      "epoch: 81,  batch step: 97, loss: 2.5039143562316895\n",
      "epoch: 81,  batch step: 98, loss: 2.265883684158325\n",
      "epoch: 81,  batch step: 99, loss: 3.134260892868042\n",
      "epoch: 81,  batch step: 100, loss: 4.353667736053467\n",
      "epoch: 81,  batch step: 101, loss: 2.32016921043396\n",
      "epoch: 81,  batch step: 102, loss: 5.052502155303955\n",
      "epoch: 81,  batch step: 103, loss: 14.573654174804688\n",
      "epoch: 81,  batch step: 104, loss: 39.24763870239258\n",
      "epoch: 81,  batch step: 105, loss: 30.898603439331055\n",
      "epoch: 81,  batch step: 106, loss: 2.2359256744384766\n",
      "epoch: 81,  batch step: 107, loss: 9.167731285095215\n",
      "epoch: 81,  batch step: 108, loss: 51.620418548583984\n",
      "epoch: 81,  batch step: 109, loss: 2.6648731231689453\n",
      "epoch: 81,  batch step: 110, loss: 4.03436803817749\n",
      "epoch: 81,  batch step: 111, loss: 3.2209253311157227\n",
      "epoch: 81,  batch step: 112, loss: 11.757955551147461\n",
      "epoch: 81,  batch step: 113, loss: 3.3220629692077637\n",
      "epoch: 81,  batch step: 114, loss: 22.909652709960938\n",
      "epoch: 81,  batch step: 115, loss: 5.796262264251709\n",
      "epoch: 81,  batch step: 116, loss: 5.0370025634765625\n",
      "epoch: 81,  batch step: 117, loss: 15.158888816833496\n",
      "epoch: 81,  batch step: 118, loss: 4.719470024108887\n",
      "epoch: 81,  batch step: 119, loss: 8.63632583618164\n",
      "epoch: 81,  batch step: 120, loss: 9.218029022216797\n",
      "epoch: 81,  batch step: 121, loss: 9.046079635620117\n",
      "epoch: 81,  batch step: 122, loss: 14.716323852539062\n",
      "epoch: 81,  batch step: 123, loss: 12.7319974899292\n",
      "epoch: 81,  batch step: 124, loss: 9.695455551147461\n",
      "epoch: 81,  batch step: 125, loss: 5.446266174316406\n",
      "epoch: 81,  batch step: 126, loss: 2.5492122173309326\n",
      "epoch: 81,  batch step: 127, loss: 2.315701484680176\n",
      "epoch: 81,  batch step: 128, loss: 2.8241987228393555\n",
      "epoch: 81,  batch step: 129, loss: 2.2719249725341797\n",
      "epoch: 81,  batch step: 130, loss: 88.95010375976562\n",
      "epoch: 81,  batch step: 131, loss: 3.1623599529266357\n",
      "epoch: 81,  batch step: 132, loss: 2.31489634513855\n",
      "epoch: 81,  batch step: 133, loss: 30.221954345703125\n",
      "epoch: 81,  batch step: 134, loss: 2.3523271083831787\n",
      "epoch: 81,  batch step: 135, loss: 1.7251479625701904\n",
      "epoch: 81,  batch step: 136, loss: 39.209930419921875\n",
      "epoch: 81,  batch step: 137, loss: 12.600607872009277\n",
      "epoch: 81,  batch step: 138, loss: 33.44965744018555\n",
      "epoch: 81,  batch step: 139, loss: 10.666301727294922\n",
      "epoch: 81,  batch step: 140, loss: 2.1407790184020996\n",
      "epoch: 81,  batch step: 141, loss: 2.180154323577881\n",
      "epoch: 81,  batch step: 142, loss: 2.501330852508545\n",
      "epoch: 81,  batch step: 143, loss: 2.711522102355957\n",
      "epoch: 81,  batch step: 144, loss: 16.23211669921875\n",
      "epoch: 81,  batch step: 145, loss: 28.405532836914062\n",
      "epoch: 81,  batch step: 146, loss: 2.680783987045288\n",
      "epoch: 81,  batch step: 147, loss: 1.493232011795044\n",
      "epoch: 81,  batch step: 148, loss: 24.409215927124023\n",
      "epoch: 81,  batch step: 149, loss: 2.6385836601257324\n",
      "epoch: 81,  batch step: 150, loss: 9.466733932495117\n",
      "epoch: 81,  batch step: 151, loss: 1.9368457794189453\n",
      "epoch: 81,  batch step: 152, loss: 10.020608901977539\n",
      "epoch: 81,  batch step: 153, loss: 2.3010053634643555\n",
      "epoch: 81,  batch step: 154, loss: 1.7620849609375\n",
      "epoch: 81,  batch step: 155, loss: 2.50093936920166\n",
      "epoch: 81,  batch step: 156, loss: 32.12944412231445\n",
      "epoch: 81,  batch step: 157, loss: 3.850987434387207\n",
      "epoch: 81,  batch step: 158, loss: 2.5335710048675537\n",
      "epoch: 81,  batch step: 159, loss: 15.238813400268555\n",
      "epoch: 81,  batch step: 160, loss: 3.8307435512542725\n",
      "epoch: 81,  batch step: 161, loss: 1.8876752853393555\n",
      "epoch: 81,  batch step: 162, loss: 33.12022018432617\n",
      "epoch: 81,  batch step: 163, loss: 3.1432301998138428\n",
      "epoch: 81,  batch step: 164, loss: 29.645179748535156\n",
      "epoch: 81,  batch step: 165, loss: 3.0853824615478516\n",
      "epoch: 81,  batch step: 166, loss: 17.674026489257812\n",
      "epoch: 81,  batch step: 167, loss: 8.387031555175781\n",
      "epoch: 81,  batch step: 168, loss: 10.21194839477539\n",
      "epoch: 81,  batch step: 169, loss: 40.33279037475586\n",
      "epoch: 81,  batch step: 170, loss: 18.98983383178711\n",
      "epoch: 81,  batch step: 171, loss: 15.413352012634277\n",
      "epoch: 81,  batch step: 172, loss: 2.489830732345581\n",
      "epoch: 81,  batch step: 173, loss: 15.493682861328125\n",
      "epoch: 81,  batch step: 174, loss: 9.149162292480469\n",
      "epoch: 81,  batch step: 175, loss: 4.820319175720215\n",
      "epoch: 81,  batch step: 176, loss: 10.039390563964844\n",
      "epoch: 81,  batch step: 177, loss: 3.095461368560791\n",
      "epoch: 81,  batch step: 178, loss: 3.6999950408935547\n",
      "epoch: 81,  batch step: 179, loss: 17.0924072265625\n",
      "epoch: 81,  batch step: 180, loss: 2.493701219558716\n",
      "epoch: 81,  batch step: 181, loss: 2.8732523918151855\n",
      "epoch: 81,  batch step: 182, loss: 15.754263877868652\n",
      "epoch: 81,  batch step: 183, loss: 11.102729797363281\n",
      "epoch: 81,  batch step: 184, loss: 10.980009078979492\n",
      "epoch: 81,  batch step: 185, loss: 13.50965404510498\n",
      "epoch: 81,  batch step: 186, loss: 4.694657325744629\n",
      "epoch: 81,  batch step: 187, loss: 17.196807861328125\n",
      "epoch: 81,  batch step: 188, loss: 2.4044547080993652\n",
      "epoch: 81,  batch step: 189, loss: 3.3850598335266113\n",
      "epoch: 81,  batch step: 190, loss: 4.740749359130859\n",
      "epoch: 81,  batch step: 191, loss: 18.207538604736328\n",
      "epoch: 81,  batch step: 192, loss: 2.831298828125\n",
      "epoch: 81,  batch step: 193, loss: 3.567002296447754\n",
      "epoch: 81,  batch step: 194, loss: 51.24958801269531\n",
      "epoch: 81,  batch step: 195, loss: 32.443450927734375\n",
      "epoch: 81,  batch step: 196, loss: 8.524669647216797\n",
      "epoch: 81,  batch step: 197, loss: 32.34677505493164\n",
      "epoch: 81,  batch step: 198, loss: 3.4159560203552246\n",
      "epoch: 81,  batch step: 199, loss: 50.538692474365234\n",
      "epoch: 81,  batch step: 200, loss: 17.869300842285156\n",
      "epoch: 81,  batch step: 201, loss: 18.926687240600586\n",
      "epoch: 81,  batch step: 202, loss: 4.839048385620117\n",
      "epoch: 81,  batch step: 203, loss: 4.406067848205566\n",
      "epoch: 81,  batch step: 204, loss: 14.811716079711914\n",
      "epoch: 81,  batch step: 205, loss: 65.03230285644531\n",
      "epoch: 81,  batch step: 206, loss: 1.9876874685287476\n",
      "epoch: 81,  batch step: 207, loss: 15.448953628540039\n",
      "epoch: 81,  batch step: 208, loss: 39.22856903076172\n",
      "epoch: 81,  batch step: 209, loss: 46.77640914916992\n",
      "epoch: 81,  batch step: 210, loss: 50.697898864746094\n",
      "epoch: 81,  batch step: 211, loss: 13.820395469665527\n",
      "epoch: 81,  batch step: 212, loss: 3.3123221397399902\n",
      "epoch: 81,  batch step: 213, loss: 3.770151138305664\n",
      "epoch: 81,  batch step: 214, loss: 20.205942153930664\n",
      "epoch: 81,  batch step: 215, loss: 2.7987747192382812\n",
      "epoch: 81,  batch step: 216, loss: 7.762350082397461\n",
      "epoch: 81,  batch step: 217, loss: 5.80461311340332\n",
      "epoch: 81,  batch step: 218, loss: 2.296914577484131\n",
      "epoch: 81,  batch step: 219, loss: 20.130447387695312\n",
      "epoch: 81,  batch step: 220, loss: 15.209345817565918\n",
      "epoch: 81,  batch step: 221, loss: 2.783140182495117\n",
      "epoch: 81,  batch step: 222, loss: 2.6433041095733643\n",
      "epoch: 81,  batch step: 223, loss: 13.431816101074219\n",
      "epoch: 81,  batch step: 224, loss: 1.963919997215271\n",
      "epoch: 81,  batch step: 225, loss: 1.9689176082611084\n",
      "epoch: 81,  batch step: 226, loss: 11.334127426147461\n",
      "epoch: 81,  batch step: 227, loss: 2.400408983230591\n",
      "epoch: 81,  batch step: 228, loss: 2.1600186824798584\n",
      "epoch: 81,  batch step: 229, loss: 14.30972671508789\n",
      "epoch: 81,  batch step: 230, loss: 9.044048309326172\n",
      "epoch: 81,  batch step: 231, loss: 1.7758376598358154\n",
      "epoch: 81,  batch step: 232, loss: 2.6884703636169434\n",
      "epoch: 81,  batch step: 233, loss: 5.214644432067871\n",
      "epoch: 81,  batch step: 234, loss: 2.210664749145508\n",
      "epoch: 81,  batch step: 235, loss: 12.26439094543457\n",
      "epoch: 81,  batch step: 236, loss: 11.48006820678711\n",
      "epoch: 81,  batch step: 237, loss: 18.765958786010742\n",
      "epoch: 81,  batch step: 238, loss: 3.177112102508545\n",
      "epoch: 81,  batch step: 239, loss: 1.914081335067749\n",
      "epoch: 81,  batch step: 240, loss: 10.570143699645996\n",
      "epoch: 81,  batch step: 241, loss: 2.3749465942382812\n",
      "epoch: 81,  batch step: 242, loss: 9.824508666992188\n",
      "epoch: 81,  batch step: 243, loss: 11.932821273803711\n",
      "epoch: 81,  batch step: 244, loss: 21.78130340576172\n",
      "epoch: 81,  batch step: 245, loss: 67.4510498046875\n",
      "epoch: 81,  batch step: 246, loss: 13.005178451538086\n",
      "epoch: 81,  batch step: 247, loss: 3.7826571464538574\n",
      "epoch: 81,  batch step: 248, loss: 12.337207794189453\n",
      "epoch: 81,  batch step: 249, loss: 31.187030792236328\n",
      "epoch: 81,  batch step: 250, loss: 12.921075820922852\n",
      "epoch: 81,  batch step: 251, loss: 2060.52685546875\n",
      "validation error epoch  81:    tensor(67.4916, device='cuda:0')\n",
      "316\n",
      "epoch: 82,  batch step: 0, loss: 4.197833061218262\n",
      "epoch: 82,  batch step: 1, loss: 18.322233200073242\n",
      "epoch: 82,  batch step: 2, loss: 17.11916732788086\n",
      "epoch: 82,  batch step: 3, loss: 79.35676574707031\n",
      "epoch: 82,  batch step: 4, loss: 40.64349365234375\n",
      "epoch: 82,  batch step: 5, loss: 48.44874572753906\n",
      "epoch: 82,  batch step: 6, loss: 34.51725387573242\n",
      "epoch: 82,  batch step: 7, loss: 10.646154403686523\n",
      "epoch: 82,  batch step: 8, loss: 13.21290111541748\n",
      "epoch: 82,  batch step: 9, loss: 104.66004943847656\n",
      "epoch: 82,  batch step: 10, loss: 41.80315399169922\n",
      "epoch: 82,  batch step: 11, loss: 88.28443908691406\n",
      "epoch: 82,  batch step: 12, loss: 62.55097579956055\n",
      "epoch: 82,  batch step: 13, loss: 25.44178581237793\n",
      "epoch: 82,  batch step: 14, loss: 8.672725677490234\n",
      "epoch: 82,  batch step: 15, loss: 14.59158706665039\n",
      "epoch: 82,  batch step: 16, loss: 29.635250091552734\n",
      "epoch: 82,  batch step: 17, loss: 11.405014038085938\n",
      "epoch: 82,  batch step: 18, loss: 17.20589256286621\n",
      "epoch: 82,  batch step: 19, loss: 9.247617721557617\n",
      "epoch: 82,  batch step: 20, loss: 103.41692352294922\n",
      "epoch: 82,  batch step: 21, loss: 26.69063377380371\n",
      "epoch: 82,  batch step: 22, loss: 52.54888916015625\n",
      "epoch: 82,  batch step: 23, loss: 10.683412551879883\n",
      "epoch: 82,  batch step: 24, loss: 33.163970947265625\n",
      "epoch: 82,  batch step: 25, loss: 11.505145072937012\n",
      "epoch: 82,  batch step: 26, loss: 25.674741744995117\n",
      "epoch: 82,  batch step: 27, loss: 17.60370445251465\n",
      "epoch: 82,  batch step: 28, loss: 13.663881301879883\n",
      "epoch: 82,  batch step: 29, loss: 28.27663230895996\n",
      "epoch: 82,  batch step: 30, loss: 32.16571044921875\n",
      "epoch: 82,  batch step: 31, loss: 74.8966064453125\n",
      "epoch: 82,  batch step: 32, loss: 31.407705307006836\n",
      "epoch: 82,  batch step: 33, loss: 6.448281288146973\n",
      "epoch: 82,  batch step: 34, loss: 11.757404327392578\n",
      "epoch: 82,  batch step: 35, loss: 12.261402130126953\n",
      "epoch: 82,  batch step: 36, loss: 57.65052795410156\n",
      "epoch: 82,  batch step: 37, loss: 16.61434555053711\n",
      "epoch: 82,  batch step: 38, loss: 84.06936645507812\n",
      "epoch: 82,  batch step: 39, loss: 5.6108551025390625\n",
      "epoch: 82,  batch step: 40, loss: 6.776235103607178\n",
      "epoch: 82,  batch step: 41, loss: 320.1921081542969\n",
      "epoch: 82,  batch step: 42, loss: 34.92277526855469\n",
      "epoch: 82,  batch step: 43, loss: 7.624480247497559\n",
      "epoch: 82,  batch step: 44, loss: 42.83927536010742\n",
      "epoch: 82,  batch step: 45, loss: 122.5995101928711\n",
      "epoch: 82,  batch step: 46, loss: 291.8618469238281\n",
      "epoch: 82,  batch step: 47, loss: 6.3414411544799805\n",
      "epoch: 82,  batch step: 48, loss: 193.90676879882812\n",
      "epoch: 82,  batch step: 49, loss: 16.764408111572266\n",
      "epoch: 82,  batch step: 50, loss: 127.15447998046875\n",
      "epoch: 82,  batch step: 51, loss: 11.450594902038574\n",
      "epoch: 82,  batch step: 52, loss: 16.595550537109375\n",
      "epoch: 82,  batch step: 53, loss: 31.512802124023438\n",
      "epoch: 82,  batch step: 54, loss: 116.93746185302734\n",
      "epoch: 82,  batch step: 55, loss: 64.63430786132812\n",
      "epoch: 82,  batch step: 56, loss: 142.03701782226562\n",
      "epoch: 82,  batch step: 57, loss: 5.890132904052734\n",
      "epoch: 82,  batch step: 58, loss: 62.269527435302734\n",
      "epoch: 82,  batch step: 59, loss: 50.18510055541992\n",
      "epoch: 82,  batch step: 60, loss: 10.584249496459961\n",
      "epoch: 82,  batch step: 61, loss: 20.413471221923828\n",
      "epoch: 82,  batch step: 62, loss: 8.67673110961914\n",
      "epoch: 82,  batch step: 63, loss: 11.018051147460938\n",
      "epoch: 82,  batch step: 64, loss: 18.183914184570312\n",
      "epoch: 82,  batch step: 65, loss: 30.674015045166016\n",
      "epoch: 82,  batch step: 66, loss: 212.2689208984375\n",
      "epoch: 82,  batch step: 67, loss: 10.50703239440918\n",
      "epoch: 82,  batch step: 68, loss: 28.183950424194336\n",
      "epoch: 82,  batch step: 69, loss: 4.0692057609558105\n",
      "epoch: 82,  batch step: 70, loss: 6.956544399261475\n",
      "epoch: 82,  batch step: 71, loss: 8.07578182220459\n",
      "epoch: 82,  batch step: 72, loss: 321.3909912109375\n",
      "epoch: 82,  batch step: 73, loss: 6.173443794250488\n",
      "epoch: 82,  batch step: 74, loss: 32.31451416015625\n",
      "epoch: 82,  batch step: 75, loss: 7.60429573059082\n",
      "epoch: 82,  batch step: 76, loss: 16.798952102661133\n",
      "epoch: 82,  batch step: 77, loss: 6.913723945617676\n",
      "epoch: 82,  batch step: 78, loss: 23.48280143737793\n",
      "epoch: 82,  batch step: 79, loss: 112.09814453125\n",
      "epoch: 82,  batch step: 80, loss: 78.63113403320312\n",
      "epoch: 82,  batch step: 81, loss: 95.66561889648438\n",
      "epoch: 82,  batch step: 82, loss: 5.4182939529418945\n",
      "epoch: 82,  batch step: 83, loss: 16.842937469482422\n",
      "epoch: 82,  batch step: 84, loss: 55.42616271972656\n",
      "epoch: 82,  batch step: 85, loss: 18.159942626953125\n",
      "epoch: 82,  batch step: 86, loss: 8.197613716125488\n",
      "epoch: 82,  batch step: 87, loss: 26.558759689331055\n",
      "epoch: 82,  batch step: 88, loss: 14.874433517456055\n",
      "epoch: 82,  batch step: 89, loss: 18.203039169311523\n",
      "epoch: 82,  batch step: 90, loss: 13.973098754882812\n",
      "epoch: 82,  batch step: 91, loss: 15.59284782409668\n",
      "epoch: 82,  batch step: 92, loss: 4.569458484649658\n",
      "epoch: 82,  batch step: 93, loss: 39.89985656738281\n",
      "epoch: 82,  batch step: 94, loss: 14.568544387817383\n",
      "epoch: 82,  batch step: 95, loss: 9.78349494934082\n",
      "epoch: 82,  batch step: 96, loss: 8.399565696716309\n",
      "epoch: 82,  batch step: 97, loss: 20.09722328186035\n",
      "epoch: 82,  batch step: 98, loss: 11.526656150817871\n",
      "epoch: 82,  batch step: 99, loss: 11.11427116394043\n",
      "epoch: 82,  batch step: 100, loss: 14.306386947631836\n",
      "epoch: 82,  batch step: 101, loss: 39.209808349609375\n",
      "epoch: 82,  batch step: 102, loss: 167.41221618652344\n",
      "epoch: 82,  batch step: 103, loss: 31.49468994140625\n",
      "epoch: 82,  batch step: 104, loss: 181.283935546875\n",
      "epoch: 82,  batch step: 105, loss: 6.355072975158691\n",
      "epoch: 82,  batch step: 106, loss: 7.607369899749756\n",
      "epoch: 82,  batch step: 107, loss: 12.653970718383789\n",
      "epoch: 82,  batch step: 108, loss: 40.5857048034668\n",
      "epoch: 82,  batch step: 109, loss: 148.27947998046875\n",
      "epoch: 82,  batch step: 110, loss: 11.764808654785156\n",
      "epoch: 82,  batch step: 111, loss: 5.6059699058532715\n",
      "epoch: 82,  batch step: 112, loss: 30.010465621948242\n",
      "epoch: 82,  batch step: 113, loss: 13.617683410644531\n",
      "epoch: 82,  batch step: 114, loss: 56.21357345581055\n",
      "epoch: 82,  batch step: 115, loss: 9.456729888916016\n",
      "epoch: 82,  batch step: 116, loss: 34.0867805480957\n",
      "epoch: 82,  batch step: 117, loss: 4.3119611740112305\n",
      "epoch: 82,  batch step: 118, loss: 5.230504989624023\n",
      "epoch: 82,  batch step: 119, loss: 30.320621490478516\n",
      "epoch: 82,  batch step: 120, loss: 44.47957992553711\n",
      "epoch: 82,  batch step: 121, loss: 30.362302780151367\n",
      "epoch: 82,  batch step: 122, loss: 9.323868751525879\n",
      "epoch: 82,  batch step: 123, loss: 4.274052619934082\n",
      "epoch: 82,  batch step: 124, loss: 56.26060485839844\n",
      "epoch: 82,  batch step: 125, loss: 7.4016642570495605\n",
      "epoch: 82,  batch step: 126, loss: 51.515403747558594\n",
      "epoch: 82,  batch step: 127, loss: 39.13286590576172\n",
      "epoch: 82,  batch step: 128, loss: 26.188426971435547\n",
      "epoch: 82,  batch step: 129, loss: 47.63628387451172\n",
      "epoch: 82,  batch step: 130, loss: 46.332977294921875\n",
      "epoch: 82,  batch step: 131, loss: 14.920924186706543\n",
      "epoch: 82,  batch step: 132, loss: 18.35460662841797\n",
      "epoch: 82,  batch step: 133, loss: 9.193181037902832\n",
      "epoch: 82,  batch step: 134, loss: 6.815262794494629\n",
      "epoch: 82,  batch step: 135, loss: 46.328548431396484\n",
      "epoch: 82,  batch step: 136, loss: 75.07630157470703\n",
      "epoch: 82,  batch step: 137, loss: 52.97057342529297\n",
      "epoch: 82,  batch step: 138, loss: 6.327690124511719\n",
      "epoch: 82,  batch step: 139, loss: 3.877342939376831\n",
      "epoch: 82,  batch step: 140, loss: 10.521965026855469\n",
      "epoch: 82,  batch step: 141, loss: 28.197647094726562\n",
      "epoch: 82,  batch step: 142, loss: 5.582262992858887\n",
      "epoch: 82,  batch step: 143, loss: 53.05971908569336\n",
      "epoch: 82,  batch step: 144, loss: 58.35862350463867\n",
      "epoch: 82,  batch step: 145, loss: 64.78119659423828\n",
      "epoch: 82,  batch step: 146, loss: 60.2611083984375\n",
      "epoch: 82,  batch step: 147, loss: 29.82040023803711\n",
      "epoch: 82,  batch step: 148, loss: 4.067359924316406\n",
      "epoch: 82,  batch step: 149, loss: 35.251773834228516\n",
      "epoch: 82,  batch step: 150, loss: 9.043497085571289\n",
      "epoch: 82,  batch step: 151, loss: 11.117415428161621\n",
      "epoch: 82,  batch step: 152, loss: 8.182079315185547\n",
      "epoch: 82,  batch step: 153, loss: 33.08353805541992\n",
      "epoch: 82,  batch step: 154, loss: 8.826542854309082\n",
      "epoch: 82,  batch step: 155, loss: 13.012714385986328\n",
      "epoch: 82,  batch step: 156, loss: 9.764642715454102\n",
      "epoch: 82,  batch step: 157, loss: 3.057424545288086\n",
      "epoch: 82,  batch step: 158, loss: 20.79499626159668\n",
      "epoch: 82,  batch step: 159, loss: 8.832963943481445\n",
      "epoch: 82,  batch step: 160, loss: 3.3911776542663574\n",
      "epoch: 82,  batch step: 161, loss: 13.681392669677734\n",
      "epoch: 82,  batch step: 162, loss: 22.929088592529297\n",
      "epoch: 82,  batch step: 163, loss: 4.069542407989502\n",
      "epoch: 82,  batch step: 164, loss: 10.60208797454834\n",
      "epoch: 82,  batch step: 165, loss: 54.57218933105469\n",
      "epoch: 82,  batch step: 166, loss: 28.458324432373047\n",
      "epoch: 82,  batch step: 167, loss: 25.4510498046875\n",
      "epoch: 82,  batch step: 168, loss: 3.564033269882202\n",
      "epoch: 82,  batch step: 169, loss: 6.602862358093262\n",
      "epoch: 82,  batch step: 170, loss: 4.991825580596924\n",
      "epoch: 82,  batch step: 171, loss: 72.36148834228516\n",
      "epoch: 82,  batch step: 172, loss: 10.171855926513672\n",
      "epoch: 82,  batch step: 173, loss: 21.95241355895996\n",
      "epoch: 82,  batch step: 174, loss: 64.87722778320312\n",
      "epoch: 82,  batch step: 175, loss: 118.7053451538086\n",
      "epoch: 82,  batch step: 176, loss: 35.76057052612305\n",
      "epoch: 82,  batch step: 177, loss: 3.364816427230835\n",
      "epoch: 82,  batch step: 178, loss: 5.283945083618164\n",
      "epoch: 82,  batch step: 179, loss: 119.11813354492188\n",
      "epoch: 82,  batch step: 180, loss: 2.9907119274139404\n",
      "epoch: 82,  batch step: 181, loss: 40.024620056152344\n",
      "epoch: 82,  batch step: 182, loss: 19.006498336791992\n",
      "epoch: 82,  batch step: 183, loss: 17.834840774536133\n",
      "epoch: 82,  batch step: 184, loss: 8.247225761413574\n",
      "epoch: 82,  batch step: 185, loss: 12.105350494384766\n",
      "epoch: 82,  batch step: 186, loss: 3.8821589946746826\n",
      "epoch: 82,  batch step: 187, loss: 4.340641021728516\n",
      "epoch: 82,  batch step: 188, loss: 21.52483367919922\n",
      "epoch: 82,  batch step: 189, loss: 23.2251033782959\n",
      "epoch: 82,  batch step: 190, loss: 43.14738464355469\n",
      "epoch: 82,  batch step: 191, loss: 44.698333740234375\n",
      "epoch: 82,  batch step: 192, loss: 24.081790924072266\n",
      "epoch: 82,  batch step: 193, loss: 3.0650057792663574\n",
      "epoch: 82,  batch step: 194, loss: 6.297896385192871\n",
      "epoch: 82,  batch step: 195, loss: 61.709434509277344\n",
      "epoch: 82,  batch step: 196, loss: 28.29663848876953\n",
      "epoch: 82,  batch step: 197, loss: 83.83216094970703\n",
      "epoch: 82,  batch step: 198, loss: 32.7230110168457\n",
      "epoch: 82,  batch step: 199, loss: 5.189307689666748\n",
      "epoch: 82,  batch step: 200, loss: 11.923251152038574\n",
      "epoch: 82,  batch step: 201, loss: 5.607312202453613\n",
      "epoch: 82,  batch step: 202, loss: 12.155485153198242\n",
      "epoch: 82,  batch step: 203, loss: 18.098041534423828\n",
      "epoch: 82,  batch step: 204, loss: 56.96434783935547\n",
      "epoch: 82,  batch step: 205, loss: 29.03917694091797\n",
      "epoch: 82,  batch step: 206, loss: 5.958856105804443\n",
      "epoch: 82,  batch step: 207, loss: 50.873016357421875\n",
      "epoch: 82,  batch step: 208, loss: 5.696093559265137\n",
      "epoch: 82,  batch step: 209, loss: 3.9878153800964355\n",
      "epoch: 82,  batch step: 210, loss: 7.43302059173584\n",
      "epoch: 82,  batch step: 211, loss: 167.71041870117188\n",
      "epoch: 82,  batch step: 212, loss: 27.389629364013672\n",
      "epoch: 82,  batch step: 213, loss: 17.26028823852539\n",
      "epoch: 82,  batch step: 214, loss: 6.678918838500977\n",
      "epoch: 82,  batch step: 215, loss: 26.234777450561523\n",
      "epoch: 82,  batch step: 216, loss: 6.305556774139404\n",
      "epoch: 82,  batch step: 217, loss: 29.097408294677734\n",
      "epoch: 82,  batch step: 218, loss: 21.49298095703125\n",
      "epoch: 82,  batch step: 219, loss: 74.92333984375\n",
      "epoch: 82,  batch step: 220, loss: 8.05001449584961\n",
      "epoch: 82,  batch step: 221, loss: 38.89288330078125\n",
      "epoch: 82,  batch step: 222, loss: 49.65516662597656\n",
      "epoch: 82,  batch step: 223, loss: 16.689258575439453\n",
      "epoch: 82,  batch step: 224, loss: 6.063174247741699\n",
      "epoch: 82,  batch step: 225, loss: 57.976009368896484\n",
      "epoch: 82,  batch step: 226, loss: 11.415538787841797\n",
      "epoch: 82,  batch step: 227, loss: 91.11384582519531\n",
      "epoch: 82,  batch step: 228, loss: 28.83802604675293\n",
      "epoch: 82,  batch step: 229, loss: 4.25295877456665\n",
      "epoch: 82,  batch step: 230, loss: 3.52999210357666\n",
      "epoch: 82,  batch step: 231, loss: 22.119915008544922\n",
      "epoch: 82,  batch step: 232, loss: 29.38582420349121\n",
      "epoch: 82,  batch step: 233, loss: 5.407939910888672\n",
      "epoch: 82,  batch step: 234, loss: 5.461259365081787\n",
      "epoch: 82,  batch step: 235, loss: 17.09084701538086\n",
      "epoch: 82,  batch step: 236, loss: 20.03822135925293\n",
      "epoch: 82,  batch step: 237, loss: 5.138707160949707\n",
      "epoch: 82,  batch step: 238, loss: 118.05412292480469\n",
      "epoch: 82,  batch step: 239, loss: 8.165401458740234\n",
      "epoch: 82,  batch step: 240, loss: 2.9626998901367188\n",
      "epoch: 82,  batch step: 241, loss: 7.592587471008301\n",
      "epoch: 82,  batch step: 242, loss: 2.6852645874023438\n",
      "epoch: 82,  batch step: 243, loss: 25.86328887939453\n",
      "epoch: 82,  batch step: 244, loss: 12.299760818481445\n",
      "epoch: 82,  batch step: 245, loss: 35.893245697021484\n",
      "epoch: 82,  batch step: 246, loss: 5.568057537078857\n",
      "epoch: 82,  batch step: 247, loss: 29.351116180419922\n",
      "epoch: 82,  batch step: 248, loss: 5.349982738494873\n",
      "epoch: 82,  batch step: 249, loss: 122.25297546386719\n",
      "epoch: 82,  batch step: 250, loss: 7.932075500488281\n",
      "epoch: 82,  batch step: 251, loss: 47.23474884033203\n",
      "validation error epoch  82:    tensor(68.3614, device='cuda:0')\n",
      "316\n",
      "epoch: 83,  batch step: 0, loss: 47.08106994628906\n",
      "epoch: 83,  batch step: 1, loss: 24.432682037353516\n",
      "epoch: 83,  batch step: 2, loss: 35.954063415527344\n",
      "epoch: 83,  batch step: 3, loss: 2.1530838012695312\n",
      "epoch: 83,  batch step: 4, loss: 15.424471855163574\n",
      "epoch: 83,  batch step: 5, loss: 22.919700622558594\n",
      "epoch: 83,  batch step: 6, loss: 3.1472716331481934\n",
      "epoch: 83,  batch step: 7, loss: 20.500089645385742\n",
      "epoch: 83,  batch step: 8, loss: 25.372894287109375\n",
      "epoch: 83,  batch step: 9, loss: 54.031951904296875\n",
      "epoch: 83,  batch step: 10, loss: 16.50394058227539\n",
      "epoch: 83,  batch step: 11, loss: 73.85880279541016\n",
      "epoch: 83,  batch step: 12, loss: 2.0315113067626953\n",
      "epoch: 83,  batch step: 13, loss: 3.9364259243011475\n",
      "epoch: 83,  batch step: 14, loss: 19.091312408447266\n",
      "epoch: 83,  batch step: 15, loss: 3.556701421737671\n",
      "epoch: 83,  batch step: 16, loss: 36.2191162109375\n",
      "epoch: 83,  batch step: 17, loss: 48.48533630371094\n",
      "epoch: 83,  batch step: 18, loss: 4.753788948059082\n",
      "epoch: 83,  batch step: 19, loss: 50.492454528808594\n",
      "epoch: 83,  batch step: 20, loss: 3.6479549407958984\n",
      "epoch: 83,  batch step: 21, loss: 32.41131591796875\n",
      "epoch: 83,  batch step: 22, loss: 10.44330883026123\n",
      "epoch: 83,  batch step: 23, loss: 18.008148193359375\n",
      "epoch: 83,  batch step: 24, loss: 20.113351821899414\n",
      "epoch: 83,  batch step: 25, loss: 88.0492172241211\n",
      "epoch: 83,  batch step: 26, loss: 31.039281845092773\n",
      "epoch: 83,  batch step: 27, loss: 24.30523681640625\n",
      "epoch: 83,  batch step: 28, loss: 5.7576398849487305\n",
      "epoch: 83,  batch step: 29, loss: 4.700300216674805\n",
      "epoch: 83,  batch step: 30, loss: 2.5971364974975586\n",
      "epoch: 83,  batch step: 31, loss: 30.639251708984375\n",
      "epoch: 83,  batch step: 32, loss: 3.386723279953003\n",
      "epoch: 83,  batch step: 33, loss: 4.868795394897461\n",
      "epoch: 83,  batch step: 34, loss: 13.37449836730957\n",
      "epoch: 83,  batch step: 35, loss: 2.679431438446045\n",
      "epoch: 83,  batch step: 36, loss: 17.91090965270996\n",
      "epoch: 83,  batch step: 37, loss: 9.434488296508789\n",
      "epoch: 83,  batch step: 38, loss: 15.26071548461914\n",
      "epoch: 83,  batch step: 39, loss: 3.2718238830566406\n",
      "epoch: 83,  batch step: 40, loss: 4.563738822937012\n",
      "epoch: 83,  batch step: 41, loss: 21.855192184448242\n",
      "epoch: 83,  batch step: 42, loss: 5.054131507873535\n",
      "epoch: 83,  batch step: 43, loss: 21.830238342285156\n",
      "epoch: 83,  batch step: 44, loss: 3.415485143661499\n",
      "epoch: 83,  batch step: 45, loss: 15.820383071899414\n",
      "epoch: 83,  batch step: 46, loss: 19.396373748779297\n",
      "epoch: 83,  batch step: 47, loss: 11.807134628295898\n",
      "epoch: 83,  batch step: 48, loss: 28.571693420410156\n",
      "epoch: 83,  batch step: 49, loss: 53.88893127441406\n",
      "epoch: 83,  batch step: 50, loss: 2.9635024070739746\n",
      "epoch: 83,  batch step: 51, loss: 15.98100471496582\n",
      "epoch: 83,  batch step: 52, loss: 2.679175853729248\n",
      "epoch: 83,  batch step: 53, loss: 8.84423542022705\n",
      "epoch: 83,  batch step: 54, loss: 4.547179698944092\n",
      "epoch: 83,  batch step: 55, loss: 2.700753688812256\n",
      "epoch: 83,  batch step: 56, loss: 22.730911254882812\n",
      "epoch: 83,  batch step: 57, loss: 2.4017770290374756\n",
      "epoch: 83,  batch step: 58, loss: 2.9300570487976074\n",
      "epoch: 83,  batch step: 59, loss: 7.472498893737793\n",
      "epoch: 83,  batch step: 60, loss: 4.30053186416626\n",
      "epoch: 83,  batch step: 61, loss: 65.18350982666016\n",
      "epoch: 83,  batch step: 62, loss: 3.711663246154785\n",
      "epoch: 83,  batch step: 63, loss: 2.7885682582855225\n",
      "epoch: 83,  batch step: 64, loss: 3.293656826019287\n",
      "epoch: 83,  batch step: 65, loss: 28.719345092773438\n",
      "epoch: 83,  batch step: 66, loss: 3.1818695068359375\n",
      "epoch: 83,  batch step: 67, loss: 51.40754699707031\n",
      "epoch: 83,  batch step: 68, loss: 16.922584533691406\n",
      "epoch: 83,  batch step: 69, loss: 12.48321533203125\n",
      "epoch: 83,  batch step: 70, loss: 2.5317912101745605\n",
      "epoch: 83,  batch step: 71, loss: 3.309931516647339\n",
      "epoch: 83,  batch step: 72, loss: 2.5845000743865967\n",
      "epoch: 83,  batch step: 73, loss: 4.179630279541016\n",
      "epoch: 83,  batch step: 74, loss: 60.587215423583984\n",
      "epoch: 83,  batch step: 75, loss: 6.227504730224609\n",
      "epoch: 83,  batch step: 76, loss: 1.722970724105835\n",
      "epoch: 83,  batch step: 77, loss: 5.645216941833496\n",
      "epoch: 83,  batch step: 78, loss: 8.726846694946289\n",
      "epoch: 83,  batch step: 79, loss: 2.3908956050872803\n",
      "epoch: 83,  batch step: 80, loss: 6.2616119384765625\n",
      "epoch: 83,  batch step: 81, loss: 19.842573165893555\n",
      "epoch: 83,  batch step: 82, loss: 2.5108914375305176\n",
      "epoch: 83,  batch step: 83, loss: 14.574176788330078\n",
      "epoch: 83,  batch step: 84, loss: 38.072593688964844\n",
      "epoch: 83,  batch step: 85, loss: 7.7032365798950195\n",
      "epoch: 83,  batch step: 86, loss: 4.516090393066406\n",
      "epoch: 83,  batch step: 87, loss: 3.0380325317382812\n",
      "epoch: 83,  batch step: 88, loss: 5.022031784057617\n",
      "epoch: 83,  batch step: 89, loss: 5.900422096252441\n",
      "epoch: 83,  batch step: 90, loss: 3.730821132659912\n",
      "epoch: 83,  batch step: 91, loss: 39.050811767578125\n",
      "epoch: 83,  batch step: 92, loss: 10.120524406433105\n",
      "epoch: 83,  batch step: 93, loss: 3.6123862266540527\n",
      "epoch: 83,  batch step: 94, loss: 13.52900505065918\n",
      "epoch: 83,  batch step: 95, loss: 34.328521728515625\n",
      "epoch: 83,  batch step: 96, loss: 11.551294326782227\n",
      "epoch: 83,  batch step: 97, loss: 4.192931175231934\n",
      "epoch: 83,  batch step: 98, loss: 63.16545867919922\n",
      "epoch: 83,  batch step: 99, loss: 14.280029296875\n",
      "epoch: 83,  batch step: 100, loss: 12.145380973815918\n",
      "epoch: 83,  batch step: 101, loss: 3.1440296173095703\n",
      "epoch: 83,  batch step: 102, loss: 3.8870437145233154\n",
      "epoch: 83,  batch step: 103, loss: 66.71659851074219\n",
      "epoch: 83,  batch step: 104, loss: 3.853135824203491\n",
      "epoch: 83,  batch step: 105, loss: 3.2700958251953125\n",
      "epoch: 83,  batch step: 106, loss: 30.631446838378906\n",
      "epoch: 83,  batch step: 107, loss: 14.72978401184082\n",
      "epoch: 83,  batch step: 108, loss: 13.372961044311523\n",
      "epoch: 83,  batch step: 109, loss: 11.291888236999512\n",
      "epoch: 83,  batch step: 110, loss: 3.120814085006714\n",
      "epoch: 83,  batch step: 111, loss: 5.423989772796631\n",
      "epoch: 83,  batch step: 112, loss: 2.712671995162964\n",
      "epoch: 83,  batch step: 113, loss: 9.903753280639648\n",
      "epoch: 83,  batch step: 114, loss: 26.197906494140625\n",
      "epoch: 83,  batch step: 115, loss: 2.34574294090271\n",
      "epoch: 83,  batch step: 116, loss: 2.3808488845825195\n",
      "epoch: 83,  batch step: 117, loss: 2.404613971710205\n",
      "epoch: 83,  batch step: 118, loss: 5.936880111694336\n",
      "epoch: 83,  batch step: 119, loss: 44.75469970703125\n",
      "epoch: 83,  batch step: 120, loss: 2.671456813812256\n",
      "epoch: 83,  batch step: 121, loss: 6.9742326736450195\n",
      "epoch: 83,  batch step: 122, loss: 3.565126895904541\n",
      "epoch: 83,  batch step: 123, loss: 27.84474754333496\n",
      "epoch: 83,  batch step: 124, loss: 24.40035629272461\n",
      "epoch: 83,  batch step: 125, loss: 30.853078842163086\n",
      "epoch: 83,  batch step: 126, loss: 12.630989074707031\n",
      "epoch: 83,  batch step: 127, loss: 11.071828842163086\n",
      "epoch: 83,  batch step: 128, loss: 49.8138542175293\n",
      "epoch: 83,  batch step: 129, loss: 4.252255916595459\n",
      "epoch: 83,  batch step: 130, loss: 3.0090951919555664\n",
      "epoch: 83,  batch step: 131, loss: 8.932958602905273\n",
      "epoch: 83,  batch step: 132, loss: 16.430450439453125\n",
      "epoch: 83,  batch step: 133, loss: 34.519134521484375\n",
      "epoch: 83,  batch step: 134, loss: 4.07358455657959\n",
      "epoch: 83,  batch step: 135, loss: 21.701549530029297\n",
      "epoch: 83,  batch step: 136, loss: 13.870784759521484\n",
      "epoch: 83,  batch step: 137, loss: 5.16550350189209\n",
      "epoch: 83,  batch step: 138, loss: 117.84165954589844\n",
      "epoch: 83,  batch step: 139, loss: 10.185364723205566\n",
      "epoch: 83,  batch step: 140, loss: 8.727575302124023\n",
      "epoch: 83,  batch step: 141, loss: 17.993377685546875\n",
      "epoch: 83,  batch step: 142, loss: 5.768092632293701\n",
      "epoch: 83,  batch step: 143, loss: 12.432746887207031\n",
      "epoch: 83,  batch step: 144, loss: 3.1211905479431152\n",
      "epoch: 83,  batch step: 145, loss: 4.806015968322754\n",
      "epoch: 83,  batch step: 146, loss: 2.141956090927124\n",
      "epoch: 83,  batch step: 147, loss: 10.01551342010498\n",
      "epoch: 83,  batch step: 148, loss: 3.1973226070404053\n",
      "epoch: 83,  batch step: 149, loss: 20.381637573242188\n",
      "epoch: 83,  batch step: 150, loss: 32.05540466308594\n",
      "epoch: 83,  batch step: 151, loss: 10.137557983398438\n",
      "epoch: 83,  batch step: 152, loss: 4.109671592712402\n",
      "epoch: 83,  batch step: 153, loss: 3.3800711631774902\n",
      "epoch: 83,  batch step: 154, loss: 11.551225662231445\n",
      "epoch: 83,  batch step: 155, loss: 11.945415496826172\n",
      "epoch: 83,  batch step: 156, loss: 13.839042663574219\n",
      "epoch: 83,  batch step: 157, loss: 25.966115951538086\n",
      "epoch: 83,  batch step: 158, loss: 6.620189189910889\n",
      "epoch: 83,  batch step: 159, loss: 37.076595306396484\n",
      "epoch: 83,  batch step: 160, loss: 5.155037879943848\n",
      "epoch: 83,  batch step: 161, loss: 14.866576194763184\n",
      "epoch: 83,  batch step: 162, loss: 41.12799072265625\n",
      "epoch: 83,  batch step: 163, loss: 2.892920970916748\n",
      "epoch: 83,  batch step: 164, loss: 3.055710792541504\n",
      "epoch: 83,  batch step: 165, loss: 37.39072036743164\n",
      "epoch: 83,  batch step: 166, loss: 16.100561141967773\n",
      "epoch: 83,  batch step: 167, loss: 3.6632823944091797\n",
      "epoch: 83,  batch step: 168, loss: 27.206905364990234\n",
      "epoch: 83,  batch step: 169, loss: 4.202533721923828\n",
      "epoch: 83,  batch step: 170, loss: 7.219161510467529\n",
      "epoch: 83,  batch step: 171, loss: 3.9617204666137695\n",
      "epoch: 83,  batch step: 172, loss: 3.602604389190674\n",
      "epoch: 83,  batch step: 173, loss: 62.435508728027344\n",
      "epoch: 83,  batch step: 174, loss: 2.113112211227417\n",
      "epoch: 83,  batch step: 175, loss: 12.926511764526367\n",
      "epoch: 83,  batch step: 176, loss: 4.121750354766846\n",
      "epoch: 83,  batch step: 177, loss: 4.540307521820068\n",
      "epoch: 83,  batch step: 178, loss: 3.374419927597046\n",
      "epoch: 83,  batch step: 179, loss: 23.121131896972656\n",
      "epoch: 83,  batch step: 180, loss: 41.112892150878906\n",
      "epoch: 83,  batch step: 181, loss: 32.46907043457031\n",
      "epoch: 83,  batch step: 182, loss: 27.301273345947266\n",
      "epoch: 83,  batch step: 183, loss: 7.589964866638184\n",
      "epoch: 83,  batch step: 184, loss: 2.8666038513183594\n",
      "epoch: 83,  batch step: 185, loss: 2.3579750061035156\n",
      "epoch: 83,  batch step: 186, loss: 18.601825714111328\n",
      "epoch: 83,  batch step: 187, loss: 42.06703186035156\n",
      "epoch: 83,  batch step: 188, loss: 2.6895899772644043\n",
      "epoch: 83,  batch step: 189, loss: 1.932784080505371\n",
      "epoch: 83,  batch step: 190, loss: 5.690099716186523\n",
      "epoch: 83,  batch step: 191, loss: 17.130216598510742\n",
      "epoch: 83,  batch step: 192, loss: 2.6738696098327637\n",
      "epoch: 83,  batch step: 193, loss: 1.9787495136260986\n",
      "epoch: 83,  batch step: 194, loss: 2.2122488021850586\n",
      "epoch: 83,  batch step: 195, loss: 19.235267639160156\n",
      "epoch: 83,  batch step: 196, loss: 2.818113327026367\n",
      "epoch: 83,  batch step: 197, loss: 27.416152954101562\n",
      "epoch: 83,  batch step: 198, loss: 17.134916305541992\n",
      "epoch: 83,  batch step: 199, loss: 21.942401885986328\n",
      "epoch: 83,  batch step: 200, loss: 24.789548873901367\n",
      "epoch: 83,  batch step: 201, loss: 2.7442612648010254\n",
      "epoch: 83,  batch step: 202, loss: 2.2306268215179443\n",
      "epoch: 83,  batch step: 203, loss: 22.562236785888672\n",
      "epoch: 83,  batch step: 204, loss: 6.931734085083008\n",
      "epoch: 83,  batch step: 205, loss: 3.8256726264953613\n",
      "epoch: 83,  batch step: 206, loss: 34.30072021484375\n",
      "epoch: 83,  batch step: 207, loss: 6.039240837097168\n",
      "epoch: 83,  batch step: 208, loss: 142.57452392578125\n",
      "epoch: 83,  batch step: 209, loss: 34.83865737915039\n",
      "epoch: 83,  batch step: 210, loss: 18.409679412841797\n",
      "epoch: 83,  batch step: 211, loss: 26.623027801513672\n",
      "epoch: 83,  batch step: 212, loss: 54.422630310058594\n",
      "epoch: 83,  batch step: 213, loss: 58.842185974121094\n",
      "epoch: 83,  batch step: 214, loss: 2.734126567840576\n",
      "epoch: 83,  batch step: 215, loss: 3.084409713745117\n",
      "epoch: 83,  batch step: 216, loss: 2.455960750579834\n",
      "epoch: 83,  batch step: 217, loss: 18.37306022644043\n",
      "epoch: 83,  batch step: 218, loss: 13.91273307800293\n",
      "epoch: 83,  batch step: 219, loss: 3.838886260986328\n",
      "epoch: 83,  batch step: 220, loss: 24.75113296508789\n",
      "epoch: 83,  batch step: 221, loss: 10.094873428344727\n",
      "epoch: 83,  batch step: 222, loss: 3.5275943279266357\n",
      "epoch: 83,  batch step: 223, loss: 4.844545364379883\n",
      "epoch: 83,  batch step: 224, loss: 21.309873580932617\n",
      "epoch: 83,  batch step: 225, loss: 24.941959381103516\n",
      "epoch: 83,  batch step: 226, loss: 9.94192123413086\n",
      "epoch: 83,  batch step: 227, loss: 3.9388046264648438\n",
      "epoch: 83,  batch step: 228, loss: 33.112632751464844\n",
      "epoch: 83,  batch step: 229, loss: 65.46623992919922\n",
      "epoch: 83,  batch step: 230, loss: 11.196247100830078\n",
      "epoch: 83,  batch step: 231, loss: 3.968526840209961\n",
      "epoch: 83,  batch step: 232, loss: 1.712998867034912\n",
      "epoch: 83,  batch step: 233, loss: 26.097976684570312\n",
      "epoch: 83,  batch step: 234, loss: 3.364032745361328\n",
      "epoch: 83,  batch step: 235, loss: 12.277758598327637\n",
      "epoch: 83,  batch step: 236, loss: 12.99048137664795\n",
      "epoch: 83,  batch step: 237, loss: 19.516475677490234\n",
      "epoch: 83,  batch step: 238, loss: 11.254226684570312\n",
      "epoch: 83,  batch step: 239, loss: 4.261495590209961\n",
      "epoch: 83,  batch step: 240, loss: 32.537025451660156\n",
      "epoch: 83,  batch step: 241, loss: 12.289697647094727\n",
      "epoch: 83,  batch step: 242, loss: 2.46382474899292\n",
      "epoch: 83,  batch step: 243, loss: 2.4739441871643066\n",
      "epoch: 83,  batch step: 244, loss: 13.568748474121094\n",
      "epoch: 83,  batch step: 245, loss: 29.385831832885742\n",
      "epoch: 83,  batch step: 246, loss: 5.4322614669799805\n",
      "epoch: 83,  batch step: 247, loss: 1.6922707557678223\n",
      "epoch: 83,  batch step: 248, loss: 26.7069149017334\n",
      "epoch: 83,  batch step: 249, loss: 20.262107849121094\n",
      "epoch: 83,  batch step: 250, loss: 15.525808334350586\n",
      "epoch: 83,  batch step: 251, loss: 7.540040493011475\n",
      "validation error epoch  83:    tensor(66.6514, device='cuda:0')\n",
      "316\n",
      "epoch: 84,  batch step: 0, loss: 52.17168426513672\n",
      "epoch: 84,  batch step: 1, loss: 6.962812423706055\n",
      "epoch: 84,  batch step: 2, loss: 7.926485061645508\n",
      "epoch: 84,  batch step: 3, loss: 3.5957019329071045\n",
      "epoch: 84,  batch step: 4, loss: 23.402484893798828\n",
      "epoch: 84,  batch step: 5, loss: 3.463996648788452\n",
      "epoch: 84,  batch step: 6, loss: 3.2423248291015625\n",
      "epoch: 84,  batch step: 7, loss: 6.377053737640381\n",
      "epoch: 84,  batch step: 8, loss: 3.0562973022460938\n",
      "epoch: 84,  batch step: 9, loss: 8.101799011230469\n",
      "epoch: 84,  batch step: 10, loss: 2.142770767211914\n",
      "epoch: 84,  batch step: 11, loss: 11.019689559936523\n",
      "epoch: 84,  batch step: 12, loss: 23.33709716796875\n",
      "epoch: 84,  batch step: 13, loss: 30.33759880065918\n",
      "epoch: 84,  batch step: 14, loss: 32.03271484375\n",
      "epoch: 84,  batch step: 15, loss: 2.242122173309326\n",
      "epoch: 84,  batch step: 16, loss: 1.7817814350128174\n",
      "epoch: 84,  batch step: 17, loss: 9.54873275756836\n",
      "epoch: 84,  batch step: 18, loss: 7.289539337158203\n",
      "epoch: 84,  batch step: 19, loss: 17.61676025390625\n",
      "epoch: 84,  batch step: 20, loss: 56.55851364135742\n",
      "epoch: 84,  batch step: 21, loss: 58.0465087890625\n",
      "epoch: 84,  batch step: 22, loss: 12.525466918945312\n",
      "epoch: 84,  batch step: 23, loss: 15.705649375915527\n",
      "epoch: 84,  batch step: 24, loss: 20.526329040527344\n",
      "epoch: 84,  batch step: 25, loss: 19.236783981323242\n",
      "epoch: 84,  batch step: 26, loss: 2.365002155303955\n",
      "epoch: 84,  batch step: 27, loss: 33.045658111572266\n",
      "epoch: 84,  batch step: 28, loss: 2.3062686920166016\n",
      "epoch: 84,  batch step: 29, loss: 5.3771185874938965\n",
      "epoch: 84,  batch step: 30, loss: 31.734203338623047\n",
      "epoch: 84,  batch step: 31, loss: 21.901409149169922\n",
      "epoch: 84,  batch step: 32, loss: 4.899597644805908\n",
      "epoch: 84,  batch step: 33, loss: 4.066226005554199\n",
      "epoch: 84,  batch step: 34, loss: 4.9210710525512695\n",
      "epoch: 84,  batch step: 35, loss: 2.213357448577881\n",
      "epoch: 84,  batch step: 36, loss: 3.761551856994629\n",
      "epoch: 84,  batch step: 37, loss: 15.752805709838867\n",
      "epoch: 84,  batch step: 38, loss: 4.429435729980469\n",
      "epoch: 84,  batch step: 39, loss: 3.4492528438568115\n",
      "epoch: 84,  batch step: 40, loss: 9.959190368652344\n",
      "epoch: 84,  batch step: 41, loss: 3.0492210388183594\n",
      "epoch: 84,  batch step: 42, loss: 187.490234375\n",
      "epoch: 84,  batch step: 43, loss: 13.137239456176758\n",
      "epoch: 84,  batch step: 44, loss: 20.097246170043945\n",
      "epoch: 84,  batch step: 45, loss: 2.5199620723724365\n",
      "epoch: 84,  batch step: 46, loss: 22.506200790405273\n",
      "epoch: 84,  batch step: 47, loss: 13.432069778442383\n",
      "epoch: 84,  batch step: 48, loss: 4.120078086853027\n",
      "epoch: 84,  batch step: 49, loss: 21.282060623168945\n",
      "epoch: 84,  batch step: 50, loss: 13.8891019821167\n",
      "epoch: 84,  batch step: 51, loss: 3.139362335205078\n",
      "epoch: 84,  batch step: 52, loss: 12.691667556762695\n",
      "epoch: 84,  batch step: 53, loss: 18.96910858154297\n",
      "epoch: 84,  batch step: 54, loss: 9.780921936035156\n",
      "epoch: 84,  batch step: 55, loss: 2.5523812770843506\n",
      "epoch: 84,  batch step: 56, loss: 28.538776397705078\n",
      "epoch: 84,  batch step: 57, loss: 4.58670711517334\n",
      "epoch: 84,  batch step: 58, loss: 19.915117263793945\n",
      "epoch: 84,  batch step: 59, loss: 15.625933647155762\n",
      "epoch: 84,  batch step: 60, loss: 10.379383087158203\n",
      "epoch: 84,  batch step: 61, loss: 9.610182762145996\n",
      "epoch: 84,  batch step: 62, loss: 3.1023948192596436\n",
      "epoch: 84,  batch step: 63, loss: 2.8222854137420654\n",
      "epoch: 84,  batch step: 64, loss: 2.8626670837402344\n",
      "epoch: 84,  batch step: 65, loss: 3.1223273277282715\n",
      "epoch: 84,  batch step: 66, loss: 13.251677513122559\n",
      "epoch: 84,  batch step: 67, loss: 3.720996856689453\n",
      "epoch: 84,  batch step: 68, loss: 2.5779337882995605\n",
      "epoch: 84,  batch step: 69, loss: 9.53128719329834\n",
      "epoch: 84,  batch step: 70, loss: 11.949573516845703\n",
      "epoch: 84,  batch step: 71, loss: 2.617213487625122\n",
      "epoch: 84,  batch step: 72, loss: 3.923182964324951\n",
      "epoch: 84,  batch step: 73, loss: 22.786945343017578\n",
      "epoch: 84,  batch step: 74, loss: 70.40533447265625\n",
      "epoch: 84,  batch step: 75, loss: 2.5029189586639404\n",
      "epoch: 84,  batch step: 76, loss: 2.447148084640503\n",
      "epoch: 84,  batch step: 77, loss: 12.995275497436523\n",
      "epoch: 84,  batch step: 78, loss: 3.5239877700805664\n",
      "epoch: 84,  batch step: 79, loss: 2.635561227798462\n",
      "epoch: 84,  batch step: 80, loss: 2.8217005729675293\n",
      "epoch: 84,  batch step: 81, loss: 18.537809371948242\n",
      "epoch: 84,  batch step: 82, loss: 2.4924566745758057\n",
      "epoch: 84,  batch step: 83, loss: 67.74916076660156\n",
      "epoch: 84,  batch step: 84, loss: 3.197049617767334\n",
      "epoch: 84,  batch step: 85, loss: 24.020606994628906\n",
      "epoch: 84,  batch step: 86, loss: 37.27039337158203\n",
      "epoch: 84,  batch step: 87, loss: 29.11608123779297\n",
      "epoch: 84,  batch step: 88, loss: 24.3678035736084\n",
      "epoch: 84,  batch step: 89, loss: 40.54798889160156\n",
      "epoch: 84,  batch step: 90, loss: 26.4858455657959\n",
      "epoch: 84,  batch step: 91, loss: 13.515762329101562\n",
      "epoch: 84,  batch step: 92, loss: 4.981282711029053\n",
      "epoch: 84,  batch step: 93, loss: 3.1375179290771484\n",
      "epoch: 84,  batch step: 94, loss: 47.544349670410156\n",
      "epoch: 84,  batch step: 95, loss: 2.263484477996826\n",
      "epoch: 84,  batch step: 96, loss: 78.87283325195312\n",
      "epoch: 84,  batch step: 97, loss: 3.152508497238159\n",
      "epoch: 84,  batch step: 98, loss: 23.422401428222656\n",
      "epoch: 84,  batch step: 99, loss: 1.980544924736023\n",
      "epoch: 84,  batch step: 100, loss: 3.161740779876709\n",
      "epoch: 84,  batch step: 101, loss: 5.590875625610352\n",
      "epoch: 84,  batch step: 102, loss: 11.694300651550293\n",
      "epoch: 84,  batch step: 103, loss: 2.7118022441864014\n",
      "epoch: 84,  batch step: 104, loss: 2.6137726306915283\n",
      "epoch: 84,  batch step: 105, loss: 2.5140066146850586\n",
      "epoch: 84,  batch step: 106, loss: 5.212833881378174\n",
      "epoch: 84,  batch step: 107, loss: 2.6012837886810303\n",
      "epoch: 84,  batch step: 108, loss: 3.0743281841278076\n",
      "epoch: 84,  batch step: 109, loss: 26.123647689819336\n",
      "epoch: 84,  batch step: 110, loss: 16.51044464111328\n",
      "epoch: 84,  batch step: 111, loss: 20.11768913269043\n",
      "epoch: 84,  batch step: 112, loss: 4.663127422332764\n",
      "epoch: 84,  batch step: 113, loss: 3.2735376358032227\n",
      "epoch: 84,  batch step: 114, loss: 63.725624084472656\n",
      "epoch: 84,  batch step: 115, loss: 10.385872840881348\n",
      "epoch: 84,  batch step: 116, loss: 18.392330169677734\n",
      "epoch: 84,  batch step: 117, loss: 11.179122924804688\n",
      "epoch: 84,  batch step: 118, loss: 2.304737091064453\n",
      "epoch: 84,  batch step: 119, loss: 2.134823799133301\n",
      "epoch: 84,  batch step: 120, loss: 8.66314697265625\n",
      "epoch: 84,  batch step: 121, loss: 33.11213302612305\n",
      "epoch: 84,  batch step: 122, loss: 42.636680603027344\n",
      "epoch: 84,  batch step: 123, loss: 2.692107915878296\n",
      "epoch: 84,  batch step: 124, loss: 5.666533470153809\n",
      "epoch: 84,  batch step: 125, loss: 20.31656837463379\n",
      "epoch: 84,  batch step: 126, loss: 15.06003189086914\n",
      "epoch: 84,  batch step: 127, loss: 2.9908854961395264\n",
      "epoch: 84,  batch step: 128, loss: 12.281684875488281\n",
      "epoch: 84,  batch step: 129, loss: 2.119016170501709\n",
      "epoch: 84,  batch step: 130, loss: 4.780068397521973\n",
      "epoch: 84,  batch step: 131, loss: 13.303145408630371\n",
      "epoch: 84,  batch step: 132, loss: 2.5281691551208496\n",
      "epoch: 84,  batch step: 133, loss: 2.9198451042175293\n",
      "epoch: 84,  batch step: 134, loss: 17.196731567382812\n",
      "epoch: 84,  batch step: 135, loss: 2.639864921569824\n",
      "epoch: 84,  batch step: 136, loss: 3.622948169708252\n",
      "epoch: 84,  batch step: 137, loss: 4.644697666168213\n",
      "epoch: 84,  batch step: 138, loss: 2.904947280883789\n",
      "epoch: 84,  batch step: 139, loss: 13.145132064819336\n",
      "epoch: 84,  batch step: 140, loss: 3.098283529281616\n",
      "epoch: 84,  batch step: 141, loss: 2.4835352897644043\n",
      "epoch: 84,  batch step: 142, loss: 8.144811630249023\n",
      "epoch: 84,  batch step: 143, loss: 19.20572853088379\n",
      "epoch: 84,  batch step: 144, loss: 23.93787956237793\n",
      "epoch: 84,  batch step: 145, loss: 20.689878463745117\n",
      "epoch: 84,  batch step: 146, loss: 16.803390502929688\n",
      "epoch: 84,  batch step: 147, loss: 2.425612449645996\n",
      "epoch: 84,  batch step: 148, loss: 3.7717480659484863\n",
      "epoch: 84,  batch step: 149, loss: 73.9268798828125\n",
      "epoch: 84,  batch step: 150, loss: 40.064361572265625\n",
      "epoch: 84,  batch step: 151, loss: 3.1727473735809326\n",
      "epoch: 84,  batch step: 152, loss: 4.067949295043945\n",
      "epoch: 84,  batch step: 153, loss: 1.53579843044281\n",
      "epoch: 84,  batch step: 154, loss: 3.575805187225342\n",
      "epoch: 84,  batch step: 155, loss: 3.744283676147461\n",
      "epoch: 84,  batch step: 156, loss: 19.1278076171875\n",
      "epoch: 84,  batch step: 157, loss: 2.030815601348877\n",
      "epoch: 84,  batch step: 158, loss: 3.0112180709838867\n",
      "epoch: 84,  batch step: 159, loss: 10.545404434204102\n",
      "epoch: 84,  batch step: 160, loss: 18.636676788330078\n",
      "epoch: 84,  batch step: 161, loss: 5.554389476776123\n",
      "epoch: 84,  batch step: 162, loss: 14.346914291381836\n",
      "epoch: 84,  batch step: 163, loss: 3.312450885772705\n",
      "epoch: 84,  batch step: 164, loss: 47.83253479003906\n",
      "epoch: 84,  batch step: 165, loss: 9.629528045654297\n",
      "epoch: 84,  batch step: 166, loss: 5.092027187347412\n",
      "epoch: 84,  batch step: 167, loss: 2.8783633708953857\n",
      "epoch: 84,  batch step: 168, loss: 4.866939544677734\n",
      "epoch: 84,  batch step: 169, loss: 15.863092422485352\n",
      "epoch: 84,  batch step: 170, loss: 8.833928108215332\n",
      "epoch: 84,  batch step: 171, loss: 6.625325679779053\n",
      "epoch: 84,  batch step: 172, loss: 2.36014461517334\n",
      "epoch: 84,  batch step: 173, loss: 61.626983642578125\n",
      "epoch: 84,  batch step: 174, loss: 6.676087856292725\n",
      "epoch: 84,  batch step: 175, loss: 1.9413392543792725\n",
      "epoch: 84,  batch step: 176, loss: 2.9630932807922363\n",
      "epoch: 84,  batch step: 177, loss: 16.906530380249023\n",
      "epoch: 84,  batch step: 178, loss: 17.72321319580078\n",
      "epoch: 84,  batch step: 179, loss: 15.393427848815918\n",
      "epoch: 84,  batch step: 180, loss: 7.823622703552246\n",
      "epoch: 84,  batch step: 181, loss: 2.492056131362915\n",
      "epoch: 84,  batch step: 182, loss: 3.3334367275238037\n",
      "epoch: 84,  batch step: 183, loss: 4.868096351623535\n",
      "epoch: 84,  batch step: 184, loss: 30.00441551208496\n",
      "epoch: 84,  batch step: 185, loss: 2.4196267127990723\n",
      "epoch: 84,  batch step: 186, loss: 16.40736198425293\n",
      "epoch: 84,  batch step: 187, loss: 10.095529556274414\n",
      "epoch: 84,  batch step: 188, loss: 8.470932006835938\n",
      "epoch: 84,  batch step: 189, loss: 1.7053544521331787\n",
      "epoch: 84,  batch step: 190, loss: 15.091669082641602\n",
      "epoch: 84,  batch step: 191, loss: 1.8552249670028687\n",
      "epoch: 84,  batch step: 192, loss: 16.817399978637695\n",
      "epoch: 84,  batch step: 193, loss: 2.3782782554626465\n",
      "epoch: 84,  batch step: 194, loss: 8.381840705871582\n",
      "epoch: 84,  batch step: 195, loss: 16.387451171875\n",
      "epoch: 84,  batch step: 196, loss: 5.76914119720459\n",
      "epoch: 84,  batch step: 197, loss: 18.546070098876953\n",
      "epoch: 84,  batch step: 198, loss: 26.741680145263672\n",
      "epoch: 84,  batch step: 199, loss: 30.38275718688965\n",
      "epoch: 84,  batch step: 200, loss: 13.678702354431152\n",
      "epoch: 84,  batch step: 201, loss: 8.954695701599121\n",
      "epoch: 84,  batch step: 202, loss: 4.340197563171387\n",
      "epoch: 84,  batch step: 203, loss: 44.78603744506836\n",
      "epoch: 84,  batch step: 204, loss: 4.112017631530762\n",
      "epoch: 84,  batch step: 205, loss: 3.256518840789795\n",
      "epoch: 84,  batch step: 206, loss: 2.846229314804077\n",
      "epoch: 84,  batch step: 207, loss: 2.7073745727539062\n",
      "epoch: 84,  batch step: 208, loss: 9.152242660522461\n",
      "epoch: 84,  batch step: 209, loss: 3.1970577239990234\n",
      "epoch: 84,  batch step: 210, loss: 12.16573715209961\n",
      "epoch: 84,  batch step: 211, loss: 21.168853759765625\n",
      "epoch: 84,  batch step: 212, loss: 2.7112631797790527\n",
      "epoch: 84,  batch step: 213, loss: 2.2130165100097656\n",
      "epoch: 84,  batch step: 214, loss: 9.281129837036133\n",
      "epoch: 84,  batch step: 215, loss: 13.72674560546875\n",
      "epoch: 84,  batch step: 216, loss: 27.11414337158203\n",
      "epoch: 84,  batch step: 217, loss: 4.322544097900391\n",
      "epoch: 84,  batch step: 218, loss: 3.180968999862671\n",
      "epoch: 84,  batch step: 219, loss: 15.684106826782227\n",
      "epoch: 84,  batch step: 220, loss: 60.84352111816406\n",
      "epoch: 84,  batch step: 221, loss: 5.978052139282227\n",
      "epoch: 84,  batch step: 222, loss: 3.310760498046875\n",
      "epoch: 84,  batch step: 223, loss: 23.128454208374023\n",
      "epoch: 84,  batch step: 224, loss: 30.96600914001465\n",
      "epoch: 84,  batch step: 225, loss: 7.009944915771484\n",
      "epoch: 84,  batch step: 226, loss: 2.5438923835754395\n",
      "epoch: 84,  batch step: 227, loss: 2.446263074874878\n",
      "epoch: 84,  batch step: 228, loss: 3.572700023651123\n",
      "epoch: 84,  batch step: 229, loss: 14.190773010253906\n",
      "epoch: 84,  batch step: 230, loss: 17.329803466796875\n",
      "epoch: 84,  batch step: 231, loss: 12.129935264587402\n",
      "epoch: 84,  batch step: 232, loss: 11.41794490814209\n",
      "epoch: 84,  batch step: 233, loss: 4.8790411949157715\n",
      "epoch: 84,  batch step: 234, loss: 3.104292392730713\n",
      "epoch: 84,  batch step: 235, loss: 9.41118049621582\n",
      "epoch: 84,  batch step: 236, loss: 2.2393453121185303\n",
      "epoch: 84,  batch step: 237, loss: 3.557048797607422\n",
      "epoch: 84,  batch step: 238, loss: 9.088018417358398\n",
      "epoch: 84,  batch step: 239, loss: 23.899490356445312\n",
      "epoch: 84,  batch step: 240, loss: 51.93885803222656\n",
      "epoch: 84,  batch step: 241, loss: 17.222558975219727\n",
      "epoch: 84,  batch step: 242, loss: 1.5386346578598022\n",
      "epoch: 84,  batch step: 243, loss: 1.888828992843628\n",
      "epoch: 84,  batch step: 244, loss: 2.7141873836517334\n",
      "epoch: 84,  batch step: 245, loss: 3.342240810394287\n",
      "epoch: 84,  batch step: 246, loss: 8.323923110961914\n",
      "epoch: 84,  batch step: 247, loss: 3.551222562789917\n",
      "epoch: 84,  batch step: 248, loss: 2.5423641204833984\n",
      "epoch: 84,  batch step: 249, loss: 10.402910232543945\n",
      "epoch: 84,  batch step: 250, loss: 16.998779296875\n",
      "epoch: 84,  batch step: 251, loss: 7.676128387451172\n",
      "validation error epoch  84:    tensor(63.9146, device='cuda:0')\n",
      "316\n",
      "epoch: 85,  batch step: 0, loss: 31.61701774597168\n",
      "epoch: 85,  batch step: 1, loss: 5.008591651916504\n",
      "epoch: 85,  batch step: 2, loss: 27.536651611328125\n",
      "epoch: 85,  batch step: 3, loss: 2.1746973991394043\n",
      "epoch: 85,  batch step: 4, loss: 18.6300106048584\n",
      "epoch: 85,  batch step: 5, loss: 12.424208641052246\n",
      "epoch: 85,  batch step: 6, loss: 4.341414928436279\n",
      "epoch: 85,  batch step: 7, loss: 2.326936721801758\n",
      "epoch: 85,  batch step: 8, loss: 5.273262977600098\n",
      "epoch: 85,  batch step: 9, loss: 8.560247421264648\n",
      "epoch: 85,  batch step: 10, loss: 5.877732753753662\n",
      "epoch: 85,  batch step: 11, loss: 6.219311714172363\n",
      "epoch: 85,  batch step: 12, loss: 16.009010314941406\n",
      "epoch: 85,  batch step: 13, loss: 1.9526870250701904\n",
      "epoch: 85,  batch step: 14, loss: 3.321383476257324\n",
      "epoch: 85,  batch step: 15, loss: 5.414070129394531\n",
      "epoch: 85,  batch step: 16, loss: 6.525971412658691\n",
      "epoch: 85,  batch step: 17, loss: 4.013790130615234\n",
      "epoch: 85,  batch step: 18, loss: 3.12654185295105\n",
      "epoch: 85,  batch step: 19, loss: 2.628882884979248\n",
      "epoch: 85,  batch step: 20, loss: 2.6298961639404297\n",
      "epoch: 85,  batch step: 21, loss: 5.6650919914245605\n",
      "epoch: 85,  batch step: 22, loss: 23.336807250976562\n",
      "epoch: 85,  batch step: 23, loss: 10.996381759643555\n",
      "epoch: 85,  batch step: 24, loss: 11.184596061706543\n",
      "epoch: 85,  batch step: 25, loss: 2.7261874675750732\n",
      "epoch: 85,  batch step: 26, loss: 6.4969587326049805\n",
      "epoch: 85,  batch step: 27, loss: 3.4116759300231934\n",
      "epoch: 85,  batch step: 28, loss: 3.417661666870117\n",
      "epoch: 85,  batch step: 29, loss: 17.9261531829834\n",
      "epoch: 85,  batch step: 30, loss: 42.241310119628906\n",
      "epoch: 85,  batch step: 31, loss: 13.653091430664062\n",
      "epoch: 85,  batch step: 32, loss: 3.166365385055542\n",
      "epoch: 85,  batch step: 33, loss: 17.424320220947266\n",
      "epoch: 85,  batch step: 34, loss: 2.3888535499572754\n",
      "epoch: 85,  batch step: 35, loss: 20.139204025268555\n",
      "epoch: 85,  batch step: 36, loss: 2.5148558616638184\n",
      "epoch: 85,  batch step: 37, loss: 3.411684036254883\n",
      "epoch: 85,  batch step: 38, loss: 10.202190399169922\n",
      "epoch: 85,  batch step: 39, loss: 1.6181172132492065\n",
      "epoch: 85,  batch step: 40, loss: 36.48857879638672\n",
      "epoch: 85,  batch step: 41, loss: 1.9640710353851318\n",
      "epoch: 85,  batch step: 42, loss: 2.637293815612793\n",
      "epoch: 85,  batch step: 43, loss: 2.2603235244750977\n",
      "epoch: 85,  batch step: 44, loss: 12.0955228805542\n",
      "epoch: 85,  batch step: 45, loss: 2.0543997287750244\n",
      "epoch: 85,  batch step: 46, loss: 28.526180267333984\n",
      "epoch: 85,  batch step: 47, loss: 16.21165657043457\n",
      "epoch: 85,  batch step: 48, loss: 1.6221270561218262\n",
      "epoch: 85,  batch step: 49, loss: 18.110374450683594\n",
      "epoch: 85,  batch step: 50, loss: 22.434810638427734\n",
      "epoch: 85,  batch step: 51, loss: 9.77859115600586\n",
      "epoch: 85,  batch step: 52, loss: 2.893056869506836\n",
      "epoch: 85,  batch step: 53, loss: 16.302223205566406\n",
      "epoch: 85,  batch step: 54, loss: 15.47006607055664\n",
      "epoch: 85,  batch step: 55, loss: 6.654572486877441\n",
      "epoch: 85,  batch step: 56, loss: 2.11785888671875\n",
      "epoch: 85,  batch step: 57, loss: 15.475970268249512\n",
      "epoch: 85,  batch step: 58, loss: 8.807151794433594\n",
      "epoch: 85,  batch step: 59, loss: 3.1272945404052734\n",
      "epoch: 85,  batch step: 60, loss: 62.843528747558594\n",
      "epoch: 85,  batch step: 61, loss: 1.6895354986190796\n",
      "epoch: 85,  batch step: 62, loss: 13.467506408691406\n",
      "epoch: 85,  batch step: 63, loss: 2.2054924964904785\n",
      "epoch: 85,  batch step: 64, loss: 3.082472562789917\n",
      "epoch: 85,  batch step: 65, loss: 1.4542756080627441\n",
      "epoch: 85,  batch step: 66, loss: 7.9195876121521\n",
      "epoch: 85,  batch step: 67, loss: 3.3843417167663574\n",
      "epoch: 85,  batch step: 68, loss: 8.534015655517578\n",
      "epoch: 85,  batch step: 69, loss: 6.887921333312988\n",
      "epoch: 85,  batch step: 70, loss: 58.74923324584961\n",
      "epoch: 85,  batch step: 71, loss: 4.129321098327637\n",
      "epoch: 85,  batch step: 72, loss: 2.355029821395874\n",
      "epoch: 85,  batch step: 73, loss: 34.901153564453125\n",
      "epoch: 85,  batch step: 74, loss: 1.7480957508087158\n",
      "epoch: 85,  batch step: 75, loss: 10.878673553466797\n",
      "epoch: 85,  batch step: 76, loss: 30.44569969177246\n",
      "epoch: 85,  batch step: 77, loss: 12.785934448242188\n",
      "epoch: 85,  batch step: 78, loss: 28.3797607421875\n",
      "epoch: 85,  batch step: 79, loss: 9.449979782104492\n",
      "epoch: 85,  batch step: 80, loss: 1.8185911178588867\n",
      "epoch: 85,  batch step: 81, loss: 4.513941764831543\n",
      "epoch: 85,  batch step: 82, loss: 4.27583122253418\n",
      "epoch: 85,  batch step: 83, loss: 5.333948135375977\n",
      "epoch: 85,  batch step: 84, loss: 3.6449077129364014\n",
      "epoch: 85,  batch step: 85, loss: 2.2754619121551514\n",
      "epoch: 85,  batch step: 86, loss: 2.1418638229370117\n",
      "epoch: 85,  batch step: 87, loss: 1.9681051969528198\n",
      "epoch: 85,  batch step: 88, loss: 3.7550182342529297\n",
      "epoch: 85,  batch step: 89, loss: 21.454357147216797\n",
      "epoch: 85,  batch step: 90, loss: 4.368934631347656\n",
      "epoch: 85,  batch step: 91, loss: 19.917871475219727\n",
      "epoch: 85,  batch step: 92, loss: 2.3314104080200195\n",
      "epoch: 85,  batch step: 93, loss: 14.972814559936523\n",
      "epoch: 85,  batch step: 94, loss: 2.6742844581604004\n",
      "epoch: 85,  batch step: 95, loss: 2.094716787338257\n",
      "epoch: 85,  batch step: 96, loss: 2.207805633544922\n",
      "epoch: 85,  batch step: 97, loss: 8.09720516204834\n",
      "epoch: 85,  batch step: 98, loss: 61.763153076171875\n",
      "epoch: 85,  batch step: 99, loss: 11.026976585388184\n",
      "epoch: 85,  batch step: 100, loss: 10.82700252532959\n",
      "epoch: 85,  batch step: 101, loss: 2.5415496826171875\n",
      "epoch: 85,  batch step: 102, loss: 4.824139595031738\n",
      "epoch: 85,  batch step: 103, loss: 1.800419569015503\n",
      "epoch: 85,  batch step: 104, loss: 2.6354475021362305\n",
      "epoch: 85,  batch step: 105, loss: 10.87201976776123\n",
      "epoch: 85,  batch step: 106, loss: 3.7399089336395264\n",
      "epoch: 85,  batch step: 107, loss: 2.1320512294769287\n",
      "epoch: 85,  batch step: 108, loss: 2.7206180095672607\n",
      "epoch: 85,  batch step: 109, loss: 2.8191354274749756\n",
      "epoch: 85,  batch step: 110, loss: 2.4257636070251465\n",
      "epoch: 85,  batch step: 111, loss: 17.93736457824707\n",
      "epoch: 85,  batch step: 112, loss: 2.5696182250976562\n",
      "epoch: 85,  batch step: 113, loss: 2.160590887069702\n",
      "epoch: 85,  batch step: 114, loss: 2.5829050540924072\n",
      "epoch: 85,  batch step: 115, loss: 2.060500144958496\n",
      "epoch: 85,  batch step: 116, loss: 2.4503936767578125\n",
      "epoch: 85,  batch step: 117, loss: 20.73491668701172\n",
      "epoch: 85,  batch step: 118, loss: 3.5196311473846436\n",
      "epoch: 85,  batch step: 119, loss: 1.749325156211853\n",
      "epoch: 85,  batch step: 120, loss: 3.4084506034851074\n",
      "epoch: 85,  batch step: 121, loss: 17.45046615600586\n",
      "epoch: 85,  batch step: 122, loss: 2.435380697250366\n",
      "epoch: 85,  batch step: 123, loss: 2.598362684249878\n",
      "epoch: 85,  batch step: 124, loss: 2.616684913635254\n",
      "epoch: 85,  batch step: 125, loss: 11.35443115234375\n",
      "epoch: 85,  batch step: 126, loss: 2.0841174125671387\n",
      "epoch: 85,  batch step: 127, loss: 8.13479232788086\n",
      "epoch: 85,  batch step: 128, loss: 3.292266607284546\n",
      "epoch: 85,  batch step: 129, loss: 2.5002970695495605\n",
      "epoch: 85,  batch step: 130, loss: 10.060218811035156\n",
      "epoch: 85,  batch step: 131, loss: 22.113950729370117\n",
      "epoch: 85,  batch step: 132, loss: 3.510451316833496\n",
      "epoch: 85,  batch step: 133, loss: 42.56997299194336\n",
      "epoch: 85,  batch step: 134, loss: 1.9622869491577148\n",
      "epoch: 85,  batch step: 135, loss: 42.882606506347656\n",
      "epoch: 85,  batch step: 136, loss: 1.6777228116989136\n",
      "epoch: 85,  batch step: 137, loss: 14.292150497436523\n",
      "epoch: 85,  batch step: 138, loss: 2.6449337005615234\n",
      "epoch: 85,  batch step: 139, loss: 24.374542236328125\n",
      "epoch: 85,  batch step: 140, loss: 22.462448120117188\n",
      "epoch: 85,  batch step: 141, loss: 28.545269012451172\n",
      "epoch: 85,  batch step: 142, loss: 19.129037857055664\n",
      "epoch: 85,  batch step: 143, loss: 3.1395297050476074\n",
      "epoch: 85,  batch step: 144, loss: 4.8274359703063965\n",
      "epoch: 85,  batch step: 145, loss: 3.576756238937378\n",
      "epoch: 85,  batch step: 146, loss: 37.914794921875\n",
      "epoch: 85,  batch step: 147, loss: 2.6198601722717285\n",
      "epoch: 85,  batch step: 148, loss: 3.047043561935425\n",
      "epoch: 85,  batch step: 149, loss: 1.9997844696044922\n",
      "epoch: 85,  batch step: 150, loss: 19.577926635742188\n",
      "epoch: 85,  batch step: 151, loss: 52.136436462402344\n",
      "epoch: 85,  batch step: 152, loss: 85.61430358886719\n",
      "epoch: 85,  batch step: 153, loss: 4.378300666809082\n",
      "epoch: 85,  batch step: 154, loss: 2.8439300060272217\n",
      "epoch: 85,  batch step: 155, loss: 10.472492218017578\n",
      "epoch: 85,  batch step: 156, loss: 3.5234646797180176\n",
      "epoch: 85,  batch step: 157, loss: 32.043697357177734\n",
      "epoch: 85,  batch step: 158, loss: 39.082027435302734\n",
      "epoch: 85,  batch step: 159, loss: 2.51798152923584\n",
      "epoch: 85,  batch step: 160, loss: 17.509523391723633\n",
      "epoch: 85,  batch step: 161, loss: 8.864397048950195\n",
      "epoch: 85,  batch step: 162, loss: 8.358457565307617\n",
      "epoch: 85,  batch step: 163, loss: 2.0120620727539062\n",
      "epoch: 85,  batch step: 164, loss: 20.57689666748047\n",
      "epoch: 85,  batch step: 165, loss: 49.681427001953125\n",
      "epoch: 85,  batch step: 166, loss: 2.6570422649383545\n",
      "epoch: 85,  batch step: 167, loss: 12.092220306396484\n",
      "epoch: 85,  batch step: 168, loss: 2.586876630783081\n",
      "epoch: 85,  batch step: 169, loss: 2.300096035003662\n",
      "epoch: 85,  batch step: 170, loss: 2.5326156616210938\n",
      "epoch: 85,  batch step: 171, loss: 2.7603747844696045\n",
      "epoch: 85,  batch step: 172, loss: 6.77082633972168\n",
      "epoch: 85,  batch step: 173, loss: 2.147261619567871\n",
      "epoch: 85,  batch step: 174, loss: 11.924009323120117\n",
      "epoch: 85,  batch step: 175, loss: 17.995254516601562\n",
      "epoch: 85,  batch step: 176, loss: 23.02911949157715\n",
      "epoch: 85,  batch step: 177, loss: 2.8021316528320312\n",
      "epoch: 85,  batch step: 178, loss: 19.4871883392334\n",
      "epoch: 85,  batch step: 179, loss: 10.391284942626953\n",
      "epoch: 85,  batch step: 180, loss: 9.80749797821045\n",
      "epoch: 85,  batch step: 181, loss: 3.5022788047790527\n",
      "epoch: 85,  batch step: 182, loss: 2.596036434173584\n",
      "epoch: 85,  batch step: 183, loss: 5.873946189880371\n",
      "epoch: 85,  batch step: 184, loss: 9.670368194580078\n",
      "epoch: 85,  batch step: 185, loss: 11.222743034362793\n",
      "epoch: 85,  batch step: 186, loss: 19.690208435058594\n",
      "epoch: 85,  batch step: 187, loss: 26.929279327392578\n",
      "epoch: 85,  batch step: 188, loss: 8.591753005981445\n",
      "epoch: 85,  batch step: 189, loss: 6.558120250701904\n",
      "epoch: 85,  batch step: 190, loss: 2.3019423484802246\n",
      "epoch: 85,  batch step: 191, loss: 17.87209129333496\n",
      "epoch: 85,  batch step: 192, loss: 30.923843383789062\n",
      "epoch: 85,  batch step: 193, loss: 21.33056640625\n",
      "epoch: 85,  batch step: 194, loss: 2.1546292304992676\n",
      "epoch: 85,  batch step: 195, loss: 23.652318954467773\n",
      "epoch: 85,  batch step: 196, loss: 18.213207244873047\n",
      "epoch: 85,  batch step: 197, loss: 77.17515563964844\n",
      "epoch: 85,  batch step: 198, loss: 4.027894020080566\n",
      "epoch: 85,  batch step: 199, loss: 11.006750106811523\n",
      "epoch: 85,  batch step: 200, loss: 2.7010059356689453\n",
      "epoch: 85,  batch step: 201, loss: 28.53737449645996\n",
      "epoch: 85,  batch step: 202, loss: 21.261049270629883\n",
      "epoch: 85,  batch step: 203, loss: 45.55166244506836\n",
      "epoch: 85,  batch step: 204, loss: 30.099721908569336\n",
      "epoch: 85,  batch step: 205, loss: 1.8483814001083374\n",
      "epoch: 85,  batch step: 206, loss: 38.0937614440918\n",
      "epoch: 85,  batch step: 207, loss: 10.316373825073242\n",
      "epoch: 85,  batch step: 208, loss: 15.939397811889648\n",
      "epoch: 85,  batch step: 209, loss: 14.911707878112793\n",
      "epoch: 85,  batch step: 210, loss: 1.8457794189453125\n",
      "epoch: 85,  batch step: 211, loss: 2.114626407623291\n",
      "epoch: 85,  batch step: 212, loss: 2.663997173309326\n",
      "epoch: 85,  batch step: 213, loss: 7.007072448730469\n",
      "epoch: 85,  batch step: 214, loss: 24.96258544921875\n",
      "epoch: 85,  batch step: 215, loss: 19.144344329833984\n",
      "epoch: 85,  batch step: 216, loss: 39.36408233642578\n",
      "epoch: 85,  batch step: 217, loss: 50.69187927246094\n",
      "epoch: 85,  batch step: 218, loss: 5.049691677093506\n",
      "epoch: 85,  batch step: 219, loss: 20.612403869628906\n",
      "epoch: 85,  batch step: 220, loss: 3.7649283409118652\n",
      "epoch: 85,  batch step: 221, loss: 2.557793617248535\n",
      "epoch: 85,  batch step: 222, loss: 10.398847579956055\n",
      "epoch: 85,  batch step: 223, loss: 29.340110778808594\n",
      "epoch: 85,  batch step: 224, loss: 39.88639831542969\n",
      "epoch: 85,  batch step: 225, loss: 14.472053527832031\n",
      "epoch: 85,  batch step: 226, loss: 15.615913391113281\n",
      "epoch: 85,  batch step: 227, loss: 16.576377868652344\n",
      "epoch: 85,  batch step: 228, loss: 2.5999693870544434\n",
      "epoch: 85,  batch step: 229, loss: 22.279043197631836\n",
      "epoch: 85,  batch step: 230, loss: 2.0801568031311035\n",
      "epoch: 85,  batch step: 231, loss: 2.6900463104248047\n",
      "epoch: 85,  batch step: 232, loss: 3.074207305908203\n",
      "epoch: 85,  batch step: 233, loss: 18.11574935913086\n",
      "epoch: 85,  batch step: 234, loss: 10.365519523620605\n",
      "epoch: 85,  batch step: 235, loss: 8.78920841217041\n",
      "epoch: 85,  batch step: 236, loss: 10.408016204833984\n",
      "epoch: 85,  batch step: 237, loss: 7.10669469833374\n",
      "epoch: 85,  batch step: 238, loss: 9.84235954284668\n",
      "epoch: 85,  batch step: 239, loss: 1.5493018627166748\n",
      "epoch: 85,  batch step: 240, loss: 131.1771697998047\n",
      "epoch: 85,  batch step: 241, loss: 42.56791687011719\n",
      "epoch: 85,  batch step: 242, loss: 5.944511413574219\n",
      "epoch: 85,  batch step: 243, loss: 19.345014572143555\n",
      "epoch: 85,  batch step: 244, loss: 1.5105082988739014\n",
      "epoch: 85,  batch step: 245, loss: 28.64369010925293\n",
      "epoch: 85,  batch step: 246, loss: 1.8708267211914062\n",
      "epoch: 85,  batch step: 247, loss: 18.083759307861328\n",
      "epoch: 85,  batch step: 248, loss: 14.10255241394043\n",
      "epoch: 85,  batch step: 249, loss: 16.423067092895508\n",
      "epoch: 85,  batch step: 250, loss: 22.205472946166992\n",
      "epoch: 85,  batch step: 251, loss: 214.50698852539062\n",
      "validation error epoch  85:    tensor(70.3439, device='cuda:0')\n",
      "316\n",
      "epoch: 86,  batch step: 0, loss: 3.3755853176116943\n",
      "epoch: 86,  batch step: 1, loss: 5.422415733337402\n",
      "epoch: 86,  batch step: 2, loss: 10.684242248535156\n",
      "epoch: 86,  batch step: 3, loss: 29.869338989257812\n",
      "epoch: 86,  batch step: 4, loss: 15.799797058105469\n",
      "epoch: 86,  batch step: 5, loss: 9.263636589050293\n",
      "epoch: 86,  batch step: 6, loss: 12.561626434326172\n",
      "epoch: 86,  batch step: 7, loss: 45.5504150390625\n",
      "epoch: 86,  batch step: 8, loss: 12.498844146728516\n",
      "epoch: 86,  batch step: 9, loss: 4.037142753601074\n",
      "epoch: 86,  batch step: 10, loss: 18.258947372436523\n",
      "epoch: 86,  batch step: 11, loss: 7.3788676261901855\n",
      "epoch: 86,  batch step: 12, loss: 6.4636993408203125\n",
      "epoch: 86,  batch step: 13, loss: 8.367484092712402\n",
      "epoch: 86,  batch step: 14, loss: 6.617097854614258\n",
      "epoch: 86,  batch step: 15, loss: 18.323558807373047\n",
      "epoch: 86,  batch step: 16, loss: 18.0852108001709\n",
      "epoch: 86,  batch step: 17, loss: 9.82624626159668\n",
      "epoch: 86,  batch step: 18, loss: 24.302291870117188\n",
      "epoch: 86,  batch step: 19, loss: 4.990842819213867\n",
      "epoch: 86,  batch step: 20, loss: 7.1702470779418945\n",
      "epoch: 86,  batch step: 21, loss: 74.68490600585938\n",
      "epoch: 86,  batch step: 22, loss: 16.266613006591797\n",
      "epoch: 86,  batch step: 23, loss: 9.808849334716797\n",
      "epoch: 86,  batch step: 24, loss: 4.925835132598877\n",
      "epoch: 86,  batch step: 25, loss: 15.106712341308594\n",
      "epoch: 86,  batch step: 26, loss: 45.264373779296875\n",
      "epoch: 86,  batch step: 27, loss: 6.957515716552734\n",
      "epoch: 86,  batch step: 28, loss: 5.7392425537109375\n",
      "epoch: 86,  batch step: 29, loss: 2.831296920776367\n",
      "epoch: 86,  batch step: 30, loss: 3.359096050262451\n",
      "epoch: 86,  batch step: 31, loss: 3.507267475128174\n",
      "epoch: 86,  batch step: 32, loss: 8.341010093688965\n",
      "epoch: 86,  batch step: 33, loss: 4.14522123336792\n",
      "epoch: 86,  batch step: 34, loss: 6.330565452575684\n",
      "epoch: 86,  batch step: 35, loss: 2.3932948112487793\n",
      "epoch: 86,  batch step: 36, loss: 2.6597461700439453\n",
      "epoch: 86,  batch step: 37, loss: 3.850706100463867\n",
      "epoch: 86,  batch step: 38, loss: 2.6549072265625\n",
      "epoch: 86,  batch step: 39, loss: 3.9073944091796875\n",
      "epoch: 86,  batch step: 40, loss: 3.1658525466918945\n",
      "epoch: 86,  batch step: 41, loss: 4.779787063598633\n",
      "epoch: 86,  batch step: 42, loss: 3.5796642303466797\n",
      "epoch: 86,  batch step: 43, loss: 10.705921173095703\n",
      "epoch: 86,  batch step: 44, loss: 44.440940856933594\n",
      "epoch: 86,  batch step: 45, loss: 38.321311950683594\n",
      "epoch: 86,  batch step: 46, loss: 156.59425354003906\n",
      "epoch: 86,  batch step: 47, loss: 3.092830181121826\n",
      "epoch: 86,  batch step: 48, loss: 2.3090317249298096\n",
      "epoch: 86,  batch step: 49, loss: 25.004865646362305\n",
      "epoch: 86,  batch step: 50, loss: 12.952428817749023\n",
      "epoch: 86,  batch step: 51, loss: 3.2009496688842773\n",
      "epoch: 86,  batch step: 52, loss: 18.403656005859375\n",
      "epoch: 86,  batch step: 53, loss: 23.076698303222656\n",
      "epoch: 86,  batch step: 54, loss: 31.093793869018555\n",
      "epoch: 86,  batch step: 55, loss: 3.9258222579956055\n",
      "epoch: 86,  batch step: 56, loss: 27.527542114257812\n",
      "epoch: 86,  batch step: 57, loss: 2.6839053630828857\n",
      "epoch: 86,  batch step: 58, loss: 29.137977600097656\n",
      "epoch: 86,  batch step: 59, loss: 3.0132670402526855\n",
      "epoch: 86,  batch step: 60, loss: 4.018224716186523\n",
      "epoch: 86,  batch step: 61, loss: 9.774093627929688\n",
      "epoch: 86,  batch step: 62, loss: 12.059051513671875\n",
      "epoch: 86,  batch step: 63, loss: 2.3466970920562744\n",
      "epoch: 86,  batch step: 64, loss: 3.6908020973205566\n",
      "epoch: 86,  batch step: 65, loss: 52.38384246826172\n",
      "epoch: 86,  batch step: 66, loss: 2.9528968334198\n",
      "epoch: 86,  batch step: 67, loss: 9.862039566040039\n",
      "epoch: 86,  batch step: 68, loss: 11.192026138305664\n",
      "epoch: 86,  batch step: 69, loss: 5.8567633628845215\n",
      "epoch: 86,  batch step: 70, loss: 1.4305623769760132\n",
      "epoch: 86,  batch step: 71, loss: 7.236069679260254\n",
      "epoch: 86,  batch step: 72, loss: 2.7558841705322266\n",
      "epoch: 86,  batch step: 73, loss: 3.933506727218628\n",
      "epoch: 86,  batch step: 74, loss: 38.907039642333984\n",
      "epoch: 86,  batch step: 75, loss: 5.737024307250977\n",
      "epoch: 86,  batch step: 76, loss: 20.548141479492188\n",
      "epoch: 86,  batch step: 77, loss: 21.71742820739746\n",
      "epoch: 86,  batch step: 78, loss: 42.09150314331055\n",
      "epoch: 86,  batch step: 79, loss: 3.0442423820495605\n",
      "epoch: 86,  batch step: 80, loss: 22.746822357177734\n",
      "epoch: 86,  batch step: 81, loss: 8.027791976928711\n",
      "epoch: 86,  batch step: 82, loss: 3.0085391998291016\n",
      "epoch: 86,  batch step: 83, loss: 4.23301887512207\n",
      "epoch: 86,  batch step: 84, loss: 15.507986068725586\n",
      "epoch: 86,  batch step: 85, loss: 36.56731033325195\n",
      "epoch: 86,  batch step: 86, loss: 22.615951538085938\n",
      "epoch: 86,  batch step: 87, loss: 10.478260040283203\n",
      "epoch: 86,  batch step: 88, loss: 46.106788635253906\n",
      "epoch: 86,  batch step: 89, loss: 2.445030450820923\n",
      "epoch: 86,  batch step: 90, loss: 5.173885345458984\n",
      "epoch: 86,  batch step: 91, loss: 16.86479377746582\n",
      "epoch: 86,  batch step: 92, loss: 18.02334213256836\n",
      "epoch: 86,  batch step: 93, loss: 19.473464965820312\n",
      "epoch: 86,  batch step: 94, loss: 2.380098342895508\n",
      "epoch: 86,  batch step: 95, loss: 22.56464385986328\n",
      "epoch: 86,  batch step: 96, loss: 13.909823417663574\n",
      "epoch: 86,  batch step: 97, loss: 44.05245590209961\n",
      "epoch: 86,  batch step: 98, loss: 26.623706817626953\n",
      "epoch: 86,  batch step: 99, loss: 16.529380798339844\n",
      "epoch: 86,  batch step: 100, loss: 52.68439483642578\n",
      "epoch: 86,  batch step: 101, loss: 23.873455047607422\n",
      "epoch: 86,  batch step: 102, loss: 63.591163635253906\n",
      "epoch: 86,  batch step: 103, loss: 28.980268478393555\n",
      "epoch: 86,  batch step: 104, loss: 2.866363525390625\n",
      "epoch: 86,  batch step: 105, loss: 2.2198305130004883\n",
      "epoch: 86,  batch step: 106, loss: 5.423951625823975\n",
      "epoch: 86,  batch step: 107, loss: 10.811068534851074\n",
      "epoch: 86,  batch step: 108, loss: 2.777953624725342\n",
      "epoch: 86,  batch step: 109, loss: 70.11688232421875\n",
      "epoch: 86,  batch step: 110, loss: 2.455565929412842\n",
      "epoch: 86,  batch step: 111, loss: 41.71305847167969\n",
      "epoch: 86,  batch step: 112, loss: 20.09299087524414\n",
      "epoch: 86,  batch step: 113, loss: 22.189115524291992\n",
      "epoch: 86,  batch step: 114, loss: 8.585977554321289\n",
      "epoch: 86,  batch step: 115, loss: 3.875361919403076\n",
      "epoch: 86,  batch step: 116, loss: 18.241310119628906\n",
      "epoch: 86,  batch step: 117, loss: 3.6089282035827637\n",
      "epoch: 86,  batch step: 118, loss: 24.507604598999023\n",
      "epoch: 86,  batch step: 119, loss: 14.698771476745605\n",
      "epoch: 86,  batch step: 120, loss: 7.962846755981445\n",
      "epoch: 86,  batch step: 121, loss: 19.529258728027344\n",
      "epoch: 86,  batch step: 122, loss: 5.208477020263672\n",
      "epoch: 86,  batch step: 123, loss: 24.657520294189453\n",
      "epoch: 86,  batch step: 124, loss: 3.5191333293914795\n",
      "epoch: 86,  batch step: 125, loss: 2.6320862770080566\n",
      "epoch: 86,  batch step: 126, loss: 9.547118186950684\n",
      "epoch: 86,  batch step: 127, loss: 19.985231399536133\n",
      "epoch: 86,  batch step: 128, loss: 2.8479676246643066\n",
      "epoch: 86,  batch step: 129, loss: 12.547063827514648\n",
      "epoch: 86,  batch step: 130, loss: 4.454282760620117\n",
      "epoch: 86,  batch step: 131, loss: 18.77000617980957\n",
      "epoch: 86,  batch step: 132, loss: 2.7469401359558105\n",
      "epoch: 86,  batch step: 133, loss: 2.7922210693359375\n",
      "epoch: 86,  batch step: 134, loss: 8.526000022888184\n",
      "epoch: 86,  batch step: 135, loss: 11.572376251220703\n",
      "epoch: 86,  batch step: 136, loss: 31.912246704101562\n",
      "epoch: 86,  batch step: 137, loss: 6.105321407318115\n",
      "epoch: 86,  batch step: 138, loss: 26.685436248779297\n",
      "epoch: 86,  batch step: 139, loss: 20.221189498901367\n",
      "epoch: 86,  batch step: 140, loss: 2.877243757247925\n",
      "epoch: 86,  batch step: 141, loss: 16.120759963989258\n",
      "epoch: 86,  batch step: 142, loss: 1.942535161972046\n",
      "epoch: 86,  batch step: 143, loss: 3.816699981689453\n",
      "epoch: 86,  batch step: 144, loss: 12.704630851745605\n",
      "epoch: 86,  batch step: 145, loss: 2.147401809692383\n",
      "epoch: 86,  batch step: 146, loss: 1.7278413772583008\n",
      "epoch: 86,  batch step: 147, loss: 3.3635013103485107\n",
      "epoch: 86,  batch step: 148, loss: 3.3439035415649414\n",
      "epoch: 86,  batch step: 149, loss: 2.023439407348633\n",
      "epoch: 86,  batch step: 150, loss: 2.6494507789611816\n",
      "epoch: 86,  batch step: 151, loss: 2.631241798400879\n",
      "epoch: 86,  batch step: 152, loss: 2.416017532348633\n",
      "epoch: 86,  batch step: 153, loss: 1.710893988609314\n",
      "epoch: 86,  batch step: 154, loss: 14.794737815856934\n",
      "epoch: 86,  batch step: 155, loss: 9.227853775024414\n",
      "epoch: 86,  batch step: 156, loss: 16.56410789489746\n",
      "epoch: 86,  batch step: 157, loss: 8.030340194702148\n",
      "epoch: 86,  batch step: 158, loss: 16.738910675048828\n",
      "epoch: 86,  batch step: 159, loss: 9.505138397216797\n",
      "epoch: 86,  batch step: 160, loss: 3.3856754302978516\n",
      "epoch: 86,  batch step: 161, loss: 9.009876251220703\n",
      "epoch: 86,  batch step: 162, loss: 1.6963766813278198\n",
      "epoch: 86,  batch step: 163, loss: 4.230118751525879\n",
      "epoch: 86,  batch step: 164, loss: 1.8040788173675537\n",
      "epoch: 86,  batch step: 165, loss: 3.3797147274017334\n",
      "epoch: 86,  batch step: 166, loss: 2.1516547203063965\n",
      "epoch: 86,  batch step: 167, loss: 36.15093231201172\n",
      "epoch: 86,  batch step: 168, loss: 3.9351840019226074\n",
      "epoch: 86,  batch step: 169, loss: 2.911370038986206\n",
      "epoch: 86,  batch step: 170, loss: 7.4638261795043945\n",
      "epoch: 86,  batch step: 171, loss: 2.9520044326782227\n",
      "epoch: 86,  batch step: 172, loss: 14.628477096557617\n",
      "epoch: 86,  batch step: 173, loss: 2.654660701751709\n",
      "epoch: 86,  batch step: 174, loss: 2.3463354110717773\n",
      "epoch: 86,  batch step: 175, loss: 26.69540023803711\n",
      "epoch: 86,  batch step: 176, loss: 29.52444839477539\n",
      "epoch: 86,  batch step: 177, loss: 5.762266159057617\n",
      "epoch: 86,  batch step: 178, loss: 2.644270896911621\n",
      "epoch: 86,  batch step: 179, loss: 4.6139678955078125\n",
      "epoch: 86,  batch step: 180, loss: 2.710615634918213\n",
      "epoch: 86,  batch step: 181, loss: 2.5258121490478516\n",
      "epoch: 86,  batch step: 182, loss: 8.410149574279785\n",
      "epoch: 86,  batch step: 183, loss: 8.08036994934082\n",
      "epoch: 86,  batch step: 184, loss: 10.632600784301758\n",
      "epoch: 86,  batch step: 185, loss: 1.6286420822143555\n",
      "epoch: 86,  batch step: 186, loss: 2.704733371734619\n",
      "epoch: 86,  batch step: 187, loss: 3.3513503074645996\n",
      "epoch: 86,  batch step: 188, loss: 3.725656032562256\n",
      "epoch: 86,  batch step: 189, loss: 4.794929027557373\n",
      "epoch: 86,  batch step: 190, loss: 3.3135757446289062\n",
      "epoch: 86,  batch step: 191, loss: 32.97235870361328\n",
      "epoch: 86,  batch step: 192, loss: 20.86203384399414\n",
      "epoch: 86,  batch step: 193, loss: 1.823043704032898\n",
      "epoch: 86,  batch step: 194, loss: 2.182583808898926\n",
      "epoch: 86,  batch step: 195, loss: 19.600778579711914\n",
      "epoch: 86,  batch step: 196, loss: 3.1436526775360107\n",
      "epoch: 86,  batch step: 197, loss: 2.666382312774658\n",
      "epoch: 86,  batch step: 198, loss: 19.751605987548828\n",
      "epoch: 86,  batch step: 199, loss: 2.745020627975464\n",
      "epoch: 86,  batch step: 200, loss: 51.131690979003906\n",
      "epoch: 86,  batch step: 201, loss: 9.716668128967285\n",
      "epoch: 86,  batch step: 202, loss: 27.681819915771484\n",
      "epoch: 86,  batch step: 203, loss: 3.149885654449463\n",
      "epoch: 86,  batch step: 204, loss: 2.1579103469848633\n",
      "epoch: 86,  batch step: 205, loss: 2.4815564155578613\n",
      "epoch: 86,  batch step: 206, loss: 21.91229820251465\n",
      "epoch: 86,  batch step: 207, loss: 42.584957122802734\n",
      "epoch: 86,  batch step: 208, loss: 2.4047136306762695\n",
      "epoch: 86,  batch step: 209, loss: 5.0320963859558105\n",
      "epoch: 86,  batch step: 210, loss: 3.5327811241149902\n",
      "epoch: 86,  batch step: 211, loss: 7.5546159744262695\n",
      "epoch: 86,  batch step: 212, loss: 17.380144119262695\n",
      "epoch: 86,  batch step: 213, loss: 2.3506364822387695\n",
      "epoch: 86,  batch step: 214, loss: 11.079649925231934\n",
      "epoch: 86,  batch step: 215, loss: 16.54439926147461\n",
      "epoch: 86,  batch step: 216, loss: 51.27448654174805\n",
      "epoch: 86,  batch step: 217, loss: 2.260127067565918\n",
      "epoch: 86,  batch step: 218, loss: 2.346963405609131\n",
      "epoch: 86,  batch step: 219, loss: 2.6176304817199707\n",
      "epoch: 86,  batch step: 220, loss: 2.288339376449585\n",
      "epoch: 86,  batch step: 221, loss: 3.820711851119995\n",
      "epoch: 86,  batch step: 222, loss: 8.709300994873047\n",
      "epoch: 86,  batch step: 223, loss: 14.842155456542969\n",
      "epoch: 86,  batch step: 224, loss: 12.653995513916016\n",
      "epoch: 86,  batch step: 225, loss: 2.3758091926574707\n",
      "epoch: 86,  batch step: 226, loss: 38.188453674316406\n",
      "epoch: 86,  batch step: 227, loss: 9.989280700683594\n",
      "epoch: 86,  batch step: 228, loss: 1.5940479040145874\n",
      "epoch: 86,  batch step: 229, loss: 13.329119682312012\n",
      "epoch: 86,  batch step: 230, loss: 13.327798843383789\n",
      "epoch: 86,  batch step: 231, loss: 7.167457580566406\n",
      "epoch: 86,  batch step: 232, loss: 3.861149549484253\n",
      "epoch: 86,  batch step: 233, loss: 2.4731125831604004\n",
      "epoch: 86,  batch step: 234, loss: 34.327880859375\n",
      "epoch: 86,  batch step: 235, loss: 17.193161010742188\n",
      "epoch: 86,  batch step: 236, loss: 50.17855453491211\n",
      "epoch: 86,  batch step: 237, loss: 2.1179380416870117\n",
      "epoch: 86,  batch step: 238, loss: 17.551036834716797\n",
      "epoch: 86,  batch step: 239, loss: 16.348262786865234\n",
      "epoch: 86,  batch step: 240, loss: 12.667089462280273\n",
      "epoch: 86,  batch step: 241, loss: 1.8808916807174683\n",
      "epoch: 86,  batch step: 242, loss: 2.9370169639587402\n",
      "epoch: 86,  batch step: 243, loss: 3.8367843627929688\n",
      "epoch: 86,  batch step: 244, loss: 3.773165702819824\n",
      "epoch: 86,  batch step: 245, loss: 2.9752726554870605\n",
      "epoch: 86,  batch step: 246, loss: 57.872108459472656\n",
      "epoch: 86,  batch step: 247, loss: 4.794559001922607\n",
      "epoch: 86,  batch step: 248, loss: 2.642540454864502\n",
      "epoch: 86,  batch step: 249, loss: 2.1078438758850098\n",
      "epoch: 86,  batch step: 250, loss: 21.914291381835938\n",
      "epoch: 86,  batch step: 251, loss: 62.015525817871094\n",
      "validation error epoch  86:    tensor(68.4801, device='cuda:0')\n",
      "316\n",
      "epoch: 87,  batch step: 0, loss: 48.2254753112793\n",
      "epoch: 87,  batch step: 1, loss: 4.139001369476318\n",
      "epoch: 87,  batch step: 2, loss: 19.57495880126953\n",
      "epoch: 87,  batch step: 3, loss: 4.2926154136657715\n",
      "epoch: 87,  batch step: 4, loss: 4.735807418823242\n",
      "epoch: 87,  batch step: 5, loss: 6.084807872772217\n",
      "epoch: 87,  batch step: 6, loss: 24.306745529174805\n",
      "epoch: 87,  batch step: 7, loss: 10.570452690124512\n",
      "epoch: 87,  batch step: 8, loss: 6.126701354980469\n",
      "epoch: 87,  batch step: 9, loss: 10.539671897888184\n",
      "epoch: 87,  batch step: 10, loss: 26.784513473510742\n",
      "epoch: 87,  batch step: 11, loss: 4.4044084548950195\n",
      "epoch: 87,  batch step: 12, loss: 30.0957088470459\n",
      "epoch: 87,  batch step: 13, loss: 4.776315689086914\n",
      "epoch: 87,  batch step: 14, loss: 5.23684549331665\n",
      "epoch: 87,  batch step: 15, loss: 9.582014083862305\n",
      "epoch: 87,  batch step: 16, loss: 23.7463436126709\n",
      "epoch: 87,  batch step: 17, loss: 24.754322052001953\n",
      "epoch: 87,  batch step: 18, loss: 62.424312591552734\n",
      "epoch: 87,  batch step: 19, loss: 7.372707366943359\n",
      "epoch: 87,  batch step: 20, loss: 13.33756160736084\n",
      "epoch: 87,  batch step: 21, loss: 4.209495544433594\n",
      "epoch: 87,  batch step: 22, loss: 27.07282066345215\n",
      "epoch: 87,  batch step: 23, loss: 12.546622276306152\n",
      "epoch: 87,  batch step: 24, loss: 9.459064483642578\n",
      "epoch: 87,  batch step: 25, loss: 5.179594039916992\n",
      "epoch: 87,  batch step: 26, loss: 11.738798141479492\n",
      "epoch: 87,  batch step: 27, loss: 7.981692314147949\n",
      "epoch: 87,  batch step: 28, loss: 30.130661010742188\n",
      "epoch: 87,  batch step: 29, loss: 3.2209677696228027\n",
      "epoch: 87,  batch step: 30, loss: 4.336572647094727\n",
      "epoch: 87,  batch step: 31, loss: 8.813910484313965\n",
      "epoch: 87,  batch step: 32, loss: 3.205463171005249\n",
      "epoch: 87,  batch step: 33, loss: 3.848787307739258\n",
      "epoch: 87,  batch step: 34, loss: 13.099847793579102\n",
      "epoch: 87,  batch step: 35, loss: 12.164609909057617\n",
      "epoch: 87,  batch step: 36, loss: 4.823669910430908\n",
      "epoch: 87,  batch step: 37, loss: 3.5510051250457764\n",
      "epoch: 87,  batch step: 38, loss: 4.9599738121032715\n",
      "epoch: 87,  batch step: 39, loss: 50.115577697753906\n",
      "epoch: 87,  batch step: 40, loss: 21.494850158691406\n",
      "epoch: 87,  batch step: 41, loss: 15.227619171142578\n",
      "epoch: 87,  batch step: 42, loss: 2.485534191131592\n",
      "epoch: 87,  batch step: 43, loss: 6.711450099945068\n",
      "epoch: 87,  batch step: 44, loss: 4.956735610961914\n",
      "epoch: 87,  batch step: 45, loss: 18.568603515625\n",
      "epoch: 87,  batch step: 46, loss: 3.1135683059692383\n",
      "epoch: 87,  batch step: 47, loss: 26.845794677734375\n",
      "epoch: 87,  batch step: 48, loss: 2.703505277633667\n",
      "epoch: 87,  batch step: 49, loss: 17.402475357055664\n",
      "epoch: 87,  batch step: 50, loss: 15.318044662475586\n",
      "epoch: 87,  batch step: 51, loss: 17.485750198364258\n",
      "epoch: 87,  batch step: 52, loss: 2.370842456817627\n",
      "epoch: 87,  batch step: 53, loss: 5.909161567687988\n",
      "epoch: 87,  batch step: 54, loss: 9.679835319519043\n",
      "epoch: 87,  batch step: 55, loss: 132.8805694580078\n",
      "epoch: 87,  batch step: 56, loss: 15.241255760192871\n",
      "epoch: 87,  batch step: 57, loss: 6.772109031677246\n",
      "epoch: 87,  batch step: 58, loss: 4.420975208282471\n",
      "epoch: 87,  batch step: 59, loss: 9.206993103027344\n",
      "epoch: 87,  batch step: 60, loss: 2.941615581512451\n",
      "epoch: 87,  batch step: 61, loss: 33.341766357421875\n",
      "epoch: 87,  batch step: 62, loss: 2.151653289794922\n",
      "epoch: 87,  batch step: 63, loss: 2.2694966793060303\n",
      "epoch: 87,  batch step: 64, loss: 26.204421997070312\n",
      "epoch: 87,  batch step: 65, loss: 3.2906227111816406\n",
      "epoch: 87,  batch step: 66, loss: 2.7030837535858154\n",
      "epoch: 87,  batch step: 67, loss: 7.9994425773620605\n",
      "epoch: 87,  batch step: 68, loss: 2.94227933883667\n",
      "epoch: 87,  batch step: 69, loss: 9.733047485351562\n",
      "epoch: 87,  batch step: 70, loss: 10.168458938598633\n",
      "epoch: 87,  batch step: 71, loss: 2.7668662071228027\n",
      "epoch: 87,  batch step: 72, loss: 2.3501319885253906\n",
      "epoch: 87,  batch step: 73, loss: 3.7980287075042725\n",
      "epoch: 87,  batch step: 74, loss: 20.374691009521484\n",
      "epoch: 87,  batch step: 75, loss: 10.086840629577637\n",
      "epoch: 87,  batch step: 76, loss: 2.0052733421325684\n",
      "epoch: 87,  batch step: 77, loss: 9.084307670593262\n",
      "epoch: 87,  batch step: 78, loss: 5.771573066711426\n",
      "epoch: 87,  batch step: 79, loss: 6.960649013519287\n",
      "epoch: 87,  batch step: 80, loss: 9.188858032226562\n",
      "epoch: 87,  batch step: 81, loss: 2.6143178939819336\n",
      "epoch: 87,  batch step: 82, loss: 2.3305253982543945\n",
      "epoch: 87,  batch step: 83, loss: 12.731951713562012\n",
      "epoch: 87,  batch step: 84, loss: 7.977941036224365\n",
      "epoch: 87,  batch step: 85, loss: 2.183347225189209\n",
      "epoch: 87,  batch step: 86, loss: 7.898976802825928\n",
      "epoch: 87,  batch step: 87, loss: 19.525253295898438\n",
      "epoch: 87,  batch step: 88, loss: 2.536966323852539\n",
      "epoch: 87,  batch step: 89, loss: 2.0824382305145264\n",
      "epoch: 87,  batch step: 90, loss: 64.08853149414062\n",
      "epoch: 87,  batch step: 91, loss: 1.9320802688598633\n",
      "epoch: 87,  batch step: 92, loss: 1.6159045696258545\n",
      "epoch: 87,  batch step: 93, loss: 3.4175963401794434\n",
      "epoch: 87,  batch step: 94, loss: 11.433966636657715\n",
      "epoch: 87,  batch step: 95, loss: 4.361730575561523\n",
      "epoch: 87,  batch step: 96, loss: 25.23792839050293\n",
      "epoch: 87,  batch step: 97, loss: 8.291799545288086\n",
      "epoch: 87,  batch step: 98, loss: 3.542409896850586\n",
      "epoch: 87,  batch step: 99, loss: 10.146371841430664\n",
      "epoch: 87,  batch step: 100, loss: 65.7650146484375\n",
      "epoch: 87,  batch step: 101, loss: 21.991451263427734\n",
      "epoch: 87,  batch step: 102, loss: 9.61875057220459\n",
      "epoch: 87,  batch step: 103, loss: 2.0310611724853516\n",
      "epoch: 87,  batch step: 104, loss: 3.3439555168151855\n",
      "epoch: 87,  batch step: 105, loss: 2.4217581748962402\n",
      "epoch: 87,  batch step: 106, loss: 3.9433345794677734\n",
      "epoch: 87,  batch step: 107, loss: 9.573614120483398\n",
      "epoch: 87,  batch step: 108, loss: 40.59307098388672\n",
      "epoch: 87,  batch step: 109, loss: 2.5625791549682617\n",
      "epoch: 87,  batch step: 110, loss: 37.2030143737793\n",
      "epoch: 87,  batch step: 111, loss: 4.138807773590088\n",
      "epoch: 87,  batch step: 112, loss: 8.158431053161621\n",
      "epoch: 87,  batch step: 113, loss: 15.57129955291748\n",
      "epoch: 87,  batch step: 114, loss: 4.216751575469971\n",
      "epoch: 87,  batch step: 115, loss: 1.3723411560058594\n",
      "epoch: 87,  batch step: 116, loss: 10.090414047241211\n",
      "epoch: 87,  batch step: 117, loss: 18.178367614746094\n",
      "epoch: 87,  batch step: 118, loss: 8.652945518493652\n",
      "epoch: 87,  batch step: 119, loss: 5.593009948730469\n",
      "epoch: 87,  batch step: 120, loss: 20.998010635375977\n",
      "epoch: 87,  batch step: 121, loss: 4.440131187438965\n",
      "epoch: 87,  batch step: 122, loss: 9.007834434509277\n",
      "epoch: 87,  batch step: 123, loss: 1.9417028427124023\n",
      "epoch: 87,  batch step: 124, loss: 56.231929779052734\n",
      "epoch: 87,  batch step: 125, loss: 3.0313682556152344\n",
      "epoch: 87,  batch step: 126, loss: 16.223745346069336\n",
      "epoch: 87,  batch step: 127, loss: 12.735613822937012\n",
      "epoch: 87,  batch step: 128, loss: 2.380133867263794\n",
      "epoch: 87,  batch step: 129, loss: 3.632882833480835\n",
      "epoch: 87,  batch step: 130, loss: 2.1084530353546143\n",
      "epoch: 87,  batch step: 131, loss: 17.391075134277344\n",
      "epoch: 87,  batch step: 132, loss: 2.7077434062957764\n",
      "epoch: 87,  batch step: 133, loss: 16.272817611694336\n",
      "epoch: 87,  batch step: 134, loss: 2.8640999794006348\n",
      "epoch: 87,  batch step: 135, loss: 3.015023946762085\n",
      "epoch: 87,  batch step: 136, loss: 10.002642631530762\n",
      "epoch: 87,  batch step: 137, loss: 3.9133241176605225\n",
      "epoch: 87,  batch step: 138, loss: 2.9226043224334717\n",
      "epoch: 87,  batch step: 139, loss: 11.472156524658203\n",
      "epoch: 87,  batch step: 140, loss: 20.32373809814453\n",
      "epoch: 87,  batch step: 141, loss: 43.98308563232422\n",
      "epoch: 87,  batch step: 142, loss: 4.749908447265625\n",
      "epoch: 87,  batch step: 143, loss: 2.2702016830444336\n",
      "epoch: 87,  batch step: 144, loss: 3.419745922088623\n",
      "epoch: 87,  batch step: 145, loss: 23.45938491821289\n",
      "epoch: 87,  batch step: 146, loss: 50.91721725463867\n",
      "epoch: 87,  batch step: 147, loss: 14.110027313232422\n",
      "epoch: 87,  batch step: 148, loss: 8.885293960571289\n",
      "epoch: 87,  batch step: 149, loss: 2.5453877449035645\n",
      "epoch: 87,  batch step: 150, loss: 2.239119529724121\n",
      "epoch: 87,  batch step: 151, loss: 3.868833541870117\n",
      "epoch: 87,  batch step: 152, loss: 14.924232482910156\n",
      "epoch: 87,  batch step: 153, loss: 108.6010971069336\n",
      "epoch: 87,  batch step: 154, loss: 2.2059383392333984\n",
      "epoch: 87,  batch step: 155, loss: 1.3357237577438354\n",
      "epoch: 87,  batch step: 156, loss: 2.359215021133423\n",
      "epoch: 87,  batch step: 157, loss: 2.4776227474212646\n",
      "epoch: 87,  batch step: 158, loss: 4.659587860107422\n",
      "epoch: 87,  batch step: 159, loss: 1.540518045425415\n",
      "epoch: 87,  batch step: 160, loss: 10.265403747558594\n",
      "epoch: 87,  batch step: 161, loss: 11.682924270629883\n",
      "epoch: 87,  batch step: 162, loss: 5.271459579467773\n",
      "epoch: 87,  batch step: 163, loss: 3.003675937652588\n",
      "epoch: 87,  batch step: 164, loss: 4.204485893249512\n",
      "epoch: 87,  batch step: 165, loss: 4.249915599822998\n",
      "epoch: 87,  batch step: 166, loss: 4.053174018859863\n",
      "epoch: 87,  batch step: 167, loss: 2.1744461059570312\n",
      "epoch: 87,  batch step: 168, loss: 2.355797290802002\n",
      "epoch: 87,  batch step: 169, loss: 14.84729290008545\n",
      "epoch: 87,  batch step: 170, loss: 8.195547103881836\n",
      "epoch: 87,  batch step: 171, loss: 15.093374252319336\n",
      "epoch: 87,  batch step: 172, loss: 12.89655876159668\n",
      "epoch: 87,  batch step: 173, loss: 3.2258527278900146\n",
      "epoch: 87,  batch step: 174, loss: 1.782223105430603\n",
      "epoch: 87,  batch step: 175, loss: 9.530882835388184\n",
      "epoch: 87,  batch step: 176, loss: 12.136019706726074\n",
      "epoch: 87,  batch step: 177, loss: 10.763166427612305\n",
      "epoch: 87,  batch step: 178, loss: 18.75019073486328\n",
      "epoch: 87,  batch step: 179, loss: 13.41489315032959\n",
      "epoch: 87,  batch step: 180, loss: 55.70171356201172\n",
      "epoch: 87,  batch step: 181, loss: 1.7588574886322021\n",
      "epoch: 87,  batch step: 182, loss: 57.724639892578125\n",
      "epoch: 87,  batch step: 183, loss: 1.694960355758667\n",
      "epoch: 87,  batch step: 184, loss: 38.348575592041016\n",
      "epoch: 87,  batch step: 185, loss: 1.9097226858139038\n",
      "epoch: 87,  batch step: 186, loss: 2.371457576751709\n",
      "epoch: 87,  batch step: 187, loss: 68.61502075195312\n",
      "epoch: 87,  batch step: 188, loss: 2.5118415355682373\n",
      "epoch: 87,  batch step: 189, loss: 1.8394278287887573\n",
      "epoch: 87,  batch step: 190, loss: 2.1902987957000732\n",
      "epoch: 87,  batch step: 191, loss: 2.65311598777771\n",
      "epoch: 87,  batch step: 192, loss: 1.6352388858795166\n",
      "epoch: 87,  batch step: 193, loss: 24.297719955444336\n",
      "epoch: 87,  batch step: 194, loss: 10.345796585083008\n",
      "epoch: 87,  batch step: 195, loss: 11.904723167419434\n",
      "epoch: 87,  batch step: 196, loss: 4.579028129577637\n",
      "epoch: 87,  batch step: 197, loss: 2.120877981185913\n",
      "epoch: 87,  batch step: 198, loss: 6.716681480407715\n",
      "epoch: 87,  batch step: 199, loss: 8.278783798217773\n",
      "epoch: 87,  batch step: 200, loss: 1.858712077140808\n",
      "epoch: 87,  batch step: 201, loss: 3.7259395122528076\n",
      "epoch: 87,  batch step: 202, loss: 2.926698684692383\n",
      "epoch: 87,  batch step: 203, loss: 10.835436820983887\n",
      "epoch: 87,  batch step: 204, loss: 6.3979315757751465\n",
      "epoch: 87,  batch step: 205, loss: 47.649131774902344\n",
      "epoch: 87,  batch step: 206, loss: 2.255448579788208\n",
      "epoch: 87,  batch step: 207, loss: 30.58403778076172\n",
      "epoch: 87,  batch step: 208, loss: 2.634669065475464\n",
      "epoch: 87,  batch step: 209, loss: 27.15285873413086\n",
      "epoch: 87,  batch step: 210, loss: 21.924388885498047\n",
      "epoch: 87,  batch step: 211, loss: 25.450611114501953\n",
      "epoch: 87,  batch step: 212, loss: 8.67660903930664\n",
      "epoch: 87,  batch step: 213, loss: 3.9882068634033203\n",
      "epoch: 87,  batch step: 214, loss: 10.271031379699707\n",
      "epoch: 87,  batch step: 215, loss: 14.095675468444824\n",
      "epoch: 87,  batch step: 216, loss: 1.6665282249450684\n",
      "epoch: 87,  batch step: 217, loss: 5.403062343597412\n",
      "epoch: 87,  batch step: 218, loss: 16.307308197021484\n",
      "epoch: 87,  batch step: 219, loss: 16.045207977294922\n",
      "epoch: 87,  batch step: 220, loss: 3.4306037425994873\n",
      "epoch: 87,  batch step: 221, loss: 32.384063720703125\n",
      "epoch: 87,  batch step: 222, loss: 30.5931453704834\n",
      "epoch: 87,  batch step: 223, loss: 10.657480239868164\n",
      "epoch: 87,  batch step: 224, loss: 10.1064453125\n",
      "epoch: 87,  batch step: 225, loss: 22.49134635925293\n",
      "epoch: 87,  batch step: 226, loss: 8.869216918945312\n",
      "epoch: 87,  batch step: 227, loss: 24.240447998046875\n",
      "epoch: 87,  batch step: 228, loss: 8.831899642944336\n",
      "epoch: 87,  batch step: 229, loss: 2.521646499633789\n",
      "epoch: 87,  batch step: 230, loss: 1.6979517936706543\n",
      "epoch: 87,  batch step: 231, loss: 8.258122444152832\n",
      "epoch: 87,  batch step: 232, loss: 16.479373931884766\n",
      "epoch: 87,  batch step: 233, loss: 2.32358980178833\n",
      "epoch: 87,  batch step: 234, loss: 27.209989547729492\n",
      "epoch: 87,  batch step: 235, loss: 7.646834850311279\n",
      "epoch: 87,  batch step: 236, loss: 3.177034378051758\n",
      "epoch: 87,  batch step: 237, loss: 2.876382827758789\n",
      "epoch: 87,  batch step: 238, loss: 2.5771284103393555\n",
      "epoch: 87,  batch step: 239, loss: 2.2322118282318115\n",
      "epoch: 87,  batch step: 240, loss: 14.664154052734375\n",
      "epoch: 87,  batch step: 241, loss: 13.216596603393555\n",
      "epoch: 87,  batch step: 242, loss: 4.076793670654297\n",
      "epoch: 87,  batch step: 243, loss: 8.478509902954102\n",
      "epoch: 87,  batch step: 244, loss: 2.8120694160461426\n",
      "epoch: 87,  batch step: 245, loss: 32.793067932128906\n",
      "epoch: 87,  batch step: 246, loss: 2.0652406215667725\n",
      "epoch: 87,  batch step: 247, loss: 2.690682888031006\n",
      "epoch: 87,  batch step: 248, loss: 2.0571703910827637\n",
      "epoch: 87,  batch step: 249, loss: 2.8756847381591797\n",
      "epoch: 87,  batch step: 250, loss: 28.08461570739746\n",
      "epoch: 87,  batch step: 251, loss: 8.458966255187988\n",
      "validation error epoch  87:    tensor(69.6606, device='cuda:0')\n",
      "316\n",
      "epoch: 88,  batch step: 0, loss: 4.792774200439453\n",
      "epoch: 88,  batch step: 1, loss: 17.993736267089844\n",
      "epoch: 88,  batch step: 2, loss: 8.093661308288574\n",
      "epoch: 88,  batch step: 3, loss: 8.392545700073242\n",
      "epoch: 88,  batch step: 4, loss: 2.7805705070495605\n",
      "epoch: 88,  batch step: 5, loss: 2.505087375640869\n",
      "epoch: 88,  batch step: 6, loss: 1.61367928981781\n",
      "epoch: 88,  batch step: 7, loss: 12.549507141113281\n",
      "epoch: 88,  batch step: 8, loss: 9.311529159545898\n",
      "epoch: 88,  batch step: 9, loss: 7.706107139587402\n",
      "epoch: 88,  batch step: 10, loss: 3.3748271465301514\n",
      "epoch: 88,  batch step: 11, loss: 29.233793258666992\n",
      "epoch: 88,  batch step: 12, loss: 1.9293086528778076\n",
      "epoch: 88,  batch step: 13, loss: 1.572676420211792\n",
      "epoch: 88,  batch step: 14, loss: 3.8838748931884766\n",
      "epoch: 88,  batch step: 15, loss: 2.6485812664031982\n",
      "epoch: 88,  batch step: 16, loss: 2.8659000396728516\n",
      "epoch: 88,  batch step: 17, loss: 27.179250717163086\n",
      "epoch: 88,  batch step: 18, loss: 3.380547523498535\n",
      "epoch: 88,  batch step: 19, loss: 2.8261325359344482\n",
      "epoch: 88,  batch step: 20, loss: 2.981199026107788\n",
      "epoch: 88,  batch step: 21, loss: 3.661968231201172\n",
      "epoch: 88,  batch step: 22, loss: 1.9925713539123535\n",
      "epoch: 88,  batch step: 23, loss: 2.1714391708374023\n",
      "epoch: 88,  batch step: 24, loss: 1.8527783155441284\n",
      "epoch: 88,  batch step: 25, loss: 3.5222270488739014\n",
      "epoch: 88,  batch step: 26, loss: 1.9686683416366577\n",
      "epoch: 88,  batch step: 27, loss: 21.51571273803711\n",
      "epoch: 88,  batch step: 28, loss: 7.2152323722839355\n",
      "epoch: 88,  batch step: 29, loss: 6.647130966186523\n",
      "epoch: 88,  batch step: 30, loss: 20.654460906982422\n",
      "epoch: 88,  batch step: 31, loss: 7.2094268798828125\n",
      "epoch: 88,  batch step: 32, loss: 36.5328369140625\n",
      "epoch: 88,  batch step: 33, loss: 1.733051061630249\n",
      "epoch: 88,  batch step: 34, loss: 1.8261469602584839\n",
      "epoch: 88,  batch step: 35, loss: 33.885128021240234\n",
      "epoch: 88,  batch step: 36, loss: 20.036428451538086\n",
      "epoch: 88,  batch step: 37, loss: 2.678495407104492\n",
      "epoch: 88,  batch step: 38, loss: 9.800583839416504\n",
      "epoch: 88,  batch step: 39, loss: 12.865614891052246\n",
      "epoch: 88,  batch step: 40, loss: 10.084338188171387\n",
      "epoch: 88,  batch step: 41, loss: 3.5377566814422607\n",
      "epoch: 88,  batch step: 42, loss: 9.544201850891113\n",
      "epoch: 88,  batch step: 43, loss: 3.2553701400756836\n",
      "epoch: 88,  batch step: 44, loss: 2.499485731124878\n",
      "epoch: 88,  batch step: 45, loss: 2.386063575744629\n",
      "epoch: 88,  batch step: 46, loss: 18.12546157836914\n",
      "epoch: 88,  batch step: 47, loss: 46.84599304199219\n",
      "epoch: 88,  batch step: 48, loss: 5.424771785736084\n",
      "epoch: 88,  batch step: 49, loss: 2.13063907623291\n",
      "epoch: 88,  batch step: 50, loss: 9.12634563446045\n",
      "epoch: 88,  batch step: 51, loss: 17.379331588745117\n",
      "epoch: 88,  batch step: 52, loss: 4.416732311248779\n",
      "epoch: 88,  batch step: 53, loss: 7.589074611663818\n",
      "epoch: 88,  batch step: 54, loss: 4.050232887268066\n",
      "epoch: 88,  batch step: 55, loss: 14.852614402770996\n",
      "epoch: 88,  batch step: 56, loss: 1.7840752601623535\n",
      "epoch: 88,  batch step: 57, loss: 2.872972011566162\n",
      "epoch: 88,  batch step: 58, loss: 3.7766222953796387\n",
      "epoch: 88,  batch step: 59, loss: 9.565324783325195\n",
      "epoch: 88,  batch step: 60, loss: 11.298553466796875\n",
      "epoch: 88,  batch step: 61, loss: 2.3392200469970703\n",
      "epoch: 88,  batch step: 62, loss: 40.76806640625\n",
      "epoch: 88,  batch step: 63, loss: 3.1367740631103516\n",
      "epoch: 88,  batch step: 64, loss: 1.9170758724212646\n",
      "epoch: 88,  batch step: 65, loss: 3.826314926147461\n",
      "epoch: 88,  batch step: 66, loss: 1.8951120376586914\n",
      "epoch: 88,  batch step: 67, loss: 2.1511008739471436\n",
      "epoch: 88,  batch step: 68, loss: 2.4400014877319336\n",
      "epoch: 88,  batch step: 69, loss: 2.244649887084961\n",
      "epoch: 88,  batch step: 70, loss: 17.784954071044922\n",
      "epoch: 88,  batch step: 71, loss: 8.438541412353516\n",
      "epoch: 88,  batch step: 72, loss: 21.79833984375\n",
      "epoch: 88,  batch step: 73, loss: 2.6313016414642334\n",
      "epoch: 88,  batch step: 74, loss: 48.35911560058594\n",
      "epoch: 88,  batch step: 75, loss: 2.291848659515381\n",
      "epoch: 88,  batch step: 76, loss: 2.1109111309051514\n",
      "epoch: 88,  batch step: 77, loss: 52.949684143066406\n",
      "epoch: 88,  batch step: 78, loss: 16.799015045166016\n",
      "epoch: 88,  batch step: 79, loss: 29.280956268310547\n",
      "epoch: 88,  batch step: 80, loss: 1.7770040035247803\n",
      "epoch: 88,  batch step: 81, loss: 11.583521842956543\n",
      "epoch: 88,  batch step: 82, loss: 11.457335472106934\n",
      "epoch: 88,  batch step: 83, loss: 2.19175386428833\n",
      "epoch: 88,  batch step: 84, loss: 10.745336532592773\n",
      "epoch: 88,  batch step: 85, loss: 1.587628722190857\n",
      "epoch: 88,  batch step: 86, loss: 2.985081195831299\n",
      "epoch: 88,  batch step: 87, loss: 7.479518890380859\n",
      "epoch: 88,  batch step: 88, loss: 38.37491226196289\n",
      "epoch: 88,  batch step: 89, loss: 2.071713447570801\n",
      "epoch: 88,  batch step: 90, loss: 20.385055541992188\n",
      "epoch: 88,  batch step: 91, loss: 2.060276508331299\n",
      "epoch: 88,  batch step: 92, loss: 2.25551176071167\n",
      "epoch: 88,  batch step: 93, loss: 2.7731971740722656\n",
      "epoch: 88,  batch step: 94, loss: 1.724548101425171\n",
      "epoch: 88,  batch step: 95, loss: 1.8929376602172852\n",
      "epoch: 88,  batch step: 96, loss: 2.7255396842956543\n",
      "epoch: 88,  batch step: 97, loss: 3.0348291397094727\n",
      "epoch: 88,  batch step: 98, loss: 2.4406332969665527\n",
      "epoch: 88,  batch step: 99, loss: 8.449212074279785\n",
      "epoch: 88,  batch step: 100, loss: 2.3293747901916504\n",
      "epoch: 88,  batch step: 101, loss: 4.411652565002441\n",
      "epoch: 88,  batch step: 102, loss: 2.975182056427002\n",
      "epoch: 88,  batch step: 103, loss: 1.6120818853378296\n",
      "epoch: 88,  batch step: 104, loss: 2.3916356563568115\n",
      "epoch: 88,  batch step: 105, loss: 11.599910736083984\n",
      "epoch: 88,  batch step: 106, loss: 73.72590637207031\n",
      "epoch: 88,  batch step: 107, loss: 4.046270370483398\n",
      "epoch: 88,  batch step: 108, loss: 22.628456115722656\n",
      "epoch: 88,  batch step: 109, loss: 38.17661666870117\n",
      "epoch: 88,  batch step: 110, loss: 3.9508519172668457\n",
      "epoch: 88,  batch step: 111, loss: 16.704343795776367\n",
      "epoch: 88,  batch step: 112, loss: 1.8029944896697998\n",
      "epoch: 88,  batch step: 113, loss: 4.139382362365723\n",
      "epoch: 88,  batch step: 114, loss: 1.742955207824707\n",
      "epoch: 88,  batch step: 115, loss: 2.2156643867492676\n",
      "epoch: 88,  batch step: 116, loss: 50.12800216674805\n",
      "epoch: 88,  batch step: 117, loss: 20.047590255737305\n",
      "epoch: 88,  batch step: 118, loss: 16.7668514251709\n",
      "epoch: 88,  batch step: 119, loss: 3.587427854537964\n",
      "epoch: 88,  batch step: 120, loss: 1.6997737884521484\n",
      "epoch: 88,  batch step: 121, loss: 2.088383436203003\n",
      "epoch: 88,  batch step: 122, loss: 9.524164199829102\n",
      "epoch: 88,  batch step: 123, loss: 2.2919652462005615\n",
      "epoch: 88,  batch step: 124, loss: 10.379276275634766\n",
      "epoch: 88,  batch step: 125, loss: 12.916290283203125\n",
      "epoch: 88,  batch step: 126, loss: 1.6557881832122803\n",
      "epoch: 88,  batch step: 127, loss: 23.9449462890625\n",
      "epoch: 88,  batch step: 128, loss: 9.658062934875488\n",
      "epoch: 88,  batch step: 129, loss: 16.43718147277832\n",
      "epoch: 88,  batch step: 130, loss: 2.2773985862731934\n",
      "epoch: 88,  batch step: 131, loss: 9.768824577331543\n",
      "epoch: 88,  batch step: 132, loss: 1.673628807067871\n",
      "epoch: 88,  batch step: 133, loss: 1.8291199207305908\n",
      "epoch: 88,  batch step: 134, loss: 9.452739715576172\n",
      "epoch: 88,  batch step: 135, loss: 102.35420989990234\n",
      "epoch: 88,  batch step: 136, loss: 9.361987113952637\n",
      "epoch: 88,  batch step: 137, loss: 5.891080379486084\n",
      "epoch: 88,  batch step: 138, loss: 15.304107666015625\n",
      "epoch: 88,  batch step: 139, loss: 6.274910926818848\n",
      "epoch: 88,  batch step: 140, loss: 1.469510555267334\n",
      "epoch: 88,  batch step: 141, loss: 1.4871957302093506\n",
      "epoch: 88,  batch step: 142, loss: 2.065945625305176\n",
      "epoch: 88,  batch step: 143, loss: 1.5016640424728394\n",
      "epoch: 88,  batch step: 144, loss: 12.380950927734375\n",
      "epoch: 88,  batch step: 145, loss: 2.817119598388672\n",
      "epoch: 88,  batch step: 146, loss: 3.877033233642578\n",
      "epoch: 88,  batch step: 147, loss: 14.568990707397461\n",
      "epoch: 88,  batch step: 148, loss: 7.906543731689453\n",
      "epoch: 88,  batch step: 149, loss: 1.451157569885254\n",
      "epoch: 88,  batch step: 150, loss: 46.970191955566406\n",
      "epoch: 88,  batch step: 151, loss: 8.034673690795898\n",
      "epoch: 88,  batch step: 152, loss: 2.801849842071533\n",
      "epoch: 88,  batch step: 153, loss: 1.7960284948349\n",
      "epoch: 88,  batch step: 154, loss: 13.299861907958984\n",
      "epoch: 88,  batch step: 155, loss: 30.47969627380371\n",
      "epoch: 88,  batch step: 156, loss: 1.9500806331634521\n",
      "epoch: 88,  batch step: 157, loss: 12.409930229187012\n",
      "epoch: 88,  batch step: 158, loss: 16.785140991210938\n",
      "epoch: 88,  batch step: 159, loss: 12.338358879089355\n",
      "epoch: 88,  batch step: 160, loss: 2.2023017406463623\n",
      "epoch: 88,  batch step: 161, loss: 2.5214481353759766\n",
      "epoch: 88,  batch step: 162, loss: 6.024857997894287\n",
      "epoch: 88,  batch step: 163, loss: 10.133334159851074\n",
      "epoch: 88,  batch step: 164, loss: 8.799755096435547\n",
      "epoch: 88,  batch step: 165, loss: 12.94505500793457\n",
      "epoch: 88,  batch step: 166, loss: 11.14588451385498\n",
      "epoch: 88,  batch step: 167, loss: 15.738485336303711\n",
      "epoch: 88,  batch step: 168, loss: 5.558009147644043\n",
      "epoch: 88,  batch step: 169, loss: 53.286109924316406\n",
      "epoch: 88,  batch step: 170, loss: 2.0527403354644775\n",
      "epoch: 88,  batch step: 171, loss: 2.011812925338745\n",
      "epoch: 88,  batch step: 172, loss: 2.3132243156433105\n",
      "epoch: 88,  batch step: 173, loss: 7.533206462860107\n",
      "epoch: 88,  batch step: 174, loss: 6.993621826171875\n",
      "epoch: 88,  batch step: 175, loss: 2.9612433910369873\n",
      "epoch: 88,  batch step: 176, loss: 140.32815551757812\n",
      "epoch: 88,  batch step: 177, loss: 16.85866355895996\n",
      "epoch: 88,  batch step: 178, loss: 71.87271881103516\n",
      "epoch: 88,  batch step: 179, loss: 25.339519500732422\n",
      "epoch: 88,  batch step: 180, loss: 2.7253012657165527\n",
      "epoch: 88,  batch step: 181, loss: 2.0904366970062256\n",
      "epoch: 88,  batch step: 182, loss: 22.229896545410156\n",
      "epoch: 88,  batch step: 183, loss: 2.6083197593688965\n",
      "epoch: 88,  batch step: 184, loss: 8.14234733581543\n",
      "epoch: 88,  batch step: 185, loss: 7.807380676269531\n",
      "epoch: 88,  batch step: 186, loss: 2.122828483581543\n",
      "epoch: 88,  batch step: 187, loss: 1.6819360256195068\n",
      "epoch: 88,  batch step: 188, loss: 3.358152389526367\n",
      "epoch: 88,  batch step: 189, loss: 2.3686697483062744\n",
      "epoch: 88,  batch step: 190, loss: 2.4073143005371094\n",
      "epoch: 88,  batch step: 191, loss: 18.16765594482422\n",
      "epoch: 88,  batch step: 192, loss: 3.630662679672241\n",
      "epoch: 88,  batch step: 193, loss: 9.838747024536133\n",
      "epoch: 88,  batch step: 194, loss: 20.398069381713867\n",
      "epoch: 88,  batch step: 195, loss: 5.060467720031738\n",
      "epoch: 88,  batch step: 196, loss: 9.810684204101562\n",
      "epoch: 88,  batch step: 197, loss: 49.98908996582031\n",
      "epoch: 88,  batch step: 198, loss: 10.407609939575195\n",
      "epoch: 88,  batch step: 199, loss: 17.03461456298828\n",
      "epoch: 88,  batch step: 200, loss: 58.71931457519531\n",
      "epoch: 88,  batch step: 201, loss: 13.826501846313477\n",
      "epoch: 88,  batch step: 202, loss: 12.591527938842773\n",
      "epoch: 88,  batch step: 203, loss: 29.6989803314209\n",
      "epoch: 88,  batch step: 204, loss: 1.5883387327194214\n",
      "epoch: 88,  batch step: 205, loss: 14.32153606414795\n",
      "epoch: 88,  batch step: 206, loss: 10.960590362548828\n",
      "epoch: 88,  batch step: 207, loss: 18.33209991455078\n",
      "epoch: 88,  batch step: 208, loss: 30.45281982421875\n",
      "epoch: 88,  batch step: 209, loss: 3.729887008666992\n",
      "epoch: 88,  batch step: 210, loss: 15.719249725341797\n",
      "epoch: 88,  batch step: 211, loss: 10.190613746643066\n",
      "epoch: 88,  batch step: 212, loss: 22.673564910888672\n",
      "epoch: 88,  batch step: 213, loss: 1.6779849529266357\n",
      "epoch: 88,  batch step: 214, loss: 3.279308319091797\n",
      "epoch: 88,  batch step: 215, loss: 1.7069305181503296\n",
      "epoch: 88,  batch step: 216, loss: 2.188960552215576\n",
      "epoch: 88,  batch step: 217, loss: 45.291988372802734\n",
      "epoch: 88,  batch step: 218, loss: 2.362031936645508\n",
      "epoch: 88,  batch step: 219, loss: 1.9592974185943604\n",
      "epoch: 88,  batch step: 220, loss: 5.008070945739746\n",
      "epoch: 88,  batch step: 221, loss: 21.79235076904297\n",
      "epoch: 88,  batch step: 222, loss: 27.882177352905273\n",
      "epoch: 88,  batch step: 223, loss: 14.787220001220703\n",
      "epoch: 88,  batch step: 224, loss: 1.7070813179016113\n",
      "epoch: 88,  batch step: 225, loss: 1.9511504173278809\n",
      "epoch: 88,  batch step: 226, loss: 2.3382010459899902\n",
      "epoch: 88,  batch step: 227, loss: 10.444530487060547\n",
      "epoch: 88,  batch step: 228, loss: 2.3002729415893555\n",
      "epoch: 88,  batch step: 229, loss: 32.2462158203125\n",
      "epoch: 88,  batch step: 230, loss: 53.486568450927734\n",
      "epoch: 88,  batch step: 231, loss: 2.5098726749420166\n",
      "epoch: 88,  batch step: 232, loss: 3.9429497718811035\n",
      "epoch: 88,  batch step: 233, loss: 22.89565086364746\n",
      "epoch: 88,  batch step: 234, loss: 3.514007091522217\n",
      "epoch: 88,  batch step: 235, loss: 30.157758712768555\n",
      "epoch: 88,  batch step: 236, loss: 8.19096565246582\n",
      "epoch: 88,  batch step: 237, loss: 4.771043300628662\n",
      "epoch: 88,  batch step: 238, loss: 14.579160690307617\n",
      "epoch: 88,  batch step: 239, loss: 2.8304905891418457\n",
      "epoch: 88,  batch step: 240, loss: 37.74653625488281\n",
      "epoch: 88,  batch step: 241, loss: 4.241642475128174\n",
      "epoch: 88,  batch step: 242, loss: 2.4044456481933594\n",
      "epoch: 88,  batch step: 243, loss: 33.084224700927734\n",
      "epoch: 88,  batch step: 244, loss: 3.899834394454956\n",
      "epoch: 88,  batch step: 245, loss: 1.6565895080566406\n",
      "epoch: 88,  batch step: 246, loss: 2.917637586593628\n",
      "epoch: 88,  batch step: 247, loss: 9.718363761901855\n",
      "epoch: 88,  batch step: 248, loss: 7.17891788482666\n",
      "epoch: 88,  batch step: 249, loss: 19.53797149658203\n",
      "epoch: 88,  batch step: 250, loss: 37.90745162963867\n",
      "epoch: 88,  batch step: 251, loss: 7.176138401031494\n",
      "validation error epoch  88:    tensor(69.3101, device='cuda:0')\n",
      "316\n",
      "epoch: 89,  batch step: 0, loss: 2.598480701446533\n",
      "epoch: 89,  batch step: 1, loss: 12.836568832397461\n",
      "epoch: 89,  batch step: 2, loss: 17.920238494873047\n",
      "epoch: 89,  batch step: 3, loss: 29.890844345092773\n",
      "epoch: 89,  batch step: 4, loss: 14.154117584228516\n",
      "epoch: 89,  batch step: 5, loss: 3.031728744506836\n",
      "epoch: 89,  batch step: 6, loss: 3.4348838329315186\n",
      "epoch: 89,  batch step: 7, loss: 12.363969802856445\n",
      "epoch: 89,  batch step: 8, loss: 28.170194625854492\n",
      "epoch: 89,  batch step: 9, loss: 39.512596130371094\n",
      "epoch: 89,  batch step: 10, loss: 8.231103897094727\n",
      "epoch: 89,  batch step: 11, loss: 7.481206893920898\n",
      "epoch: 89,  batch step: 12, loss: 3.442821502685547\n",
      "epoch: 89,  batch step: 13, loss: 2.737466335296631\n",
      "epoch: 89,  batch step: 14, loss: 36.90336990356445\n",
      "epoch: 89,  batch step: 15, loss: 4.500415802001953\n",
      "epoch: 89,  batch step: 16, loss: 2.3545539379119873\n",
      "epoch: 89,  batch step: 17, loss: 3.679318904876709\n",
      "epoch: 89,  batch step: 18, loss: 35.39353942871094\n",
      "epoch: 89,  batch step: 19, loss: 15.87818431854248\n",
      "epoch: 89,  batch step: 20, loss: 5.918536186218262\n",
      "epoch: 89,  batch step: 21, loss: 2.547388792037964\n",
      "epoch: 89,  batch step: 22, loss: 24.040279388427734\n",
      "epoch: 89,  batch step: 23, loss: 3.69465708732605\n",
      "epoch: 89,  batch step: 24, loss: 13.519798278808594\n",
      "epoch: 89,  batch step: 25, loss: 2.0321202278137207\n",
      "epoch: 89,  batch step: 26, loss: 15.416282653808594\n",
      "epoch: 89,  batch step: 27, loss: 19.091636657714844\n",
      "epoch: 89,  batch step: 28, loss: 5.341031551361084\n",
      "epoch: 89,  batch step: 29, loss: 2.6915407180786133\n",
      "epoch: 89,  batch step: 30, loss: 3.9344730377197266\n",
      "epoch: 89,  batch step: 31, loss: 13.957316398620605\n",
      "epoch: 89,  batch step: 32, loss: 3.9838924407958984\n",
      "epoch: 89,  batch step: 33, loss: 3.1387507915496826\n",
      "epoch: 89,  batch step: 34, loss: 8.744372367858887\n",
      "epoch: 89,  batch step: 35, loss: 4.389943599700928\n",
      "epoch: 89,  batch step: 36, loss: 2.366885185241699\n",
      "epoch: 89,  batch step: 37, loss: 28.926326751708984\n",
      "epoch: 89,  batch step: 38, loss: 2.1206564903259277\n",
      "epoch: 89,  batch step: 39, loss: 16.934602737426758\n",
      "epoch: 89,  batch step: 40, loss: 8.603649139404297\n",
      "epoch: 89,  batch step: 41, loss: 57.85439682006836\n",
      "epoch: 89,  batch step: 42, loss: 15.47646713256836\n",
      "epoch: 89,  batch step: 43, loss: 2.565462827682495\n",
      "epoch: 89,  batch step: 44, loss: 45.270050048828125\n",
      "epoch: 89,  batch step: 45, loss: 11.045448303222656\n",
      "epoch: 89,  batch step: 46, loss: 11.440139770507812\n",
      "epoch: 89,  batch step: 47, loss: 9.483062744140625\n",
      "epoch: 89,  batch step: 48, loss: 1.6756181716918945\n",
      "epoch: 89,  batch step: 49, loss: 2.1988465785980225\n",
      "epoch: 89,  batch step: 50, loss: 8.281624794006348\n",
      "epoch: 89,  batch step: 51, loss: 34.17083740234375\n",
      "epoch: 89,  batch step: 52, loss: 1.5956487655639648\n",
      "epoch: 89,  batch step: 53, loss: 131.7751007080078\n",
      "epoch: 89,  batch step: 54, loss: 14.94953441619873\n",
      "epoch: 89,  batch step: 55, loss: 7.601424694061279\n",
      "epoch: 89,  batch step: 56, loss: 5.022353649139404\n",
      "epoch: 89,  batch step: 57, loss: 2.0073904991149902\n",
      "epoch: 89,  batch step: 58, loss: 18.026073455810547\n",
      "epoch: 89,  batch step: 59, loss: 2.073187828063965\n",
      "epoch: 89,  batch step: 60, loss: 4.238046646118164\n",
      "epoch: 89,  batch step: 61, loss: 62.17118453979492\n",
      "epoch: 89,  batch step: 62, loss: 4.192432403564453\n",
      "epoch: 89,  batch step: 63, loss: 36.33444595336914\n",
      "epoch: 89,  batch step: 64, loss: 24.191730499267578\n",
      "epoch: 89,  batch step: 65, loss: 21.873014450073242\n",
      "epoch: 89,  batch step: 66, loss: 4.095466613769531\n",
      "epoch: 89,  batch step: 67, loss: 15.97092342376709\n",
      "epoch: 89,  batch step: 68, loss: 64.53279876708984\n",
      "epoch: 89,  batch step: 69, loss: 54.48371124267578\n",
      "epoch: 89,  batch step: 70, loss: 10.711803436279297\n",
      "epoch: 89,  batch step: 71, loss: 1.5130574703216553\n",
      "epoch: 89,  batch step: 72, loss: 1.7052229642868042\n",
      "epoch: 89,  batch step: 73, loss: 8.9944486618042\n",
      "epoch: 89,  batch step: 74, loss: 48.20537567138672\n",
      "epoch: 89,  batch step: 75, loss: 2.465789318084717\n",
      "epoch: 89,  batch step: 76, loss: 8.77607536315918\n",
      "epoch: 89,  batch step: 77, loss: 2.5604264736175537\n",
      "epoch: 89,  batch step: 78, loss: 4.320897579193115\n",
      "epoch: 89,  batch step: 79, loss: 3.605642080307007\n",
      "epoch: 89,  batch step: 80, loss: 29.925527572631836\n",
      "epoch: 89,  batch step: 81, loss: 7.636343955993652\n",
      "epoch: 89,  batch step: 82, loss: 2.334592819213867\n",
      "epoch: 89,  batch step: 83, loss: 8.232434272766113\n",
      "epoch: 89,  batch step: 84, loss: 2.289827823638916\n",
      "epoch: 89,  batch step: 85, loss: 11.87290096282959\n",
      "epoch: 89,  batch step: 86, loss: 2.2633464336395264\n",
      "epoch: 89,  batch step: 87, loss: 21.5748348236084\n",
      "epoch: 89,  batch step: 88, loss: 3.296018123626709\n",
      "epoch: 89,  batch step: 89, loss: 3.324981212615967\n",
      "epoch: 89,  batch step: 90, loss: 3.4712817668914795\n",
      "epoch: 89,  batch step: 91, loss: 4.0264105796813965\n",
      "epoch: 89,  batch step: 92, loss: 3.281400680541992\n",
      "epoch: 89,  batch step: 93, loss: 23.752912521362305\n",
      "epoch: 89,  batch step: 94, loss: 8.548912048339844\n",
      "epoch: 89,  batch step: 95, loss: 3.269723653793335\n",
      "epoch: 89,  batch step: 96, loss: 3.28023099899292\n",
      "epoch: 89,  batch step: 97, loss: 3.399585485458374\n",
      "epoch: 89,  batch step: 98, loss: 10.017827987670898\n",
      "epoch: 89,  batch step: 99, loss: 15.700799942016602\n",
      "epoch: 89,  batch step: 100, loss: 2.6430792808532715\n",
      "epoch: 89,  batch step: 101, loss: 3.0639092922210693\n",
      "epoch: 89,  batch step: 102, loss: 25.390850067138672\n",
      "epoch: 89,  batch step: 103, loss: 20.833927154541016\n",
      "epoch: 89,  batch step: 104, loss: 10.480558395385742\n",
      "epoch: 89,  batch step: 105, loss: 8.02759838104248\n",
      "epoch: 89,  batch step: 106, loss: 15.048128128051758\n",
      "epoch: 89,  batch step: 107, loss: 4.704907417297363\n",
      "epoch: 89,  batch step: 108, loss: 20.813100814819336\n",
      "epoch: 89,  batch step: 109, loss: 7.808076858520508\n",
      "epoch: 89,  batch step: 110, loss: 8.03046989440918\n",
      "epoch: 89,  batch step: 111, loss: 5.774704933166504\n",
      "epoch: 89,  batch step: 112, loss: 11.185871124267578\n",
      "epoch: 89,  batch step: 113, loss: 1.5505170822143555\n",
      "epoch: 89,  batch step: 114, loss: 12.312076568603516\n",
      "epoch: 89,  batch step: 115, loss: 57.17852020263672\n",
      "epoch: 89,  batch step: 116, loss: 14.586626052856445\n",
      "epoch: 89,  batch step: 117, loss: 40.520225524902344\n",
      "epoch: 89,  batch step: 118, loss: 2.819589376449585\n",
      "epoch: 89,  batch step: 119, loss: 11.036170959472656\n",
      "epoch: 89,  batch step: 120, loss: 2.1489498615264893\n",
      "epoch: 89,  batch step: 121, loss: 11.458656311035156\n",
      "epoch: 89,  batch step: 122, loss: 67.80580139160156\n",
      "epoch: 89,  batch step: 123, loss: 30.648815155029297\n",
      "epoch: 89,  batch step: 124, loss: 38.16726303100586\n",
      "epoch: 89,  batch step: 125, loss: 2.2586817741394043\n",
      "epoch: 89,  batch step: 126, loss: 8.224416732788086\n",
      "epoch: 89,  batch step: 127, loss: 2.211576461791992\n",
      "epoch: 89,  batch step: 128, loss: 25.3896541595459\n",
      "epoch: 89,  batch step: 129, loss: 2.7597789764404297\n",
      "epoch: 89,  batch step: 130, loss: 53.59096908569336\n",
      "epoch: 89,  batch step: 131, loss: 11.88340950012207\n",
      "epoch: 89,  batch step: 132, loss: 8.987154960632324\n",
      "epoch: 89,  batch step: 133, loss: 2.609225273132324\n",
      "epoch: 89,  batch step: 134, loss: 16.187185287475586\n",
      "epoch: 89,  batch step: 135, loss: 2.637328863143921\n",
      "epoch: 89,  batch step: 136, loss: 9.396635055541992\n",
      "epoch: 89,  batch step: 137, loss: 15.523494720458984\n",
      "epoch: 89,  batch step: 138, loss: 5.262335300445557\n",
      "epoch: 89,  batch step: 139, loss: 2.7320384979248047\n",
      "epoch: 89,  batch step: 140, loss: 2.579357147216797\n",
      "epoch: 89,  batch step: 141, loss: 2.658811569213867\n",
      "epoch: 89,  batch step: 142, loss: 3.4335315227508545\n",
      "epoch: 89,  batch step: 143, loss: 2.3021388053894043\n",
      "epoch: 89,  batch step: 144, loss: 4.987616539001465\n",
      "epoch: 89,  batch step: 145, loss: 2.5586793422698975\n",
      "epoch: 89,  batch step: 146, loss: 2.4460463523864746\n",
      "epoch: 89,  batch step: 147, loss: 3.891373634338379\n",
      "epoch: 89,  batch step: 148, loss: 16.125457763671875\n",
      "epoch: 89,  batch step: 149, loss: 64.50493621826172\n",
      "epoch: 89,  batch step: 150, loss: 2.9256598949432373\n",
      "epoch: 89,  batch step: 151, loss: 2.8552160263061523\n",
      "epoch: 89,  batch step: 152, loss: 2.5902528762817383\n",
      "epoch: 89,  batch step: 153, loss: 50.260780334472656\n",
      "epoch: 89,  batch step: 154, loss: 15.23791790008545\n",
      "epoch: 89,  batch step: 155, loss: 36.663658142089844\n",
      "epoch: 89,  batch step: 156, loss: 2.629345417022705\n",
      "epoch: 89,  batch step: 157, loss: 3.644972562789917\n",
      "epoch: 89,  batch step: 158, loss: 2.7588374614715576\n",
      "epoch: 89,  batch step: 159, loss: 22.869510650634766\n",
      "epoch: 89,  batch step: 160, loss: 2.3402438163757324\n",
      "epoch: 89,  batch step: 161, loss: 3.720801830291748\n",
      "epoch: 89,  batch step: 162, loss: 4.277857303619385\n",
      "epoch: 89,  batch step: 163, loss: 56.63810729980469\n",
      "epoch: 89,  batch step: 164, loss: 4.945046424865723\n",
      "epoch: 89,  batch step: 165, loss: 3.5552117824554443\n",
      "epoch: 89,  batch step: 166, loss: 28.652111053466797\n",
      "epoch: 89,  batch step: 167, loss: 16.844633102416992\n",
      "epoch: 89,  batch step: 168, loss: 2.662475824356079\n",
      "epoch: 89,  batch step: 169, loss: 16.023975372314453\n",
      "epoch: 89,  batch step: 170, loss: 4.201409816741943\n",
      "epoch: 89,  batch step: 171, loss: 3.2027947902679443\n",
      "epoch: 89,  batch step: 172, loss: 30.552467346191406\n",
      "epoch: 89,  batch step: 173, loss: 2.1390507221221924\n",
      "epoch: 89,  batch step: 174, loss: 16.363719940185547\n",
      "epoch: 89,  batch step: 175, loss: 4.610230445861816\n",
      "epoch: 89,  batch step: 176, loss: 14.211932182312012\n",
      "epoch: 89,  batch step: 177, loss: 2.6915106773376465\n",
      "epoch: 89,  batch step: 178, loss: 23.108440399169922\n",
      "epoch: 89,  batch step: 179, loss: 2.8366715908050537\n",
      "epoch: 89,  batch step: 180, loss: 33.81503677368164\n",
      "epoch: 89,  batch step: 181, loss: 13.763395309448242\n",
      "epoch: 89,  batch step: 182, loss: 5.895068645477295\n",
      "epoch: 89,  batch step: 183, loss: 4.242650508880615\n",
      "epoch: 89,  batch step: 184, loss: 3.2573299407958984\n",
      "epoch: 89,  batch step: 185, loss: 3.860360860824585\n",
      "epoch: 89,  batch step: 186, loss: 26.409740447998047\n",
      "epoch: 89,  batch step: 187, loss: 9.051619529724121\n",
      "epoch: 89,  batch step: 188, loss: 3.3195252418518066\n",
      "epoch: 89,  batch step: 189, loss: 3.6483707427978516\n",
      "epoch: 89,  batch step: 190, loss: 58.49713897705078\n",
      "epoch: 89,  batch step: 191, loss: 2.8092029094696045\n",
      "epoch: 89,  batch step: 192, loss: 2.9562079906463623\n",
      "epoch: 89,  batch step: 193, loss: 3.034090042114258\n",
      "epoch: 89,  batch step: 194, loss: 2.59627103805542\n",
      "epoch: 89,  batch step: 195, loss: 18.871856689453125\n",
      "epoch: 89,  batch step: 196, loss: 3.648463010787964\n",
      "epoch: 89,  batch step: 197, loss: 7.894073486328125\n",
      "epoch: 89,  batch step: 198, loss: 45.46049499511719\n",
      "epoch: 89,  batch step: 199, loss: 27.937292098999023\n",
      "epoch: 89,  batch step: 200, loss: 31.906116485595703\n",
      "epoch: 89,  batch step: 201, loss: 8.01500129699707\n",
      "epoch: 89,  batch step: 202, loss: 33.152042388916016\n",
      "epoch: 89,  batch step: 203, loss: 14.29733657836914\n",
      "epoch: 89,  batch step: 204, loss: 5.12879753112793\n",
      "epoch: 89,  batch step: 205, loss: 37.37687683105469\n",
      "epoch: 89,  batch step: 206, loss: 63.30714416503906\n",
      "epoch: 89,  batch step: 207, loss: 8.554773330688477\n",
      "epoch: 89,  batch step: 208, loss: 37.0472297668457\n",
      "epoch: 89,  batch step: 209, loss: 16.2420654296875\n",
      "epoch: 89,  batch step: 210, loss: 3.298062801361084\n",
      "epoch: 89,  batch step: 211, loss: 2.9700496196746826\n",
      "epoch: 89,  batch step: 212, loss: 8.274087905883789\n",
      "epoch: 89,  batch step: 213, loss: 29.460105895996094\n",
      "epoch: 89,  batch step: 214, loss: 16.099668502807617\n",
      "epoch: 89,  batch step: 215, loss: 26.213308334350586\n",
      "epoch: 89,  batch step: 216, loss: 81.61508178710938\n",
      "epoch: 89,  batch step: 217, loss: 9.109480857849121\n",
      "epoch: 89,  batch step: 218, loss: 5.53000020980835\n",
      "epoch: 89,  batch step: 219, loss: 5.752306938171387\n",
      "epoch: 89,  batch step: 220, loss: 4.282375335693359\n",
      "epoch: 89,  batch step: 221, loss: 5.689393520355225\n",
      "epoch: 89,  batch step: 222, loss: 67.65695190429688\n",
      "epoch: 89,  batch step: 223, loss: 12.50602912902832\n",
      "epoch: 89,  batch step: 224, loss: 4.282876014709473\n",
      "epoch: 89,  batch step: 225, loss: 50.52683639526367\n",
      "epoch: 89,  batch step: 226, loss: 4.37775993347168\n",
      "epoch: 89,  batch step: 227, loss: 5.120497703552246\n",
      "epoch: 89,  batch step: 228, loss: 16.224761962890625\n",
      "epoch: 89,  batch step: 229, loss: 31.47322654724121\n",
      "epoch: 89,  batch step: 230, loss: 21.25690460205078\n",
      "epoch: 89,  batch step: 231, loss: 11.140399932861328\n",
      "epoch: 89,  batch step: 232, loss: 5.582376003265381\n",
      "epoch: 89,  batch step: 233, loss: 37.99885559082031\n",
      "epoch: 89,  batch step: 234, loss: 28.290863037109375\n",
      "epoch: 89,  batch step: 235, loss: 5.670358180999756\n",
      "epoch: 89,  batch step: 236, loss: 6.048332214355469\n",
      "epoch: 89,  batch step: 237, loss: 8.040730476379395\n",
      "epoch: 89,  batch step: 238, loss: 34.52595520019531\n",
      "epoch: 89,  batch step: 239, loss: 22.62792205810547\n",
      "epoch: 89,  batch step: 240, loss: 3.0129871368408203\n",
      "epoch: 89,  batch step: 241, loss: 41.96419143676758\n",
      "epoch: 89,  batch step: 242, loss: 9.54407787322998\n",
      "epoch: 89,  batch step: 243, loss: 29.593107223510742\n",
      "epoch: 89,  batch step: 244, loss: 4.487704753875732\n",
      "epoch: 89,  batch step: 245, loss: 3.772500991821289\n",
      "epoch: 89,  batch step: 246, loss: 4.196412086486816\n",
      "epoch: 89,  batch step: 247, loss: 19.89468002319336\n",
      "epoch: 89,  batch step: 248, loss: 3.2102203369140625\n",
      "epoch: 89,  batch step: 249, loss: 18.375673294067383\n",
      "epoch: 89,  batch step: 250, loss: 20.8974609375\n",
      "epoch: 89,  batch step: 251, loss: 3270.19873046875\n",
      "finished saving checkpoints\n",
      "validation error epoch  89:    tensor(93.2475, device='cuda:0')\n",
      "316\n",
      "epoch: 90,  batch step: 0, loss: 25.582015991210938\n",
      "epoch: 90,  batch step: 1, loss: 59.75896453857422\n",
      "epoch: 90,  batch step: 2, loss: 18.39440155029297\n",
      "epoch: 90,  batch step: 3, loss: 112.62973022460938\n",
      "epoch: 90,  batch step: 4, loss: 213.7635498046875\n",
      "epoch: 90,  batch step: 5, loss: 71.49669647216797\n",
      "epoch: 90,  batch step: 6, loss: 165.3677978515625\n",
      "epoch: 90,  batch step: 7, loss: 107.75242614746094\n",
      "epoch: 90,  batch step: 8, loss: 49.66839599609375\n",
      "epoch: 90,  batch step: 9, loss: 111.65982055664062\n",
      "epoch: 90,  batch step: 10, loss: 55.23992919921875\n",
      "epoch: 90,  batch step: 11, loss: 59.634849548339844\n",
      "epoch: 90,  batch step: 12, loss: 22.21292495727539\n",
      "epoch: 90,  batch step: 13, loss: 22.019222259521484\n",
      "epoch: 90,  batch step: 14, loss: 125.22838592529297\n",
      "epoch: 90,  batch step: 15, loss: 29.031761169433594\n",
      "epoch: 90,  batch step: 16, loss: 15.785995483398438\n",
      "epoch: 90,  batch step: 17, loss: 358.14520263671875\n",
      "epoch: 90,  batch step: 18, loss: 38.98141860961914\n",
      "epoch: 90,  batch step: 19, loss: 34.78900909423828\n",
      "epoch: 90,  batch step: 20, loss: 25.98096466064453\n",
      "epoch: 90,  batch step: 21, loss: 27.37983512878418\n",
      "epoch: 90,  batch step: 22, loss: 17.248191833496094\n",
      "epoch: 90,  batch step: 23, loss: 105.30155944824219\n",
      "epoch: 90,  batch step: 24, loss: 14.023035049438477\n",
      "epoch: 90,  batch step: 25, loss: 37.87401580810547\n",
      "epoch: 90,  batch step: 26, loss: 13.96647834777832\n",
      "epoch: 90,  batch step: 27, loss: 8.497013092041016\n",
      "epoch: 90,  batch step: 28, loss: 27.760601043701172\n",
      "epoch: 90,  batch step: 29, loss: 13.488792419433594\n",
      "epoch: 90,  batch step: 30, loss: 24.628929138183594\n",
      "epoch: 90,  batch step: 31, loss: 33.382598876953125\n",
      "epoch: 90,  batch step: 32, loss: 14.899467468261719\n",
      "epoch: 90,  batch step: 33, loss: 9.645624160766602\n",
      "epoch: 90,  batch step: 34, loss: 38.51057815551758\n",
      "epoch: 90,  batch step: 35, loss: 13.25793743133545\n",
      "epoch: 90,  batch step: 36, loss: 28.809749603271484\n",
      "epoch: 90,  batch step: 37, loss: 256.7938232421875\n",
      "epoch: 90,  batch step: 38, loss: 189.80319213867188\n",
      "epoch: 90,  batch step: 39, loss: 15.904200553894043\n",
      "epoch: 90,  batch step: 40, loss: 143.66249084472656\n",
      "epoch: 90,  batch step: 41, loss: 13.24186897277832\n",
      "epoch: 90,  batch step: 42, loss: 12.118003845214844\n",
      "epoch: 90,  batch step: 43, loss: 12.295977592468262\n",
      "epoch: 90,  batch step: 44, loss: 15.924365997314453\n",
      "epoch: 90,  batch step: 45, loss: 17.39910125732422\n",
      "epoch: 90,  batch step: 46, loss: 23.133716583251953\n",
      "epoch: 90,  batch step: 47, loss: 40.088558197021484\n",
      "epoch: 90,  batch step: 48, loss: 8.505587577819824\n",
      "epoch: 90,  batch step: 49, loss: 8.066664695739746\n",
      "epoch: 90,  batch step: 50, loss: 249.3751220703125\n",
      "epoch: 90,  batch step: 51, loss: 30.736621856689453\n",
      "epoch: 90,  batch step: 52, loss: 6.603093147277832\n",
      "epoch: 90,  batch step: 53, loss: 24.918880462646484\n",
      "epoch: 90,  batch step: 54, loss: 8.600408554077148\n",
      "epoch: 90,  batch step: 55, loss: 7.691649436950684\n",
      "epoch: 90,  batch step: 56, loss: 124.15486907958984\n",
      "epoch: 90,  batch step: 57, loss: 255.017822265625\n",
      "epoch: 90,  batch step: 58, loss: 190.0703582763672\n",
      "epoch: 90,  batch step: 59, loss: 7.354292869567871\n",
      "epoch: 90,  batch step: 60, loss: 34.59830093383789\n",
      "epoch: 90,  batch step: 61, loss: 28.86244773864746\n",
      "epoch: 90,  batch step: 62, loss: 13.006223678588867\n",
      "epoch: 90,  batch step: 63, loss: 253.69500732421875\n",
      "epoch: 90,  batch step: 64, loss: 108.24386596679688\n",
      "epoch: 90,  batch step: 65, loss: 10.989638328552246\n",
      "epoch: 90,  batch step: 66, loss: 270.53839111328125\n",
      "epoch: 90,  batch step: 67, loss: 40.125579833984375\n",
      "epoch: 90,  batch step: 68, loss: 39.35047149658203\n",
      "epoch: 90,  batch step: 69, loss: 21.798877716064453\n",
      "epoch: 90,  batch step: 70, loss: 82.03170776367188\n",
      "epoch: 90,  batch step: 71, loss: 14.365446090698242\n",
      "epoch: 90,  batch step: 72, loss: 82.67770385742188\n",
      "epoch: 90,  batch step: 73, loss: 94.20298767089844\n",
      "epoch: 90,  batch step: 74, loss: 11.179048538208008\n",
      "epoch: 90,  batch step: 75, loss: 12.49454116821289\n",
      "epoch: 90,  batch step: 76, loss: 9.649415969848633\n",
      "epoch: 90,  batch step: 77, loss: 23.509090423583984\n",
      "epoch: 90,  batch step: 78, loss: 10.47669506072998\n",
      "epoch: 90,  batch step: 79, loss: 45.94057846069336\n",
      "epoch: 90,  batch step: 80, loss: 10.01992130279541\n",
      "epoch: 90,  batch step: 81, loss: 36.777305603027344\n",
      "epoch: 90,  batch step: 82, loss: 9.160218238830566\n",
      "epoch: 90,  batch step: 83, loss: 12.480033874511719\n",
      "epoch: 90,  batch step: 84, loss: 12.696487426757812\n",
      "epoch: 90,  batch step: 85, loss: 178.47323608398438\n",
      "epoch: 90,  batch step: 86, loss: 96.11946105957031\n",
      "epoch: 90,  batch step: 87, loss: 8.586108207702637\n",
      "epoch: 90,  batch step: 88, loss: 85.71700286865234\n",
      "epoch: 90,  batch step: 89, loss: 100.47505950927734\n",
      "epoch: 90,  batch step: 90, loss: 27.483871459960938\n",
      "epoch: 90,  batch step: 91, loss: 29.606708526611328\n",
      "epoch: 90,  batch step: 92, loss: 7.231391906738281\n",
      "epoch: 90,  batch step: 93, loss: 10.774883270263672\n",
      "epoch: 90,  batch step: 94, loss: 55.06303405761719\n",
      "epoch: 90,  batch step: 95, loss: 31.110340118408203\n",
      "epoch: 90,  batch step: 96, loss: 13.152113914489746\n",
      "epoch: 90,  batch step: 97, loss: 6.472911357879639\n",
      "epoch: 90,  batch step: 98, loss: 8.208196640014648\n",
      "epoch: 90,  batch step: 99, loss: 40.302066802978516\n",
      "epoch: 90,  batch step: 100, loss: 21.812007904052734\n",
      "epoch: 90,  batch step: 101, loss: 148.6068572998047\n",
      "epoch: 90,  batch step: 102, loss: 109.17552185058594\n",
      "epoch: 90,  batch step: 103, loss: 19.44208526611328\n",
      "epoch: 90,  batch step: 104, loss: 137.47869873046875\n",
      "epoch: 90,  batch step: 105, loss: 49.71778869628906\n",
      "epoch: 90,  batch step: 106, loss: 20.554113388061523\n",
      "epoch: 90,  batch step: 107, loss: 14.936266899108887\n",
      "epoch: 90,  batch step: 108, loss: 12.037348747253418\n",
      "epoch: 90,  batch step: 109, loss: 193.1974334716797\n",
      "epoch: 90,  batch step: 110, loss: 26.178447723388672\n",
      "epoch: 90,  batch step: 111, loss: 39.208595275878906\n",
      "epoch: 90,  batch step: 112, loss: 13.71575927734375\n",
      "epoch: 90,  batch step: 113, loss: 6.019418239593506\n",
      "epoch: 90,  batch step: 114, loss: 8.611930847167969\n",
      "epoch: 90,  batch step: 115, loss: 76.57195281982422\n",
      "epoch: 90,  batch step: 116, loss: 154.6801300048828\n",
      "epoch: 90,  batch step: 117, loss: 15.688127517700195\n",
      "epoch: 90,  batch step: 118, loss: 69.5545654296875\n",
      "epoch: 90,  batch step: 119, loss: 12.153264999389648\n",
      "epoch: 90,  batch step: 120, loss: 6.8997697830200195\n",
      "epoch: 90,  batch step: 121, loss: 10.5294771194458\n",
      "epoch: 90,  batch step: 122, loss: 44.839874267578125\n",
      "epoch: 90,  batch step: 123, loss: 15.588140487670898\n",
      "epoch: 90,  batch step: 124, loss: 6.58103084564209\n",
      "epoch: 90,  batch step: 125, loss: 23.38262176513672\n",
      "epoch: 90,  batch step: 126, loss: 5.587114334106445\n",
      "epoch: 90,  batch step: 127, loss: 65.95245361328125\n",
      "epoch: 90,  batch step: 128, loss: 60.487144470214844\n",
      "epoch: 90,  batch step: 129, loss: 11.964991569519043\n",
      "epoch: 90,  batch step: 130, loss: 171.2935791015625\n",
      "epoch: 90,  batch step: 131, loss: 38.406898498535156\n",
      "epoch: 90,  batch step: 132, loss: 20.823829650878906\n",
      "epoch: 90,  batch step: 133, loss: 35.91487121582031\n",
      "epoch: 90,  batch step: 134, loss: 36.109092712402344\n",
      "epoch: 90,  batch step: 135, loss: 9.000909805297852\n",
      "epoch: 90,  batch step: 136, loss: 26.73996353149414\n",
      "epoch: 90,  batch step: 137, loss: 66.65086364746094\n",
      "epoch: 90,  batch step: 138, loss: 57.222225189208984\n",
      "epoch: 90,  batch step: 139, loss: 27.549434661865234\n",
      "epoch: 90,  batch step: 140, loss: 13.14094066619873\n",
      "epoch: 90,  batch step: 141, loss: 55.139862060546875\n",
      "epoch: 90,  batch step: 142, loss: 5.07249641418457\n",
      "epoch: 90,  batch step: 143, loss: 65.59043884277344\n",
      "epoch: 90,  batch step: 144, loss: 5.097833156585693\n",
      "epoch: 90,  batch step: 145, loss: 47.85448455810547\n",
      "epoch: 90,  batch step: 146, loss: 66.35692596435547\n",
      "epoch: 90,  batch step: 147, loss: 3.986180305480957\n",
      "epoch: 90,  batch step: 148, loss: 38.39300537109375\n",
      "epoch: 90,  batch step: 149, loss: 69.64199829101562\n",
      "epoch: 90,  batch step: 150, loss: 183.71884155273438\n",
      "epoch: 90,  batch step: 151, loss: 5.433425426483154\n",
      "epoch: 90,  batch step: 152, loss: 7.551653861999512\n",
      "epoch: 90,  batch step: 153, loss: 59.26704406738281\n",
      "epoch: 90,  batch step: 154, loss: 55.08672332763672\n",
      "epoch: 90,  batch step: 155, loss: 34.21894073486328\n",
      "epoch: 90,  batch step: 156, loss: 129.31631469726562\n",
      "epoch: 90,  batch step: 157, loss: 127.45699310302734\n",
      "epoch: 90,  batch step: 158, loss: 5.85658597946167\n",
      "epoch: 90,  batch step: 159, loss: 19.5791015625\n",
      "epoch: 90,  batch step: 160, loss: 8.83266544342041\n",
      "epoch: 90,  batch step: 161, loss: 7.047386169433594\n",
      "epoch: 90,  batch step: 162, loss: 8.704240798950195\n",
      "epoch: 90,  batch step: 163, loss: 39.76806640625\n",
      "epoch: 90,  batch step: 164, loss: 55.59667205810547\n",
      "epoch: 90,  batch step: 165, loss: 8.873095512390137\n",
      "epoch: 90,  batch step: 166, loss: 125.70521545410156\n",
      "epoch: 90,  batch step: 167, loss: 52.90201950073242\n",
      "epoch: 90,  batch step: 168, loss: 56.9549560546875\n",
      "epoch: 90,  batch step: 169, loss: 6.0650458335876465\n",
      "epoch: 90,  batch step: 170, loss: 35.44546890258789\n",
      "epoch: 90,  batch step: 171, loss: 6.707401275634766\n",
      "epoch: 90,  batch step: 172, loss: 54.9498405456543\n",
      "epoch: 90,  batch step: 173, loss: 60.97858428955078\n",
      "epoch: 90,  batch step: 174, loss: 29.44072723388672\n",
      "epoch: 90,  batch step: 175, loss: 8.47387981414795\n",
      "epoch: 90,  batch step: 176, loss: 5.0210185050964355\n",
      "epoch: 90,  batch step: 177, loss: 131.48475646972656\n",
      "epoch: 90,  batch step: 178, loss: 23.173179626464844\n",
      "epoch: 90,  batch step: 179, loss: 5.278362274169922\n",
      "epoch: 90,  batch step: 180, loss: 11.138265609741211\n",
      "epoch: 90,  batch step: 181, loss: 27.2882137298584\n",
      "epoch: 90,  batch step: 182, loss: 12.421073913574219\n",
      "epoch: 90,  batch step: 183, loss: 36.50945281982422\n",
      "epoch: 90,  batch step: 184, loss: 6.668285369873047\n",
      "epoch: 90,  batch step: 185, loss: 128.06277465820312\n",
      "epoch: 90,  batch step: 186, loss: 7.444272041320801\n",
      "epoch: 90,  batch step: 187, loss: 4.561652660369873\n",
      "epoch: 90,  batch step: 188, loss: 70.18024444580078\n",
      "epoch: 90,  batch step: 189, loss: 7.628334045410156\n",
      "epoch: 90,  batch step: 190, loss: 51.335479736328125\n",
      "epoch: 90,  batch step: 191, loss: 26.363361358642578\n",
      "epoch: 90,  batch step: 192, loss: 15.172422409057617\n",
      "epoch: 90,  batch step: 193, loss: 38.33030700683594\n",
      "epoch: 90,  batch step: 194, loss: 118.26445007324219\n",
      "epoch: 90,  batch step: 195, loss: 57.971099853515625\n",
      "epoch: 90,  batch step: 196, loss: 80.13616180419922\n",
      "epoch: 90,  batch step: 197, loss: 12.268671989440918\n",
      "epoch: 90,  batch step: 198, loss: 8.915952682495117\n",
      "epoch: 90,  batch step: 199, loss: 4.568721771240234\n",
      "epoch: 90,  batch step: 200, loss: 12.744110107421875\n",
      "epoch: 90,  batch step: 201, loss: 5.487471580505371\n",
      "epoch: 90,  batch step: 202, loss: 6.892783164978027\n",
      "epoch: 90,  batch step: 203, loss: 20.286035537719727\n",
      "epoch: 90,  batch step: 204, loss: 19.36212921142578\n",
      "epoch: 90,  batch step: 205, loss: 6.0233917236328125\n",
      "epoch: 90,  batch step: 206, loss: 150.575439453125\n",
      "epoch: 90,  batch step: 207, loss: 47.54060363769531\n",
      "epoch: 90,  batch step: 208, loss: 40.781883239746094\n",
      "epoch: 90,  batch step: 209, loss: 76.48095703125\n",
      "epoch: 90,  batch step: 210, loss: 143.2024383544922\n",
      "epoch: 90,  batch step: 211, loss: 33.26532745361328\n",
      "epoch: 90,  batch step: 212, loss: 21.723119735717773\n",
      "epoch: 90,  batch step: 213, loss: 6.860379695892334\n",
      "epoch: 90,  batch step: 214, loss: 6.025113582611084\n",
      "epoch: 90,  batch step: 215, loss: 7.0482025146484375\n",
      "epoch: 90,  batch step: 216, loss: 11.082965850830078\n",
      "epoch: 90,  batch step: 217, loss: 10.942584037780762\n",
      "epoch: 90,  batch step: 218, loss: 7.224836349487305\n",
      "epoch: 90,  batch step: 219, loss: 54.05500030517578\n",
      "epoch: 90,  batch step: 220, loss: 23.92786979675293\n",
      "epoch: 90,  batch step: 221, loss: 18.246997833251953\n",
      "epoch: 90,  batch step: 222, loss: 82.82379150390625\n",
      "epoch: 90,  batch step: 223, loss: 62.04203414916992\n",
      "epoch: 90,  batch step: 224, loss: 7.080525875091553\n",
      "epoch: 90,  batch step: 225, loss: 12.438165664672852\n",
      "epoch: 90,  batch step: 226, loss: 4.214123725891113\n",
      "epoch: 90,  batch step: 227, loss: 6.460466384887695\n",
      "epoch: 90,  batch step: 228, loss: 60.634300231933594\n",
      "epoch: 90,  batch step: 229, loss: 2.848001480102539\n",
      "epoch: 90,  batch step: 230, loss: 28.18562126159668\n",
      "epoch: 90,  batch step: 231, loss: 21.873525619506836\n",
      "epoch: 90,  batch step: 232, loss: 7.695003509521484\n",
      "epoch: 90,  batch step: 233, loss: 8.26426887512207\n",
      "epoch: 90,  batch step: 234, loss: 26.709564208984375\n",
      "epoch: 90,  batch step: 235, loss: 6.788520812988281\n",
      "epoch: 90,  batch step: 236, loss: 62.11190414428711\n",
      "epoch: 90,  batch step: 237, loss: 2.894683361053467\n",
      "epoch: 90,  batch step: 238, loss: 16.05863380432129\n",
      "epoch: 90,  batch step: 239, loss: 8.35050106048584\n",
      "epoch: 90,  batch step: 240, loss: 16.672508239746094\n",
      "epoch: 90,  batch step: 241, loss: 2.5016634464263916\n",
      "epoch: 90,  batch step: 242, loss: 6.617862701416016\n",
      "epoch: 90,  batch step: 243, loss: 23.03912353515625\n",
      "epoch: 90,  batch step: 244, loss: 3.549652576446533\n",
      "epoch: 90,  batch step: 245, loss: 35.030357360839844\n",
      "epoch: 90,  batch step: 246, loss: 13.621671676635742\n",
      "epoch: 90,  batch step: 247, loss: 4.0292067527771\n",
      "epoch: 90,  batch step: 248, loss: 24.734375\n",
      "epoch: 90,  batch step: 249, loss: 35.61648941040039\n",
      "epoch: 90,  batch step: 250, loss: 4.468794822692871\n",
      "epoch: 90,  batch step: 251, loss: 423.3839111328125\n",
      "validation error epoch  90:    tensor(70.5538, device='cuda:0')\n",
      "316\n",
      "epoch: 91,  batch step: 0, loss: 9.477694511413574\n",
      "epoch: 91,  batch step: 1, loss: 7.172234535217285\n",
      "epoch: 91,  batch step: 2, loss: 10.0691499710083\n",
      "epoch: 91,  batch step: 3, loss: 39.46558380126953\n",
      "epoch: 91,  batch step: 4, loss: 24.5742244720459\n",
      "epoch: 91,  batch step: 5, loss: 11.983034133911133\n",
      "epoch: 91,  batch step: 6, loss: 13.437582015991211\n",
      "epoch: 91,  batch step: 7, loss: 78.42625427246094\n",
      "epoch: 91,  batch step: 8, loss: 6.205328941345215\n",
      "epoch: 91,  batch step: 9, loss: 13.735525131225586\n",
      "epoch: 91,  batch step: 10, loss: 79.72493743896484\n",
      "epoch: 91,  batch step: 11, loss: 138.7115478515625\n",
      "epoch: 91,  batch step: 12, loss: 113.66407775878906\n",
      "epoch: 91,  batch step: 13, loss: 13.982938766479492\n",
      "epoch: 91,  batch step: 14, loss: 31.240934371948242\n",
      "epoch: 91,  batch step: 15, loss: 6.842851161956787\n",
      "epoch: 91,  batch step: 16, loss: 17.012775421142578\n",
      "epoch: 91,  batch step: 17, loss: 6.065467834472656\n",
      "epoch: 91,  batch step: 18, loss: 4.331337928771973\n",
      "epoch: 91,  batch step: 19, loss: 3.721803665161133\n",
      "epoch: 91,  batch step: 20, loss: 4.488722801208496\n",
      "epoch: 91,  batch step: 21, loss: 5.202491760253906\n",
      "epoch: 91,  batch step: 22, loss: 48.32316207885742\n",
      "epoch: 91,  batch step: 23, loss: 3.3274240493774414\n",
      "epoch: 91,  batch step: 24, loss: 6.134076118469238\n",
      "epoch: 91,  batch step: 25, loss: 20.829687118530273\n",
      "epoch: 91,  batch step: 26, loss: 59.31077575683594\n",
      "epoch: 91,  batch step: 27, loss: 5.280941009521484\n",
      "epoch: 91,  batch step: 28, loss: 4.263844013214111\n",
      "epoch: 91,  batch step: 29, loss: 8.942755699157715\n",
      "epoch: 91,  batch step: 30, loss: 58.10790252685547\n",
      "epoch: 91,  batch step: 31, loss: 4.617647171020508\n",
      "epoch: 91,  batch step: 32, loss: 20.691314697265625\n",
      "epoch: 91,  batch step: 33, loss: 184.27178955078125\n",
      "epoch: 91,  batch step: 34, loss: 6.037814140319824\n",
      "epoch: 91,  batch step: 35, loss: 5.27640438079834\n",
      "epoch: 91,  batch step: 36, loss: 5.618167877197266\n",
      "epoch: 91,  batch step: 37, loss: 5.242118835449219\n",
      "epoch: 91,  batch step: 38, loss: 8.877469062805176\n",
      "epoch: 91,  batch step: 39, loss: 61.18722152709961\n",
      "epoch: 91,  batch step: 40, loss: 71.71316528320312\n",
      "epoch: 91,  batch step: 41, loss: 30.786182403564453\n",
      "epoch: 91,  batch step: 42, loss: 41.820289611816406\n",
      "epoch: 91,  batch step: 43, loss: 21.514097213745117\n",
      "epoch: 91,  batch step: 44, loss: 16.832721710205078\n",
      "epoch: 91,  batch step: 45, loss: 5.165027618408203\n",
      "epoch: 91,  batch step: 46, loss: 3.395320415496826\n",
      "epoch: 91,  batch step: 47, loss: 12.492770195007324\n",
      "epoch: 91,  batch step: 48, loss: 60.46662139892578\n",
      "epoch: 91,  batch step: 49, loss: 15.1337890625\n",
      "epoch: 91,  batch step: 50, loss: 33.29848098754883\n",
      "epoch: 91,  batch step: 51, loss: 27.513465881347656\n",
      "epoch: 91,  batch step: 52, loss: 26.014942169189453\n",
      "epoch: 91,  batch step: 53, loss: 36.01653289794922\n",
      "epoch: 91,  batch step: 54, loss: 9.623499870300293\n",
      "epoch: 91,  batch step: 55, loss: 112.97377014160156\n",
      "epoch: 91,  batch step: 56, loss: 23.697620391845703\n",
      "epoch: 91,  batch step: 57, loss: 30.06416130065918\n",
      "epoch: 91,  batch step: 58, loss: 6.931806564331055\n",
      "epoch: 91,  batch step: 59, loss: 4.449004650115967\n",
      "epoch: 91,  batch step: 60, loss: 31.153112411499023\n",
      "epoch: 91,  batch step: 61, loss: 46.91710662841797\n",
      "epoch: 91,  batch step: 62, loss: 25.7133846282959\n",
      "epoch: 91,  batch step: 63, loss: 8.134531021118164\n",
      "epoch: 91,  batch step: 64, loss: 47.61246871948242\n",
      "epoch: 91,  batch step: 65, loss: 5.7721266746521\n",
      "epoch: 91,  batch step: 66, loss: 62.5432243347168\n",
      "epoch: 91,  batch step: 67, loss: 8.954294204711914\n",
      "epoch: 91,  batch step: 68, loss: 8.003042221069336\n",
      "epoch: 91,  batch step: 69, loss: 72.04291534423828\n",
      "epoch: 91,  batch step: 70, loss: 23.826589584350586\n",
      "epoch: 91,  batch step: 71, loss: 6.113312721252441\n",
      "epoch: 91,  batch step: 72, loss: 18.573610305786133\n",
      "epoch: 91,  batch step: 73, loss: 3.745760917663574\n",
      "epoch: 91,  batch step: 74, loss: 22.670080184936523\n",
      "epoch: 91,  batch step: 75, loss: 90.36898040771484\n",
      "epoch: 91,  batch step: 76, loss: 3.8754191398620605\n",
      "epoch: 91,  batch step: 77, loss: 9.054731369018555\n",
      "epoch: 91,  batch step: 78, loss: 54.26090621948242\n",
      "epoch: 91,  batch step: 79, loss: 7.137416362762451\n",
      "epoch: 91,  batch step: 80, loss: 20.43400764465332\n",
      "epoch: 91,  batch step: 81, loss: 63.96282958984375\n",
      "epoch: 91,  batch step: 82, loss: 67.03829956054688\n",
      "epoch: 91,  batch step: 83, loss: 4.694978713989258\n",
      "epoch: 91,  batch step: 84, loss: 66.5149154663086\n",
      "epoch: 91,  batch step: 85, loss: 51.234291076660156\n",
      "epoch: 91,  batch step: 86, loss: 58.8489990234375\n",
      "epoch: 91,  batch step: 87, loss: 10.153571128845215\n",
      "epoch: 91,  batch step: 88, loss: 22.894378662109375\n",
      "epoch: 91,  batch step: 89, loss: 25.311813354492188\n",
      "epoch: 91,  batch step: 90, loss: 7.985897064208984\n",
      "epoch: 91,  batch step: 91, loss: 3.854025363922119\n",
      "epoch: 91,  batch step: 92, loss: 7.4720869064331055\n",
      "epoch: 91,  batch step: 93, loss: 22.37946319580078\n",
      "epoch: 91,  batch step: 94, loss: 14.709756851196289\n",
      "epoch: 91,  batch step: 95, loss: 53.48667526245117\n",
      "epoch: 91,  batch step: 96, loss: 5.122949123382568\n",
      "epoch: 91,  batch step: 97, loss: 15.399711608886719\n",
      "epoch: 91,  batch step: 98, loss: 14.696599960327148\n",
      "epoch: 91,  batch step: 99, loss: 31.795438766479492\n",
      "epoch: 91,  batch step: 100, loss: 103.36122131347656\n",
      "epoch: 91,  batch step: 101, loss: 3.5652031898498535\n",
      "epoch: 91,  batch step: 102, loss: 2.9195923805236816\n",
      "epoch: 91,  batch step: 103, loss: 11.37362289428711\n",
      "epoch: 91,  batch step: 104, loss: 37.055809020996094\n",
      "epoch: 91,  batch step: 105, loss: 15.099325180053711\n",
      "epoch: 91,  batch step: 106, loss: 4.180543899536133\n",
      "epoch: 91,  batch step: 107, loss: 31.273723602294922\n",
      "epoch: 91,  batch step: 108, loss: 125.483642578125\n",
      "epoch: 91,  batch step: 109, loss: 25.401212692260742\n",
      "epoch: 91,  batch step: 110, loss: 16.6567440032959\n",
      "epoch: 91,  batch step: 111, loss: 37.96977615356445\n",
      "epoch: 91,  batch step: 112, loss: 3.9074790477752686\n",
      "epoch: 91,  batch step: 113, loss: 6.700255393981934\n",
      "epoch: 91,  batch step: 114, loss: 7.472484111785889\n",
      "epoch: 91,  batch step: 115, loss: 20.7968807220459\n",
      "epoch: 91,  batch step: 116, loss: 3.296738386154175\n",
      "epoch: 91,  batch step: 117, loss: 37.21796417236328\n",
      "epoch: 91,  batch step: 118, loss: 5.865273952484131\n",
      "epoch: 91,  batch step: 119, loss: 21.379613876342773\n",
      "epoch: 91,  batch step: 120, loss: 5.106461524963379\n",
      "epoch: 91,  batch step: 121, loss: 10.450357437133789\n",
      "epoch: 91,  batch step: 122, loss: 4.046597957611084\n",
      "epoch: 91,  batch step: 123, loss: 22.714614868164062\n",
      "epoch: 91,  batch step: 124, loss: 6.641933441162109\n",
      "epoch: 91,  batch step: 125, loss: 17.148332595825195\n",
      "epoch: 91,  batch step: 126, loss: 3.1528165340423584\n",
      "epoch: 91,  batch step: 127, loss: 6.471652507781982\n",
      "epoch: 91,  batch step: 128, loss: 7.982600688934326\n",
      "epoch: 91,  batch step: 129, loss: 4.561763763427734\n",
      "epoch: 91,  batch step: 130, loss: 13.79516887664795\n",
      "epoch: 91,  batch step: 131, loss: 5.790127754211426\n",
      "epoch: 91,  batch step: 132, loss: 3.406186580657959\n",
      "epoch: 91,  batch step: 133, loss: 18.407569885253906\n",
      "epoch: 91,  batch step: 134, loss: 5.922457695007324\n",
      "epoch: 91,  batch step: 135, loss: 2.875117778778076\n",
      "epoch: 91,  batch step: 136, loss: 48.12262725830078\n",
      "epoch: 91,  batch step: 137, loss: 3.3120925426483154\n",
      "epoch: 91,  batch step: 138, loss: 5.353282928466797\n",
      "epoch: 91,  batch step: 139, loss: 33.73664093017578\n",
      "epoch: 91,  batch step: 140, loss: 9.99203109741211\n",
      "epoch: 91,  batch step: 141, loss: 25.002859115600586\n",
      "epoch: 91,  batch step: 142, loss: 3.16444730758667\n",
      "epoch: 91,  batch step: 143, loss: 25.539669036865234\n",
      "epoch: 91,  batch step: 144, loss: 42.10008239746094\n",
      "epoch: 91,  batch step: 145, loss: 17.9204158782959\n",
      "epoch: 91,  batch step: 146, loss: 65.5237045288086\n",
      "epoch: 91,  batch step: 147, loss: 3.4924705028533936\n",
      "epoch: 91,  batch step: 148, loss: 11.547137260437012\n",
      "epoch: 91,  batch step: 149, loss: 35.03982162475586\n",
      "epoch: 91,  batch step: 150, loss: 21.464290618896484\n",
      "epoch: 91,  batch step: 151, loss: 3.04822039604187\n",
      "epoch: 91,  batch step: 152, loss: 15.9413480758667\n",
      "epoch: 91,  batch step: 153, loss: 23.538986206054688\n",
      "epoch: 91,  batch step: 154, loss: 142.9783935546875\n",
      "epoch: 91,  batch step: 155, loss: 3.9273784160614014\n",
      "epoch: 91,  batch step: 156, loss: 44.61185836791992\n",
      "epoch: 91,  batch step: 157, loss: 5.532191276550293\n",
      "epoch: 91,  batch step: 158, loss: 29.379013061523438\n",
      "epoch: 91,  batch step: 159, loss: 55.541969299316406\n",
      "epoch: 91,  batch step: 160, loss: 3.087925910949707\n",
      "epoch: 91,  batch step: 161, loss: 3.6688270568847656\n",
      "epoch: 91,  batch step: 162, loss: 4.352381706237793\n",
      "epoch: 91,  batch step: 163, loss: 17.973960876464844\n",
      "epoch: 91,  batch step: 164, loss: 16.998096466064453\n",
      "epoch: 91,  batch step: 165, loss: 39.356971740722656\n",
      "epoch: 91,  batch step: 166, loss: 19.180503845214844\n",
      "epoch: 91,  batch step: 167, loss: 23.972915649414062\n",
      "epoch: 91,  batch step: 168, loss: 2.3477230072021484\n",
      "epoch: 91,  batch step: 169, loss: 4.393991470336914\n",
      "epoch: 91,  batch step: 170, loss: 56.153907775878906\n",
      "epoch: 91,  batch step: 171, loss: 5.528459548950195\n",
      "epoch: 91,  batch step: 172, loss: 3.23396372795105\n",
      "epoch: 91,  batch step: 173, loss: 55.202362060546875\n",
      "epoch: 91,  batch step: 174, loss: 12.903745651245117\n",
      "epoch: 91,  batch step: 175, loss: 20.38052749633789\n",
      "epoch: 91,  batch step: 176, loss: 69.04338836669922\n",
      "epoch: 91,  batch step: 177, loss: 5.317426681518555\n",
      "epoch: 91,  batch step: 178, loss: 11.53242301940918\n",
      "epoch: 91,  batch step: 179, loss: 24.649185180664062\n",
      "epoch: 91,  batch step: 180, loss: 2.9280829429626465\n",
      "epoch: 91,  batch step: 181, loss: 14.470621109008789\n",
      "epoch: 91,  batch step: 182, loss: 4.267870903015137\n",
      "epoch: 91,  batch step: 183, loss: 48.70500183105469\n",
      "epoch: 91,  batch step: 184, loss: 2.4044644832611084\n",
      "epoch: 91,  batch step: 185, loss: 6.078857421875\n",
      "epoch: 91,  batch step: 186, loss: 42.38658905029297\n",
      "epoch: 91,  batch step: 187, loss: 65.68701171875\n",
      "epoch: 91,  batch step: 188, loss: 33.41919708251953\n",
      "epoch: 91,  batch step: 189, loss: 5.872756004333496\n",
      "epoch: 91,  batch step: 190, loss: 8.757159233093262\n",
      "epoch: 91,  batch step: 191, loss: 13.98073959350586\n",
      "epoch: 91,  batch step: 192, loss: 21.47113037109375\n",
      "epoch: 91,  batch step: 193, loss: 15.300235748291016\n",
      "epoch: 91,  batch step: 194, loss: 14.055257797241211\n",
      "epoch: 91,  batch step: 195, loss: 30.481155395507812\n",
      "epoch: 91,  batch step: 196, loss: 11.718606948852539\n",
      "epoch: 91,  batch step: 197, loss: 2.271310329437256\n",
      "epoch: 91,  batch step: 198, loss: 4.015604019165039\n",
      "epoch: 91,  batch step: 199, loss: 8.808652877807617\n",
      "epoch: 91,  batch step: 200, loss: 2.5598583221435547\n",
      "epoch: 91,  batch step: 201, loss: 3.510596990585327\n",
      "epoch: 91,  batch step: 202, loss: 3.130200147628784\n",
      "epoch: 91,  batch step: 203, loss: 14.073466300964355\n",
      "epoch: 91,  batch step: 204, loss: 2.633101463317871\n",
      "epoch: 91,  batch step: 205, loss: 16.11691665649414\n",
      "epoch: 91,  batch step: 206, loss: 4.185540199279785\n",
      "epoch: 91,  batch step: 207, loss: 5.027148246765137\n",
      "epoch: 91,  batch step: 208, loss: 27.620037078857422\n",
      "epoch: 91,  batch step: 209, loss: 11.606844902038574\n",
      "epoch: 91,  batch step: 210, loss: 5.621169090270996\n",
      "epoch: 91,  batch step: 211, loss: 40.8033447265625\n",
      "epoch: 91,  batch step: 212, loss: 16.231422424316406\n",
      "epoch: 91,  batch step: 213, loss: 4.506290435791016\n",
      "epoch: 91,  batch step: 214, loss: 1.8264133930206299\n",
      "epoch: 91,  batch step: 215, loss: 6.186372756958008\n",
      "epoch: 91,  batch step: 216, loss: 2.402858257293701\n",
      "epoch: 91,  batch step: 217, loss: 28.798147201538086\n",
      "epoch: 91,  batch step: 218, loss: 50.19866943359375\n",
      "epoch: 91,  batch step: 219, loss: 28.452085494995117\n",
      "epoch: 91,  batch step: 220, loss: 5.18541145324707\n",
      "epoch: 91,  batch step: 221, loss: 21.38009262084961\n",
      "epoch: 91,  batch step: 222, loss: 9.124300003051758\n",
      "epoch: 91,  batch step: 223, loss: 52.91801071166992\n",
      "epoch: 91,  batch step: 224, loss: 5.593952178955078\n",
      "epoch: 91,  batch step: 225, loss: 8.05202865600586\n",
      "epoch: 91,  batch step: 226, loss: 4.5273942947387695\n",
      "epoch: 91,  batch step: 227, loss: 2.023111581802368\n",
      "epoch: 91,  batch step: 228, loss: 31.058059692382812\n",
      "epoch: 91,  batch step: 229, loss: 3.3564600944519043\n",
      "epoch: 91,  batch step: 230, loss: 43.63674545288086\n",
      "epoch: 91,  batch step: 231, loss: 2.883782386779785\n",
      "epoch: 91,  batch step: 232, loss: 32.09519958496094\n",
      "epoch: 91,  batch step: 233, loss: 58.0810546875\n",
      "epoch: 91,  batch step: 234, loss: 2.008833646774292\n",
      "epoch: 91,  batch step: 235, loss: 2.7800662517547607\n",
      "epoch: 91,  batch step: 236, loss: 21.580322265625\n",
      "epoch: 91,  batch step: 237, loss: 35.185997009277344\n",
      "epoch: 91,  batch step: 238, loss: 5.8406219482421875\n",
      "epoch: 91,  batch step: 239, loss: 17.542482376098633\n",
      "epoch: 91,  batch step: 240, loss: 6.643743515014648\n",
      "epoch: 91,  batch step: 241, loss: 2.9055638313293457\n",
      "epoch: 91,  batch step: 242, loss: 4.197267532348633\n",
      "epoch: 91,  batch step: 243, loss: 5.778325080871582\n",
      "epoch: 91,  batch step: 244, loss: 14.496871948242188\n",
      "epoch: 91,  batch step: 245, loss: 5.225251197814941\n",
      "epoch: 91,  batch step: 246, loss: 8.5267333984375\n",
      "epoch: 91,  batch step: 247, loss: 29.674190521240234\n",
      "epoch: 91,  batch step: 248, loss: 2.9249188899993896\n",
      "epoch: 91,  batch step: 249, loss: 2.1783177852630615\n",
      "epoch: 91,  batch step: 250, loss: 14.23865795135498\n",
      "epoch: 91,  batch step: 251, loss: 94.25180053710938\n",
      "validation error epoch  91:    tensor(69.0163, device='cuda:0')\n",
      "316\n",
      "epoch: 92,  batch step: 0, loss: 2.467808246612549\n",
      "epoch: 92,  batch step: 1, loss: 4.1639404296875\n",
      "epoch: 92,  batch step: 2, loss: 2.9262959957122803\n",
      "epoch: 92,  batch step: 3, loss: 13.701072692871094\n",
      "epoch: 92,  batch step: 4, loss: 16.373281478881836\n",
      "epoch: 92,  batch step: 5, loss: 7.007031440734863\n",
      "epoch: 92,  batch step: 6, loss: 14.080554962158203\n",
      "epoch: 92,  batch step: 7, loss: 14.0911865234375\n",
      "epoch: 92,  batch step: 8, loss: 17.881450653076172\n",
      "epoch: 92,  batch step: 9, loss: 19.877836227416992\n",
      "epoch: 92,  batch step: 10, loss: 6.552722454071045\n",
      "epoch: 92,  batch step: 11, loss: 3.8262650966644287\n",
      "epoch: 92,  batch step: 12, loss: 3.251108169555664\n",
      "epoch: 92,  batch step: 13, loss: 4.452268600463867\n",
      "epoch: 92,  batch step: 14, loss: 4.513320446014404\n",
      "epoch: 92,  batch step: 15, loss: 17.088809967041016\n",
      "epoch: 92,  batch step: 16, loss: 51.93598937988281\n",
      "epoch: 92,  batch step: 17, loss: 5.660121440887451\n",
      "epoch: 92,  batch step: 18, loss: 26.99947738647461\n",
      "epoch: 92,  batch step: 19, loss: 7.809014797210693\n",
      "epoch: 92,  batch step: 20, loss: 5.421437740325928\n",
      "epoch: 92,  batch step: 21, loss: 12.569473266601562\n",
      "epoch: 92,  batch step: 22, loss: 6.387851715087891\n",
      "epoch: 92,  batch step: 23, loss: 34.871307373046875\n",
      "epoch: 92,  batch step: 24, loss: 2.953103542327881\n",
      "epoch: 92,  batch step: 25, loss: 48.35894012451172\n",
      "epoch: 92,  batch step: 26, loss: 2.7044942378997803\n",
      "epoch: 92,  batch step: 27, loss: 8.244345664978027\n",
      "epoch: 92,  batch step: 28, loss: 29.57187271118164\n",
      "epoch: 92,  batch step: 29, loss: 18.85726547241211\n",
      "epoch: 92,  batch step: 30, loss: 3.261230230331421\n",
      "epoch: 92,  batch step: 31, loss: 3.551617383956909\n",
      "epoch: 92,  batch step: 32, loss: 5.452624320983887\n",
      "epoch: 92,  batch step: 33, loss: 4.3872785568237305\n",
      "epoch: 92,  batch step: 34, loss: 11.373777389526367\n",
      "epoch: 92,  batch step: 35, loss: 14.174636840820312\n",
      "epoch: 92,  batch step: 36, loss: 2.593289852142334\n",
      "epoch: 92,  batch step: 37, loss: 4.327586650848389\n",
      "epoch: 92,  batch step: 38, loss: 2.2120656967163086\n",
      "epoch: 92,  batch step: 39, loss: 66.75398254394531\n",
      "epoch: 92,  batch step: 40, loss: 15.126269340515137\n",
      "epoch: 92,  batch step: 41, loss: 25.006690979003906\n",
      "epoch: 92,  batch step: 42, loss: 26.064191818237305\n",
      "epoch: 92,  batch step: 43, loss: 23.454727172851562\n",
      "epoch: 92,  batch step: 44, loss: 7.100724697113037\n",
      "epoch: 92,  batch step: 45, loss: 36.58279037475586\n",
      "epoch: 92,  batch step: 46, loss: 14.144414901733398\n",
      "epoch: 92,  batch step: 47, loss: 55.202178955078125\n",
      "epoch: 92,  batch step: 48, loss: 3.914660930633545\n",
      "epoch: 92,  batch step: 49, loss: 31.45648765563965\n",
      "epoch: 92,  batch step: 50, loss: 3.060786008834839\n",
      "epoch: 92,  batch step: 51, loss: 3.9923617839813232\n",
      "epoch: 92,  batch step: 52, loss: 4.0760955810546875\n",
      "epoch: 92,  batch step: 53, loss: 31.752416610717773\n",
      "epoch: 92,  batch step: 54, loss: 3.354609489440918\n",
      "epoch: 92,  batch step: 55, loss: 31.080215454101562\n",
      "epoch: 92,  batch step: 56, loss: 16.470415115356445\n",
      "epoch: 92,  batch step: 57, loss: 4.01929235458374\n",
      "epoch: 92,  batch step: 58, loss: 4.685011863708496\n",
      "epoch: 92,  batch step: 59, loss: 134.9886474609375\n",
      "epoch: 92,  batch step: 60, loss: 5.365496635437012\n",
      "epoch: 92,  batch step: 61, loss: 17.900001525878906\n",
      "epoch: 92,  batch step: 62, loss: 11.830218315124512\n",
      "epoch: 92,  batch step: 63, loss: 7.01978063583374\n",
      "epoch: 92,  batch step: 64, loss: 4.06307315826416\n",
      "epoch: 92,  batch step: 65, loss: 17.335477828979492\n",
      "epoch: 92,  batch step: 66, loss: 27.81542205810547\n",
      "epoch: 92,  batch step: 67, loss: 6.045722484588623\n",
      "epoch: 92,  batch step: 68, loss: 15.461380004882812\n",
      "epoch: 92,  batch step: 69, loss: 4.751132965087891\n",
      "epoch: 92,  batch step: 70, loss: 11.303773880004883\n",
      "epoch: 92,  batch step: 71, loss: 11.853010177612305\n",
      "epoch: 92,  batch step: 72, loss: 6.803999900817871\n",
      "epoch: 92,  batch step: 73, loss: 4.0635294914245605\n",
      "epoch: 92,  batch step: 74, loss: 3.916837215423584\n",
      "epoch: 92,  batch step: 75, loss: 4.931955337524414\n",
      "epoch: 92,  batch step: 76, loss: 3.582017660140991\n",
      "epoch: 92,  batch step: 77, loss: 17.392372131347656\n",
      "epoch: 92,  batch step: 78, loss: 2.273940086364746\n",
      "epoch: 92,  batch step: 79, loss: 7.623404502868652\n",
      "epoch: 92,  batch step: 80, loss: 12.97286605834961\n",
      "epoch: 92,  batch step: 81, loss: 10.608856201171875\n",
      "epoch: 92,  batch step: 82, loss: 28.736190795898438\n",
      "epoch: 92,  batch step: 83, loss: 2.826118230819702\n",
      "epoch: 92,  batch step: 84, loss: 17.561614990234375\n",
      "epoch: 92,  batch step: 85, loss: 3.374256134033203\n",
      "epoch: 92,  batch step: 86, loss: 2.7243621349334717\n",
      "epoch: 92,  batch step: 87, loss: 3.2986769676208496\n",
      "epoch: 92,  batch step: 88, loss: 2.881685733795166\n",
      "epoch: 92,  batch step: 89, loss: 33.63743591308594\n",
      "epoch: 92,  batch step: 90, loss: 14.709269523620605\n",
      "epoch: 92,  batch step: 91, loss: 20.030431747436523\n",
      "epoch: 92,  batch step: 92, loss: 21.47018814086914\n",
      "epoch: 92,  batch step: 93, loss: 20.12200164794922\n",
      "epoch: 92,  batch step: 94, loss: 6.124046325683594\n",
      "epoch: 92,  batch step: 95, loss: 4.018801689147949\n",
      "epoch: 92,  batch step: 96, loss: 4.150692939758301\n",
      "epoch: 92,  batch step: 97, loss: 32.53507614135742\n",
      "epoch: 92,  batch step: 98, loss: 2.430844306945801\n",
      "epoch: 92,  batch step: 99, loss: 74.3798599243164\n",
      "epoch: 92,  batch step: 100, loss: 30.18573760986328\n",
      "epoch: 92,  batch step: 101, loss: 7.185939788818359\n",
      "epoch: 92,  batch step: 102, loss: 17.04264259338379\n",
      "epoch: 92,  batch step: 103, loss: 3.2249698638916016\n",
      "epoch: 92,  batch step: 104, loss: 25.954200744628906\n",
      "epoch: 92,  batch step: 105, loss: 4.153369903564453\n",
      "epoch: 92,  batch step: 106, loss: 60.81733322143555\n",
      "epoch: 92,  batch step: 107, loss: 71.8530502319336\n",
      "epoch: 92,  batch step: 108, loss: 2.973351240158081\n",
      "epoch: 92,  batch step: 109, loss: 12.8887357711792\n",
      "epoch: 92,  batch step: 110, loss: 35.30085372924805\n",
      "epoch: 92,  batch step: 111, loss: 25.462337493896484\n",
      "epoch: 92,  batch step: 112, loss: 29.56465721130371\n",
      "epoch: 92,  batch step: 113, loss: 2.911315441131592\n",
      "epoch: 92,  batch step: 114, loss: 15.079001426696777\n",
      "epoch: 92,  batch step: 115, loss: 18.967187881469727\n",
      "epoch: 92,  batch step: 116, loss: 57.34929656982422\n",
      "epoch: 92,  batch step: 117, loss: 3.6902377605438232\n",
      "epoch: 92,  batch step: 118, loss: 3.10211443901062\n",
      "epoch: 92,  batch step: 119, loss: 68.3314208984375\n",
      "epoch: 92,  batch step: 120, loss: 39.40135192871094\n",
      "epoch: 92,  batch step: 121, loss: 13.517504692077637\n",
      "epoch: 92,  batch step: 122, loss: 1.7888352870941162\n",
      "epoch: 92,  batch step: 123, loss: 21.101028442382812\n",
      "epoch: 92,  batch step: 124, loss: 23.415790557861328\n",
      "epoch: 92,  batch step: 125, loss: 28.620906829833984\n",
      "epoch: 92,  batch step: 126, loss: 6.172883987426758\n",
      "epoch: 92,  batch step: 127, loss: 3.1566162109375\n",
      "epoch: 92,  batch step: 128, loss: 15.553491592407227\n",
      "epoch: 92,  batch step: 129, loss: 14.046674728393555\n",
      "epoch: 92,  batch step: 130, loss: 18.114383697509766\n",
      "epoch: 92,  batch step: 131, loss: 2.762732982635498\n",
      "epoch: 92,  batch step: 132, loss: 3.001234531402588\n",
      "epoch: 92,  batch step: 133, loss: 13.710067749023438\n",
      "epoch: 92,  batch step: 134, loss: 2.4677181243896484\n",
      "epoch: 92,  batch step: 135, loss: 4.30940580368042\n",
      "epoch: 92,  batch step: 136, loss: 37.97537612915039\n",
      "epoch: 92,  batch step: 137, loss: 16.71050262451172\n",
      "epoch: 92,  batch step: 138, loss: 19.468204498291016\n",
      "epoch: 92,  batch step: 139, loss: 31.506607055664062\n",
      "epoch: 92,  batch step: 140, loss: 6.502398490905762\n",
      "epoch: 92,  batch step: 141, loss: 9.48649787902832\n",
      "epoch: 92,  batch step: 142, loss: 3.7311301231384277\n",
      "epoch: 92,  batch step: 143, loss: 3.5278091430664062\n",
      "epoch: 92,  batch step: 144, loss: 4.84468412399292\n",
      "epoch: 92,  batch step: 145, loss: 4.8559370040893555\n",
      "epoch: 92,  batch step: 146, loss: 25.836864471435547\n",
      "epoch: 92,  batch step: 147, loss: 18.472383499145508\n",
      "epoch: 92,  batch step: 148, loss: 10.983747482299805\n",
      "epoch: 92,  batch step: 149, loss: 10.538166046142578\n",
      "epoch: 92,  batch step: 150, loss: 3.67227840423584\n",
      "epoch: 92,  batch step: 151, loss: 6.2599310874938965\n",
      "epoch: 92,  batch step: 152, loss: 25.87460708618164\n",
      "epoch: 92,  batch step: 153, loss: 3.1554484367370605\n",
      "epoch: 92,  batch step: 154, loss: 14.393421173095703\n",
      "epoch: 92,  batch step: 155, loss: 6.219379425048828\n",
      "epoch: 92,  batch step: 156, loss: 5.443917274475098\n",
      "epoch: 92,  batch step: 157, loss: 48.840301513671875\n",
      "epoch: 92,  batch step: 158, loss: 18.62921142578125\n",
      "epoch: 92,  batch step: 159, loss: 10.49429988861084\n",
      "epoch: 92,  batch step: 160, loss: 6.898746490478516\n",
      "epoch: 92,  batch step: 161, loss: 2.2241015434265137\n",
      "epoch: 92,  batch step: 162, loss: 5.344195365905762\n",
      "epoch: 92,  batch step: 163, loss: 17.932056427001953\n",
      "epoch: 92,  batch step: 164, loss: 4.433385848999023\n",
      "epoch: 92,  batch step: 165, loss: 16.94841194152832\n",
      "epoch: 92,  batch step: 166, loss: 4.154447078704834\n",
      "epoch: 92,  batch step: 167, loss: 13.399921417236328\n",
      "epoch: 92,  batch step: 168, loss: 22.984477996826172\n",
      "epoch: 92,  batch step: 169, loss: 2.2978034019470215\n",
      "epoch: 92,  batch step: 170, loss: 2.60945463180542\n",
      "epoch: 92,  batch step: 171, loss: 4.117648601531982\n",
      "epoch: 92,  batch step: 172, loss: 12.627614974975586\n",
      "epoch: 92,  batch step: 173, loss: 2.327599287033081\n",
      "epoch: 92,  batch step: 174, loss: 4.2822675704956055\n",
      "epoch: 92,  batch step: 175, loss: 1.97623872756958\n",
      "epoch: 92,  batch step: 176, loss: 2.571575880050659\n",
      "epoch: 92,  batch step: 177, loss: 2.6939187049865723\n",
      "epoch: 92,  batch step: 178, loss: 2.628020763397217\n",
      "epoch: 92,  batch step: 179, loss: 3.892974615097046\n",
      "epoch: 92,  batch step: 180, loss: 11.858430862426758\n",
      "epoch: 92,  batch step: 181, loss: 48.38728332519531\n",
      "epoch: 92,  batch step: 182, loss: 29.501068115234375\n",
      "epoch: 92,  batch step: 183, loss: 2.066587209701538\n",
      "epoch: 92,  batch step: 184, loss: 23.748672485351562\n",
      "epoch: 92,  batch step: 185, loss: 11.146746635437012\n",
      "epoch: 92,  batch step: 186, loss: 2.9270405769348145\n",
      "epoch: 92,  batch step: 187, loss: 20.670482635498047\n",
      "epoch: 92,  batch step: 188, loss: 11.358781814575195\n",
      "epoch: 92,  batch step: 189, loss: 5.873187065124512\n",
      "epoch: 92,  batch step: 190, loss: 44.49217224121094\n",
      "epoch: 92,  batch step: 191, loss: 43.398826599121094\n",
      "epoch: 92,  batch step: 192, loss: 10.901926040649414\n",
      "epoch: 92,  batch step: 193, loss: 12.073978424072266\n",
      "epoch: 92,  batch step: 194, loss: 28.7021541595459\n",
      "epoch: 92,  batch step: 195, loss: 3.928680181503296\n",
      "epoch: 92,  batch step: 196, loss: 11.452058792114258\n",
      "epoch: 92,  batch step: 197, loss: 3.3215126991271973\n",
      "epoch: 92,  batch step: 198, loss: 2.8091654777526855\n",
      "epoch: 92,  batch step: 199, loss: 9.48470401763916\n",
      "epoch: 92,  batch step: 200, loss: 9.8313570022583\n",
      "epoch: 92,  batch step: 201, loss: 3.295456647872925\n",
      "epoch: 92,  batch step: 202, loss: 2.4844632148742676\n",
      "epoch: 92,  batch step: 203, loss: 3.0094375610351562\n",
      "epoch: 92,  batch step: 204, loss: 2.4499175548553467\n",
      "epoch: 92,  batch step: 205, loss: 13.001523971557617\n",
      "epoch: 92,  batch step: 206, loss: 75.27497863769531\n",
      "epoch: 92,  batch step: 207, loss: 8.994264602661133\n",
      "epoch: 92,  batch step: 208, loss: 3.56644606590271\n",
      "epoch: 92,  batch step: 209, loss: 11.800079345703125\n",
      "epoch: 92,  batch step: 210, loss: 3.292090892791748\n",
      "epoch: 92,  batch step: 211, loss: 47.75708770751953\n",
      "epoch: 92,  batch step: 212, loss: 10.677812576293945\n",
      "epoch: 92,  batch step: 213, loss: 3.442417860031128\n",
      "epoch: 92,  batch step: 214, loss: 32.823631286621094\n",
      "epoch: 92,  batch step: 215, loss: 16.954349517822266\n",
      "epoch: 92,  batch step: 216, loss: 3.1328980922698975\n",
      "epoch: 92,  batch step: 217, loss: 4.404553413391113\n",
      "epoch: 92,  batch step: 218, loss: 2.932626485824585\n",
      "epoch: 92,  batch step: 219, loss: 4.579198837280273\n",
      "epoch: 92,  batch step: 220, loss: 16.160329818725586\n",
      "epoch: 92,  batch step: 221, loss: 17.31285858154297\n",
      "epoch: 92,  batch step: 222, loss: 13.327804565429688\n",
      "epoch: 92,  batch step: 223, loss: 7.921796798706055\n",
      "epoch: 92,  batch step: 224, loss: 31.416301727294922\n",
      "epoch: 92,  batch step: 225, loss: 35.65068435668945\n",
      "epoch: 92,  batch step: 226, loss: 11.091654777526855\n",
      "epoch: 92,  batch step: 227, loss: 1.8294687271118164\n",
      "epoch: 92,  batch step: 228, loss: 52.081748962402344\n",
      "epoch: 92,  batch step: 229, loss: 9.489971160888672\n",
      "epoch: 92,  batch step: 230, loss: 5.656663417816162\n",
      "epoch: 92,  batch step: 231, loss: 32.613487243652344\n",
      "epoch: 92,  batch step: 232, loss: 12.396961212158203\n",
      "epoch: 92,  batch step: 233, loss: 3.9308815002441406\n",
      "epoch: 92,  batch step: 234, loss: 3.484269618988037\n",
      "epoch: 92,  batch step: 235, loss: 11.205233573913574\n",
      "epoch: 92,  batch step: 236, loss: 56.36933898925781\n",
      "epoch: 92,  batch step: 237, loss: 9.801778793334961\n",
      "epoch: 92,  batch step: 238, loss: 8.579642295837402\n",
      "epoch: 92,  batch step: 239, loss: 2.7934212684631348\n",
      "epoch: 92,  batch step: 240, loss: 3.6525673866271973\n",
      "epoch: 92,  batch step: 241, loss: 5.297987461090088\n",
      "epoch: 92,  batch step: 242, loss: 15.101016998291016\n",
      "epoch: 92,  batch step: 243, loss: 64.58523559570312\n",
      "epoch: 92,  batch step: 244, loss: 11.790380477905273\n",
      "epoch: 92,  batch step: 245, loss: 2.46494460105896\n",
      "epoch: 92,  batch step: 246, loss: 18.8856143951416\n",
      "epoch: 92,  batch step: 247, loss: 2.635894775390625\n",
      "epoch: 92,  batch step: 248, loss: 2.549903392791748\n",
      "epoch: 92,  batch step: 249, loss: 18.075664520263672\n",
      "epoch: 92,  batch step: 250, loss: 3.952011823654175\n",
      "epoch: 92,  batch step: 251, loss: 5.553715229034424\n",
      "validation error epoch  92:    tensor(68.6683, device='cuda:0')\n",
      "316\n",
      "epoch: 93,  batch step: 0, loss: 3.2647080421447754\n",
      "epoch: 93,  batch step: 1, loss: 21.42184066772461\n",
      "epoch: 93,  batch step: 2, loss: 2.866142988204956\n",
      "epoch: 93,  batch step: 3, loss: 2.169038772583008\n",
      "epoch: 93,  batch step: 4, loss: 16.80994415283203\n",
      "epoch: 93,  batch step: 5, loss: 5.737174987792969\n",
      "epoch: 93,  batch step: 6, loss: 2.7117278575897217\n",
      "epoch: 93,  batch step: 7, loss: 8.223984718322754\n",
      "epoch: 93,  batch step: 8, loss: 4.293466567993164\n",
      "epoch: 93,  batch step: 9, loss: 13.35671615600586\n",
      "epoch: 93,  batch step: 10, loss: 10.017822265625\n",
      "epoch: 93,  batch step: 11, loss: 4.809286117553711\n",
      "epoch: 93,  batch step: 12, loss: 10.901396751403809\n",
      "epoch: 93,  batch step: 13, loss: 11.654672622680664\n",
      "epoch: 93,  batch step: 14, loss: 2.44376277923584\n",
      "epoch: 93,  batch step: 15, loss: 2.621824264526367\n",
      "epoch: 93,  batch step: 16, loss: 3.4951510429382324\n",
      "epoch: 93,  batch step: 17, loss: 19.69811248779297\n",
      "epoch: 93,  batch step: 18, loss: 8.363237380981445\n",
      "epoch: 93,  batch step: 19, loss: 2.328789710998535\n",
      "epoch: 93,  batch step: 20, loss: 32.23196792602539\n",
      "epoch: 93,  batch step: 21, loss: 10.548844337463379\n",
      "epoch: 93,  batch step: 22, loss: 3.954127788543701\n",
      "epoch: 93,  batch step: 23, loss: 9.359667778015137\n",
      "epoch: 93,  batch step: 24, loss: 13.779411315917969\n",
      "epoch: 93,  batch step: 25, loss: 5.687398433685303\n",
      "epoch: 93,  batch step: 26, loss: 23.195945739746094\n",
      "epoch: 93,  batch step: 27, loss: 3.2402796745300293\n",
      "epoch: 93,  batch step: 28, loss: 25.660640716552734\n",
      "epoch: 93,  batch step: 29, loss: 33.81533432006836\n",
      "epoch: 93,  batch step: 30, loss: 11.26776123046875\n",
      "epoch: 93,  batch step: 31, loss: 3.271474838256836\n",
      "epoch: 93,  batch step: 32, loss: 2.2337937355041504\n",
      "epoch: 93,  batch step: 33, loss: 4.420390605926514\n",
      "epoch: 93,  batch step: 34, loss: 22.418773651123047\n",
      "epoch: 93,  batch step: 35, loss: 19.202899932861328\n",
      "epoch: 93,  batch step: 36, loss: 3.4967846870422363\n",
      "epoch: 93,  batch step: 37, loss: 5.40994930267334\n",
      "epoch: 93,  batch step: 38, loss: 4.012905120849609\n",
      "epoch: 93,  batch step: 39, loss: 10.06325912475586\n",
      "epoch: 93,  batch step: 40, loss: 13.058895111083984\n",
      "epoch: 93,  batch step: 41, loss: 14.107847213745117\n",
      "epoch: 93,  batch step: 42, loss: 10.886930465698242\n",
      "epoch: 93,  batch step: 43, loss: 2.820725440979004\n",
      "epoch: 93,  batch step: 44, loss: 7.6339874267578125\n",
      "epoch: 93,  batch step: 45, loss: 2.6569161415100098\n",
      "epoch: 93,  batch step: 46, loss: 3.0819873809814453\n",
      "epoch: 93,  batch step: 47, loss: 1.9800941944122314\n",
      "epoch: 93,  batch step: 48, loss: 14.531867027282715\n",
      "epoch: 93,  batch step: 49, loss: 7.4885478019714355\n",
      "epoch: 93,  batch step: 50, loss: 3.840320110321045\n",
      "epoch: 93,  batch step: 51, loss: 10.130411148071289\n",
      "epoch: 93,  batch step: 52, loss: 30.677749633789062\n",
      "epoch: 93,  batch step: 53, loss: 3.0190296173095703\n",
      "epoch: 93,  batch step: 54, loss: 29.188674926757812\n",
      "epoch: 93,  batch step: 55, loss: 4.330276966094971\n",
      "epoch: 93,  batch step: 56, loss: 47.5262451171875\n",
      "epoch: 93,  batch step: 57, loss: 6.1404876708984375\n",
      "epoch: 93,  batch step: 58, loss: 2.707611083984375\n",
      "epoch: 93,  batch step: 59, loss: 1.5307996273040771\n",
      "epoch: 93,  batch step: 60, loss: 1.937278151512146\n",
      "epoch: 93,  batch step: 61, loss: 5.275383472442627\n",
      "epoch: 93,  batch step: 62, loss: 2.630970001220703\n",
      "epoch: 93,  batch step: 63, loss: 57.5321159362793\n",
      "epoch: 93,  batch step: 64, loss: 4.954532623291016\n",
      "epoch: 93,  batch step: 65, loss: 56.820960998535156\n",
      "epoch: 93,  batch step: 66, loss: 2.093494176864624\n",
      "epoch: 93,  batch step: 67, loss: 3.0759377479553223\n",
      "epoch: 93,  batch step: 68, loss: 16.938005447387695\n",
      "epoch: 93,  batch step: 69, loss: 47.60973358154297\n",
      "epoch: 93,  batch step: 70, loss: 2.977339744567871\n",
      "epoch: 93,  batch step: 71, loss: 60.939083099365234\n",
      "epoch: 93,  batch step: 72, loss: 11.295222282409668\n",
      "epoch: 93,  batch step: 73, loss: 3.3309943675994873\n",
      "epoch: 93,  batch step: 74, loss: 2.357295036315918\n",
      "epoch: 93,  batch step: 75, loss: 2.102156162261963\n",
      "epoch: 93,  batch step: 76, loss: 2.9364287853240967\n",
      "epoch: 93,  batch step: 77, loss: 2.0788416862487793\n",
      "epoch: 93,  batch step: 78, loss: 5.418097972869873\n",
      "epoch: 93,  batch step: 79, loss: 2.2472238540649414\n",
      "epoch: 93,  batch step: 80, loss: 5.314162731170654\n",
      "epoch: 93,  batch step: 81, loss: 2.0936813354492188\n",
      "epoch: 93,  batch step: 82, loss: 32.21327209472656\n",
      "epoch: 93,  batch step: 83, loss: 10.936147689819336\n",
      "epoch: 93,  batch step: 84, loss: 39.37185287475586\n",
      "epoch: 93,  batch step: 85, loss: 4.073633193969727\n",
      "epoch: 93,  batch step: 86, loss: 14.537386894226074\n",
      "epoch: 93,  batch step: 87, loss: 32.33721160888672\n",
      "epoch: 93,  batch step: 88, loss: 2.1993682384490967\n",
      "epoch: 93,  batch step: 89, loss: 15.050922393798828\n",
      "epoch: 93,  batch step: 90, loss: 134.44764709472656\n",
      "epoch: 93,  batch step: 91, loss: 4.575145244598389\n",
      "epoch: 93,  batch step: 92, loss: 1.6178245544433594\n",
      "epoch: 93,  batch step: 93, loss: 16.508377075195312\n",
      "epoch: 93,  batch step: 94, loss: 17.637813568115234\n",
      "epoch: 93,  batch step: 95, loss: 2.855640172958374\n",
      "epoch: 93,  batch step: 96, loss: 11.23310375213623\n",
      "epoch: 93,  batch step: 97, loss: 31.027915954589844\n",
      "epoch: 93,  batch step: 98, loss: 2.922459125518799\n",
      "epoch: 93,  batch step: 99, loss: 2.172304391860962\n",
      "epoch: 93,  batch step: 100, loss: 3.476020574569702\n",
      "epoch: 93,  batch step: 101, loss: 4.130657196044922\n",
      "epoch: 93,  batch step: 102, loss: 3.1479928493499756\n",
      "epoch: 93,  batch step: 103, loss: 2.1403398513793945\n",
      "epoch: 93,  batch step: 104, loss: 3.5196309089660645\n",
      "epoch: 93,  batch step: 105, loss: 13.18641471862793\n",
      "epoch: 93,  batch step: 106, loss: 3.534524440765381\n",
      "epoch: 93,  batch step: 107, loss: 3.765454053878784\n",
      "epoch: 93,  batch step: 108, loss: 25.559362411499023\n",
      "epoch: 93,  batch step: 109, loss: 3.4734420776367188\n",
      "epoch: 93,  batch step: 110, loss: 3.0874547958374023\n",
      "epoch: 93,  batch step: 111, loss: 2.006714344024658\n",
      "epoch: 93,  batch step: 112, loss: 11.763221740722656\n",
      "epoch: 93,  batch step: 113, loss: 18.286460876464844\n",
      "epoch: 93,  batch step: 114, loss: 23.90181541442871\n",
      "epoch: 93,  batch step: 115, loss: 20.158681869506836\n",
      "epoch: 93,  batch step: 116, loss: 18.435134887695312\n",
      "epoch: 93,  batch step: 117, loss: 16.89288902282715\n",
      "epoch: 93,  batch step: 118, loss: 1.853961706161499\n",
      "epoch: 93,  batch step: 119, loss: 3.43290114402771\n",
      "epoch: 93,  batch step: 120, loss: 44.51850891113281\n",
      "epoch: 93,  batch step: 121, loss: 3.2078311443328857\n",
      "epoch: 93,  batch step: 122, loss: 4.663272380828857\n",
      "epoch: 93,  batch step: 123, loss: 7.404099941253662\n",
      "epoch: 93,  batch step: 124, loss: 67.31330108642578\n",
      "epoch: 93,  batch step: 125, loss: 3.736220121383667\n",
      "epoch: 93,  batch step: 126, loss: 24.027624130249023\n",
      "epoch: 93,  batch step: 127, loss: 8.408812522888184\n",
      "epoch: 93,  batch step: 128, loss: 14.0029935836792\n",
      "epoch: 93,  batch step: 129, loss: 1.6336934566497803\n",
      "epoch: 93,  batch step: 130, loss: 8.485739707946777\n",
      "epoch: 93,  batch step: 131, loss: 2.2809221744537354\n",
      "epoch: 93,  batch step: 132, loss: 2.1150879859924316\n",
      "epoch: 93,  batch step: 133, loss: 2.6697559356689453\n",
      "epoch: 93,  batch step: 134, loss: 46.58789825439453\n",
      "epoch: 93,  batch step: 135, loss: 16.031455993652344\n",
      "epoch: 93,  batch step: 136, loss: 1.8441798686981201\n",
      "epoch: 93,  batch step: 137, loss: 6.361562728881836\n",
      "epoch: 93,  batch step: 138, loss: 17.958744049072266\n",
      "epoch: 93,  batch step: 139, loss: 42.97943878173828\n",
      "epoch: 93,  batch step: 140, loss: 2.974581480026245\n",
      "epoch: 93,  batch step: 141, loss: 15.837943077087402\n",
      "epoch: 93,  batch step: 142, loss: 2.9017207622528076\n",
      "epoch: 93,  batch step: 143, loss: 2.4026522636413574\n",
      "epoch: 93,  batch step: 144, loss: 2.23583984375\n",
      "epoch: 93,  batch step: 145, loss: 3.4075074195861816\n",
      "epoch: 93,  batch step: 146, loss: 91.24102020263672\n",
      "epoch: 93,  batch step: 147, loss: 25.18536376953125\n",
      "epoch: 93,  batch step: 148, loss: 3.0991005897521973\n",
      "epoch: 93,  batch step: 149, loss: 4.30586576461792\n",
      "epoch: 93,  batch step: 150, loss: 2.706003189086914\n",
      "epoch: 93,  batch step: 151, loss: 5.906371593475342\n",
      "epoch: 93,  batch step: 152, loss: 3.671227216720581\n",
      "epoch: 93,  batch step: 153, loss: 2.1551361083984375\n",
      "epoch: 93,  batch step: 154, loss: 3.828836441040039\n",
      "epoch: 93,  batch step: 155, loss: 25.60900115966797\n",
      "epoch: 93,  batch step: 156, loss: 18.437271118164062\n",
      "epoch: 93,  batch step: 157, loss: 10.36220645904541\n",
      "epoch: 93,  batch step: 158, loss: 3.708617687225342\n",
      "epoch: 93,  batch step: 159, loss: 6.896777629852295\n",
      "epoch: 93,  batch step: 160, loss: 43.489498138427734\n",
      "epoch: 93,  batch step: 161, loss: 10.292364120483398\n",
      "epoch: 93,  batch step: 162, loss: 2.248471736907959\n",
      "epoch: 93,  batch step: 163, loss: 10.45327377319336\n",
      "epoch: 93,  batch step: 164, loss: 30.40261459350586\n",
      "epoch: 93,  batch step: 165, loss: 22.330158233642578\n",
      "epoch: 93,  batch step: 166, loss: 3.4013843536376953\n",
      "epoch: 93,  batch step: 167, loss: 23.689594268798828\n",
      "epoch: 93,  batch step: 168, loss: 8.905324935913086\n",
      "epoch: 93,  batch step: 169, loss: 8.928901672363281\n",
      "epoch: 93,  batch step: 170, loss: 25.124298095703125\n",
      "epoch: 93,  batch step: 171, loss: 17.812341690063477\n",
      "epoch: 93,  batch step: 172, loss: 2.1150169372558594\n",
      "epoch: 93,  batch step: 173, loss: 2.823012351989746\n",
      "epoch: 93,  batch step: 174, loss: 13.811429977416992\n",
      "epoch: 93,  batch step: 175, loss: 51.00002670288086\n",
      "epoch: 93,  batch step: 176, loss: 21.649259567260742\n",
      "epoch: 93,  batch step: 177, loss: 2.6762232780456543\n",
      "epoch: 93,  batch step: 178, loss: 6.500937461853027\n",
      "epoch: 93,  batch step: 179, loss: 3.4263129234313965\n",
      "epoch: 93,  batch step: 180, loss: 20.70454978942871\n",
      "epoch: 93,  batch step: 181, loss: 16.280426025390625\n",
      "epoch: 93,  batch step: 182, loss: 3.1243820190429688\n",
      "epoch: 93,  batch step: 183, loss: 5.148311614990234\n",
      "epoch: 93,  batch step: 184, loss: 29.494312286376953\n",
      "epoch: 93,  batch step: 185, loss: 7.5630316734313965\n",
      "epoch: 93,  batch step: 186, loss: 37.679840087890625\n",
      "epoch: 93,  batch step: 187, loss: 15.890134811401367\n",
      "epoch: 93,  batch step: 188, loss: 16.463031768798828\n",
      "epoch: 93,  batch step: 189, loss: 18.992759704589844\n",
      "epoch: 93,  batch step: 190, loss: 10.420380592346191\n",
      "epoch: 93,  batch step: 191, loss: 19.529987335205078\n",
      "epoch: 93,  batch step: 192, loss: 8.206013679504395\n",
      "epoch: 93,  batch step: 193, loss: 8.888946533203125\n",
      "epoch: 93,  batch step: 194, loss: 29.97220230102539\n",
      "epoch: 93,  batch step: 195, loss: 2.5985612869262695\n",
      "epoch: 93,  batch step: 196, loss: 5.2739152908325195\n",
      "epoch: 93,  batch step: 197, loss: 4.32186222076416\n",
      "epoch: 93,  batch step: 198, loss: 3.758173704147339\n",
      "epoch: 93,  batch step: 199, loss: 73.1345443725586\n",
      "epoch: 93,  batch step: 200, loss: 3.88061785697937\n",
      "epoch: 93,  batch step: 201, loss: 10.566887855529785\n",
      "epoch: 93,  batch step: 202, loss: 4.78749418258667\n",
      "epoch: 93,  batch step: 203, loss: 12.805089950561523\n",
      "epoch: 93,  batch step: 204, loss: 3.421459913253784\n",
      "epoch: 93,  batch step: 205, loss: 37.25110626220703\n",
      "epoch: 93,  batch step: 206, loss: 2.2513484954833984\n",
      "epoch: 93,  batch step: 207, loss: 4.370131969451904\n",
      "epoch: 93,  batch step: 208, loss: 14.521575927734375\n",
      "epoch: 93,  batch step: 209, loss: 2.6203532218933105\n",
      "epoch: 93,  batch step: 210, loss: 2.4503931999206543\n",
      "epoch: 93,  batch step: 211, loss: 17.203720092773438\n",
      "epoch: 93,  batch step: 212, loss: 11.295278549194336\n",
      "epoch: 93,  batch step: 213, loss: 29.152944564819336\n",
      "epoch: 93,  batch step: 214, loss: 2.8814873695373535\n",
      "epoch: 93,  batch step: 215, loss: 2.1252832412719727\n",
      "epoch: 93,  batch step: 216, loss: 23.318084716796875\n",
      "epoch: 93,  batch step: 217, loss: 3.149963140487671\n",
      "epoch: 93,  batch step: 218, loss: 2.0336825847625732\n",
      "epoch: 93,  batch step: 219, loss: 2.521618366241455\n",
      "epoch: 93,  batch step: 220, loss: 8.579940795898438\n",
      "epoch: 93,  batch step: 221, loss: 1.7897913455963135\n",
      "epoch: 93,  batch step: 222, loss: 2.5420522689819336\n",
      "epoch: 93,  batch step: 223, loss: 1.9127496480941772\n",
      "epoch: 93,  batch step: 224, loss: 12.827301025390625\n",
      "epoch: 93,  batch step: 225, loss: 9.823448181152344\n",
      "epoch: 93,  batch step: 226, loss: 2.475951671600342\n",
      "epoch: 93,  batch step: 227, loss: 3.8238208293914795\n",
      "epoch: 93,  batch step: 228, loss: 21.560619354248047\n",
      "epoch: 93,  batch step: 229, loss: 60.750511169433594\n",
      "epoch: 93,  batch step: 230, loss: 3.87439227104187\n",
      "epoch: 93,  batch step: 231, loss: 16.781803131103516\n",
      "epoch: 93,  batch step: 232, loss: 5.24114990234375\n",
      "epoch: 93,  batch step: 233, loss: 3.1327195167541504\n",
      "epoch: 93,  batch step: 234, loss: 15.268430709838867\n",
      "epoch: 93,  batch step: 235, loss: 17.659439086914062\n",
      "epoch: 93,  batch step: 236, loss: 21.040082931518555\n",
      "epoch: 93,  batch step: 237, loss: 14.685083389282227\n",
      "epoch: 93,  batch step: 238, loss: 12.16763687133789\n",
      "epoch: 93,  batch step: 239, loss: 2.806952476501465\n",
      "epoch: 93,  batch step: 240, loss: 3.4709696769714355\n",
      "epoch: 93,  batch step: 241, loss: 9.949858665466309\n",
      "epoch: 93,  batch step: 242, loss: 8.071989059448242\n",
      "epoch: 93,  batch step: 243, loss: 11.81900691986084\n",
      "epoch: 93,  batch step: 244, loss: 2.385695695877075\n",
      "epoch: 93,  batch step: 245, loss: 7.669650554656982\n",
      "epoch: 93,  batch step: 246, loss: 8.629003524780273\n",
      "epoch: 93,  batch step: 247, loss: 51.94335174560547\n",
      "epoch: 93,  batch step: 248, loss: 4.4252753257751465\n",
      "epoch: 93,  batch step: 249, loss: 11.933696746826172\n",
      "epoch: 93,  batch step: 250, loss: 9.33670425415039\n",
      "epoch: 93,  batch step: 251, loss: 11.065670013427734\n",
      "validation error epoch  93:    tensor(69.5606, device='cuda:0')\n",
      "316\n",
      "epoch: 94,  batch step: 0, loss: 13.871901512145996\n",
      "epoch: 94,  batch step: 1, loss: 2.2723233699798584\n",
      "epoch: 94,  batch step: 2, loss: 4.673182487487793\n",
      "epoch: 94,  batch step: 3, loss: 4.02020263671875\n",
      "epoch: 94,  batch step: 4, loss: 2.6481757164001465\n",
      "epoch: 94,  batch step: 5, loss: 11.009211540222168\n",
      "epoch: 94,  batch step: 6, loss: 3.298964500427246\n",
      "epoch: 94,  batch step: 7, loss: 21.71478843688965\n",
      "epoch: 94,  batch step: 8, loss: 2.93491530418396\n",
      "epoch: 94,  batch step: 9, loss: 3.1808078289031982\n",
      "epoch: 94,  batch step: 10, loss: 13.353384017944336\n",
      "epoch: 94,  batch step: 11, loss: 14.164907455444336\n",
      "epoch: 94,  batch step: 12, loss: 3.284719944000244\n",
      "epoch: 94,  batch step: 13, loss: 5.793831825256348\n",
      "epoch: 94,  batch step: 14, loss: 4.262078285217285\n",
      "epoch: 94,  batch step: 15, loss: 2.9316372871398926\n",
      "epoch: 94,  batch step: 16, loss: 13.36509895324707\n",
      "epoch: 94,  batch step: 17, loss: 3.6083714962005615\n",
      "epoch: 94,  batch step: 18, loss: 12.26081657409668\n",
      "epoch: 94,  batch step: 19, loss: 11.4978666305542\n",
      "epoch: 94,  batch step: 20, loss: 4.114321231842041\n",
      "epoch: 94,  batch step: 21, loss: 2.3207719326019287\n",
      "epoch: 94,  batch step: 22, loss: 56.516807556152344\n",
      "epoch: 94,  batch step: 23, loss: 5.5879225730896\n",
      "epoch: 94,  batch step: 24, loss: 2.491915702819824\n",
      "epoch: 94,  batch step: 25, loss: 14.899919509887695\n",
      "epoch: 94,  batch step: 26, loss: 19.52466583251953\n",
      "epoch: 94,  batch step: 27, loss: 9.714044570922852\n",
      "epoch: 94,  batch step: 28, loss: 8.286698341369629\n",
      "epoch: 94,  batch step: 29, loss: 4.12911319732666\n",
      "epoch: 94,  batch step: 30, loss: 13.63884162902832\n",
      "epoch: 94,  batch step: 31, loss: 45.923274993896484\n",
      "epoch: 94,  batch step: 32, loss: 14.06573486328125\n",
      "epoch: 94,  batch step: 33, loss: 2.20694899559021\n",
      "epoch: 94,  batch step: 34, loss: 15.03419303894043\n",
      "epoch: 94,  batch step: 35, loss: 12.635601043701172\n",
      "epoch: 94,  batch step: 36, loss: 2.545527935028076\n",
      "epoch: 94,  batch step: 37, loss: 11.453479766845703\n",
      "epoch: 94,  batch step: 38, loss: 2.57572078704834\n",
      "epoch: 94,  batch step: 39, loss: 6.599604606628418\n",
      "epoch: 94,  batch step: 40, loss: 21.704998016357422\n",
      "epoch: 94,  batch step: 41, loss: 11.391838073730469\n",
      "epoch: 94,  batch step: 42, loss: 6.24810791015625\n",
      "epoch: 94,  batch step: 43, loss: 2.6611716747283936\n",
      "epoch: 94,  batch step: 44, loss: 4.472451210021973\n",
      "epoch: 94,  batch step: 45, loss: 10.108892440795898\n",
      "epoch: 94,  batch step: 46, loss: 10.597591400146484\n",
      "epoch: 94,  batch step: 47, loss: 26.264860153198242\n",
      "epoch: 94,  batch step: 48, loss: 18.67238998413086\n",
      "epoch: 94,  batch step: 49, loss: 20.201040267944336\n",
      "epoch: 94,  batch step: 50, loss: 19.0509090423584\n",
      "epoch: 94,  batch step: 51, loss: 2.776397705078125\n",
      "epoch: 94,  batch step: 52, loss: 2.896048069000244\n",
      "epoch: 94,  batch step: 53, loss: 48.566986083984375\n",
      "epoch: 94,  batch step: 54, loss: 15.944156646728516\n",
      "epoch: 94,  batch step: 55, loss: 7.522795677185059\n",
      "epoch: 94,  batch step: 56, loss: 19.5726261138916\n",
      "epoch: 94,  batch step: 57, loss: 11.854796409606934\n",
      "epoch: 94,  batch step: 58, loss: 2.0337514877319336\n",
      "epoch: 94,  batch step: 59, loss: 3.2007880210876465\n",
      "epoch: 94,  batch step: 60, loss: 19.029930114746094\n",
      "epoch: 94,  batch step: 61, loss: 16.015111923217773\n",
      "epoch: 94,  batch step: 62, loss: 2.0949819087982178\n",
      "epoch: 94,  batch step: 63, loss: 1.7696139812469482\n",
      "epoch: 94,  batch step: 64, loss: 15.659640312194824\n",
      "epoch: 94,  batch step: 65, loss: 2.6187102794647217\n",
      "epoch: 94,  batch step: 66, loss: 3.51145076751709\n",
      "epoch: 94,  batch step: 67, loss: 8.648494720458984\n",
      "epoch: 94,  batch step: 68, loss: 16.831798553466797\n",
      "epoch: 94,  batch step: 69, loss: 7.921156883239746\n",
      "epoch: 94,  batch step: 70, loss: 2.038180351257324\n",
      "epoch: 94,  batch step: 71, loss: 17.161441802978516\n",
      "epoch: 94,  batch step: 72, loss: 4.275068283081055\n",
      "epoch: 94,  batch step: 73, loss: 4.129208564758301\n",
      "epoch: 94,  batch step: 74, loss: 2.0880274772644043\n",
      "epoch: 94,  batch step: 75, loss: 2.1596460342407227\n",
      "epoch: 94,  batch step: 76, loss: 2.798739194869995\n",
      "epoch: 94,  batch step: 77, loss: 19.2890625\n",
      "epoch: 94,  batch step: 78, loss: 1.7390193939208984\n",
      "epoch: 94,  batch step: 79, loss: 26.973325729370117\n",
      "epoch: 94,  batch step: 80, loss: 8.120964050292969\n",
      "epoch: 94,  batch step: 81, loss: 6.0935821533203125\n",
      "epoch: 94,  batch step: 82, loss: 3.134852886199951\n",
      "epoch: 94,  batch step: 83, loss: 52.42536544799805\n",
      "epoch: 94,  batch step: 84, loss: 4.639095306396484\n",
      "epoch: 94,  batch step: 85, loss: 3.659733533859253\n",
      "epoch: 94,  batch step: 86, loss: 155.30624389648438\n",
      "epoch: 94,  batch step: 87, loss: 3.601348400115967\n",
      "epoch: 94,  batch step: 88, loss: 11.910514831542969\n",
      "epoch: 94,  batch step: 89, loss: 2.8712286949157715\n",
      "epoch: 94,  batch step: 90, loss: 18.771183013916016\n",
      "epoch: 94,  batch step: 91, loss: 2.1618497371673584\n",
      "epoch: 94,  batch step: 92, loss: 2.2572972774505615\n",
      "epoch: 94,  batch step: 93, loss: 5.165158271789551\n",
      "epoch: 94,  batch step: 94, loss: 3.8524980545043945\n",
      "epoch: 94,  batch step: 95, loss: 2.9187324047088623\n",
      "epoch: 94,  batch step: 96, loss: 3.077584981918335\n",
      "epoch: 94,  batch step: 97, loss: 28.32985496520996\n",
      "epoch: 94,  batch step: 98, loss: 2.973771810531616\n",
      "epoch: 94,  batch step: 99, loss: 1.8952605724334717\n",
      "epoch: 94,  batch step: 100, loss: 3.1411752700805664\n",
      "epoch: 94,  batch step: 101, loss: 4.220978736877441\n",
      "epoch: 94,  batch step: 102, loss: 17.12465476989746\n",
      "epoch: 94,  batch step: 103, loss: 4.466853618621826\n",
      "epoch: 94,  batch step: 104, loss: 7.043595314025879\n",
      "epoch: 94,  batch step: 105, loss: 8.655173301696777\n",
      "epoch: 94,  batch step: 106, loss: 5.6428632736206055\n",
      "epoch: 94,  batch step: 107, loss: 3.330876350402832\n",
      "epoch: 94,  batch step: 108, loss: 2.202812671661377\n",
      "epoch: 94,  batch step: 109, loss: 3.0497069358825684\n",
      "epoch: 94,  batch step: 110, loss: 15.033888816833496\n",
      "epoch: 94,  batch step: 111, loss: 21.547710418701172\n",
      "epoch: 94,  batch step: 112, loss: 18.717878341674805\n",
      "epoch: 94,  batch step: 113, loss: 9.312193870544434\n",
      "epoch: 94,  batch step: 114, loss: 10.008363723754883\n",
      "epoch: 94,  batch step: 115, loss: 12.295702934265137\n",
      "epoch: 94,  batch step: 116, loss: 9.611154556274414\n",
      "epoch: 94,  batch step: 117, loss: 2.3685214519500732\n",
      "epoch: 94,  batch step: 118, loss: 1.6227878332138062\n",
      "epoch: 94,  batch step: 119, loss: 5.038653373718262\n",
      "epoch: 94,  batch step: 120, loss: 9.136868476867676\n",
      "epoch: 94,  batch step: 121, loss: 48.99383544921875\n",
      "epoch: 94,  batch step: 122, loss: 10.336647987365723\n",
      "epoch: 94,  batch step: 123, loss: 4.7235107421875\n",
      "epoch: 94,  batch step: 124, loss: 34.36748504638672\n",
      "epoch: 94,  batch step: 125, loss: 3.9559226036071777\n",
      "epoch: 94,  batch step: 126, loss: 76.70893096923828\n",
      "epoch: 94,  batch step: 127, loss: 3.4452247619628906\n",
      "epoch: 94,  batch step: 128, loss: 4.523926258087158\n",
      "epoch: 94,  batch step: 129, loss: 14.606081008911133\n",
      "epoch: 94,  batch step: 130, loss: 2.588606595993042\n",
      "epoch: 94,  batch step: 131, loss: 2.4589099884033203\n",
      "epoch: 94,  batch step: 132, loss: 45.57384490966797\n",
      "epoch: 94,  batch step: 133, loss: 38.674835205078125\n",
      "epoch: 94,  batch step: 134, loss: 21.294172286987305\n",
      "epoch: 94,  batch step: 135, loss: 5.556094169616699\n",
      "epoch: 94,  batch step: 136, loss: 11.260089874267578\n",
      "epoch: 94,  batch step: 137, loss: 11.772541046142578\n",
      "epoch: 94,  batch step: 138, loss: 33.49436569213867\n",
      "epoch: 94,  batch step: 139, loss: 3.108144521713257\n",
      "epoch: 94,  batch step: 140, loss: 2.9631495475769043\n",
      "epoch: 94,  batch step: 141, loss: 3.152027130126953\n",
      "epoch: 94,  batch step: 142, loss: 11.63372802734375\n",
      "epoch: 94,  batch step: 143, loss: 3.5861291885375977\n",
      "epoch: 94,  batch step: 144, loss: 3.2178237438201904\n",
      "epoch: 94,  batch step: 145, loss: 18.483062744140625\n",
      "epoch: 94,  batch step: 146, loss: 8.661850929260254\n",
      "epoch: 94,  batch step: 147, loss: 7.640862464904785\n",
      "epoch: 94,  batch step: 148, loss: 24.05891990661621\n",
      "epoch: 94,  batch step: 149, loss: 30.47779083251953\n",
      "epoch: 94,  batch step: 150, loss: 29.880712509155273\n",
      "epoch: 94,  batch step: 151, loss: 2.0400214195251465\n",
      "epoch: 94,  batch step: 152, loss: 10.153647422790527\n",
      "epoch: 94,  batch step: 153, loss: 2.8707823753356934\n",
      "epoch: 94,  batch step: 154, loss: 13.010986328125\n",
      "epoch: 94,  batch step: 155, loss: 2.0866785049438477\n",
      "epoch: 94,  batch step: 156, loss: 3.8260304927825928\n",
      "epoch: 94,  batch step: 157, loss: 12.196115493774414\n",
      "epoch: 94,  batch step: 158, loss: 10.526580810546875\n",
      "epoch: 94,  batch step: 159, loss: 7.415868282318115\n",
      "epoch: 94,  batch step: 160, loss: 46.05009078979492\n",
      "epoch: 94,  batch step: 161, loss: 4.835515975952148\n",
      "epoch: 94,  batch step: 162, loss: 4.317552089691162\n",
      "epoch: 94,  batch step: 163, loss: 3.7529983520507812\n",
      "epoch: 94,  batch step: 164, loss: 2.495123863220215\n",
      "epoch: 94,  batch step: 165, loss: 11.550628662109375\n",
      "epoch: 94,  batch step: 166, loss: 2.5681381225585938\n",
      "epoch: 94,  batch step: 167, loss: 13.658456802368164\n",
      "epoch: 94,  batch step: 168, loss: 50.72855758666992\n",
      "epoch: 94,  batch step: 169, loss: 4.227680206298828\n",
      "epoch: 94,  batch step: 170, loss: 45.838172912597656\n",
      "epoch: 94,  batch step: 171, loss: 9.600534439086914\n",
      "epoch: 94,  batch step: 172, loss: 9.138267517089844\n",
      "epoch: 94,  batch step: 173, loss: 13.75619888305664\n",
      "epoch: 94,  batch step: 174, loss: 14.906962394714355\n",
      "epoch: 94,  batch step: 175, loss: 2.8348422050476074\n",
      "epoch: 94,  batch step: 176, loss: 2.6733813285827637\n",
      "epoch: 94,  batch step: 177, loss: 6.299799919128418\n",
      "epoch: 94,  batch step: 178, loss: 67.67680358886719\n",
      "epoch: 94,  batch step: 179, loss: 55.08530807495117\n",
      "epoch: 94,  batch step: 180, loss: 2.8446884155273438\n",
      "epoch: 94,  batch step: 181, loss: 4.449779510498047\n",
      "epoch: 94,  batch step: 182, loss: 8.168069839477539\n",
      "epoch: 94,  batch step: 183, loss: 2.508404493331909\n",
      "epoch: 94,  batch step: 184, loss: 1.7594528198242188\n",
      "epoch: 94,  batch step: 185, loss: 4.377769470214844\n",
      "epoch: 94,  batch step: 186, loss: 19.539363861083984\n",
      "epoch: 94,  batch step: 187, loss: 1.6377370357513428\n",
      "epoch: 94,  batch step: 188, loss: 10.247735023498535\n",
      "epoch: 94,  batch step: 189, loss: 1.9111979007720947\n",
      "epoch: 94,  batch step: 190, loss: 19.668678283691406\n",
      "epoch: 94,  batch step: 191, loss: 2.0995166301727295\n",
      "epoch: 94,  batch step: 192, loss: 9.64258861541748\n",
      "epoch: 94,  batch step: 193, loss: 8.901762008666992\n",
      "epoch: 94,  batch step: 194, loss: 7.871947765350342\n",
      "epoch: 94,  batch step: 195, loss: 7.295755386352539\n",
      "epoch: 94,  batch step: 196, loss: 3.167097806930542\n",
      "epoch: 94,  batch step: 197, loss: 2.2617671489715576\n",
      "epoch: 94,  batch step: 198, loss: 2.6515047550201416\n",
      "epoch: 94,  batch step: 199, loss: 10.053564071655273\n",
      "epoch: 94,  batch step: 200, loss: 1.9666775465011597\n",
      "epoch: 94,  batch step: 201, loss: 14.73465633392334\n",
      "epoch: 94,  batch step: 202, loss: 2.018531322479248\n",
      "epoch: 94,  batch step: 203, loss: 1.9227404594421387\n",
      "epoch: 94,  batch step: 204, loss: 4.323240280151367\n",
      "epoch: 94,  batch step: 205, loss: 13.850006103515625\n",
      "epoch: 94,  batch step: 206, loss: 41.38357162475586\n",
      "epoch: 94,  batch step: 207, loss: 9.913554191589355\n",
      "epoch: 94,  batch step: 208, loss: 15.437426567077637\n",
      "epoch: 94,  batch step: 209, loss: 49.052268981933594\n",
      "epoch: 94,  batch step: 210, loss: 2.2999868392944336\n",
      "epoch: 94,  batch step: 211, loss: 28.85031509399414\n",
      "epoch: 94,  batch step: 212, loss: 13.25110149383545\n",
      "epoch: 94,  batch step: 213, loss: 2.7831084728240967\n",
      "epoch: 94,  batch step: 214, loss: 14.213118553161621\n",
      "epoch: 94,  batch step: 215, loss: 2.4465742111206055\n",
      "epoch: 94,  batch step: 216, loss: 3.733386516571045\n",
      "epoch: 94,  batch step: 217, loss: 7.513970851898193\n",
      "epoch: 94,  batch step: 218, loss: 17.48619270324707\n",
      "epoch: 94,  batch step: 219, loss: 23.817543029785156\n",
      "epoch: 94,  batch step: 220, loss: 2.2114462852478027\n",
      "epoch: 94,  batch step: 221, loss: 1.6607108116149902\n",
      "epoch: 94,  batch step: 222, loss: 10.095051765441895\n",
      "epoch: 94,  batch step: 223, loss: 27.28864097595215\n",
      "epoch: 94,  batch step: 224, loss: 1.8352935314178467\n",
      "epoch: 94,  batch step: 225, loss: 3.4645495414733887\n",
      "epoch: 94,  batch step: 226, loss: 2.315427780151367\n",
      "epoch: 94,  batch step: 227, loss: 2.1419339179992676\n",
      "epoch: 94,  batch step: 228, loss: 16.02642822265625\n",
      "epoch: 94,  batch step: 229, loss: 2.2401976585388184\n",
      "epoch: 94,  batch step: 230, loss: 10.993281364440918\n",
      "epoch: 94,  batch step: 231, loss: 3.7715346813201904\n",
      "epoch: 94,  batch step: 232, loss: 10.313567161560059\n",
      "epoch: 94,  batch step: 233, loss: 20.50594711303711\n",
      "epoch: 94,  batch step: 234, loss: 9.341499328613281\n",
      "epoch: 94,  batch step: 235, loss: 8.0787353515625\n",
      "epoch: 94,  batch step: 236, loss: 3.368967056274414\n",
      "epoch: 94,  batch step: 237, loss: 2.101548433303833\n",
      "epoch: 94,  batch step: 238, loss: 11.54333782196045\n",
      "epoch: 94,  batch step: 239, loss: 2.786301374435425\n",
      "epoch: 94,  batch step: 240, loss: 3.104341983795166\n",
      "epoch: 94,  batch step: 241, loss: 1.863888144493103\n",
      "epoch: 94,  batch step: 242, loss: 12.76953411102295\n",
      "epoch: 94,  batch step: 243, loss: 8.774200439453125\n",
      "epoch: 94,  batch step: 244, loss: 3.507206678390503\n",
      "epoch: 94,  batch step: 245, loss: 7.537524700164795\n",
      "epoch: 94,  batch step: 246, loss: 44.729454040527344\n",
      "epoch: 94,  batch step: 247, loss: 73.20442199707031\n",
      "epoch: 94,  batch step: 248, loss: 17.329299926757812\n",
      "epoch: 94,  batch step: 249, loss: 16.846118927001953\n",
      "epoch: 94,  batch step: 250, loss: 17.996137619018555\n",
      "epoch: 94,  batch step: 251, loss: 10.5385103225708\n",
      "validation error epoch  94:    tensor(70.8972, device='cuda:0')\n",
      "316\n",
      "epoch: 95,  batch step: 0, loss: 20.36046600341797\n",
      "epoch: 95,  batch step: 1, loss: 11.849384307861328\n",
      "epoch: 95,  batch step: 2, loss: 31.394298553466797\n",
      "epoch: 95,  batch step: 3, loss: 2.616088390350342\n",
      "epoch: 95,  batch step: 4, loss: 2.5422215461730957\n",
      "epoch: 95,  batch step: 5, loss: 18.47553253173828\n",
      "epoch: 95,  batch step: 6, loss: 2.34352445602417\n",
      "epoch: 95,  batch step: 7, loss: 8.077421188354492\n",
      "epoch: 95,  batch step: 8, loss: 25.306625366210938\n",
      "epoch: 95,  batch step: 9, loss: 2.023550510406494\n",
      "epoch: 95,  batch step: 10, loss: 15.424860000610352\n",
      "epoch: 95,  batch step: 11, loss: 7.954455375671387\n",
      "epoch: 95,  batch step: 12, loss: 2.3113956451416016\n",
      "epoch: 95,  batch step: 13, loss: 2.747544050216675\n",
      "epoch: 95,  batch step: 14, loss: 2.2220520973205566\n",
      "epoch: 95,  batch step: 15, loss: 1.9598031044006348\n",
      "epoch: 95,  batch step: 16, loss: 3.136855125427246\n",
      "epoch: 95,  batch step: 17, loss: 2.4697842597961426\n",
      "epoch: 95,  batch step: 18, loss: 2.6751821041107178\n",
      "epoch: 95,  batch step: 19, loss: 21.74263572692871\n",
      "epoch: 95,  batch step: 20, loss: 2.195185422897339\n",
      "epoch: 95,  batch step: 21, loss: 2.605207681655884\n",
      "epoch: 95,  batch step: 22, loss: 1.613883376121521\n",
      "epoch: 95,  batch step: 23, loss: 4.854837894439697\n",
      "epoch: 95,  batch step: 24, loss: 1.973804235458374\n",
      "epoch: 95,  batch step: 25, loss: 9.806920051574707\n",
      "epoch: 95,  batch step: 26, loss: 10.113794326782227\n",
      "epoch: 95,  batch step: 27, loss: 2.269594192504883\n",
      "epoch: 95,  batch step: 28, loss: 3.890353202819824\n",
      "epoch: 95,  batch step: 29, loss: 5.568714141845703\n",
      "epoch: 95,  batch step: 30, loss: 9.551543235778809\n",
      "epoch: 95,  batch step: 31, loss: 5.297868728637695\n",
      "epoch: 95,  batch step: 32, loss: 12.684549331665039\n",
      "epoch: 95,  batch step: 33, loss: 9.25202751159668\n",
      "epoch: 95,  batch step: 34, loss: 3.629591941833496\n",
      "epoch: 95,  batch step: 35, loss: 2.323831558227539\n",
      "epoch: 95,  batch step: 36, loss: 72.00906372070312\n",
      "epoch: 95,  batch step: 37, loss: 16.361995697021484\n",
      "epoch: 95,  batch step: 38, loss: 84.35861206054688\n",
      "epoch: 95,  batch step: 39, loss: 3.147825241088867\n",
      "epoch: 95,  batch step: 40, loss: 29.344860076904297\n",
      "epoch: 95,  batch step: 41, loss: 56.773345947265625\n",
      "epoch: 95,  batch step: 42, loss: 2.520956516265869\n",
      "epoch: 95,  batch step: 43, loss: 15.431312561035156\n",
      "epoch: 95,  batch step: 44, loss: 3.998351573944092\n",
      "epoch: 95,  batch step: 45, loss: 6.75777530670166\n",
      "epoch: 95,  batch step: 46, loss: 2.072443962097168\n",
      "epoch: 95,  batch step: 47, loss: 3.0802388191223145\n",
      "epoch: 95,  batch step: 48, loss: 3.122251033782959\n",
      "epoch: 95,  batch step: 49, loss: 1.9726157188415527\n",
      "epoch: 95,  batch step: 50, loss: 3.9606847763061523\n",
      "epoch: 95,  batch step: 51, loss: 11.57076644897461\n",
      "epoch: 95,  batch step: 52, loss: 10.980989456176758\n",
      "epoch: 95,  batch step: 53, loss: 1.7754905223846436\n",
      "epoch: 95,  batch step: 54, loss: 6.183793544769287\n",
      "epoch: 95,  batch step: 55, loss: 17.69536590576172\n",
      "epoch: 95,  batch step: 56, loss: 7.123417854309082\n",
      "epoch: 95,  batch step: 57, loss: 3.8070106506347656\n",
      "epoch: 95,  batch step: 58, loss: 12.884096145629883\n",
      "epoch: 95,  batch step: 59, loss: 3.7141237258911133\n",
      "epoch: 95,  batch step: 60, loss: 21.185382843017578\n",
      "epoch: 95,  batch step: 61, loss: 5.369321823120117\n",
      "epoch: 95,  batch step: 62, loss: 17.809551239013672\n",
      "epoch: 95,  batch step: 63, loss: 10.961347579956055\n",
      "epoch: 95,  batch step: 64, loss: 3.5088553428649902\n",
      "epoch: 95,  batch step: 65, loss: 28.228622436523438\n",
      "epoch: 95,  batch step: 66, loss: 7.193601131439209\n",
      "epoch: 95,  batch step: 67, loss: 46.90609359741211\n",
      "epoch: 95,  batch step: 68, loss: 11.796599388122559\n",
      "epoch: 95,  batch step: 69, loss: 2.4707658290863037\n",
      "epoch: 95,  batch step: 70, loss: 3.3427798748016357\n",
      "epoch: 95,  batch step: 71, loss: 39.015323638916016\n",
      "epoch: 95,  batch step: 72, loss: 15.695737838745117\n",
      "epoch: 95,  batch step: 73, loss: 18.97802734375\n",
      "epoch: 95,  batch step: 74, loss: 3.6414194107055664\n",
      "epoch: 95,  batch step: 75, loss: 9.021160125732422\n",
      "epoch: 95,  batch step: 76, loss: 11.868667602539062\n",
      "epoch: 95,  batch step: 77, loss: 70.13899993896484\n",
      "epoch: 95,  batch step: 78, loss: 1.7220290899276733\n",
      "epoch: 95,  batch step: 79, loss: 35.89007568359375\n",
      "epoch: 95,  batch step: 80, loss: 2.098734140396118\n",
      "epoch: 95,  batch step: 81, loss: 2.545884132385254\n",
      "epoch: 95,  batch step: 82, loss: 2.311951160430908\n",
      "epoch: 95,  batch step: 83, loss: 1.950668215751648\n",
      "epoch: 95,  batch step: 84, loss: 2.7018890380859375\n",
      "epoch: 95,  batch step: 85, loss: 15.700551986694336\n",
      "epoch: 95,  batch step: 86, loss: 2.8629488945007324\n",
      "epoch: 95,  batch step: 87, loss: 2.9363255500793457\n",
      "epoch: 95,  batch step: 88, loss: 11.218572616577148\n",
      "epoch: 95,  batch step: 89, loss: 2.594336986541748\n",
      "epoch: 95,  batch step: 90, loss: 12.241134643554688\n",
      "epoch: 95,  batch step: 91, loss: 1.9161155223846436\n",
      "epoch: 95,  batch step: 92, loss: 3.2176990509033203\n",
      "epoch: 95,  batch step: 93, loss: 18.334911346435547\n",
      "epoch: 95,  batch step: 94, loss: 56.202545166015625\n",
      "epoch: 95,  batch step: 95, loss: 4.3980536460876465\n",
      "epoch: 95,  batch step: 96, loss: 5.73240852355957\n",
      "epoch: 95,  batch step: 97, loss: 1.9044733047485352\n",
      "epoch: 95,  batch step: 98, loss: 14.990880966186523\n",
      "epoch: 95,  batch step: 99, loss: 2.1521615982055664\n",
      "epoch: 95,  batch step: 100, loss: 4.23759651184082\n",
      "epoch: 95,  batch step: 101, loss: 8.760016441345215\n",
      "epoch: 95,  batch step: 102, loss: 15.486364364624023\n",
      "epoch: 95,  batch step: 103, loss: 13.409579277038574\n",
      "epoch: 95,  batch step: 104, loss: 9.239206314086914\n",
      "epoch: 95,  batch step: 105, loss: 6.265693187713623\n",
      "epoch: 95,  batch step: 106, loss: 7.370879173278809\n",
      "epoch: 95,  batch step: 107, loss: 5.243638038635254\n",
      "epoch: 95,  batch step: 108, loss: 21.004112243652344\n",
      "epoch: 95,  batch step: 109, loss: 3.435859203338623\n",
      "epoch: 95,  batch step: 110, loss: 2.3423166275024414\n",
      "epoch: 95,  batch step: 111, loss: 17.473478317260742\n",
      "epoch: 95,  batch step: 112, loss: 14.092875480651855\n",
      "epoch: 95,  batch step: 113, loss: 4.357613563537598\n",
      "epoch: 95,  batch step: 114, loss: 2.7990567684173584\n",
      "epoch: 95,  batch step: 115, loss: 6.907878398895264\n",
      "epoch: 95,  batch step: 116, loss: 1.9160938262939453\n",
      "epoch: 95,  batch step: 117, loss: 10.612410545349121\n",
      "epoch: 95,  batch step: 118, loss: 75.83894348144531\n",
      "epoch: 95,  batch step: 119, loss: 6.213187217712402\n",
      "epoch: 95,  batch step: 120, loss: 7.805468559265137\n",
      "epoch: 95,  batch step: 121, loss: 3.437689781188965\n",
      "epoch: 95,  batch step: 122, loss: 1.9765965938568115\n",
      "epoch: 95,  batch step: 123, loss: 20.01097869873047\n",
      "epoch: 95,  batch step: 124, loss: 8.016047477722168\n",
      "epoch: 95,  batch step: 125, loss: 7.555712699890137\n",
      "epoch: 95,  batch step: 126, loss: 10.962514877319336\n",
      "epoch: 95,  batch step: 127, loss: 2.8105006217956543\n",
      "epoch: 95,  batch step: 128, loss: 2.733555555343628\n",
      "epoch: 95,  batch step: 129, loss: 77.31083679199219\n",
      "epoch: 95,  batch step: 130, loss: 14.320379257202148\n",
      "epoch: 95,  batch step: 131, loss: 1.8745607137680054\n",
      "epoch: 95,  batch step: 132, loss: 2.334700584411621\n",
      "epoch: 95,  batch step: 133, loss: 12.231828689575195\n",
      "epoch: 95,  batch step: 134, loss: 3.5821616649627686\n",
      "epoch: 95,  batch step: 135, loss: 10.011353492736816\n",
      "epoch: 95,  batch step: 136, loss: 37.97960662841797\n",
      "epoch: 95,  batch step: 137, loss: 34.38655090332031\n",
      "epoch: 95,  batch step: 138, loss: 11.705045700073242\n",
      "epoch: 95,  batch step: 139, loss: 47.92911911010742\n",
      "epoch: 95,  batch step: 140, loss: 6.4527082443237305\n",
      "epoch: 95,  batch step: 141, loss: 34.58966827392578\n",
      "epoch: 95,  batch step: 142, loss: 14.459945678710938\n",
      "epoch: 95,  batch step: 143, loss: 3.313685178756714\n",
      "epoch: 95,  batch step: 144, loss: 3.0058341026306152\n",
      "epoch: 95,  batch step: 145, loss: 6.651873588562012\n",
      "epoch: 95,  batch step: 146, loss: 2.320507049560547\n",
      "epoch: 95,  batch step: 147, loss: 9.69651985168457\n",
      "epoch: 95,  batch step: 148, loss: 20.48662567138672\n",
      "epoch: 95,  batch step: 149, loss: 7.848660469055176\n",
      "epoch: 95,  batch step: 150, loss: 15.134069442749023\n",
      "epoch: 95,  batch step: 151, loss: 2.6741814613342285\n",
      "epoch: 95,  batch step: 152, loss: 5.4838666915893555\n",
      "epoch: 95,  batch step: 153, loss: 5.722127914428711\n",
      "epoch: 95,  batch step: 154, loss: 56.14148712158203\n",
      "epoch: 95,  batch step: 155, loss: 7.656876564025879\n",
      "epoch: 95,  batch step: 156, loss: 2.67997145652771\n",
      "epoch: 95,  batch step: 157, loss: 2.9576120376586914\n",
      "epoch: 95,  batch step: 158, loss: 2.181408405303955\n",
      "epoch: 95,  batch step: 159, loss: 20.48111343383789\n",
      "epoch: 95,  batch step: 160, loss: 16.985029220581055\n",
      "epoch: 95,  batch step: 161, loss: 3.1870381832122803\n",
      "epoch: 95,  batch step: 162, loss: 6.747720241546631\n",
      "epoch: 95,  batch step: 163, loss: 2.0874240398406982\n",
      "epoch: 95,  batch step: 164, loss: 15.118556022644043\n",
      "epoch: 95,  batch step: 165, loss: 26.868408203125\n",
      "epoch: 95,  batch step: 166, loss: 8.920906066894531\n",
      "epoch: 95,  batch step: 167, loss: 2.218398094177246\n",
      "epoch: 95,  batch step: 168, loss: 2.37557315826416\n",
      "epoch: 95,  batch step: 169, loss: 2.4342257976531982\n",
      "epoch: 95,  batch step: 170, loss: 2.116131544113159\n",
      "epoch: 95,  batch step: 171, loss: 2.765956401824951\n",
      "epoch: 95,  batch step: 172, loss: 2.6917121410369873\n",
      "epoch: 95,  batch step: 173, loss: 24.88155174255371\n",
      "epoch: 95,  batch step: 174, loss: 3.1722216606140137\n",
      "epoch: 95,  batch step: 175, loss: 4.152338027954102\n",
      "epoch: 95,  batch step: 176, loss: 20.240785598754883\n",
      "epoch: 95,  batch step: 177, loss: 2.431664228439331\n",
      "epoch: 95,  batch step: 178, loss: 3.3705573081970215\n",
      "epoch: 95,  batch step: 179, loss: 2.7236974239349365\n",
      "epoch: 95,  batch step: 180, loss: 14.789915084838867\n",
      "epoch: 95,  batch step: 181, loss: 44.386653900146484\n",
      "epoch: 95,  batch step: 182, loss: 10.312389373779297\n",
      "epoch: 95,  batch step: 183, loss: 13.528548240661621\n",
      "epoch: 95,  batch step: 184, loss: 8.814996719360352\n",
      "epoch: 95,  batch step: 185, loss: 1.8562781810760498\n",
      "epoch: 95,  batch step: 186, loss: 2.65608549118042\n",
      "epoch: 95,  batch step: 187, loss: 15.121915817260742\n",
      "epoch: 95,  batch step: 188, loss: 22.910314559936523\n",
      "epoch: 95,  batch step: 189, loss: 10.199110984802246\n",
      "epoch: 95,  batch step: 190, loss: 2.3834030628204346\n",
      "epoch: 95,  batch step: 191, loss: 2.18387508392334\n",
      "epoch: 95,  batch step: 192, loss: 6.437810897827148\n",
      "epoch: 95,  batch step: 193, loss: 3.2539749145507812\n",
      "epoch: 95,  batch step: 194, loss: 2.875333786010742\n",
      "epoch: 95,  batch step: 195, loss: 9.858497619628906\n",
      "epoch: 95,  batch step: 196, loss: 44.345664978027344\n",
      "epoch: 95,  batch step: 197, loss: 3.52485728263855\n",
      "epoch: 95,  batch step: 198, loss: 3.5969512462615967\n",
      "epoch: 95,  batch step: 199, loss: 7.923482418060303\n",
      "epoch: 95,  batch step: 200, loss: 3.404468297958374\n",
      "epoch: 95,  batch step: 201, loss: 9.623804092407227\n",
      "epoch: 95,  batch step: 202, loss: 6.979207515716553\n",
      "epoch: 95,  batch step: 203, loss: 10.239639282226562\n",
      "epoch: 95,  batch step: 204, loss: 10.44677734375\n",
      "epoch: 95,  batch step: 205, loss: 2.8321683406829834\n",
      "epoch: 95,  batch step: 206, loss: 4.816979885101318\n",
      "epoch: 95,  batch step: 207, loss: 2.1433560848236084\n",
      "epoch: 95,  batch step: 208, loss: 1.9362046718597412\n",
      "epoch: 95,  batch step: 209, loss: 1.6557958126068115\n",
      "epoch: 95,  batch step: 210, loss: 7.424832344055176\n",
      "epoch: 95,  batch step: 211, loss: 2.115246534347534\n",
      "epoch: 95,  batch step: 212, loss: 3.6370770931243896\n",
      "epoch: 95,  batch step: 213, loss: 34.90127182006836\n",
      "epoch: 95,  batch step: 214, loss: 17.568477630615234\n",
      "epoch: 95,  batch step: 215, loss: 24.616680145263672\n",
      "epoch: 95,  batch step: 216, loss: 7.958049774169922\n",
      "epoch: 95,  batch step: 217, loss: 2.6637396812438965\n",
      "epoch: 95,  batch step: 218, loss: 3.2364070415496826\n",
      "epoch: 95,  batch step: 219, loss: 2.0512008666992188\n",
      "epoch: 95,  batch step: 220, loss: 2.6909873485565186\n",
      "epoch: 95,  batch step: 221, loss: 4.111205577850342\n",
      "epoch: 95,  batch step: 222, loss: 5.3995490074157715\n",
      "epoch: 95,  batch step: 223, loss: 1.721774935722351\n",
      "epoch: 95,  batch step: 224, loss: 4.17378568649292\n",
      "epoch: 95,  batch step: 225, loss: 11.190977096557617\n",
      "epoch: 95,  batch step: 226, loss: 4.410717964172363\n",
      "epoch: 95,  batch step: 227, loss: 3.5953073501586914\n",
      "epoch: 95,  batch step: 228, loss: 3.0856118202209473\n",
      "epoch: 95,  batch step: 229, loss: 2.3808932304382324\n",
      "epoch: 95,  batch step: 230, loss: 2.8970391750335693\n",
      "epoch: 95,  batch step: 231, loss: 15.506570816040039\n",
      "epoch: 95,  batch step: 232, loss: 49.97550582885742\n",
      "epoch: 95,  batch step: 233, loss: 7.992131233215332\n",
      "epoch: 95,  batch step: 234, loss: 2.316063404083252\n",
      "epoch: 95,  batch step: 235, loss: 2.767457962036133\n",
      "epoch: 95,  batch step: 236, loss: 32.16550827026367\n",
      "epoch: 95,  batch step: 237, loss: 7.785317420959473\n",
      "epoch: 95,  batch step: 238, loss: 69.15782165527344\n",
      "epoch: 95,  batch step: 239, loss: 3.0550525188446045\n",
      "epoch: 95,  batch step: 240, loss: 12.593809127807617\n",
      "epoch: 95,  batch step: 241, loss: 4.1843767166137695\n",
      "epoch: 95,  batch step: 242, loss: 2.6421658992767334\n",
      "epoch: 95,  batch step: 243, loss: 7.231906414031982\n",
      "epoch: 95,  batch step: 244, loss: 15.893238067626953\n",
      "epoch: 95,  batch step: 245, loss: 133.8184814453125\n",
      "epoch: 95,  batch step: 246, loss: 2.375624656677246\n",
      "epoch: 95,  batch step: 247, loss: 2.1335248947143555\n",
      "epoch: 95,  batch step: 248, loss: 2.5600011348724365\n",
      "epoch: 95,  batch step: 249, loss: 8.038280487060547\n",
      "epoch: 95,  batch step: 250, loss: 2.371063232421875\n",
      "epoch: 95,  batch step: 251, loss: 126.45158386230469\n",
      "validation error epoch  95:    tensor(70.0418, device='cuda:0')\n",
      "316\n",
      "epoch: 96,  batch step: 0, loss: 2.5954294204711914\n",
      "epoch: 96,  batch step: 1, loss: 6.186864376068115\n",
      "epoch: 96,  batch step: 2, loss: 12.53913402557373\n",
      "epoch: 96,  batch step: 3, loss: 7.8214521408081055\n",
      "epoch: 96,  batch step: 4, loss: 6.452569484710693\n",
      "epoch: 96,  batch step: 5, loss: 90.75105285644531\n",
      "epoch: 96,  batch step: 6, loss: 11.472349166870117\n",
      "epoch: 96,  batch step: 7, loss: 49.56886291503906\n",
      "epoch: 96,  batch step: 8, loss: 9.346355438232422\n",
      "epoch: 96,  batch step: 9, loss: 106.2315902709961\n",
      "epoch: 96,  batch step: 10, loss: 72.45962524414062\n",
      "epoch: 96,  batch step: 11, loss: 7.845021724700928\n",
      "epoch: 96,  batch step: 12, loss: 8.428470611572266\n",
      "epoch: 96,  batch step: 13, loss: 48.67716598510742\n",
      "epoch: 96,  batch step: 14, loss: 8.767409324645996\n",
      "epoch: 96,  batch step: 15, loss: 39.05219268798828\n",
      "epoch: 96,  batch step: 16, loss: 13.775945663452148\n",
      "epoch: 96,  batch step: 17, loss: 9.161306381225586\n",
      "epoch: 96,  batch step: 18, loss: 63.66327667236328\n",
      "epoch: 96,  batch step: 19, loss: 28.893781661987305\n",
      "epoch: 96,  batch step: 20, loss: 18.14442253112793\n",
      "epoch: 96,  batch step: 21, loss: 23.416601181030273\n",
      "epoch: 96,  batch step: 22, loss: 7.581798553466797\n",
      "epoch: 96,  batch step: 23, loss: 10.0711088180542\n",
      "epoch: 96,  batch step: 24, loss: 4.349729061126709\n",
      "epoch: 96,  batch step: 25, loss: 4.557145595550537\n",
      "epoch: 96,  batch step: 26, loss: 14.630849838256836\n",
      "epoch: 96,  batch step: 27, loss: 8.109418869018555\n",
      "epoch: 96,  batch step: 28, loss: 80.21293640136719\n",
      "epoch: 96,  batch step: 29, loss: 27.848613739013672\n",
      "epoch: 96,  batch step: 30, loss: 7.428930282592773\n",
      "epoch: 96,  batch step: 31, loss: 3.738302230834961\n",
      "epoch: 96,  batch step: 32, loss: 26.76471519470215\n",
      "epoch: 96,  batch step: 33, loss: 17.988887786865234\n",
      "epoch: 96,  batch step: 34, loss: 20.321094512939453\n",
      "epoch: 96,  batch step: 35, loss: 5.385242462158203\n",
      "epoch: 96,  batch step: 36, loss: 17.24958610534668\n",
      "epoch: 96,  batch step: 37, loss: 12.306194305419922\n",
      "epoch: 96,  batch step: 38, loss: 27.24347686767578\n",
      "epoch: 96,  batch step: 39, loss: 8.005861282348633\n",
      "epoch: 96,  batch step: 40, loss: 4.90438175201416\n",
      "epoch: 96,  batch step: 41, loss: 25.254650115966797\n",
      "epoch: 96,  batch step: 42, loss: 18.110883712768555\n",
      "epoch: 96,  batch step: 43, loss: 16.545827865600586\n",
      "epoch: 96,  batch step: 44, loss: 17.488689422607422\n",
      "epoch: 96,  batch step: 45, loss: 28.017200469970703\n",
      "epoch: 96,  batch step: 46, loss: 26.072765350341797\n",
      "epoch: 96,  batch step: 47, loss: 18.380977630615234\n",
      "epoch: 96,  batch step: 48, loss: 3.7747955322265625\n",
      "epoch: 96,  batch step: 49, loss: 36.904563903808594\n",
      "epoch: 96,  batch step: 50, loss: 23.81148338317871\n",
      "epoch: 96,  batch step: 51, loss: 12.039709091186523\n",
      "epoch: 96,  batch step: 52, loss: 80.21800231933594\n",
      "epoch: 96,  batch step: 53, loss: 14.639961242675781\n",
      "epoch: 96,  batch step: 54, loss: 3.5128092765808105\n",
      "epoch: 96,  batch step: 55, loss: 3.678300380706787\n",
      "epoch: 96,  batch step: 56, loss: 7.998472213745117\n",
      "epoch: 96,  batch step: 57, loss: 53.399810791015625\n",
      "epoch: 96,  batch step: 58, loss: 3.9102065563201904\n",
      "epoch: 96,  batch step: 59, loss: 11.047679901123047\n",
      "epoch: 96,  batch step: 60, loss: 9.280745506286621\n",
      "epoch: 96,  batch step: 61, loss: 11.903226852416992\n",
      "epoch: 96,  batch step: 62, loss: 57.06240463256836\n",
      "epoch: 96,  batch step: 63, loss: 18.328460693359375\n",
      "epoch: 96,  batch step: 64, loss: 17.881071090698242\n",
      "epoch: 96,  batch step: 65, loss: 4.288942337036133\n",
      "epoch: 96,  batch step: 66, loss: 10.362260818481445\n",
      "epoch: 96,  batch step: 67, loss: 35.637821197509766\n",
      "epoch: 96,  batch step: 68, loss: 11.093043327331543\n",
      "epoch: 96,  batch step: 69, loss: 10.203577995300293\n",
      "epoch: 96,  batch step: 70, loss: 19.11389923095703\n",
      "epoch: 96,  batch step: 71, loss: 20.389286041259766\n",
      "epoch: 96,  batch step: 72, loss: 2.3855671882629395\n",
      "epoch: 96,  batch step: 73, loss: 7.711167812347412\n",
      "epoch: 96,  batch step: 74, loss: 31.02766990661621\n",
      "epoch: 96,  batch step: 75, loss: 52.246673583984375\n",
      "epoch: 96,  batch step: 76, loss: 49.20794677734375\n",
      "epoch: 96,  batch step: 77, loss: 22.587444305419922\n",
      "epoch: 96,  batch step: 78, loss: 3.6867504119873047\n",
      "epoch: 96,  batch step: 79, loss: 20.7407283782959\n",
      "epoch: 96,  batch step: 80, loss: 20.216707229614258\n",
      "epoch: 96,  batch step: 81, loss: 6.481740474700928\n",
      "epoch: 96,  batch step: 82, loss: 6.029084205627441\n",
      "epoch: 96,  batch step: 83, loss: 6.057086944580078\n",
      "epoch: 96,  batch step: 84, loss: 2.8137290477752686\n",
      "epoch: 96,  batch step: 85, loss: 20.002973556518555\n",
      "epoch: 96,  batch step: 86, loss: 3.8022847175598145\n",
      "epoch: 96,  batch step: 87, loss: 40.62465286254883\n",
      "epoch: 96,  batch step: 88, loss: 4.038576602935791\n",
      "epoch: 96,  batch step: 89, loss: 4.579550266265869\n",
      "epoch: 96,  batch step: 90, loss: 2.909085273742676\n",
      "epoch: 96,  batch step: 91, loss: 21.54144859313965\n",
      "epoch: 96,  batch step: 92, loss: 4.655108451843262\n",
      "epoch: 96,  batch step: 93, loss: 2.9539527893066406\n",
      "epoch: 96,  batch step: 94, loss: 3.9847571849823\n",
      "epoch: 96,  batch step: 95, loss: 5.300818920135498\n",
      "epoch: 96,  batch step: 96, loss: 11.245027542114258\n",
      "epoch: 96,  batch step: 97, loss: 33.94227981567383\n",
      "epoch: 96,  batch step: 98, loss: 22.608150482177734\n",
      "epoch: 96,  batch step: 99, loss: 69.19779968261719\n",
      "epoch: 96,  batch step: 100, loss: 3.9659667015075684\n",
      "epoch: 96,  batch step: 101, loss: 3.6714513301849365\n",
      "epoch: 96,  batch step: 102, loss: 55.98624801635742\n",
      "epoch: 96,  batch step: 103, loss: 59.32592010498047\n",
      "epoch: 96,  batch step: 104, loss: 3.815277099609375\n",
      "epoch: 96,  batch step: 105, loss: 6.953701019287109\n",
      "epoch: 96,  batch step: 106, loss: 10.70572566986084\n",
      "epoch: 96,  batch step: 107, loss: 13.672904014587402\n",
      "epoch: 96,  batch step: 108, loss: 62.61747360229492\n",
      "epoch: 96,  batch step: 109, loss: 45.86909484863281\n",
      "epoch: 96,  batch step: 110, loss: 40.75938415527344\n",
      "epoch: 96,  batch step: 111, loss: 2.775989532470703\n",
      "epoch: 96,  batch step: 112, loss: 4.069578647613525\n",
      "epoch: 96,  batch step: 113, loss: 13.642786026000977\n",
      "epoch: 96,  batch step: 114, loss: 4.8226823806762695\n",
      "epoch: 96,  batch step: 115, loss: 3.7224597930908203\n",
      "epoch: 96,  batch step: 116, loss: 4.743072509765625\n",
      "epoch: 96,  batch step: 117, loss: 3.0589985847473145\n",
      "epoch: 96,  batch step: 118, loss: 19.383270263671875\n",
      "epoch: 96,  batch step: 119, loss: 49.401123046875\n",
      "epoch: 96,  batch step: 120, loss: 28.261112213134766\n",
      "epoch: 96,  batch step: 121, loss: 12.69843864440918\n",
      "epoch: 96,  batch step: 122, loss: 5.564375877380371\n",
      "epoch: 96,  batch step: 123, loss: 28.722370147705078\n",
      "epoch: 96,  batch step: 124, loss: 11.228570938110352\n",
      "epoch: 96,  batch step: 125, loss: 3.628835439682007\n",
      "epoch: 96,  batch step: 126, loss: 3.8013343811035156\n",
      "epoch: 96,  batch step: 127, loss: 6.12884521484375\n",
      "epoch: 96,  batch step: 128, loss: 2.3979110717773438\n",
      "epoch: 96,  batch step: 129, loss: 15.078231811523438\n",
      "epoch: 96,  batch step: 130, loss: 4.981820106506348\n",
      "epoch: 96,  batch step: 131, loss: 28.04909324645996\n",
      "epoch: 96,  batch step: 132, loss: 27.40993881225586\n",
      "epoch: 96,  batch step: 133, loss: 2.507821559906006\n",
      "epoch: 96,  batch step: 134, loss: 20.247188568115234\n",
      "epoch: 96,  batch step: 135, loss: 4.588468074798584\n",
      "epoch: 96,  batch step: 136, loss: 33.491432189941406\n",
      "epoch: 96,  batch step: 137, loss: 4.143640041351318\n",
      "epoch: 96,  batch step: 138, loss: 29.241308212280273\n",
      "epoch: 96,  batch step: 139, loss: 3.341517210006714\n",
      "epoch: 96,  batch step: 140, loss: 15.680415153503418\n",
      "epoch: 96,  batch step: 141, loss: 20.03516960144043\n",
      "epoch: 96,  batch step: 142, loss: 7.317117214202881\n",
      "epoch: 96,  batch step: 143, loss: 10.635856628417969\n",
      "epoch: 96,  batch step: 144, loss: 13.241717338562012\n",
      "epoch: 96,  batch step: 145, loss: 3.185152530670166\n",
      "epoch: 96,  batch step: 146, loss: 21.997119903564453\n",
      "epoch: 96,  batch step: 147, loss: 6.486438751220703\n",
      "epoch: 96,  batch step: 148, loss: 47.74345397949219\n",
      "epoch: 96,  batch step: 149, loss: 8.147329330444336\n",
      "epoch: 96,  batch step: 150, loss: 5.597537040710449\n",
      "epoch: 96,  batch step: 151, loss: 25.27540397644043\n",
      "epoch: 96,  batch step: 152, loss: 67.75634765625\n",
      "epoch: 96,  batch step: 153, loss: 58.81257247924805\n",
      "epoch: 96,  batch step: 154, loss: 6.0707197189331055\n",
      "epoch: 96,  batch step: 155, loss: 4.1402506828308105\n",
      "epoch: 96,  batch step: 156, loss: 12.282366752624512\n",
      "epoch: 96,  batch step: 157, loss: 2.929368019104004\n",
      "epoch: 96,  batch step: 158, loss: 18.052778244018555\n",
      "epoch: 96,  batch step: 159, loss: 1.9247386455535889\n",
      "epoch: 96,  batch step: 160, loss: 18.237342834472656\n",
      "epoch: 96,  batch step: 161, loss: 52.86396408081055\n",
      "epoch: 96,  batch step: 162, loss: 11.439326286315918\n",
      "epoch: 96,  batch step: 163, loss: 12.465518951416016\n",
      "epoch: 96,  batch step: 164, loss: 11.577322959899902\n",
      "epoch: 96,  batch step: 165, loss: 4.459392547607422\n",
      "epoch: 96,  batch step: 166, loss: 2.280486583709717\n",
      "epoch: 96,  batch step: 167, loss: 3.164201498031616\n",
      "epoch: 96,  batch step: 168, loss: 2.5208749771118164\n",
      "epoch: 96,  batch step: 169, loss: 4.017901420593262\n",
      "epoch: 96,  batch step: 170, loss: 6.159587860107422\n",
      "epoch: 96,  batch step: 171, loss: 12.47037410736084\n",
      "epoch: 96,  batch step: 172, loss: 11.998157501220703\n",
      "epoch: 96,  batch step: 173, loss: 30.4312801361084\n",
      "epoch: 96,  batch step: 174, loss: 49.7570915222168\n",
      "epoch: 96,  batch step: 175, loss: 2.723849058151245\n",
      "epoch: 96,  batch step: 176, loss: 3.0322794914245605\n",
      "epoch: 96,  batch step: 177, loss: 6.2548828125\n",
      "epoch: 96,  batch step: 178, loss: 8.050188064575195\n",
      "epoch: 96,  batch step: 179, loss: 10.420560836791992\n",
      "epoch: 96,  batch step: 180, loss: 2.4256792068481445\n",
      "epoch: 96,  batch step: 181, loss: 2.3541855812072754\n",
      "epoch: 96,  batch step: 182, loss: 12.379622459411621\n",
      "epoch: 96,  batch step: 183, loss: 2.228759765625\n",
      "epoch: 96,  batch step: 184, loss: 3.081183433532715\n",
      "epoch: 96,  batch step: 185, loss: 4.247889995574951\n",
      "epoch: 96,  batch step: 186, loss: 3.628892421722412\n",
      "epoch: 96,  batch step: 187, loss: 17.565195083618164\n",
      "epoch: 96,  batch step: 188, loss: 5.268205642700195\n",
      "epoch: 96,  batch step: 189, loss: 1.9928816556930542\n",
      "epoch: 96,  batch step: 190, loss: 4.1600189208984375\n",
      "epoch: 96,  batch step: 191, loss: 61.488059997558594\n",
      "epoch: 96,  batch step: 192, loss: 36.34528732299805\n",
      "epoch: 96,  batch step: 193, loss: 27.417736053466797\n",
      "epoch: 96,  batch step: 194, loss: 19.669710159301758\n",
      "epoch: 96,  batch step: 195, loss: 2.6006264686584473\n",
      "epoch: 96,  batch step: 196, loss: 9.239105224609375\n",
      "epoch: 96,  batch step: 197, loss: 15.844550132751465\n",
      "epoch: 96,  batch step: 198, loss: 4.04245662689209\n",
      "epoch: 96,  batch step: 199, loss: 2.2033870220184326\n",
      "epoch: 96,  batch step: 200, loss: 27.32124900817871\n",
      "epoch: 96,  batch step: 201, loss: 3.374755382537842\n",
      "epoch: 96,  batch step: 202, loss: 3.86273193359375\n",
      "epoch: 96,  batch step: 203, loss: 5.277153968811035\n",
      "epoch: 96,  batch step: 204, loss: 14.788063049316406\n",
      "epoch: 96,  batch step: 205, loss: 2.0931098461151123\n",
      "epoch: 96,  batch step: 206, loss: 57.05474853515625\n",
      "epoch: 96,  batch step: 207, loss: 22.076335906982422\n",
      "epoch: 96,  batch step: 208, loss: 2.105421543121338\n",
      "epoch: 96,  batch step: 209, loss: 12.298480033874512\n",
      "epoch: 96,  batch step: 210, loss: 31.09742546081543\n",
      "epoch: 96,  batch step: 211, loss: 2.5845696926116943\n",
      "epoch: 96,  batch step: 212, loss: 18.037559509277344\n",
      "epoch: 96,  batch step: 213, loss: 1.7352294921875\n",
      "epoch: 96,  batch step: 214, loss: 34.4535026550293\n",
      "epoch: 96,  batch step: 215, loss: 34.261573791503906\n",
      "epoch: 96,  batch step: 216, loss: 21.506214141845703\n",
      "epoch: 96,  batch step: 217, loss: 3.2030701637268066\n",
      "epoch: 96,  batch step: 218, loss: 12.673210144042969\n",
      "epoch: 96,  batch step: 219, loss: 5.526358604431152\n",
      "epoch: 96,  batch step: 220, loss: 2.718428134918213\n",
      "epoch: 96,  batch step: 221, loss: 19.795698165893555\n",
      "epoch: 96,  batch step: 222, loss: 13.639368057250977\n",
      "epoch: 96,  batch step: 223, loss: 2.329819679260254\n",
      "epoch: 96,  batch step: 224, loss: 3.7721214294433594\n",
      "epoch: 96,  batch step: 225, loss: 4.747781753540039\n",
      "epoch: 96,  batch step: 226, loss: 2.4508883953094482\n",
      "epoch: 96,  batch step: 227, loss: 21.70064353942871\n",
      "epoch: 96,  batch step: 228, loss: 12.850212097167969\n",
      "epoch: 96,  batch step: 229, loss: 46.70050811767578\n",
      "epoch: 96,  batch step: 230, loss: 2.427855968475342\n",
      "epoch: 96,  batch step: 231, loss: 53.06307601928711\n",
      "epoch: 96,  batch step: 232, loss: 3.049031972885132\n",
      "epoch: 96,  batch step: 233, loss: 14.34449291229248\n",
      "epoch: 96,  batch step: 234, loss: 19.677520751953125\n",
      "epoch: 96,  batch step: 235, loss: 2.1616878509521484\n",
      "epoch: 96,  batch step: 236, loss: 5.4075846672058105\n",
      "epoch: 96,  batch step: 237, loss: 13.830973625183105\n",
      "epoch: 96,  batch step: 238, loss: 2.7700395584106445\n",
      "epoch: 96,  batch step: 239, loss: 3.1132686138153076\n",
      "epoch: 96,  batch step: 240, loss: 16.749948501586914\n",
      "epoch: 96,  batch step: 241, loss: 2.2850000858306885\n",
      "epoch: 96,  batch step: 242, loss: 21.0816650390625\n",
      "epoch: 96,  batch step: 243, loss: 7.350025177001953\n",
      "epoch: 96,  batch step: 244, loss: 7.419328212738037\n",
      "epoch: 96,  batch step: 245, loss: 21.927173614501953\n",
      "epoch: 96,  batch step: 246, loss: 2.5358214378356934\n",
      "epoch: 96,  batch step: 247, loss: 10.71562385559082\n",
      "epoch: 96,  batch step: 248, loss: 14.3236722946167\n",
      "epoch: 96,  batch step: 249, loss: 134.757568359375\n",
      "epoch: 96,  batch step: 250, loss: 30.86205291748047\n",
      "epoch: 96,  batch step: 251, loss: 64.50466918945312\n",
      "validation error epoch  96:    tensor(71.9060, device='cuda:0')\n",
      "316\n",
      "epoch: 97,  batch step: 0, loss: 17.68250274658203\n",
      "epoch: 97,  batch step: 1, loss: 18.32603645324707\n",
      "epoch: 97,  batch step: 2, loss: 4.247922897338867\n",
      "epoch: 97,  batch step: 3, loss: 8.046570777893066\n",
      "epoch: 97,  batch step: 4, loss: 42.31522750854492\n",
      "epoch: 97,  batch step: 5, loss: 2.7405622005462646\n",
      "epoch: 97,  batch step: 6, loss: 9.319297790527344\n",
      "epoch: 97,  batch step: 7, loss: 3.215938091278076\n",
      "epoch: 97,  batch step: 8, loss: 4.231350898742676\n",
      "epoch: 97,  batch step: 9, loss: 13.92464828491211\n",
      "epoch: 97,  batch step: 10, loss: 68.00128173828125\n",
      "epoch: 97,  batch step: 11, loss: 2.9654242992401123\n",
      "epoch: 97,  batch step: 12, loss: 7.465873718261719\n",
      "epoch: 97,  batch step: 13, loss: 11.371952056884766\n",
      "epoch: 97,  batch step: 14, loss: 7.371480464935303\n",
      "epoch: 97,  batch step: 15, loss: 3.90854811668396\n",
      "epoch: 97,  batch step: 16, loss: 26.71384048461914\n",
      "epoch: 97,  batch step: 17, loss: 2.762892484664917\n",
      "epoch: 97,  batch step: 18, loss: 13.64127254486084\n",
      "epoch: 97,  batch step: 19, loss: 3.429525375366211\n",
      "epoch: 97,  batch step: 20, loss: 31.681739807128906\n",
      "epoch: 97,  batch step: 21, loss: 6.962553977966309\n",
      "epoch: 97,  batch step: 22, loss: 6.309543609619141\n",
      "epoch: 97,  batch step: 23, loss: 9.637259483337402\n",
      "epoch: 97,  batch step: 24, loss: 3.904927968978882\n",
      "epoch: 97,  batch step: 25, loss: 2.586246967315674\n",
      "epoch: 97,  batch step: 26, loss: 2.807605743408203\n",
      "epoch: 97,  batch step: 27, loss: 3.78828763961792\n",
      "epoch: 97,  batch step: 28, loss: 13.41081428527832\n",
      "epoch: 97,  batch step: 29, loss: 6.364221572875977\n",
      "epoch: 97,  batch step: 30, loss: 3.04963755607605\n",
      "epoch: 97,  batch step: 31, loss: 11.672008514404297\n",
      "epoch: 97,  batch step: 32, loss: 2.482119560241699\n",
      "epoch: 97,  batch step: 33, loss: 12.819547653198242\n",
      "epoch: 97,  batch step: 34, loss: 10.445792198181152\n",
      "epoch: 97,  batch step: 35, loss: 50.72315979003906\n",
      "epoch: 97,  batch step: 36, loss: 3.6023528575897217\n",
      "epoch: 97,  batch step: 37, loss: 3.9387431144714355\n",
      "epoch: 97,  batch step: 38, loss: 10.727609634399414\n",
      "epoch: 97,  batch step: 39, loss: 2.3814234733581543\n",
      "epoch: 97,  batch step: 40, loss: 2.5873863697052\n",
      "epoch: 97,  batch step: 41, loss: 3.7605624198913574\n",
      "epoch: 97,  batch step: 42, loss: 57.82225799560547\n",
      "epoch: 97,  batch step: 43, loss: 16.611221313476562\n",
      "epoch: 97,  batch step: 44, loss: 2.352385997772217\n",
      "epoch: 97,  batch step: 45, loss: 2.262023448944092\n",
      "epoch: 97,  batch step: 46, loss: 6.889860153198242\n",
      "epoch: 97,  batch step: 47, loss: 22.806928634643555\n",
      "epoch: 97,  batch step: 48, loss: 8.647533416748047\n",
      "epoch: 97,  batch step: 49, loss: 2.182142734527588\n",
      "epoch: 97,  batch step: 50, loss: 7.861095428466797\n",
      "epoch: 97,  batch step: 51, loss: 8.805529594421387\n",
      "epoch: 97,  batch step: 52, loss: 12.733041763305664\n",
      "epoch: 97,  batch step: 53, loss: 3.3540096282958984\n",
      "epoch: 97,  batch step: 54, loss: 16.623987197875977\n",
      "epoch: 97,  batch step: 55, loss: 3.5082716941833496\n",
      "epoch: 97,  batch step: 56, loss: 8.894643783569336\n",
      "epoch: 97,  batch step: 57, loss: 29.30002212524414\n",
      "epoch: 97,  batch step: 58, loss: 33.588592529296875\n",
      "epoch: 97,  batch step: 59, loss: 6.810693264007568\n",
      "epoch: 97,  batch step: 60, loss: 4.298164367675781\n",
      "epoch: 97,  batch step: 61, loss: 2.3617568016052246\n",
      "epoch: 97,  batch step: 62, loss: 27.169639587402344\n",
      "epoch: 97,  batch step: 63, loss: 18.33894920349121\n",
      "epoch: 97,  batch step: 64, loss: 1.8895905017852783\n",
      "epoch: 97,  batch step: 65, loss: 2.61702299118042\n",
      "epoch: 97,  batch step: 66, loss: 2.559830665588379\n",
      "epoch: 97,  batch step: 67, loss: 49.556976318359375\n",
      "epoch: 97,  batch step: 68, loss: 4.278895378112793\n",
      "epoch: 97,  batch step: 69, loss: 2.789644956588745\n",
      "epoch: 97,  batch step: 70, loss: 8.407591819763184\n",
      "epoch: 97,  batch step: 71, loss: 26.441648483276367\n",
      "epoch: 97,  batch step: 72, loss: 3.4526631832122803\n",
      "epoch: 97,  batch step: 73, loss: 2.032838821411133\n",
      "epoch: 97,  batch step: 74, loss: 40.010986328125\n",
      "epoch: 97,  batch step: 75, loss: 54.12191390991211\n",
      "epoch: 97,  batch step: 76, loss: 27.963600158691406\n",
      "epoch: 97,  batch step: 77, loss: 2.5793490409851074\n",
      "epoch: 97,  batch step: 78, loss: 135.0460205078125\n",
      "epoch: 97,  batch step: 79, loss: 9.264408111572266\n",
      "epoch: 97,  batch step: 80, loss: 3.5108513832092285\n",
      "epoch: 97,  batch step: 81, loss: 2.3935890197753906\n",
      "epoch: 97,  batch step: 82, loss: 11.125883102416992\n",
      "epoch: 97,  batch step: 83, loss: 4.047008514404297\n",
      "epoch: 97,  batch step: 84, loss: 4.868111610412598\n",
      "epoch: 97,  batch step: 85, loss: 27.313352584838867\n",
      "epoch: 97,  batch step: 86, loss: 11.22203254699707\n",
      "epoch: 97,  batch step: 87, loss: 50.65599060058594\n",
      "epoch: 97,  batch step: 88, loss: 2.2102322578430176\n",
      "epoch: 97,  batch step: 89, loss: 28.738004684448242\n",
      "epoch: 97,  batch step: 90, loss: 10.794326782226562\n",
      "epoch: 97,  batch step: 91, loss: 17.980247497558594\n",
      "epoch: 97,  batch step: 92, loss: 53.6153678894043\n",
      "epoch: 97,  batch step: 93, loss: 12.671534538269043\n",
      "epoch: 97,  batch step: 94, loss: 28.848691940307617\n",
      "epoch: 97,  batch step: 95, loss: 3.8840370178222656\n",
      "epoch: 97,  batch step: 96, loss: 1.8480145931243896\n",
      "epoch: 97,  batch step: 97, loss: 25.85616683959961\n",
      "epoch: 97,  batch step: 98, loss: 3.1542415618896484\n",
      "epoch: 97,  batch step: 99, loss: 15.834124565124512\n",
      "epoch: 97,  batch step: 100, loss: 119.73941040039062\n",
      "epoch: 97,  batch step: 101, loss: 2.08113694190979\n",
      "epoch: 97,  batch step: 102, loss: 27.032413482666016\n",
      "epoch: 97,  batch step: 103, loss: 7.750480651855469\n",
      "epoch: 97,  batch step: 104, loss: 11.275308609008789\n",
      "epoch: 97,  batch step: 105, loss: 2.8212878704071045\n",
      "epoch: 97,  batch step: 106, loss: 2.1542868614196777\n",
      "epoch: 97,  batch step: 107, loss: 2.9549646377563477\n",
      "epoch: 97,  batch step: 108, loss: 9.53792667388916\n",
      "epoch: 97,  batch step: 109, loss: 1.322009563446045\n",
      "epoch: 97,  batch step: 110, loss: 3.526794910430908\n",
      "epoch: 97,  batch step: 111, loss: 2.3497440814971924\n",
      "epoch: 97,  batch step: 112, loss: 4.278043270111084\n",
      "epoch: 97,  batch step: 113, loss: 9.460982322692871\n",
      "epoch: 97,  batch step: 114, loss: 3.5186421871185303\n",
      "epoch: 97,  batch step: 115, loss: 21.046491622924805\n",
      "epoch: 97,  batch step: 116, loss: 2.4645142555236816\n",
      "epoch: 97,  batch step: 117, loss: 27.334854125976562\n",
      "epoch: 97,  batch step: 118, loss: 27.371875762939453\n",
      "epoch: 97,  batch step: 119, loss: 37.1650390625\n",
      "epoch: 97,  batch step: 120, loss: 2.101773738861084\n",
      "epoch: 97,  batch step: 121, loss: 13.610595703125\n",
      "epoch: 97,  batch step: 122, loss: 3.5066261291503906\n",
      "epoch: 97,  batch step: 123, loss: 3.5098819732666016\n",
      "epoch: 97,  batch step: 124, loss: 41.82833480834961\n",
      "epoch: 97,  batch step: 125, loss: 9.404455184936523\n",
      "epoch: 97,  batch step: 126, loss: 3.6761021614074707\n",
      "epoch: 97,  batch step: 127, loss: 3.5106894969940186\n",
      "epoch: 97,  batch step: 128, loss: 3.63667368888855\n",
      "epoch: 97,  batch step: 129, loss: 2.962916374206543\n",
      "epoch: 97,  batch step: 130, loss: 2.37182354927063\n",
      "epoch: 97,  batch step: 131, loss: 2.0572190284729004\n",
      "epoch: 97,  batch step: 132, loss: 2.356464385986328\n",
      "epoch: 97,  batch step: 133, loss: 3.514549970626831\n",
      "epoch: 97,  batch step: 134, loss: 15.366792678833008\n",
      "epoch: 97,  batch step: 135, loss: 4.416123390197754\n",
      "epoch: 97,  batch step: 136, loss: 47.53730773925781\n",
      "epoch: 97,  batch step: 137, loss: 2.3763227462768555\n",
      "epoch: 97,  batch step: 138, loss: 30.954769134521484\n",
      "epoch: 97,  batch step: 139, loss: 2.410207509994507\n",
      "epoch: 97,  batch step: 140, loss: 1.6260490417480469\n",
      "epoch: 97,  batch step: 141, loss: 15.989082336425781\n",
      "epoch: 97,  batch step: 142, loss: 17.64048957824707\n",
      "epoch: 97,  batch step: 143, loss: 3.631485939025879\n",
      "epoch: 97,  batch step: 144, loss: 1.7575883865356445\n",
      "epoch: 97,  batch step: 145, loss: 9.271763801574707\n",
      "epoch: 97,  batch step: 146, loss: 13.687544822692871\n",
      "epoch: 97,  batch step: 147, loss: 2.4321422576904297\n",
      "epoch: 97,  batch step: 148, loss: 18.175966262817383\n",
      "epoch: 97,  batch step: 149, loss: 12.266473770141602\n",
      "epoch: 97,  batch step: 150, loss: 15.379270553588867\n",
      "epoch: 97,  batch step: 151, loss: 2.8848843574523926\n",
      "epoch: 97,  batch step: 152, loss: 4.225183486938477\n",
      "epoch: 97,  batch step: 153, loss: 2.5263290405273438\n",
      "epoch: 97,  batch step: 154, loss: 23.7291316986084\n",
      "epoch: 97,  batch step: 155, loss: 26.56554412841797\n",
      "epoch: 97,  batch step: 156, loss: 9.888598442077637\n",
      "epoch: 97,  batch step: 157, loss: 2.151675224304199\n",
      "epoch: 97,  batch step: 158, loss: 31.043935775756836\n",
      "epoch: 97,  batch step: 159, loss: 1.9635156393051147\n",
      "epoch: 97,  batch step: 160, loss: 4.736024856567383\n",
      "epoch: 97,  batch step: 161, loss: 21.616870880126953\n",
      "epoch: 97,  batch step: 162, loss: 1.6576359272003174\n",
      "epoch: 97,  batch step: 163, loss: 15.70728588104248\n",
      "epoch: 97,  batch step: 164, loss: 2.903519630432129\n",
      "epoch: 97,  batch step: 165, loss: 7.509036064147949\n",
      "epoch: 97,  batch step: 166, loss: 4.0357666015625\n",
      "epoch: 97,  batch step: 167, loss: 2.929600477218628\n",
      "epoch: 97,  batch step: 168, loss: 6.939191818237305\n",
      "epoch: 97,  batch step: 169, loss: 1.8095191717147827\n",
      "epoch: 97,  batch step: 170, loss: 2.2624707221984863\n",
      "epoch: 97,  batch step: 171, loss: 45.59342956542969\n",
      "epoch: 97,  batch step: 172, loss: 10.88827133178711\n",
      "epoch: 97,  batch step: 173, loss: 14.899940490722656\n",
      "epoch: 97,  batch step: 174, loss: 12.374308586120605\n",
      "epoch: 97,  batch step: 175, loss: 3.439267158508301\n",
      "epoch: 97,  batch step: 176, loss: 2.59578800201416\n",
      "epoch: 97,  batch step: 177, loss: 1.9040883779525757\n",
      "epoch: 97,  batch step: 178, loss: 4.704061031341553\n",
      "epoch: 97,  batch step: 179, loss: 17.340652465820312\n",
      "epoch: 97,  batch step: 180, loss: 8.016478538513184\n",
      "epoch: 97,  batch step: 181, loss: 27.955991744995117\n",
      "epoch: 97,  batch step: 182, loss: 19.610523223876953\n",
      "epoch: 97,  batch step: 183, loss: 20.8249568939209\n",
      "epoch: 97,  batch step: 184, loss: 5.100465774536133\n",
      "epoch: 97,  batch step: 185, loss: 2.5000531673431396\n",
      "epoch: 97,  batch step: 186, loss: 3.8672285079956055\n",
      "epoch: 97,  batch step: 187, loss: 23.906700134277344\n",
      "epoch: 97,  batch step: 188, loss: 3.081934690475464\n",
      "epoch: 97,  batch step: 189, loss: 30.516071319580078\n",
      "epoch: 97,  batch step: 190, loss: 20.917095184326172\n",
      "epoch: 97,  batch step: 191, loss: 32.496768951416016\n",
      "epoch: 97,  batch step: 192, loss: 9.545562744140625\n",
      "epoch: 97,  batch step: 193, loss: 2.8280816078186035\n",
      "epoch: 97,  batch step: 194, loss: 49.83686447143555\n",
      "epoch: 97,  batch step: 195, loss: 4.311474800109863\n",
      "epoch: 97,  batch step: 196, loss: 41.35639190673828\n",
      "epoch: 97,  batch step: 197, loss: 10.093317031860352\n",
      "epoch: 97,  batch step: 198, loss: 3.102046489715576\n",
      "epoch: 97,  batch step: 199, loss: 1.864776611328125\n",
      "epoch: 97,  batch step: 200, loss: 3.2956435680389404\n",
      "epoch: 97,  batch step: 201, loss: 2.480940341949463\n",
      "epoch: 97,  batch step: 202, loss: 68.60398864746094\n",
      "epoch: 97,  batch step: 203, loss: 14.393965721130371\n",
      "epoch: 97,  batch step: 204, loss: 5.215238571166992\n",
      "epoch: 97,  batch step: 205, loss: 3.22772479057312\n",
      "epoch: 97,  batch step: 206, loss: 42.24700927734375\n",
      "epoch: 97,  batch step: 207, loss: 8.622188568115234\n",
      "epoch: 97,  batch step: 208, loss: 67.00032806396484\n",
      "epoch: 97,  batch step: 209, loss: 1.88370680809021\n",
      "epoch: 97,  batch step: 210, loss: 10.327899932861328\n",
      "epoch: 97,  batch step: 211, loss: 38.520362854003906\n",
      "epoch: 97,  batch step: 212, loss: 3.53332781791687\n",
      "epoch: 97,  batch step: 213, loss: 5.529332637786865\n",
      "epoch: 97,  batch step: 214, loss: 10.443412780761719\n",
      "epoch: 97,  batch step: 215, loss: 7.423936367034912\n",
      "epoch: 97,  batch step: 216, loss: 41.67601013183594\n",
      "epoch: 97,  batch step: 217, loss: 5.308241844177246\n",
      "epoch: 97,  batch step: 218, loss: 21.824764251708984\n",
      "epoch: 97,  batch step: 219, loss: 2.737812042236328\n",
      "epoch: 97,  batch step: 220, loss: 2.0649664402008057\n",
      "epoch: 97,  batch step: 221, loss: 17.352703094482422\n",
      "epoch: 97,  batch step: 222, loss: 4.532535076141357\n",
      "epoch: 97,  batch step: 223, loss: 37.15000915527344\n",
      "epoch: 97,  batch step: 224, loss: 2.7156035900115967\n",
      "epoch: 97,  batch step: 225, loss: 22.663969039916992\n",
      "epoch: 97,  batch step: 226, loss: 16.785865783691406\n",
      "epoch: 97,  batch step: 227, loss: 2.886669158935547\n",
      "epoch: 97,  batch step: 228, loss: 4.904648780822754\n",
      "epoch: 97,  batch step: 229, loss: 15.212199211120605\n",
      "epoch: 97,  batch step: 230, loss: 2.9292049407958984\n",
      "epoch: 97,  batch step: 231, loss: 14.933058738708496\n",
      "epoch: 97,  batch step: 232, loss: 1.7582309246063232\n",
      "epoch: 97,  batch step: 233, loss: 2.422989845275879\n",
      "epoch: 97,  batch step: 234, loss: 2.3010103702545166\n",
      "epoch: 97,  batch step: 235, loss: 2.098926544189453\n",
      "epoch: 97,  batch step: 236, loss: 2.8590407371520996\n",
      "epoch: 97,  batch step: 237, loss: 17.81661605834961\n",
      "epoch: 97,  batch step: 238, loss: 18.77292251586914\n",
      "epoch: 97,  batch step: 239, loss: 7.240239143371582\n",
      "epoch: 97,  batch step: 240, loss: 2.5037190914154053\n",
      "epoch: 97,  batch step: 241, loss: 4.778905868530273\n",
      "epoch: 97,  batch step: 242, loss: 2.9778947830200195\n",
      "epoch: 97,  batch step: 243, loss: 16.305400848388672\n",
      "epoch: 97,  batch step: 244, loss: 2.4323413372039795\n",
      "epoch: 97,  batch step: 245, loss: 12.061247825622559\n",
      "epoch: 97,  batch step: 246, loss: 2.5959725379943848\n",
      "epoch: 97,  batch step: 247, loss: 29.54401206970215\n",
      "epoch: 97,  batch step: 248, loss: 41.097347259521484\n",
      "epoch: 97,  batch step: 249, loss: 2.0806641578674316\n",
      "epoch: 97,  batch step: 250, loss: 2.3640875816345215\n",
      "epoch: 97,  batch step: 251, loss: 46.1369514465332\n",
      "validation error epoch  97:    tensor(68.0375, device='cuda:0')\n",
      "316\n",
      "epoch: 98,  batch step: 0, loss: 6.111995697021484\n",
      "epoch: 98,  batch step: 1, loss: 5.500890254974365\n",
      "epoch: 98,  batch step: 2, loss: 2.4517970085144043\n",
      "epoch: 98,  batch step: 3, loss: 12.257296562194824\n",
      "epoch: 98,  batch step: 4, loss: 17.81691551208496\n",
      "epoch: 98,  batch step: 5, loss: 4.243960380554199\n",
      "epoch: 98,  batch step: 6, loss: 3.6083316802978516\n",
      "epoch: 98,  batch step: 7, loss: 91.85374450683594\n",
      "epoch: 98,  batch step: 8, loss: 42.7219352722168\n",
      "epoch: 98,  batch step: 9, loss: 10.846418380737305\n",
      "epoch: 98,  batch step: 10, loss: 13.526390075683594\n",
      "epoch: 98,  batch step: 11, loss: 3.64437198638916\n",
      "epoch: 98,  batch step: 12, loss: 7.534989833831787\n",
      "epoch: 98,  batch step: 13, loss: 13.055915832519531\n",
      "epoch: 98,  batch step: 14, loss: 9.972804069519043\n",
      "epoch: 98,  batch step: 15, loss: 48.45314025878906\n",
      "epoch: 98,  batch step: 16, loss: 3.321122646331787\n",
      "epoch: 98,  batch step: 17, loss: 12.585249900817871\n",
      "epoch: 98,  batch step: 18, loss: 3.3008852005004883\n",
      "epoch: 98,  batch step: 19, loss: 12.384272575378418\n",
      "epoch: 98,  batch step: 20, loss: 6.317632675170898\n",
      "epoch: 98,  batch step: 21, loss: 24.098392486572266\n",
      "epoch: 98,  batch step: 22, loss: 13.032755851745605\n",
      "epoch: 98,  batch step: 23, loss: 9.086576461791992\n",
      "epoch: 98,  batch step: 24, loss: 15.728361129760742\n",
      "epoch: 98,  batch step: 25, loss: 13.552584648132324\n",
      "epoch: 98,  batch step: 26, loss: 2.5186715126037598\n",
      "epoch: 98,  batch step: 27, loss: 2.8961992263793945\n",
      "epoch: 98,  batch step: 28, loss: 2.801093578338623\n",
      "epoch: 98,  batch step: 29, loss: 3.6029014587402344\n",
      "epoch: 98,  batch step: 30, loss: 4.566494941711426\n",
      "epoch: 98,  batch step: 31, loss: 2.5105082988739014\n",
      "epoch: 98,  batch step: 32, loss: 1.7808096408843994\n",
      "epoch: 98,  batch step: 33, loss: 10.872363090515137\n",
      "epoch: 98,  batch step: 34, loss: 32.97262191772461\n",
      "epoch: 98,  batch step: 35, loss: 79.3526382446289\n",
      "epoch: 98,  batch step: 36, loss: 2.15189790725708\n",
      "epoch: 98,  batch step: 37, loss: 7.377680778503418\n",
      "epoch: 98,  batch step: 38, loss: 2.687809705734253\n",
      "epoch: 98,  batch step: 39, loss: 4.855503082275391\n",
      "epoch: 98,  batch step: 40, loss: 56.5020866394043\n",
      "epoch: 98,  batch step: 41, loss: 8.276922225952148\n",
      "epoch: 98,  batch step: 42, loss: 7.035720348358154\n",
      "epoch: 98,  batch step: 43, loss: 6.236689567565918\n",
      "epoch: 98,  batch step: 44, loss: 48.97196960449219\n",
      "epoch: 98,  batch step: 45, loss: 4.910412311553955\n",
      "epoch: 98,  batch step: 46, loss: 1.813054084777832\n",
      "epoch: 98,  batch step: 47, loss: 30.079641342163086\n",
      "epoch: 98,  batch step: 48, loss: 2.0800037384033203\n",
      "epoch: 98,  batch step: 49, loss: 3.0800271034240723\n",
      "epoch: 98,  batch step: 50, loss: 17.81266975402832\n",
      "epoch: 98,  batch step: 51, loss: 4.447179794311523\n",
      "epoch: 98,  batch step: 52, loss: 8.250043869018555\n",
      "epoch: 98,  batch step: 53, loss: 2.0689141750335693\n",
      "epoch: 98,  batch step: 54, loss: 11.614141464233398\n",
      "epoch: 98,  batch step: 55, loss: 1.6310372352600098\n",
      "epoch: 98,  batch step: 56, loss: 4.0596418380737305\n",
      "epoch: 98,  batch step: 57, loss: 2.939943790435791\n",
      "epoch: 98,  batch step: 58, loss: 7.690067291259766\n",
      "epoch: 98,  batch step: 59, loss: 67.86155700683594\n",
      "epoch: 98,  batch step: 60, loss: 3.503884792327881\n",
      "epoch: 98,  batch step: 61, loss: 9.204760551452637\n",
      "epoch: 98,  batch step: 62, loss: 2.3763883113861084\n",
      "epoch: 98,  batch step: 63, loss: 1.461876392364502\n",
      "epoch: 98,  batch step: 64, loss: 132.25482177734375\n",
      "epoch: 98,  batch step: 65, loss: 10.083396911621094\n",
      "epoch: 98,  batch step: 66, loss: 4.055063247680664\n",
      "epoch: 98,  batch step: 67, loss: 5.394590377807617\n",
      "epoch: 98,  batch step: 68, loss: 2.9369163513183594\n",
      "epoch: 98,  batch step: 69, loss: 1.7802808284759521\n",
      "epoch: 98,  batch step: 70, loss: 18.858097076416016\n",
      "epoch: 98,  batch step: 71, loss: 2.451827049255371\n",
      "epoch: 98,  batch step: 72, loss: 18.549030303955078\n",
      "epoch: 98,  batch step: 73, loss: 7.501972198486328\n",
      "epoch: 98,  batch step: 74, loss: 2.6290454864501953\n",
      "epoch: 98,  batch step: 75, loss: 18.897010803222656\n",
      "epoch: 98,  batch step: 76, loss: 20.590190887451172\n",
      "epoch: 98,  batch step: 77, loss: 15.796823501586914\n",
      "epoch: 98,  batch step: 78, loss: 15.034463882446289\n",
      "epoch: 98,  batch step: 79, loss: 2.4306766986846924\n",
      "epoch: 98,  batch step: 80, loss: 8.176315307617188\n",
      "epoch: 98,  batch step: 81, loss: 5.35284423828125\n",
      "epoch: 98,  batch step: 82, loss: 3.758104085922241\n",
      "epoch: 98,  batch step: 83, loss: 7.7442426681518555\n",
      "epoch: 98,  batch step: 84, loss: 4.303075313568115\n",
      "epoch: 98,  batch step: 85, loss: 19.104557037353516\n",
      "epoch: 98,  batch step: 86, loss: 8.180510520935059\n",
      "epoch: 98,  batch step: 87, loss: 29.444128036499023\n",
      "epoch: 98,  batch step: 88, loss: 6.610713005065918\n",
      "epoch: 98,  batch step: 89, loss: 2.028949499130249\n",
      "epoch: 98,  batch step: 90, loss: 6.7660627365112305\n",
      "epoch: 98,  batch step: 91, loss: 3.898710250854492\n",
      "epoch: 98,  batch step: 92, loss: 14.960765838623047\n",
      "epoch: 98,  batch step: 93, loss: 3.019869804382324\n",
      "epoch: 98,  batch step: 94, loss: 2.8951430320739746\n",
      "epoch: 98,  batch step: 95, loss: 2.5269765853881836\n",
      "epoch: 98,  batch step: 96, loss: 18.99440574645996\n",
      "epoch: 98,  batch step: 97, loss: 51.24937438964844\n",
      "epoch: 98,  batch step: 98, loss: 7.655986785888672\n",
      "epoch: 98,  batch step: 99, loss: 23.50738525390625\n",
      "epoch: 98,  batch step: 100, loss: 43.942787170410156\n",
      "epoch: 98,  batch step: 101, loss: 41.70238494873047\n",
      "epoch: 98,  batch step: 102, loss: 16.713430404663086\n",
      "epoch: 98,  batch step: 103, loss: 10.177616119384766\n",
      "epoch: 98,  batch step: 104, loss: 2.104173183441162\n",
      "epoch: 98,  batch step: 105, loss: 5.476890563964844\n",
      "epoch: 98,  batch step: 106, loss: 2.2577903270721436\n",
      "epoch: 98,  batch step: 107, loss: 27.42462730407715\n",
      "epoch: 98,  batch step: 108, loss: 14.093356132507324\n",
      "epoch: 98,  batch step: 109, loss: 29.875545501708984\n",
      "epoch: 98,  batch step: 110, loss: 2.6399502754211426\n",
      "epoch: 98,  batch step: 111, loss: 14.015716552734375\n",
      "epoch: 98,  batch step: 112, loss: 7.265838146209717\n",
      "epoch: 98,  batch step: 113, loss: 4.012452602386475\n",
      "epoch: 98,  batch step: 114, loss: 2.4448447227478027\n",
      "epoch: 98,  batch step: 115, loss: 16.377731323242188\n",
      "epoch: 98,  batch step: 116, loss: 2.7915587425231934\n",
      "epoch: 98,  batch step: 117, loss: 2.4150514602661133\n",
      "epoch: 98,  batch step: 118, loss: 43.75179672241211\n",
      "epoch: 98,  batch step: 119, loss: 16.01587677001953\n",
      "epoch: 98,  batch step: 120, loss: 2.5864944458007812\n",
      "epoch: 98,  batch step: 121, loss: 26.450164794921875\n",
      "epoch: 98,  batch step: 122, loss: 29.105268478393555\n",
      "epoch: 98,  batch step: 123, loss: 4.670938491821289\n",
      "epoch: 98,  batch step: 124, loss: 2.3369431495666504\n",
      "epoch: 98,  batch step: 125, loss: 3.284126043319702\n",
      "epoch: 98,  batch step: 126, loss: 2.569016933441162\n",
      "epoch: 98,  batch step: 127, loss: 49.455806732177734\n",
      "epoch: 98,  batch step: 128, loss: 6.546095848083496\n",
      "epoch: 98,  batch step: 129, loss: 3.21820068359375\n",
      "epoch: 98,  batch step: 130, loss: 1.986891746520996\n",
      "epoch: 98,  batch step: 131, loss: 2.0755674839019775\n",
      "epoch: 98,  batch step: 132, loss: 4.37348747253418\n",
      "epoch: 98,  batch step: 133, loss: 5.86731481552124\n",
      "epoch: 98,  batch step: 134, loss: 2.122121572494507\n",
      "epoch: 98,  batch step: 135, loss: 23.454975128173828\n",
      "epoch: 98,  batch step: 136, loss: 11.473146438598633\n",
      "epoch: 98,  batch step: 137, loss: 16.897518157958984\n",
      "epoch: 98,  batch step: 138, loss: 2.6837337017059326\n",
      "epoch: 98,  batch step: 139, loss: 2.775303363800049\n",
      "epoch: 98,  batch step: 140, loss: 26.69220733642578\n",
      "epoch: 98,  batch step: 141, loss: 9.485118865966797\n",
      "epoch: 98,  batch step: 142, loss: 46.783653259277344\n",
      "epoch: 98,  batch step: 143, loss: 9.002689361572266\n",
      "epoch: 98,  batch step: 144, loss: 22.72804832458496\n",
      "epoch: 98,  batch step: 145, loss: 1.4027576446533203\n",
      "epoch: 98,  batch step: 146, loss: 1.8829684257507324\n",
      "epoch: 98,  batch step: 147, loss: 5.884957313537598\n",
      "epoch: 98,  batch step: 148, loss: 3.3031673431396484\n",
      "epoch: 98,  batch step: 149, loss: 2.0815391540527344\n",
      "epoch: 98,  batch step: 150, loss: 24.06561851501465\n",
      "epoch: 98,  batch step: 151, loss: 4.241868019104004\n",
      "epoch: 98,  batch step: 152, loss: 4.779989242553711\n",
      "epoch: 98,  batch step: 153, loss: 1.8354718685150146\n",
      "epoch: 98,  batch step: 154, loss: 33.85641098022461\n",
      "epoch: 98,  batch step: 155, loss: 6.748919486999512\n",
      "epoch: 98,  batch step: 156, loss: 10.299324035644531\n",
      "epoch: 98,  batch step: 157, loss: 2.9820191860198975\n",
      "epoch: 98,  batch step: 158, loss: 1.589234709739685\n",
      "epoch: 98,  batch step: 159, loss: 2.7196807861328125\n",
      "epoch: 98,  batch step: 160, loss: 14.03930950164795\n",
      "epoch: 98,  batch step: 161, loss: 11.642291069030762\n",
      "epoch: 98,  batch step: 162, loss: 17.701175689697266\n",
      "epoch: 98,  batch step: 163, loss: 2.2590835094451904\n",
      "epoch: 98,  batch step: 164, loss: 5.029284477233887\n",
      "epoch: 98,  batch step: 165, loss: 48.37298583984375\n",
      "epoch: 98,  batch step: 166, loss: 11.788642883300781\n",
      "epoch: 98,  batch step: 167, loss: 1.9558042287826538\n",
      "epoch: 98,  batch step: 168, loss: 2.4398484230041504\n",
      "epoch: 98,  batch step: 169, loss: 11.19187068939209\n",
      "epoch: 98,  batch step: 170, loss: 2.5046308040618896\n",
      "epoch: 98,  batch step: 171, loss: 1.9145915508270264\n",
      "epoch: 98,  batch step: 172, loss: 7.292244911193848\n",
      "epoch: 98,  batch step: 173, loss: 18.920883178710938\n",
      "epoch: 98,  batch step: 174, loss: 2.2645654678344727\n",
      "epoch: 98,  batch step: 175, loss: 14.122798919677734\n",
      "epoch: 98,  batch step: 176, loss: 4.753135681152344\n",
      "epoch: 98,  batch step: 177, loss: 2.374244213104248\n",
      "epoch: 98,  batch step: 178, loss: 25.213529586791992\n",
      "epoch: 98,  batch step: 179, loss: 3.6181790828704834\n",
      "epoch: 98,  batch step: 180, loss: 2.479837656021118\n",
      "epoch: 98,  batch step: 181, loss: 6.829793453216553\n",
      "epoch: 98,  batch step: 182, loss: 1.879457950592041\n",
      "epoch: 98,  batch step: 183, loss: 13.765981674194336\n",
      "epoch: 98,  batch step: 184, loss: 5.204689025878906\n",
      "epoch: 98,  batch step: 185, loss: 2.210824966430664\n",
      "epoch: 98,  batch step: 186, loss: 11.879783630371094\n",
      "epoch: 98,  batch step: 187, loss: 15.101360321044922\n",
      "epoch: 98,  batch step: 188, loss: 68.42479705810547\n",
      "epoch: 98,  batch step: 189, loss: 5.535236358642578\n",
      "epoch: 98,  batch step: 190, loss: 9.365005493164062\n",
      "epoch: 98,  batch step: 191, loss: 27.179580688476562\n",
      "epoch: 98,  batch step: 192, loss: 27.067920684814453\n",
      "epoch: 98,  batch step: 193, loss: 3.9337356090545654\n",
      "epoch: 98,  batch step: 194, loss: 2.4188263416290283\n",
      "epoch: 98,  batch step: 195, loss: 5.002757549285889\n",
      "epoch: 98,  batch step: 196, loss: 2.589003086090088\n",
      "epoch: 98,  batch step: 197, loss: 14.647513389587402\n",
      "epoch: 98,  batch step: 198, loss: 3.4429385662078857\n",
      "epoch: 98,  batch step: 199, loss: 2.219404697418213\n",
      "epoch: 98,  batch step: 200, loss: 10.976847648620605\n",
      "epoch: 98,  batch step: 201, loss: 29.546628952026367\n",
      "epoch: 98,  batch step: 202, loss: 14.4703369140625\n",
      "epoch: 98,  batch step: 203, loss: 3.752908706665039\n",
      "epoch: 98,  batch step: 204, loss: 1.9388797283172607\n",
      "epoch: 98,  batch step: 205, loss: 3.211421489715576\n",
      "epoch: 98,  batch step: 206, loss: 16.50543975830078\n",
      "epoch: 98,  batch step: 207, loss: 14.639579772949219\n",
      "epoch: 98,  batch step: 208, loss: 34.23290252685547\n",
      "epoch: 98,  batch step: 209, loss: 12.494819641113281\n",
      "epoch: 98,  batch step: 210, loss: 18.297683715820312\n",
      "epoch: 98,  batch step: 211, loss: 13.016463279724121\n",
      "epoch: 98,  batch step: 212, loss: 16.190425872802734\n",
      "epoch: 98,  batch step: 213, loss: 7.131455421447754\n",
      "epoch: 98,  batch step: 214, loss: 5.444690704345703\n",
      "epoch: 98,  batch step: 215, loss: 12.001729965209961\n",
      "epoch: 98,  batch step: 216, loss: 10.201468467712402\n",
      "epoch: 98,  batch step: 217, loss: 17.77753257751465\n",
      "epoch: 98,  batch step: 218, loss: 8.046892166137695\n",
      "epoch: 98,  batch step: 219, loss: 6.751346588134766\n",
      "epoch: 98,  batch step: 220, loss: 2.5831146240234375\n",
      "epoch: 98,  batch step: 221, loss: 2.3674724102020264\n",
      "epoch: 98,  batch step: 222, loss: 14.301467895507812\n",
      "epoch: 98,  batch step: 223, loss: 2.194528102874756\n",
      "epoch: 98,  batch step: 224, loss: 6.595430374145508\n",
      "epoch: 98,  batch step: 225, loss: 5.29106330871582\n",
      "epoch: 98,  batch step: 226, loss: 26.182851791381836\n",
      "epoch: 98,  batch step: 227, loss: 8.451173782348633\n",
      "epoch: 98,  batch step: 228, loss: 2.1507468223571777\n",
      "epoch: 98,  batch step: 229, loss: 16.41627311706543\n",
      "epoch: 98,  batch step: 230, loss: 1.5642905235290527\n",
      "epoch: 98,  batch step: 231, loss: 1.722163438796997\n",
      "epoch: 98,  batch step: 232, loss: 13.889852523803711\n",
      "epoch: 98,  batch step: 233, loss: 7.710292816162109\n",
      "epoch: 98,  batch step: 234, loss: 27.908931732177734\n",
      "epoch: 98,  batch step: 235, loss: 5.762877941131592\n",
      "epoch: 98,  batch step: 236, loss: 15.347606658935547\n",
      "epoch: 98,  batch step: 237, loss: 10.064369201660156\n",
      "epoch: 98,  batch step: 238, loss: 12.402069091796875\n",
      "epoch: 98,  batch step: 239, loss: 1.8135379552841187\n",
      "epoch: 98,  batch step: 240, loss: 3.156646728515625\n",
      "epoch: 98,  batch step: 241, loss: 9.196551322937012\n",
      "epoch: 98,  batch step: 242, loss: 2.911787986755371\n",
      "epoch: 98,  batch step: 243, loss: 2.6635780334472656\n",
      "epoch: 98,  batch step: 244, loss: 2.7286603450775146\n",
      "epoch: 98,  batch step: 245, loss: 2.300161123275757\n",
      "epoch: 98,  batch step: 246, loss: 3.9131922721862793\n",
      "epoch: 98,  batch step: 247, loss: 1.9288676977157593\n",
      "epoch: 98,  batch step: 248, loss: 4.441778182983398\n",
      "epoch: 98,  batch step: 249, loss: 2.339475154876709\n",
      "epoch: 98,  batch step: 250, loss: 11.246199607849121\n",
      "epoch: 98,  batch step: 251, loss: 52.41057586669922\n",
      "validation error epoch  98:    tensor(68.1627, device='cuda:0')\n",
      "316\n",
      "epoch: 99,  batch step: 0, loss: 3.015007495880127\n",
      "epoch: 99,  batch step: 1, loss: 4.155137538909912\n",
      "epoch: 99,  batch step: 2, loss: 29.379179000854492\n",
      "epoch: 99,  batch step: 3, loss: 25.481298446655273\n",
      "epoch: 99,  batch step: 4, loss: 12.366455078125\n",
      "epoch: 99,  batch step: 5, loss: 14.230158805847168\n",
      "epoch: 99,  batch step: 6, loss: 2.2253215312957764\n",
      "epoch: 99,  batch step: 7, loss: 9.800156593322754\n",
      "epoch: 99,  batch step: 8, loss: 12.599294662475586\n",
      "epoch: 99,  batch step: 9, loss: 7.170766353607178\n",
      "epoch: 99,  batch step: 10, loss: 85.5852279663086\n",
      "epoch: 99,  batch step: 11, loss: 4.332616329193115\n",
      "epoch: 99,  batch step: 12, loss: 15.303407669067383\n",
      "epoch: 99,  batch step: 13, loss: 17.117145538330078\n",
      "epoch: 99,  batch step: 14, loss: 2.0614781379699707\n",
      "epoch: 99,  batch step: 15, loss: 4.6324052810668945\n",
      "epoch: 99,  batch step: 16, loss: 12.631224632263184\n",
      "epoch: 99,  batch step: 17, loss: 6.333230495452881\n",
      "epoch: 99,  batch step: 18, loss: 2.512066602706909\n",
      "epoch: 99,  batch step: 19, loss: 5.019662857055664\n",
      "epoch: 99,  batch step: 20, loss: 1.8406332731246948\n",
      "epoch: 99,  batch step: 21, loss: 15.691473007202148\n",
      "epoch: 99,  batch step: 22, loss: 2.2556304931640625\n",
      "epoch: 99,  batch step: 23, loss: 35.116607666015625\n",
      "epoch: 99,  batch step: 24, loss: 12.606525421142578\n",
      "epoch: 99,  batch step: 25, loss: 2.69381046295166\n",
      "epoch: 99,  batch step: 26, loss: 13.717514038085938\n",
      "epoch: 99,  batch step: 27, loss: 31.09012222290039\n",
      "epoch: 99,  batch step: 28, loss: 2.9424073696136475\n",
      "epoch: 99,  batch step: 29, loss: 43.884788513183594\n",
      "epoch: 99,  batch step: 30, loss: 13.237985610961914\n",
      "epoch: 99,  batch step: 31, loss: 9.654024124145508\n",
      "epoch: 99,  batch step: 32, loss: 2.356757640838623\n",
      "epoch: 99,  batch step: 33, loss: 2.148690700531006\n",
      "epoch: 99,  batch step: 34, loss: 3.0716798305511475\n",
      "epoch: 99,  batch step: 35, loss: 2.3858823776245117\n",
      "epoch: 99,  batch step: 36, loss: 3.8809666633605957\n",
      "epoch: 99,  batch step: 37, loss: 39.81700897216797\n",
      "epoch: 99,  batch step: 38, loss: 22.048072814941406\n",
      "epoch: 99,  batch step: 39, loss: 9.420378684997559\n",
      "epoch: 99,  batch step: 40, loss: 2.4116930961608887\n",
      "epoch: 99,  batch step: 41, loss: 3.178466796875\n",
      "epoch: 99,  batch step: 42, loss: 13.599063873291016\n",
      "epoch: 99,  batch step: 43, loss: 21.89920425415039\n",
      "epoch: 99,  batch step: 44, loss: 10.119800567626953\n",
      "epoch: 99,  batch step: 45, loss: 9.864953994750977\n",
      "epoch: 99,  batch step: 46, loss: 48.803916931152344\n",
      "epoch: 99,  batch step: 47, loss: 2.5790555477142334\n",
      "epoch: 99,  batch step: 48, loss: 12.55324935913086\n",
      "epoch: 99,  batch step: 49, loss: 25.680158615112305\n",
      "epoch: 99,  batch step: 50, loss: 2.612828493118286\n",
      "epoch: 99,  batch step: 51, loss: 11.940058708190918\n",
      "epoch: 99,  batch step: 52, loss: 24.3015193939209\n",
      "epoch: 99,  batch step: 53, loss: 3.2200675010681152\n",
      "epoch: 99,  batch step: 54, loss: 3.2117958068847656\n",
      "epoch: 99,  batch step: 55, loss: 2.918886184692383\n",
      "epoch: 99,  batch step: 56, loss: 9.798337936401367\n",
      "epoch: 99,  batch step: 57, loss: 2.9004154205322266\n",
      "epoch: 99,  batch step: 58, loss: 21.05109977722168\n",
      "epoch: 99,  batch step: 59, loss: 13.73224925994873\n",
      "epoch: 99,  batch step: 60, loss: 10.171751022338867\n",
      "epoch: 99,  batch step: 61, loss: 4.1873369216918945\n",
      "epoch: 99,  batch step: 62, loss: 1.8244813680648804\n",
      "epoch: 99,  batch step: 63, loss: 2.271480083465576\n",
      "epoch: 99,  batch step: 64, loss: 3.4766650199890137\n",
      "epoch: 99,  batch step: 65, loss: 2.141765594482422\n",
      "epoch: 99,  batch step: 66, loss: 2.926084518432617\n",
      "epoch: 99,  batch step: 67, loss: 7.648963451385498\n",
      "epoch: 99,  batch step: 68, loss: 28.794057846069336\n",
      "epoch: 99,  batch step: 69, loss: 3.2362582683563232\n",
      "epoch: 99,  batch step: 70, loss: 2.1200194358825684\n",
      "epoch: 99,  batch step: 71, loss: 17.292133331298828\n",
      "epoch: 99,  batch step: 72, loss: 8.140560150146484\n",
      "epoch: 99,  batch step: 73, loss: 2.7920942306518555\n",
      "epoch: 99,  batch step: 74, loss: 12.847631454467773\n",
      "epoch: 99,  batch step: 75, loss: 2.226485252380371\n",
      "epoch: 99,  batch step: 76, loss: 3.173154354095459\n",
      "epoch: 99,  batch step: 77, loss: 10.080833435058594\n",
      "epoch: 99,  batch step: 78, loss: 2.6515989303588867\n",
      "epoch: 99,  batch step: 79, loss: 1.679422378540039\n",
      "epoch: 99,  batch step: 80, loss: 2.0439205169677734\n",
      "epoch: 99,  batch step: 81, loss: 2.166264057159424\n",
      "epoch: 99,  batch step: 82, loss: 1.8583641052246094\n",
      "epoch: 99,  batch step: 83, loss: 1.7332900762557983\n",
      "epoch: 99,  batch step: 84, loss: 51.05048751831055\n",
      "epoch: 99,  batch step: 85, loss: 40.23040771484375\n",
      "epoch: 99,  batch step: 86, loss: 2.2885873317718506\n",
      "epoch: 99,  batch step: 87, loss: 6.538329124450684\n",
      "epoch: 99,  batch step: 88, loss: 15.368277549743652\n",
      "epoch: 99,  batch step: 89, loss: 2.4247570037841797\n",
      "epoch: 99,  batch step: 90, loss: 2.4743728637695312\n",
      "epoch: 99,  batch step: 91, loss: 16.077150344848633\n",
      "epoch: 99,  batch step: 92, loss: 2.4807934761047363\n",
      "epoch: 99,  batch step: 93, loss: 2.132127523422241\n",
      "epoch: 99,  batch step: 94, loss: 1.7008941173553467\n",
      "epoch: 99,  batch step: 95, loss: 12.328115463256836\n",
      "epoch: 99,  batch step: 96, loss: 9.973810195922852\n",
      "epoch: 99,  batch step: 97, loss: 118.77071380615234\n",
      "epoch: 99,  batch step: 98, loss: 10.796652793884277\n",
      "epoch: 99,  batch step: 99, loss: 2.78946852684021\n",
      "epoch: 99,  batch step: 100, loss: 3.5795788764953613\n",
      "epoch: 99,  batch step: 101, loss: 2.3568906784057617\n",
      "epoch: 99,  batch step: 102, loss: 2.1564688682556152\n",
      "epoch: 99,  batch step: 103, loss: 2.0498478412628174\n",
      "epoch: 99,  batch step: 104, loss: 1.4057859182357788\n",
      "epoch: 99,  batch step: 105, loss: 1.8233369588851929\n",
      "epoch: 99,  batch step: 106, loss: 25.64568328857422\n",
      "epoch: 99,  batch step: 107, loss: 2.180001974105835\n",
      "epoch: 99,  batch step: 108, loss: 1.7648708820343018\n",
      "epoch: 99,  batch step: 109, loss: 5.113844871520996\n",
      "epoch: 99,  batch step: 110, loss: 10.576603889465332\n",
      "epoch: 99,  batch step: 111, loss: 3.3224618434906006\n",
      "epoch: 99,  batch step: 112, loss: 18.324283599853516\n",
      "epoch: 99,  batch step: 113, loss: 6.816370964050293\n",
      "epoch: 99,  batch step: 114, loss: 26.299453735351562\n",
      "epoch: 99,  batch step: 115, loss: 2.8725574016571045\n",
      "epoch: 99,  batch step: 116, loss: 48.694190979003906\n",
      "epoch: 99,  batch step: 117, loss: 2.3290646076202393\n",
      "epoch: 99,  batch step: 118, loss: 8.32883071899414\n",
      "epoch: 99,  batch step: 119, loss: 1.8085979223251343\n",
      "epoch: 99,  batch step: 120, loss: 38.98249816894531\n",
      "epoch: 99,  batch step: 121, loss: 6.741412162780762\n",
      "epoch: 99,  batch step: 122, loss: 10.317056655883789\n",
      "epoch: 99,  batch step: 123, loss: 10.483457565307617\n",
      "epoch: 99,  batch step: 124, loss: 2.680607318878174\n",
      "epoch: 99,  batch step: 125, loss: 11.115622520446777\n",
      "epoch: 99,  batch step: 126, loss: 83.39002990722656\n",
      "epoch: 99,  batch step: 127, loss: 5.662379264831543\n",
      "epoch: 99,  batch step: 128, loss: 7.165536403656006\n",
      "epoch: 99,  batch step: 129, loss: 4.433548927307129\n",
      "epoch: 99,  batch step: 130, loss: 1.6816915273666382\n",
      "epoch: 99,  batch step: 131, loss: 25.407520294189453\n",
      "epoch: 99,  batch step: 132, loss: 3.557457685470581\n",
      "epoch: 99,  batch step: 133, loss: 1.8520338535308838\n",
      "epoch: 99,  batch step: 134, loss: 1.7910966873168945\n",
      "epoch: 99,  batch step: 135, loss: 2.8640007972717285\n",
      "epoch: 99,  batch step: 136, loss: 5.232361316680908\n",
      "epoch: 99,  batch step: 137, loss: 1.729101538658142\n",
      "epoch: 99,  batch step: 138, loss: 8.488059043884277\n",
      "epoch: 99,  batch step: 139, loss: 25.046859741210938\n",
      "epoch: 99,  batch step: 140, loss: 21.982955932617188\n",
      "epoch: 99,  batch step: 141, loss: 16.229774475097656\n",
      "epoch: 99,  batch step: 142, loss: 3.0508334636688232\n",
      "epoch: 99,  batch step: 143, loss: 9.02591323852539\n",
      "epoch: 99,  batch step: 144, loss: 20.185230255126953\n",
      "epoch: 99,  batch step: 145, loss: 15.335126876831055\n",
      "epoch: 99,  batch step: 146, loss: 54.92824935913086\n",
      "epoch: 99,  batch step: 147, loss: 6.6753435134887695\n",
      "epoch: 99,  batch step: 148, loss: 8.305856704711914\n",
      "epoch: 99,  batch step: 149, loss: 2.757253646850586\n",
      "epoch: 99,  batch step: 150, loss: 3.4694533348083496\n",
      "epoch: 99,  batch step: 151, loss: 4.174637794494629\n",
      "epoch: 99,  batch step: 152, loss: 2.84162974357605\n",
      "epoch: 99,  batch step: 153, loss: 8.014535903930664\n",
      "epoch: 99,  batch step: 154, loss: 10.715033531188965\n",
      "epoch: 99,  batch step: 155, loss: 12.291078567504883\n",
      "epoch: 99,  batch step: 156, loss: 18.97208023071289\n",
      "epoch: 99,  batch step: 157, loss: 2.0977494716644287\n",
      "epoch: 99,  batch step: 158, loss: 5.377587795257568\n",
      "epoch: 99,  batch step: 159, loss: 27.178611755371094\n",
      "epoch: 99,  batch step: 160, loss: 52.65807342529297\n",
      "epoch: 99,  batch step: 161, loss: 8.492415428161621\n",
      "epoch: 99,  batch step: 162, loss: 25.27701187133789\n",
      "epoch: 99,  batch step: 163, loss: 2.215622663497925\n",
      "epoch: 99,  batch step: 164, loss: 8.344223976135254\n",
      "epoch: 99,  batch step: 165, loss: 1.943008303642273\n",
      "epoch: 99,  batch step: 166, loss: 15.685400009155273\n",
      "epoch: 99,  batch step: 167, loss: 8.503327369689941\n",
      "epoch: 99,  batch step: 168, loss: 33.523441314697266\n",
      "epoch: 99,  batch step: 169, loss: 1.3936545848846436\n",
      "epoch: 99,  batch step: 170, loss: 23.50850486755371\n",
      "epoch: 99,  batch step: 171, loss: 2.7211413383483887\n",
      "epoch: 99,  batch step: 172, loss: 2.486386299133301\n",
      "epoch: 99,  batch step: 173, loss: 1.8017534017562866\n",
      "epoch: 99,  batch step: 174, loss: 4.76064395904541\n",
      "epoch: 99,  batch step: 175, loss: 2.56900691986084\n",
      "epoch: 99,  batch step: 176, loss: 2.3109917640686035\n",
      "epoch: 99,  batch step: 177, loss: 3.0925004482269287\n",
      "epoch: 99,  batch step: 178, loss: 15.19478988647461\n",
      "epoch: 99,  batch step: 179, loss: 2.164926052093506\n",
      "epoch: 99,  batch step: 180, loss: 3.3675696849823\n",
      "epoch: 99,  batch step: 181, loss: 2.9607348442077637\n",
      "epoch: 99,  batch step: 182, loss: 3.7498278617858887\n",
      "epoch: 99,  batch step: 183, loss: 28.6387939453125\n",
      "epoch: 99,  batch step: 184, loss: 3.369774580001831\n",
      "epoch: 99,  batch step: 185, loss: 2.8228707313537598\n",
      "epoch: 99,  batch step: 186, loss: 13.064784049987793\n",
      "epoch: 99,  batch step: 187, loss: 2.5360989570617676\n",
      "epoch: 99,  batch step: 188, loss: 10.125086784362793\n",
      "epoch: 99,  batch step: 189, loss: 10.843169212341309\n",
      "epoch: 99,  batch step: 190, loss: 7.418429851531982\n",
      "epoch: 99,  batch step: 191, loss: 2.267928123474121\n",
      "epoch: 99,  batch step: 192, loss: 2.624375104904175\n",
      "epoch: 99,  batch step: 193, loss: 8.753511428833008\n",
      "epoch: 99,  batch step: 194, loss: 23.745101928710938\n",
      "epoch: 99,  batch step: 195, loss: 15.515295028686523\n",
      "epoch: 99,  batch step: 196, loss: 4.430679798126221\n",
      "epoch: 99,  batch step: 197, loss: 2.8095874786376953\n",
      "epoch: 99,  batch step: 198, loss: 15.096822738647461\n",
      "epoch: 99,  batch step: 199, loss: 5.8042216300964355\n",
      "epoch: 99,  batch step: 200, loss: 2.5895073413848877\n",
      "epoch: 99,  batch step: 201, loss: 10.549436569213867\n",
      "epoch: 99,  batch step: 202, loss: 1.7341606616973877\n",
      "epoch: 99,  batch step: 203, loss: 6.25222110748291\n",
      "epoch: 99,  batch step: 204, loss: 2.4718029499053955\n",
      "epoch: 99,  batch step: 205, loss: 2.6461377143859863\n",
      "epoch: 99,  batch step: 206, loss: 4.101649761199951\n",
      "epoch: 99,  batch step: 207, loss: 14.465555191040039\n",
      "epoch: 99,  batch step: 208, loss: 2.4330968856811523\n",
      "epoch: 99,  batch step: 209, loss: 9.055097579956055\n",
      "epoch: 99,  batch step: 210, loss: 20.596561431884766\n",
      "epoch: 99,  batch step: 211, loss: 22.044208526611328\n",
      "epoch: 99,  batch step: 212, loss: 7.580778121948242\n",
      "epoch: 99,  batch step: 213, loss: 2.3924570083618164\n",
      "epoch: 99,  batch step: 214, loss: 3.413567066192627\n",
      "epoch: 99,  batch step: 215, loss: 2.8085601329803467\n",
      "epoch: 99,  batch step: 216, loss: 25.024307250976562\n",
      "epoch: 99,  batch step: 217, loss: 45.751644134521484\n",
      "epoch: 99,  batch step: 218, loss: 34.579925537109375\n",
      "epoch: 99,  batch step: 219, loss: 2.5635383129119873\n",
      "epoch: 99,  batch step: 220, loss: 4.043412208557129\n",
      "epoch: 99,  batch step: 221, loss: 10.987039566040039\n",
      "epoch: 99,  batch step: 222, loss: 31.898635864257812\n",
      "epoch: 99,  batch step: 223, loss: 15.675121307373047\n",
      "epoch: 99,  batch step: 224, loss: 2.0328383445739746\n",
      "epoch: 99,  batch step: 225, loss: 2.4136154651641846\n",
      "epoch: 99,  batch step: 226, loss: 6.888186454772949\n",
      "epoch: 99,  batch step: 227, loss: 3.2180018424987793\n",
      "epoch: 99,  batch step: 228, loss: 37.87676239013672\n",
      "epoch: 99,  batch step: 229, loss: 2.364417791366577\n",
      "epoch: 99,  batch step: 230, loss: 2.4718761444091797\n",
      "epoch: 99,  batch step: 231, loss: 10.076464653015137\n",
      "epoch: 99,  batch step: 232, loss: 15.203872680664062\n",
      "epoch: 99,  batch step: 233, loss: 2.4874768257141113\n",
      "epoch: 99,  batch step: 234, loss: 2.9381680488586426\n",
      "epoch: 99,  batch step: 235, loss: 33.12493896484375\n",
      "epoch: 99,  batch step: 236, loss: 1.759670615196228\n",
      "epoch: 99,  batch step: 237, loss: 2.4731924533843994\n",
      "epoch: 99,  batch step: 238, loss: 10.088227272033691\n",
      "epoch: 99,  batch step: 239, loss: 12.14755630493164\n",
      "epoch: 99,  batch step: 240, loss: 2.2173423767089844\n",
      "epoch: 99,  batch step: 241, loss: 3.4008054733276367\n",
      "epoch: 99,  batch step: 242, loss: 131.65286254882812\n",
      "epoch: 99,  batch step: 243, loss: 29.530447006225586\n",
      "epoch: 99,  batch step: 244, loss: 2.8635101318359375\n",
      "epoch: 99,  batch step: 245, loss: 6.658604621887207\n",
      "epoch: 99,  batch step: 246, loss: 19.695571899414062\n",
      "epoch: 99,  batch step: 247, loss: 17.0821475982666\n",
      "epoch: 99,  batch step: 248, loss: 5.903866767883301\n",
      "epoch: 99,  batch step: 249, loss: 10.951705932617188\n",
      "epoch: 99,  batch step: 250, loss: 8.627143859863281\n",
      "epoch: 99,  batch step: 251, loss: 83.67082214355469\n",
      "finished saving checkpoints\n",
      "validation error epoch  99:    tensor(68.8265, device='cuda:0')\n",
      "316\n",
      "epoch: 100,  batch step: 0, loss: 2.1699142456054688\n",
      "epoch: 100,  batch step: 1, loss: 27.511417388916016\n",
      "epoch: 100,  batch step: 2, loss: 7.034141540527344\n",
      "epoch: 100,  batch step: 3, loss: 12.473162651062012\n",
      "epoch: 100,  batch step: 4, loss: 14.208385467529297\n",
      "epoch: 100,  batch step: 5, loss: 6.3051252365112305\n",
      "epoch: 100,  batch step: 6, loss: 12.52833366394043\n",
      "epoch: 100,  batch step: 7, loss: 3.397958517074585\n",
      "epoch: 100,  batch step: 8, loss: 3.2548599243164062\n",
      "epoch: 100,  batch step: 9, loss: 5.188503265380859\n",
      "epoch: 100,  batch step: 10, loss: 5.1081037521362305\n",
      "epoch: 100,  batch step: 11, loss: 5.238635063171387\n",
      "epoch: 100,  batch step: 12, loss: 10.955345153808594\n",
      "epoch: 100,  batch step: 13, loss: 4.669641494750977\n",
      "epoch: 100,  batch step: 14, loss: 10.526810646057129\n",
      "epoch: 100,  batch step: 15, loss: 3.7088725566864014\n",
      "epoch: 100,  batch step: 16, loss: 6.210898399353027\n",
      "epoch: 100,  batch step: 17, loss: 2.523665189743042\n",
      "epoch: 100,  batch step: 18, loss: 10.890467643737793\n",
      "epoch: 100,  batch step: 19, loss: 18.033811569213867\n",
      "epoch: 100,  batch step: 20, loss: 4.708193778991699\n",
      "epoch: 100,  batch step: 21, loss: 6.062674522399902\n",
      "epoch: 100,  batch step: 22, loss: 41.74946212768555\n",
      "epoch: 100,  batch step: 23, loss: 50.361366271972656\n",
      "epoch: 100,  batch step: 24, loss: 8.515890121459961\n",
      "epoch: 100,  batch step: 25, loss: 10.523752212524414\n",
      "epoch: 100,  batch step: 26, loss: 7.799018383026123\n",
      "epoch: 100,  batch step: 27, loss: 7.36566686630249\n",
      "epoch: 100,  batch step: 28, loss: 12.167464256286621\n",
      "epoch: 100,  batch step: 29, loss: 5.532732009887695\n",
      "epoch: 100,  batch step: 30, loss: 55.3502311706543\n",
      "epoch: 100,  batch step: 31, loss: 8.328062057495117\n",
      "epoch: 100,  batch step: 32, loss: 5.982652187347412\n",
      "epoch: 100,  batch step: 33, loss: 52.39234924316406\n",
      "epoch: 100,  batch step: 34, loss: 2.7628207206726074\n",
      "epoch: 100,  batch step: 35, loss: 2.8574018478393555\n",
      "epoch: 100,  batch step: 36, loss: 11.299059867858887\n",
      "epoch: 100,  batch step: 37, loss: 21.73794937133789\n",
      "epoch: 100,  batch step: 38, loss: 7.9543375968933105\n",
      "epoch: 100,  batch step: 39, loss: 3.246610641479492\n",
      "epoch: 100,  batch step: 40, loss: 21.052997589111328\n",
      "epoch: 100,  batch step: 41, loss: 3.7151572704315186\n",
      "epoch: 100,  batch step: 42, loss: 21.64460563659668\n",
      "epoch: 100,  batch step: 43, loss: 2.1501588821411133\n",
      "epoch: 100,  batch step: 44, loss: 9.167665481567383\n",
      "epoch: 100,  batch step: 45, loss: 2.701138973236084\n",
      "epoch: 100,  batch step: 46, loss: 5.643049240112305\n",
      "epoch: 100,  batch step: 47, loss: 26.38384246826172\n",
      "epoch: 100,  batch step: 48, loss: 7.03653621673584\n",
      "epoch: 100,  batch step: 49, loss: 2.378617763519287\n",
      "epoch: 100,  batch step: 50, loss: 3.0804800987243652\n",
      "epoch: 100,  batch step: 51, loss: 12.883232116699219\n",
      "epoch: 100,  batch step: 52, loss: 25.708908081054688\n",
      "epoch: 100,  batch step: 53, loss: 2.646461009979248\n",
      "epoch: 100,  batch step: 54, loss: 1.8002278804779053\n",
      "epoch: 100,  batch step: 55, loss: 9.256607055664062\n",
      "epoch: 100,  batch step: 56, loss: 2.569324493408203\n",
      "epoch: 100,  batch step: 57, loss: 9.308660507202148\n",
      "epoch: 100,  batch step: 58, loss: 14.483697891235352\n",
      "epoch: 100,  batch step: 59, loss: 1.938136339187622\n",
      "epoch: 100,  batch step: 60, loss: 10.470048904418945\n",
      "epoch: 100,  batch step: 61, loss: 12.261085510253906\n",
      "epoch: 100,  batch step: 62, loss: 32.23363494873047\n",
      "epoch: 100,  batch step: 63, loss: 18.308900833129883\n",
      "epoch: 100,  batch step: 64, loss: 2.5211360454559326\n",
      "epoch: 100,  batch step: 65, loss: 1.8909820318222046\n",
      "epoch: 100,  batch step: 66, loss: 2.4143762588500977\n",
      "epoch: 100,  batch step: 67, loss: 2.248091697692871\n",
      "epoch: 100,  batch step: 68, loss: 2.325866937637329\n",
      "epoch: 100,  batch step: 69, loss: 3.255129814147949\n",
      "epoch: 100,  batch step: 70, loss: 2.317814350128174\n",
      "epoch: 100,  batch step: 71, loss: 4.535065650939941\n",
      "epoch: 100,  batch step: 72, loss: 1.6074268817901611\n",
      "epoch: 100,  batch step: 73, loss: 3.782435894012451\n",
      "epoch: 100,  batch step: 74, loss: 12.503287315368652\n",
      "epoch: 100,  batch step: 75, loss: 30.16671371459961\n",
      "epoch: 100,  batch step: 76, loss: 10.617181777954102\n",
      "epoch: 100,  batch step: 77, loss: 2.1495206356048584\n",
      "epoch: 100,  batch step: 78, loss: 32.19530487060547\n",
      "epoch: 100,  batch step: 79, loss: 7.059052467346191\n",
      "epoch: 100,  batch step: 80, loss: 2.508192539215088\n",
      "epoch: 100,  batch step: 81, loss: 2.138422966003418\n",
      "epoch: 100,  batch step: 82, loss: 2.179508686065674\n",
      "epoch: 100,  batch step: 83, loss: 3.034996509552002\n",
      "epoch: 100,  batch step: 84, loss: 30.849594116210938\n",
      "epoch: 100,  batch step: 85, loss: 3.0145297050476074\n",
      "epoch: 100,  batch step: 86, loss: 11.440303802490234\n",
      "epoch: 100,  batch step: 87, loss: 1.7713279724121094\n",
      "epoch: 100,  batch step: 88, loss: 10.872297286987305\n",
      "epoch: 100,  batch step: 89, loss: 11.371667861938477\n",
      "epoch: 100,  batch step: 90, loss: 3.7932958602905273\n",
      "epoch: 100,  batch step: 91, loss: 1.5437209606170654\n",
      "epoch: 100,  batch step: 92, loss: 2.991058826446533\n",
      "epoch: 100,  batch step: 93, loss: 1.7681312561035156\n",
      "epoch: 100,  batch step: 94, loss: 52.5993766784668\n",
      "epoch: 100,  batch step: 95, loss: 7.462187767028809\n",
      "epoch: 100,  batch step: 96, loss: 3.0551764965057373\n",
      "epoch: 100,  batch step: 97, loss: 6.183295726776123\n",
      "epoch: 100,  batch step: 98, loss: 7.164332389831543\n",
      "epoch: 100,  batch step: 99, loss: 4.534498691558838\n",
      "epoch: 100,  batch step: 100, loss: 1.9481971263885498\n",
      "epoch: 100,  batch step: 101, loss: 8.946107864379883\n",
      "epoch: 100,  batch step: 102, loss: 2.698601484298706\n",
      "epoch: 100,  batch step: 103, loss: 8.45974349975586\n",
      "epoch: 100,  batch step: 104, loss: 2.083693742752075\n",
      "epoch: 100,  batch step: 105, loss: 3.115436553955078\n",
      "epoch: 100,  batch step: 106, loss: 4.033092975616455\n",
      "epoch: 100,  batch step: 107, loss: 3.021301031112671\n",
      "epoch: 100,  batch step: 108, loss: 2.8990230560302734\n",
      "epoch: 100,  batch step: 109, loss: 18.40584945678711\n",
      "epoch: 100,  batch step: 110, loss: 2.267848491668701\n",
      "epoch: 100,  batch step: 111, loss: 6.8690595626831055\n",
      "epoch: 100,  batch step: 112, loss: 10.00613784790039\n",
      "epoch: 100,  batch step: 113, loss: 1.8480842113494873\n",
      "epoch: 100,  batch step: 114, loss: 13.552477836608887\n",
      "epoch: 100,  batch step: 115, loss: 6.046565532684326\n",
      "epoch: 100,  batch step: 116, loss: 4.9503326416015625\n",
      "epoch: 100,  batch step: 117, loss: 3.1719937324523926\n",
      "epoch: 100,  batch step: 118, loss: 27.051733016967773\n",
      "epoch: 100,  batch step: 119, loss: 12.844734191894531\n",
      "epoch: 100,  batch step: 120, loss: 4.542801380157471\n",
      "epoch: 100,  batch step: 121, loss: 15.118853569030762\n",
      "epoch: 100,  batch step: 122, loss: 31.299327850341797\n",
      "epoch: 100,  batch step: 123, loss: 2.259438991546631\n",
      "epoch: 100,  batch step: 124, loss: 2.9510064125061035\n",
      "epoch: 100,  batch step: 125, loss: 11.261263847351074\n",
      "epoch: 100,  batch step: 126, loss: 6.5726165771484375\n",
      "epoch: 100,  batch step: 127, loss: 2.2858214378356934\n",
      "epoch: 100,  batch step: 128, loss: 10.43974494934082\n",
      "epoch: 100,  batch step: 129, loss: 2.561798572540283\n",
      "epoch: 100,  batch step: 130, loss: 8.070146560668945\n",
      "epoch: 100,  batch step: 131, loss: 36.585426330566406\n",
      "epoch: 100,  batch step: 132, loss: 6.7726850509643555\n",
      "epoch: 100,  batch step: 133, loss: 3.7846572399139404\n",
      "epoch: 100,  batch step: 134, loss: 2.7867188453674316\n",
      "epoch: 100,  batch step: 135, loss: 15.963607788085938\n",
      "epoch: 100,  batch step: 136, loss: 10.759581565856934\n",
      "epoch: 100,  batch step: 137, loss: 16.179548263549805\n",
      "epoch: 100,  batch step: 138, loss: 1.7966108322143555\n",
      "epoch: 100,  batch step: 139, loss: 3.0055012702941895\n",
      "epoch: 100,  batch step: 140, loss: 4.2766876220703125\n",
      "epoch: 100,  batch step: 141, loss: 1.9210213422775269\n",
      "epoch: 100,  batch step: 142, loss: 42.78440475463867\n",
      "epoch: 100,  batch step: 143, loss: 18.807636260986328\n",
      "epoch: 100,  batch step: 144, loss: 17.642986297607422\n",
      "epoch: 100,  batch step: 145, loss: 1.740995168685913\n",
      "epoch: 100,  batch step: 146, loss: 4.887447357177734\n",
      "epoch: 100,  batch step: 147, loss: 7.0581159591674805\n",
      "epoch: 100,  batch step: 148, loss: 2.204761505126953\n",
      "epoch: 100,  batch step: 149, loss: 47.64447784423828\n",
      "epoch: 100,  batch step: 150, loss: 30.785171508789062\n",
      "epoch: 100,  batch step: 151, loss: 1.676123023033142\n",
      "epoch: 100,  batch step: 152, loss: 12.6997709274292\n",
      "epoch: 100,  batch step: 153, loss: 7.717292308807373\n",
      "epoch: 100,  batch step: 154, loss: 1.8888356685638428\n",
      "epoch: 100,  batch step: 155, loss: 10.37646484375\n",
      "epoch: 100,  batch step: 156, loss: 15.716773986816406\n",
      "epoch: 100,  batch step: 157, loss: 1.8061192035675049\n",
      "epoch: 100,  batch step: 158, loss: 2.243905544281006\n",
      "epoch: 100,  batch step: 159, loss: 13.106123924255371\n",
      "epoch: 100,  batch step: 160, loss: 7.709751129150391\n",
      "epoch: 100,  batch step: 161, loss: 8.121715545654297\n",
      "epoch: 100,  batch step: 162, loss: 1.9678411483764648\n",
      "epoch: 100,  batch step: 163, loss: 4.6001200675964355\n",
      "epoch: 100,  batch step: 164, loss: 11.671018600463867\n",
      "epoch: 100,  batch step: 165, loss: 1.752164602279663\n",
      "epoch: 100,  batch step: 166, loss: 9.981199264526367\n",
      "epoch: 100,  batch step: 167, loss: 29.63453483581543\n",
      "epoch: 100,  batch step: 168, loss: 7.206765651702881\n",
      "epoch: 100,  batch step: 169, loss: 51.972774505615234\n",
      "epoch: 100,  batch step: 170, loss: 8.741628646850586\n",
      "epoch: 100,  batch step: 171, loss: 2.0576581954956055\n",
      "epoch: 100,  batch step: 172, loss: 16.753568649291992\n",
      "epoch: 100,  batch step: 173, loss: 1.6793029308319092\n",
      "epoch: 100,  batch step: 174, loss: 13.402626037597656\n",
      "epoch: 100,  batch step: 175, loss: 2.3617286682128906\n",
      "epoch: 100,  batch step: 176, loss: 15.58259105682373\n",
      "epoch: 100,  batch step: 177, loss: 66.8149185180664\n",
      "epoch: 100,  batch step: 178, loss: 37.605873107910156\n",
      "epoch: 100,  batch step: 179, loss: 7.309944152832031\n",
      "epoch: 100,  batch step: 180, loss: 5.05320930480957\n",
      "epoch: 100,  batch step: 181, loss: 1.7911882400512695\n",
      "epoch: 100,  batch step: 182, loss: 1.846674919128418\n",
      "epoch: 100,  batch step: 183, loss: 23.018247604370117\n",
      "epoch: 100,  batch step: 184, loss: 2.6267032623291016\n",
      "epoch: 100,  batch step: 185, loss: 8.691829681396484\n",
      "epoch: 100,  batch step: 186, loss: 38.20552062988281\n",
      "epoch: 100,  batch step: 187, loss: 20.645885467529297\n",
      "epoch: 100,  batch step: 188, loss: 23.299901962280273\n",
      "epoch: 100,  batch step: 189, loss: 2.3423094749450684\n",
      "epoch: 100,  batch step: 190, loss: 5.11592435836792\n",
      "epoch: 100,  batch step: 191, loss: 3.2997283935546875\n",
      "epoch: 100,  batch step: 192, loss: 57.10490417480469\n",
      "epoch: 100,  batch step: 193, loss: 16.172908782958984\n",
      "epoch: 100,  batch step: 194, loss: 12.881071090698242\n",
      "epoch: 100,  batch step: 195, loss: 16.037996292114258\n",
      "epoch: 100,  batch step: 196, loss: 9.72659683227539\n",
      "epoch: 100,  batch step: 197, loss: 2.492252826690674\n",
      "epoch: 100,  batch step: 198, loss: 8.191946029663086\n",
      "epoch: 100,  batch step: 199, loss: 8.18051528930664\n",
      "epoch: 100,  batch step: 200, loss: 26.019084930419922\n",
      "epoch: 100,  batch step: 201, loss: 6.769392967224121\n",
      "epoch: 100,  batch step: 202, loss: 2.0029921531677246\n",
      "epoch: 100,  batch step: 203, loss: 3.213557720184326\n",
      "epoch: 100,  batch step: 204, loss: 10.89588737487793\n",
      "epoch: 100,  batch step: 205, loss: 135.7049560546875\n",
      "epoch: 100,  batch step: 206, loss: 2.513005256652832\n",
      "epoch: 100,  batch step: 207, loss: 17.61636734008789\n",
      "epoch: 100,  batch step: 208, loss: 2.801954746246338\n",
      "epoch: 100,  batch step: 209, loss: 10.74534797668457\n",
      "epoch: 100,  batch step: 210, loss: 18.22555923461914\n",
      "epoch: 100,  batch step: 211, loss: 1.602403998374939\n",
      "epoch: 100,  batch step: 212, loss: 21.408092498779297\n",
      "epoch: 100,  batch step: 213, loss: 7.401719093322754\n",
      "epoch: 100,  batch step: 214, loss: 13.3485107421875\n",
      "epoch: 100,  batch step: 215, loss: 5.814876079559326\n",
      "epoch: 100,  batch step: 216, loss: 1.8283185958862305\n",
      "epoch: 100,  batch step: 217, loss: 7.013465881347656\n",
      "epoch: 100,  batch step: 218, loss: 15.573527336120605\n",
      "epoch: 100,  batch step: 219, loss: 7.944403648376465\n",
      "epoch: 100,  batch step: 220, loss: 30.90987205505371\n",
      "epoch: 100,  batch step: 221, loss: 3.1268696784973145\n",
      "epoch: 100,  batch step: 222, loss: 14.751673698425293\n",
      "epoch: 100,  batch step: 223, loss: 2.1095616817474365\n",
      "epoch: 100,  batch step: 224, loss: 11.30540657043457\n",
      "epoch: 100,  batch step: 225, loss: 8.90359115600586\n",
      "epoch: 100,  batch step: 226, loss: 3.354037284851074\n",
      "epoch: 100,  batch step: 227, loss: 31.64393424987793\n",
      "epoch: 100,  batch step: 228, loss: 12.684263229370117\n",
      "epoch: 100,  batch step: 229, loss: 3.0108461380004883\n",
      "epoch: 100,  batch step: 230, loss: 3.3777549266815186\n",
      "epoch: 100,  batch step: 231, loss: 47.6555290222168\n",
      "epoch: 100,  batch step: 232, loss: 1.127077341079712\n",
      "epoch: 100,  batch step: 233, loss: 3.2992939949035645\n",
      "epoch: 100,  batch step: 234, loss: 11.908689498901367\n",
      "epoch: 100,  batch step: 235, loss: 2.5154569149017334\n",
      "epoch: 100,  batch step: 236, loss: 2.616560935974121\n",
      "epoch: 100,  batch step: 237, loss: 2.161834239959717\n",
      "epoch: 100,  batch step: 238, loss: 1.346688151359558\n",
      "epoch: 100,  batch step: 239, loss: 9.155604362487793\n",
      "epoch: 100,  batch step: 240, loss: 12.08716106414795\n",
      "epoch: 100,  batch step: 241, loss: 3.575596570968628\n",
      "epoch: 100,  batch step: 242, loss: 35.662132263183594\n",
      "epoch: 100,  batch step: 243, loss: 22.748428344726562\n",
      "epoch: 100,  batch step: 244, loss: 5.810086250305176\n",
      "epoch: 100,  batch step: 245, loss: 1.53512704372406\n",
      "epoch: 100,  batch step: 246, loss: 5.703310966491699\n",
      "epoch: 100,  batch step: 247, loss: 4.516162395477295\n",
      "epoch: 100,  batch step: 248, loss: 8.784116744995117\n",
      "epoch: 100,  batch step: 249, loss: 2.042050838470459\n",
      "epoch: 100,  batch step: 250, loss: 1.5859506130218506\n",
      "epoch: 100,  batch step: 251, loss: 38.82622528076172\n",
      "validation error epoch  100:    tensor(70.7564, device='cuda:0')\n",
      "316\n",
      "epoch: 101,  batch step: 0, loss: 29.86250877380371\n",
      "epoch: 101,  batch step: 1, loss: 3.813131809234619\n",
      "epoch: 101,  batch step: 2, loss: 2.7558512687683105\n",
      "epoch: 101,  batch step: 3, loss: 10.526262283325195\n",
      "epoch: 101,  batch step: 4, loss: 9.431060791015625\n",
      "epoch: 101,  batch step: 5, loss: 55.667999267578125\n",
      "epoch: 101,  batch step: 6, loss: 4.018918514251709\n",
      "epoch: 101,  batch step: 7, loss: 7.4116644859313965\n",
      "epoch: 101,  batch step: 8, loss: 6.052790641784668\n",
      "epoch: 101,  batch step: 9, loss: 2.126218795776367\n",
      "epoch: 101,  batch step: 10, loss: 3.9363603591918945\n",
      "epoch: 101,  batch step: 11, loss: 3.0447871685028076\n",
      "epoch: 101,  batch step: 12, loss: 2.9728636741638184\n",
      "epoch: 101,  batch step: 13, loss: 8.81605339050293\n",
      "epoch: 101,  batch step: 14, loss: 31.86860466003418\n",
      "epoch: 101,  batch step: 15, loss: 1.5576976537704468\n",
      "epoch: 101,  batch step: 16, loss: 12.509288787841797\n",
      "epoch: 101,  batch step: 17, loss: 4.568044662475586\n",
      "epoch: 101,  batch step: 18, loss: 16.657615661621094\n",
      "epoch: 101,  batch step: 19, loss: 1.8120553493499756\n",
      "epoch: 101,  batch step: 20, loss: 9.47718620300293\n",
      "epoch: 101,  batch step: 21, loss: 16.14553451538086\n",
      "epoch: 101,  batch step: 22, loss: 9.857062339782715\n",
      "epoch: 101,  batch step: 23, loss: 5.750077724456787\n",
      "epoch: 101,  batch step: 24, loss: 2.6576123237609863\n",
      "epoch: 101,  batch step: 25, loss: 56.681724548339844\n",
      "epoch: 101,  batch step: 26, loss: 12.296290397644043\n",
      "epoch: 101,  batch step: 27, loss: 7.306039333343506\n",
      "epoch: 101,  batch step: 28, loss: 21.600622177124023\n",
      "epoch: 101,  batch step: 29, loss: 2.7143187522888184\n",
      "epoch: 101,  batch step: 30, loss: 1.9207242727279663\n",
      "epoch: 101,  batch step: 31, loss: 3.1171445846557617\n",
      "epoch: 101,  batch step: 32, loss: 2.8256592750549316\n",
      "epoch: 101,  batch step: 33, loss: 34.67805099487305\n",
      "epoch: 101,  batch step: 34, loss: 9.472926139831543\n",
      "epoch: 101,  batch step: 35, loss: 3.755695343017578\n",
      "epoch: 101,  batch step: 36, loss: 2.5984301567077637\n",
      "epoch: 101,  batch step: 37, loss: 2.0232529640197754\n",
      "epoch: 101,  batch step: 38, loss: 10.390954971313477\n",
      "epoch: 101,  batch step: 39, loss: 5.757333755493164\n",
      "epoch: 101,  batch step: 40, loss: 64.00779724121094\n",
      "epoch: 101,  batch step: 41, loss: 5.221717834472656\n",
      "epoch: 101,  batch step: 42, loss: 5.242312431335449\n",
      "epoch: 101,  batch step: 43, loss: 2.486086845397949\n",
      "epoch: 101,  batch step: 44, loss: 8.087047576904297\n",
      "epoch: 101,  batch step: 45, loss: 2.4600865840911865\n",
      "epoch: 101,  batch step: 46, loss: 16.113407135009766\n",
      "epoch: 101,  batch step: 47, loss: 2.188373565673828\n",
      "epoch: 101,  batch step: 48, loss: 2.6871337890625\n",
      "epoch: 101,  batch step: 49, loss: 5.220181941986084\n",
      "epoch: 101,  batch step: 50, loss: 2.3607425689697266\n",
      "epoch: 101,  batch step: 51, loss: 2.4609007835388184\n",
      "epoch: 101,  batch step: 52, loss: 2.5758776664733887\n",
      "epoch: 101,  batch step: 53, loss: 14.379020690917969\n",
      "epoch: 101,  batch step: 54, loss: 2.294088125228882\n",
      "epoch: 101,  batch step: 55, loss: 2.791759967803955\n",
      "epoch: 101,  batch step: 56, loss: 2.322902202606201\n",
      "epoch: 101,  batch step: 57, loss: 16.141399383544922\n",
      "epoch: 101,  batch step: 58, loss: 29.686140060424805\n",
      "epoch: 101,  batch step: 59, loss: 19.056325912475586\n",
      "epoch: 101,  batch step: 60, loss: 9.405264854431152\n",
      "epoch: 101,  batch step: 61, loss: 2.900712490081787\n",
      "epoch: 101,  batch step: 62, loss: 16.728946685791016\n",
      "epoch: 101,  batch step: 63, loss: 13.243514060974121\n",
      "epoch: 101,  batch step: 64, loss: 31.24378204345703\n",
      "epoch: 101,  batch step: 65, loss: 13.318316459655762\n",
      "epoch: 101,  batch step: 66, loss: 8.09090805053711\n",
      "epoch: 101,  batch step: 67, loss: 1.4550036191940308\n",
      "epoch: 101,  batch step: 68, loss: 6.156405448913574\n",
      "epoch: 101,  batch step: 69, loss: 3.2556891441345215\n",
      "epoch: 101,  batch step: 70, loss: 3.2901837825775146\n",
      "epoch: 101,  batch step: 71, loss: 16.097919464111328\n",
      "epoch: 101,  batch step: 72, loss: 39.67674255371094\n",
      "epoch: 101,  batch step: 73, loss: 2.7804408073425293\n",
      "epoch: 101,  batch step: 74, loss: 3.3336219787597656\n",
      "epoch: 101,  batch step: 75, loss: 3.075554370880127\n",
      "epoch: 101,  batch step: 76, loss: 2.805777072906494\n",
      "epoch: 101,  batch step: 77, loss: 2.0630626678466797\n",
      "epoch: 101,  batch step: 78, loss: 5.597713470458984\n",
      "epoch: 101,  batch step: 79, loss: 1.2650070190429688\n",
      "epoch: 101,  batch step: 80, loss: 11.078773498535156\n",
      "epoch: 101,  batch step: 81, loss: 2.4053311347961426\n",
      "epoch: 101,  batch step: 82, loss: 7.577956199645996\n",
      "epoch: 101,  batch step: 83, loss: 2.1711983680725098\n",
      "epoch: 101,  batch step: 84, loss: 2.7766215801239014\n",
      "epoch: 101,  batch step: 85, loss: 2.3380675315856934\n",
      "epoch: 101,  batch step: 86, loss: 41.82589340209961\n",
      "epoch: 101,  batch step: 87, loss: 36.07989501953125\n",
      "epoch: 101,  batch step: 88, loss: 7.343535423278809\n",
      "epoch: 101,  batch step: 89, loss: 1.466698169708252\n",
      "epoch: 101,  batch step: 90, loss: 1.6905854940414429\n",
      "epoch: 101,  batch step: 91, loss: 43.960689544677734\n",
      "epoch: 101,  batch step: 92, loss: 18.33082389831543\n",
      "epoch: 101,  batch step: 93, loss: 2.5479907989501953\n",
      "epoch: 101,  batch step: 94, loss: 1.7542884349822998\n",
      "epoch: 101,  batch step: 95, loss: 5.037705898284912\n",
      "epoch: 101,  batch step: 96, loss: 5.494711875915527\n",
      "epoch: 101,  batch step: 97, loss: 3.6908767223358154\n",
      "epoch: 101,  batch step: 98, loss: 3.0421359539031982\n",
      "epoch: 101,  batch step: 99, loss: 3.1682815551757812\n",
      "epoch: 101,  batch step: 100, loss: 3.2094922065734863\n",
      "epoch: 101,  batch step: 101, loss: 65.22276306152344\n",
      "epoch: 101,  batch step: 102, loss: 1.1684085130691528\n",
      "epoch: 101,  batch step: 103, loss: 1.5167276859283447\n",
      "epoch: 101,  batch step: 104, loss: 6.093523025512695\n",
      "epoch: 101,  batch step: 105, loss: 2.118640899658203\n",
      "epoch: 101,  batch step: 106, loss: 5.28887939453125\n",
      "epoch: 101,  batch step: 107, loss: 1.5651500225067139\n",
      "epoch: 101,  batch step: 108, loss: 12.844671249389648\n",
      "epoch: 101,  batch step: 109, loss: 56.57500457763672\n",
      "epoch: 101,  batch step: 110, loss: 20.735828399658203\n",
      "epoch: 101,  batch step: 111, loss: 2.7145144939422607\n",
      "epoch: 101,  batch step: 112, loss: 3.334489583969116\n",
      "epoch: 101,  batch step: 113, loss: 1.6387975215911865\n",
      "epoch: 101,  batch step: 114, loss: 6.196767807006836\n",
      "epoch: 101,  batch step: 115, loss: 1.763641357421875\n",
      "epoch: 101,  batch step: 116, loss: 10.740057945251465\n",
      "epoch: 101,  batch step: 117, loss: 11.750800132751465\n",
      "epoch: 101,  batch step: 118, loss: 3.325777053833008\n",
      "epoch: 101,  batch step: 119, loss: 2.4196817874908447\n",
      "epoch: 101,  batch step: 120, loss: 2.1389760971069336\n",
      "epoch: 101,  batch step: 121, loss: 11.512659072875977\n",
      "epoch: 101,  batch step: 122, loss: 2.941441297531128\n",
      "epoch: 101,  batch step: 123, loss: 1.5759592056274414\n",
      "epoch: 101,  batch step: 124, loss: 1.7895512580871582\n",
      "epoch: 101,  batch step: 125, loss: 3.1238207817077637\n",
      "epoch: 101,  batch step: 126, loss: 18.456867218017578\n",
      "epoch: 101,  batch step: 127, loss: 1.6113080978393555\n",
      "epoch: 101,  batch step: 128, loss: 7.838273525238037\n",
      "epoch: 101,  batch step: 129, loss: 7.929622650146484\n",
      "epoch: 101,  batch step: 130, loss: 1.390089988708496\n",
      "epoch: 101,  batch step: 131, loss: 151.91094970703125\n",
      "epoch: 101,  batch step: 132, loss: 11.986532211303711\n",
      "epoch: 101,  batch step: 133, loss: 55.15923309326172\n",
      "epoch: 101,  batch step: 134, loss: 9.064537048339844\n",
      "epoch: 101,  batch step: 135, loss: 2.563530445098877\n",
      "epoch: 101,  batch step: 136, loss: 21.121349334716797\n",
      "epoch: 101,  batch step: 137, loss: 2.6902551651000977\n",
      "epoch: 101,  batch step: 138, loss: 16.717147827148438\n",
      "epoch: 101,  batch step: 139, loss: 12.42207145690918\n",
      "epoch: 101,  batch step: 140, loss: 5.973654747009277\n",
      "epoch: 101,  batch step: 141, loss: 2.1584818363189697\n",
      "epoch: 101,  batch step: 142, loss: 15.946290969848633\n",
      "epoch: 101,  batch step: 143, loss: 2.765434741973877\n",
      "epoch: 101,  batch step: 144, loss: 8.902900695800781\n",
      "epoch: 101,  batch step: 145, loss: 3.689600706100464\n",
      "epoch: 101,  batch step: 146, loss: 60.498756408691406\n",
      "epoch: 101,  batch step: 147, loss: 2.4808590412139893\n",
      "epoch: 101,  batch step: 148, loss: 4.15327787399292\n",
      "epoch: 101,  batch step: 149, loss: 6.908998012542725\n",
      "epoch: 101,  batch step: 150, loss: 26.83273696899414\n",
      "epoch: 101,  batch step: 151, loss: 4.097449779510498\n",
      "epoch: 101,  batch step: 152, loss: 20.56951904296875\n",
      "epoch: 101,  batch step: 153, loss: 2.039555549621582\n",
      "epoch: 101,  batch step: 154, loss: 2.5821361541748047\n",
      "epoch: 101,  batch step: 155, loss: 10.670583724975586\n",
      "epoch: 101,  batch step: 156, loss: 3.2295682430267334\n",
      "epoch: 101,  batch step: 157, loss: 16.373533248901367\n",
      "epoch: 101,  batch step: 158, loss: 30.04876708984375\n",
      "epoch: 101,  batch step: 159, loss: 65.0601577758789\n",
      "epoch: 101,  batch step: 160, loss: 2.4187753200531006\n",
      "epoch: 101,  batch step: 161, loss: 5.198978424072266\n",
      "epoch: 101,  batch step: 162, loss: 2.1736972332000732\n",
      "epoch: 101,  batch step: 163, loss: 11.44410514831543\n",
      "epoch: 101,  batch step: 164, loss: 7.827972412109375\n",
      "epoch: 101,  batch step: 165, loss: 1.3539104461669922\n",
      "epoch: 101,  batch step: 166, loss: 1.9634971618652344\n",
      "epoch: 101,  batch step: 167, loss: 1.7483789920806885\n",
      "epoch: 101,  batch step: 168, loss: 3.180795192718506\n",
      "epoch: 101,  batch step: 169, loss: 2.7012510299682617\n",
      "epoch: 101,  batch step: 170, loss: 24.971019744873047\n",
      "epoch: 101,  batch step: 171, loss: 2.02382755279541\n",
      "epoch: 101,  batch step: 172, loss: 13.443830490112305\n",
      "epoch: 101,  batch step: 173, loss: 1.2735700607299805\n",
      "epoch: 101,  batch step: 174, loss: 1.6124881505966187\n",
      "epoch: 101,  batch step: 175, loss: 6.7879462242126465\n",
      "epoch: 101,  batch step: 176, loss: 16.30524253845215\n",
      "epoch: 101,  batch step: 177, loss: 29.352874755859375\n",
      "epoch: 101,  batch step: 178, loss: 13.854066848754883\n",
      "epoch: 101,  batch step: 179, loss: 3.384812355041504\n",
      "epoch: 101,  batch step: 180, loss: 1.6969869136810303\n",
      "epoch: 101,  batch step: 181, loss: 2.440934181213379\n",
      "epoch: 101,  batch step: 182, loss: 1.801203727722168\n",
      "epoch: 101,  batch step: 183, loss: 1.9389138221740723\n",
      "epoch: 101,  batch step: 184, loss: 10.061934471130371\n",
      "epoch: 101,  batch step: 185, loss: 65.14058685302734\n",
      "epoch: 101,  batch step: 186, loss: 2.078906774520874\n",
      "epoch: 101,  batch step: 187, loss: 7.516376495361328\n",
      "epoch: 101,  batch step: 188, loss: 9.420819282531738\n",
      "epoch: 101,  batch step: 189, loss: 3.5368638038635254\n",
      "epoch: 101,  batch step: 190, loss: 2.36812686920166\n",
      "epoch: 101,  batch step: 191, loss: 17.464040756225586\n",
      "epoch: 101,  batch step: 192, loss: 4.846102714538574\n",
      "epoch: 101,  batch step: 193, loss: 12.834348678588867\n",
      "epoch: 101,  batch step: 194, loss: 10.537044525146484\n",
      "epoch: 101,  batch step: 195, loss: 12.242067337036133\n",
      "epoch: 101,  batch step: 196, loss: 1.666473388671875\n",
      "epoch: 101,  batch step: 197, loss: 2.332517147064209\n",
      "epoch: 101,  batch step: 198, loss: 1.5539555549621582\n",
      "epoch: 101,  batch step: 199, loss: 2.4598207473754883\n",
      "epoch: 101,  batch step: 200, loss: 4.582767009735107\n",
      "epoch: 101,  batch step: 201, loss: 2.1438112258911133\n",
      "epoch: 101,  batch step: 202, loss: 8.978477478027344\n",
      "epoch: 101,  batch step: 203, loss: 22.538890838623047\n",
      "epoch: 101,  batch step: 204, loss: 1.9153130054473877\n",
      "epoch: 101,  batch step: 205, loss: 1.6583185195922852\n",
      "epoch: 101,  batch step: 206, loss: 8.507526397705078\n",
      "epoch: 101,  batch step: 207, loss: 11.974154472351074\n",
      "epoch: 101,  batch step: 208, loss: 3.9795312881469727\n",
      "epoch: 101,  batch step: 209, loss: 6.103919982910156\n",
      "epoch: 101,  batch step: 210, loss: 2.1231350898742676\n",
      "epoch: 101,  batch step: 211, loss: 2.3139524459838867\n",
      "epoch: 101,  batch step: 212, loss: 21.48225212097168\n",
      "epoch: 101,  batch step: 213, loss: 1.8358392715454102\n",
      "epoch: 101,  batch step: 214, loss: 51.498130798339844\n",
      "epoch: 101,  batch step: 215, loss: 3.173213481903076\n",
      "epoch: 101,  batch step: 216, loss: 55.87568283081055\n",
      "epoch: 101,  batch step: 217, loss: 10.018126487731934\n",
      "epoch: 101,  batch step: 218, loss: 2.1112771034240723\n",
      "epoch: 101,  batch step: 219, loss: 36.058563232421875\n",
      "epoch: 101,  batch step: 220, loss: 38.25199508666992\n",
      "epoch: 101,  batch step: 221, loss: 8.462080955505371\n",
      "epoch: 101,  batch step: 222, loss: 4.673288822174072\n",
      "epoch: 101,  batch step: 223, loss: 5.91377067565918\n",
      "epoch: 101,  batch step: 224, loss: 8.547511100769043\n",
      "epoch: 101,  batch step: 225, loss: 5.607564449310303\n",
      "epoch: 101,  batch step: 226, loss: 15.826811790466309\n",
      "epoch: 101,  batch step: 227, loss: 2.7691867351531982\n",
      "epoch: 101,  batch step: 228, loss: 10.211525917053223\n",
      "epoch: 101,  batch step: 229, loss: 2.7458713054656982\n",
      "epoch: 101,  batch step: 230, loss: 1.615192174911499\n",
      "epoch: 101,  batch step: 231, loss: 4.598128795623779\n",
      "epoch: 101,  batch step: 232, loss: 3.227418899536133\n",
      "epoch: 101,  batch step: 233, loss: 16.701519012451172\n",
      "epoch: 101,  batch step: 234, loss: 2.3054556846618652\n",
      "epoch: 101,  batch step: 235, loss: 3.594834804534912\n",
      "epoch: 101,  batch step: 236, loss: 4.723233222961426\n",
      "epoch: 101,  batch step: 237, loss: 3.9290218353271484\n",
      "epoch: 101,  batch step: 238, loss: 13.283472061157227\n",
      "epoch: 101,  batch step: 239, loss: 7.772124290466309\n",
      "epoch: 101,  batch step: 240, loss: 1.8775261640548706\n",
      "epoch: 101,  batch step: 241, loss: 2.760986328125\n",
      "epoch: 101,  batch step: 242, loss: 2.971102476119995\n",
      "epoch: 101,  batch step: 243, loss: 2.525381088256836\n",
      "epoch: 101,  batch step: 244, loss: 4.759408473968506\n",
      "epoch: 101,  batch step: 245, loss: 67.70931243896484\n",
      "epoch: 101,  batch step: 246, loss: 17.814407348632812\n",
      "epoch: 101,  batch step: 247, loss: 4.291642189025879\n",
      "epoch: 101,  batch step: 248, loss: 1.9552385807037354\n",
      "epoch: 101,  batch step: 249, loss: 11.2232666015625\n",
      "epoch: 101,  batch step: 250, loss: 15.118589401245117\n",
      "epoch: 101,  batch step: 251, loss: 15.469873428344727\n",
      "validation error epoch  101:    tensor(67.5879, device='cuda:0')\n",
      "316\n",
      "epoch: 102,  batch step: 0, loss: 1.7106746435165405\n",
      "epoch: 102,  batch step: 1, loss: 8.960254669189453\n",
      "epoch: 102,  batch step: 2, loss: 5.821876525878906\n",
      "epoch: 102,  batch step: 3, loss: 3.593018054962158\n",
      "epoch: 102,  batch step: 4, loss: 51.16961669921875\n",
      "epoch: 102,  batch step: 5, loss: 2.116594076156616\n",
      "epoch: 102,  batch step: 6, loss: 3.593087673187256\n",
      "epoch: 102,  batch step: 7, loss: 23.31106185913086\n",
      "epoch: 102,  batch step: 8, loss: 3.477893829345703\n",
      "epoch: 102,  batch step: 9, loss: 8.326791763305664\n",
      "epoch: 102,  batch step: 10, loss: 2.115065097808838\n",
      "epoch: 102,  batch step: 11, loss: 3.2941207885742188\n",
      "epoch: 102,  batch step: 12, loss: 22.990921020507812\n",
      "epoch: 102,  batch step: 13, loss: 2.8634300231933594\n",
      "epoch: 102,  batch step: 14, loss: 1.9881460666656494\n",
      "epoch: 102,  batch step: 15, loss: 2.616297721862793\n",
      "epoch: 102,  batch step: 16, loss: 2.1908183097839355\n",
      "epoch: 102,  batch step: 17, loss: 3.3702850341796875\n",
      "epoch: 102,  batch step: 18, loss: 1.9185161590576172\n",
      "epoch: 102,  batch step: 19, loss: 10.19001579284668\n",
      "epoch: 102,  batch step: 20, loss: 1.8146781921386719\n",
      "epoch: 102,  batch step: 21, loss: 8.277680397033691\n",
      "epoch: 102,  batch step: 22, loss: 2.516000747680664\n",
      "epoch: 102,  batch step: 23, loss: 14.925069808959961\n",
      "epoch: 102,  batch step: 24, loss: 4.821906566619873\n",
      "epoch: 102,  batch step: 25, loss: 7.847846031188965\n",
      "epoch: 102,  batch step: 26, loss: 13.429232597351074\n",
      "epoch: 102,  batch step: 27, loss: 2.366110324859619\n",
      "epoch: 102,  batch step: 28, loss: 3.5603041648864746\n",
      "epoch: 102,  batch step: 29, loss: 6.246153831481934\n",
      "epoch: 102,  batch step: 30, loss: 2.551182746887207\n",
      "epoch: 102,  batch step: 31, loss: 2.1274542808532715\n",
      "epoch: 102,  batch step: 32, loss: 2.8752923011779785\n",
      "epoch: 102,  batch step: 33, loss: 19.27405548095703\n",
      "epoch: 102,  batch step: 34, loss: 15.805173873901367\n",
      "epoch: 102,  batch step: 35, loss: 6.095612525939941\n",
      "epoch: 102,  batch step: 36, loss: 4.57280158996582\n",
      "epoch: 102,  batch step: 37, loss: 2.3932313919067383\n",
      "epoch: 102,  batch step: 38, loss: 4.1887617111206055\n",
      "epoch: 102,  batch step: 39, loss: 21.277233123779297\n",
      "epoch: 102,  batch step: 40, loss: 2.2535276412963867\n",
      "epoch: 102,  batch step: 41, loss: 1.4882124662399292\n",
      "epoch: 102,  batch step: 42, loss: 9.515771865844727\n",
      "epoch: 102,  batch step: 43, loss: 2.719034194946289\n",
      "epoch: 102,  batch step: 44, loss: 3.2474894523620605\n",
      "epoch: 102,  batch step: 45, loss: 12.227274894714355\n",
      "epoch: 102,  batch step: 46, loss: 3.897380828857422\n",
      "epoch: 102,  batch step: 47, loss: 40.96658706665039\n",
      "epoch: 102,  batch step: 48, loss: 2.6257152557373047\n",
      "epoch: 102,  batch step: 49, loss: 141.1226806640625\n",
      "epoch: 102,  batch step: 50, loss: 9.6307954788208\n",
      "epoch: 102,  batch step: 51, loss: 34.19686508178711\n",
      "epoch: 102,  batch step: 52, loss: 3.170351982116699\n",
      "epoch: 102,  batch step: 53, loss: 10.492133140563965\n",
      "epoch: 102,  batch step: 54, loss: 3.2259700298309326\n",
      "epoch: 102,  batch step: 55, loss: 3.512022018432617\n",
      "epoch: 102,  batch step: 56, loss: 2.5769882202148438\n",
      "epoch: 102,  batch step: 57, loss: 2.243551015853882\n",
      "epoch: 102,  batch step: 58, loss: 7.241461753845215\n",
      "epoch: 102,  batch step: 59, loss: 2.3209216594696045\n",
      "epoch: 102,  batch step: 60, loss: 17.66091537475586\n",
      "epoch: 102,  batch step: 61, loss: 10.359750747680664\n",
      "epoch: 102,  batch step: 62, loss: 1.4771016836166382\n",
      "epoch: 102,  batch step: 63, loss: 13.922980308532715\n",
      "epoch: 102,  batch step: 64, loss: 8.147159576416016\n",
      "epoch: 102,  batch step: 65, loss: 2.087010383605957\n",
      "epoch: 102,  batch step: 66, loss: 2.5580739974975586\n",
      "epoch: 102,  batch step: 67, loss: 12.591447830200195\n",
      "epoch: 102,  batch step: 68, loss: 67.19268798828125\n",
      "epoch: 102,  batch step: 69, loss: 64.22543334960938\n",
      "epoch: 102,  batch step: 70, loss: 27.41139793395996\n",
      "epoch: 102,  batch step: 71, loss: 3.3006796836853027\n",
      "epoch: 102,  batch step: 72, loss: 7.029080867767334\n",
      "epoch: 102,  batch step: 73, loss: 5.904778480529785\n",
      "epoch: 102,  batch step: 74, loss: 10.656387329101562\n",
      "epoch: 102,  batch step: 75, loss: 5.898952484130859\n",
      "epoch: 102,  batch step: 76, loss: 2.487104892730713\n",
      "epoch: 102,  batch step: 77, loss: 9.570268630981445\n",
      "epoch: 102,  batch step: 78, loss: 9.79212474822998\n",
      "epoch: 102,  batch step: 79, loss: 10.379708290100098\n",
      "epoch: 102,  batch step: 80, loss: 1.9743388891220093\n",
      "epoch: 102,  batch step: 81, loss: 1.6051595211029053\n",
      "epoch: 102,  batch step: 82, loss: 2.137946844100952\n",
      "epoch: 102,  batch step: 83, loss: 39.02109909057617\n",
      "epoch: 102,  batch step: 84, loss: 2.5139260292053223\n",
      "epoch: 102,  batch step: 85, loss: 2.0573365688323975\n",
      "epoch: 102,  batch step: 86, loss: 7.177303314208984\n",
      "epoch: 102,  batch step: 87, loss: 1.8551197052001953\n",
      "epoch: 102,  batch step: 88, loss: 1.8106544017791748\n",
      "epoch: 102,  batch step: 89, loss: 1.6816744804382324\n",
      "epoch: 102,  batch step: 90, loss: 19.137588500976562\n",
      "epoch: 102,  batch step: 91, loss: 5.600490093231201\n",
      "epoch: 102,  batch step: 92, loss: 17.8330078125\n",
      "epoch: 102,  batch step: 93, loss: 53.49247360229492\n",
      "epoch: 102,  batch step: 94, loss: 7.284169673919678\n",
      "epoch: 102,  batch step: 95, loss: 2.875659704208374\n",
      "epoch: 102,  batch step: 96, loss: 17.67047119140625\n",
      "epoch: 102,  batch step: 97, loss: 4.168759346008301\n",
      "epoch: 102,  batch step: 98, loss: 7.463120937347412\n",
      "epoch: 102,  batch step: 99, loss: 1.6675466299057007\n",
      "epoch: 102,  batch step: 100, loss: 5.013279914855957\n",
      "epoch: 102,  batch step: 101, loss: 1.6372909545898438\n",
      "epoch: 102,  batch step: 102, loss: 38.458377838134766\n",
      "epoch: 102,  batch step: 103, loss: 2.0180671215057373\n",
      "epoch: 102,  batch step: 104, loss: 7.393580913543701\n",
      "epoch: 102,  batch step: 105, loss: 9.193037986755371\n",
      "epoch: 102,  batch step: 106, loss: 7.304621696472168\n",
      "epoch: 102,  batch step: 107, loss: 18.0992488861084\n",
      "epoch: 102,  batch step: 108, loss: 3.3887572288513184\n",
      "epoch: 102,  batch step: 109, loss: 8.078888893127441\n",
      "epoch: 102,  batch step: 110, loss: 2.5426416397094727\n",
      "epoch: 102,  batch step: 111, loss: 2.89223575592041\n",
      "epoch: 102,  batch step: 112, loss: 1.5007785558700562\n",
      "epoch: 102,  batch step: 113, loss: 2.732189178466797\n",
      "epoch: 102,  batch step: 114, loss: 37.9696159362793\n",
      "epoch: 102,  batch step: 115, loss: 2.0506234169006348\n",
      "epoch: 102,  batch step: 116, loss: 25.73625946044922\n",
      "epoch: 102,  batch step: 117, loss: 26.065855026245117\n",
      "epoch: 102,  batch step: 118, loss: 7.490340232849121\n",
      "epoch: 102,  batch step: 119, loss: 9.577953338623047\n",
      "epoch: 102,  batch step: 120, loss: 11.084135055541992\n",
      "epoch: 102,  batch step: 121, loss: 11.875754356384277\n",
      "epoch: 102,  batch step: 122, loss: 1.7557446956634521\n",
      "epoch: 102,  batch step: 123, loss: 14.903346061706543\n",
      "epoch: 102,  batch step: 124, loss: 16.267597198486328\n",
      "epoch: 102,  batch step: 125, loss: 2.399548053741455\n",
      "epoch: 102,  batch step: 126, loss: 1.4505388736724854\n",
      "epoch: 102,  batch step: 127, loss: 13.797991752624512\n",
      "epoch: 102,  batch step: 128, loss: 2.8005409240722656\n",
      "epoch: 102,  batch step: 129, loss: 13.310133934020996\n",
      "epoch: 102,  batch step: 130, loss: 2.4958596229553223\n",
      "epoch: 102,  batch step: 131, loss: 12.679973602294922\n",
      "epoch: 102,  batch step: 132, loss: 1.5748157501220703\n",
      "epoch: 102,  batch step: 133, loss: 2.4271116256713867\n",
      "epoch: 102,  batch step: 134, loss: 2.152188301086426\n",
      "epoch: 102,  batch step: 135, loss: 2.4768872261047363\n",
      "epoch: 102,  batch step: 136, loss: 1.3076188564300537\n",
      "epoch: 102,  batch step: 137, loss: 11.33200740814209\n",
      "epoch: 102,  batch step: 138, loss: 9.811915397644043\n",
      "epoch: 102,  batch step: 139, loss: 1.9656442403793335\n",
      "epoch: 102,  batch step: 140, loss: 3.109117269515991\n",
      "epoch: 102,  batch step: 141, loss: 13.062956809997559\n",
      "epoch: 102,  batch step: 142, loss: 5.803908824920654\n",
      "epoch: 102,  batch step: 143, loss: 12.35505199432373\n",
      "epoch: 102,  batch step: 144, loss: 1.7297991514205933\n",
      "epoch: 102,  batch step: 145, loss: 14.653676986694336\n",
      "epoch: 102,  batch step: 146, loss: 18.493295669555664\n",
      "epoch: 102,  batch step: 147, loss: 1.5898040533065796\n",
      "epoch: 102,  batch step: 148, loss: 6.269680023193359\n",
      "epoch: 102,  batch step: 149, loss: 6.20393705368042\n",
      "epoch: 102,  batch step: 150, loss: 2.507256031036377\n",
      "epoch: 102,  batch step: 151, loss: 1.8644160032272339\n",
      "epoch: 102,  batch step: 152, loss: 48.01488494873047\n",
      "epoch: 102,  batch step: 153, loss: 15.696810722351074\n",
      "epoch: 102,  batch step: 154, loss: 2.6461472511291504\n",
      "epoch: 102,  batch step: 155, loss: 22.893423080444336\n",
      "epoch: 102,  batch step: 156, loss: 1.98318612575531\n",
      "epoch: 102,  batch step: 157, loss: 11.44365119934082\n",
      "epoch: 102,  batch step: 158, loss: 12.542895317077637\n",
      "epoch: 102,  batch step: 159, loss: 8.259797096252441\n",
      "epoch: 102,  batch step: 160, loss: 12.168926239013672\n",
      "epoch: 102,  batch step: 161, loss: 16.882102966308594\n",
      "epoch: 102,  batch step: 162, loss: 8.419689178466797\n",
      "epoch: 102,  batch step: 163, loss: 14.474102020263672\n",
      "epoch: 102,  batch step: 164, loss: 1.977189064025879\n",
      "epoch: 102,  batch step: 165, loss: 12.837181091308594\n",
      "epoch: 102,  batch step: 166, loss: 3.9015603065490723\n",
      "epoch: 102,  batch step: 167, loss: 1.6504607200622559\n",
      "epoch: 102,  batch step: 168, loss: 8.44339370727539\n",
      "epoch: 102,  batch step: 169, loss: 1.7488534450531006\n",
      "epoch: 102,  batch step: 170, loss: 1.9686716794967651\n",
      "epoch: 102,  batch step: 171, loss: 2.4660239219665527\n",
      "epoch: 102,  batch step: 172, loss: 3.7174105644226074\n",
      "epoch: 102,  batch step: 173, loss: 3.176222801208496\n",
      "epoch: 102,  batch step: 174, loss: 28.720623016357422\n",
      "epoch: 102,  batch step: 175, loss: 4.358999252319336\n",
      "epoch: 102,  batch step: 176, loss: 2.092860698699951\n",
      "epoch: 102,  batch step: 177, loss: 17.771081924438477\n",
      "epoch: 102,  batch step: 178, loss: 15.032785415649414\n",
      "epoch: 102,  batch step: 179, loss: 6.540307521820068\n",
      "epoch: 102,  batch step: 180, loss: 22.2684383392334\n",
      "epoch: 102,  batch step: 181, loss: 31.925060272216797\n",
      "epoch: 102,  batch step: 182, loss: 12.5577392578125\n",
      "epoch: 102,  batch step: 183, loss: 3.3664863109588623\n",
      "epoch: 102,  batch step: 184, loss: 5.221933841705322\n",
      "epoch: 102,  batch step: 185, loss: 15.731340408325195\n",
      "epoch: 102,  batch step: 186, loss: 50.10493087768555\n",
      "epoch: 102,  batch step: 187, loss: 2.094177722930908\n",
      "epoch: 102,  batch step: 188, loss: 3.8645687103271484\n",
      "epoch: 102,  batch step: 189, loss: 3.704845905303955\n",
      "epoch: 102,  batch step: 190, loss: 27.749828338623047\n",
      "epoch: 102,  batch step: 191, loss: 9.338736534118652\n",
      "epoch: 102,  batch step: 192, loss: 69.64344787597656\n",
      "epoch: 102,  batch step: 193, loss: 2.9135189056396484\n",
      "epoch: 102,  batch step: 194, loss: 3.1266894340515137\n",
      "epoch: 102,  batch step: 195, loss: 17.595645904541016\n",
      "epoch: 102,  batch step: 196, loss: 3.0269298553466797\n",
      "epoch: 102,  batch step: 197, loss: 2.5642147064208984\n",
      "epoch: 102,  batch step: 198, loss: 3.643066167831421\n",
      "epoch: 102,  batch step: 199, loss: 1.6622799634933472\n",
      "epoch: 102,  batch step: 200, loss: 6.950721263885498\n",
      "epoch: 102,  batch step: 201, loss: 7.180133819580078\n",
      "epoch: 102,  batch step: 202, loss: 22.069091796875\n",
      "epoch: 102,  batch step: 203, loss: 27.64409637451172\n",
      "epoch: 102,  batch step: 204, loss: 17.566566467285156\n",
      "epoch: 102,  batch step: 205, loss: 8.184843063354492\n",
      "epoch: 102,  batch step: 206, loss: 1.7672786712646484\n",
      "epoch: 102,  batch step: 207, loss: 2.197828769683838\n",
      "epoch: 102,  batch step: 208, loss: 3.347696304321289\n",
      "epoch: 102,  batch step: 209, loss: 40.237831115722656\n",
      "epoch: 102,  batch step: 210, loss: 4.384210109710693\n",
      "epoch: 102,  batch step: 211, loss: 2.4821383953094482\n",
      "epoch: 102,  batch step: 212, loss: 2.6709136962890625\n",
      "epoch: 102,  batch step: 213, loss: 129.51083374023438\n",
      "epoch: 102,  batch step: 214, loss: 2.6820790767669678\n",
      "epoch: 102,  batch step: 215, loss: 13.645597457885742\n",
      "epoch: 102,  batch step: 216, loss: 2.3500118255615234\n",
      "epoch: 102,  batch step: 217, loss: 5.303067684173584\n",
      "epoch: 102,  batch step: 218, loss: 2.4573142528533936\n",
      "epoch: 102,  batch step: 219, loss: 2.722705364227295\n",
      "epoch: 102,  batch step: 220, loss: 2.2502176761627197\n",
      "epoch: 102,  batch step: 221, loss: 1.9705628156661987\n",
      "epoch: 102,  batch step: 222, loss: 2.0696067810058594\n",
      "epoch: 102,  batch step: 223, loss: 27.254356384277344\n",
      "epoch: 102,  batch step: 224, loss: 2.2693357467651367\n",
      "epoch: 102,  batch step: 225, loss: 1.9225530624389648\n",
      "epoch: 102,  batch step: 226, loss: 22.73754119873047\n",
      "epoch: 102,  batch step: 227, loss: 1.7881519794464111\n",
      "epoch: 102,  batch step: 228, loss: 34.31364822387695\n",
      "epoch: 102,  batch step: 229, loss: 13.563773155212402\n",
      "epoch: 102,  batch step: 230, loss: 9.97320556640625\n",
      "epoch: 102,  batch step: 231, loss: 2.2717232704162598\n",
      "epoch: 102,  batch step: 232, loss: 21.144832611083984\n",
      "epoch: 102,  batch step: 233, loss: 18.12018394470215\n",
      "epoch: 102,  batch step: 234, loss: 5.275468349456787\n",
      "epoch: 102,  batch step: 235, loss: 2.4396142959594727\n",
      "epoch: 102,  batch step: 236, loss: 6.129782676696777\n",
      "epoch: 102,  batch step: 237, loss: 1.5381520986557007\n",
      "epoch: 102,  batch step: 238, loss: 17.922115325927734\n",
      "epoch: 102,  batch step: 239, loss: 1.9268081188201904\n",
      "epoch: 102,  batch step: 240, loss: 10.42558479309082\n",
      "epoch: 102,  batch step: 241, loss: 1.5149834156036377\n",
      "epoch: 102,  batch step: 242, loss: 22.47615623474121\n",
      "epoch: 102,  batch step: 243, loss: 18.33466339111328\n",
      "epoch: 102,  batch step: 244, loss: 2.918196678161621\n",
      "epoch: 102,  batch step: 245, loss: 41.405757904052734\n",
      "epoch: 102,  batch step: 246, loss: 56.246978759765625\n",
      "epoch: 102,  batch step: 247, loss: 1.8240302801132202\n",
      "epoch: 102,  batch step: 248, loss: 3.9843080043792725\n",
      "epoch: 102,  batch step: 249, loss: 2.0851593017578125\n",
      "epoch: 102,  batch step: 250, loss: 9.945579528808594\n",
      "epoch: 102,  batch step: 251, loss: 71.09996032714844\n",
      "validation error epoch  102:    tensor(69.3091, device='cuda:0')\n",
      "316\n",
      "epoch: 103,  batch step: 0, loss: 17.287261962890625\n",
      "epoch: 103,  batch step: 1, loss: 15.105409622192383\n",
      "epoch: 103,  batch step: 2, loss: 3.9760262966156006\n",
      "epoch: 103,  batch step: 3, loss: 34.095115661621094\n",
      "epoch: 103,  batch step: 4, loss: 6.409397125244141\n",
      "epoch: 103,  batch step: 5, loss: 10.536394119262695\n",
      "epoch: 103,  batch step: 6, loss: 2.9147307872772217\n",
      "epoch: 103,  batch step: 7, loss: 18.69558334350586\n",
      "epoch: 103,  batch step: 8, loss: 28.249008178710938\n",
      "epoch: 103,  batch step: 9, loss: 4.134160041809082\n",
      "epoch: 103,  batch step: 10, loss: 3.610863447189331\n",
      "epoch: 103,  batch step: 11, loss: 6.435948848724365\n",
      "epoch: 103,  batch step: 12, loss: 53.58769226074219\n",
      "epoch: 103,  batch step: 13, loss: 10.467732429504395\n",
      "epoch: 103,  batch step: 14, loss: 3.7048709392547607\n",
      "epoch: 103,  batch step: 15, loss: 5.701521873474121\n",
      "epoch: 103,  batch step: 16, loss: 10.311991691589355\n",
      "epoch: 103,  batch step: 17, loss: 4.957630157470703\n",
      "epoch: 103,  batch step: 18, loss: 9.720610618591309\n",
      "epoch: 103,  batch step: 19, loss: 5.390214443206787\n",
      "epoch: 103,  batch step: 20, loss: 10.35098648071289\n",
      "epoch: 103,  batch step: 21, loss: 13.32686996459961\n",
      "epoch: 103,  batch step: 22, loss: 13.631209373474121\n",
      "epoch: 103,  batch step: 23, loss: 19.20497703552246\n",
      "epoch: 103,  batch step: 24, loss: 4.881034851074219\n",
      "epoch: 103,  batch step: 25, loss: 12.686347007751465\n",
      "epoch: 103,  batch step: 26, loss: 39.615055084228516\n",
      "epoch: 103,  batch step: 27, loss: 30.006834030151367\n",
      "epoch: 103,  batch step: 28, loss: 4.129422664642334\n",
      "epoch: 103,  batch step: 29, loss: 4.3671770095825195\n",
      "epoch: 103,  batch step: 30, loss: 3.1648268699645996\n",
      "epoch: 103,  batch step: 31, loss: 6.961544990539551\n",
      "epoch: 103,  batch step: 32, loss: 11.713245391845703\n",
      "epoch: 103,  batch step: 33, loss: 19.950061798095703\n",
      "epoch: 103,  batch step: 34, loss: 42.75895309448242\n",
      "epoch: 103,  batch step: 35, loss: 20.647811889648438\n",
      "epoch: 103,  batch step: 36, loss: 3.019331693649292\n",
      "epoch: 103,  batch step: 37, loss: 19.805261611938477\n",
      "epoch: 103,  batch step: 38, loss: 12.974294662475586\n",
      "epoch: 103,  batch step: 39, loss: 14.679159164428711\n",
      "epoch: 103,  batch step: 40, loss: 5.014227390289307\n",
      "epoch: 103,  batch step: 41, loss: 41.869537353515625\n",
      "epoch: 103,  batch step: 42, loss: 3.232302665710449\n",
      "epoch: 103,  batch step: 43, loss: 3.3462934494018555\n",
      "epoch: 103,  batch step: 44, loss: 2.514904260635376\n",
      "epoch: 103,  batch step: 45, loss: 15.090694427490234\n",
      "epoch: 103,  batch step: 46, loss: 19.922348022460938\n",
      "epoch: 103,  batch step: 47, loss: 4.360649108886719\n",
      "epoch: 103,  batch step: 48, loss: 5.007582187652588\n",
      "epoch: 103,  batch step: 49, loss: 27.115819931030273\n",
      "epoch: 103,  batch step: 50, loss: 23.82288360595703\n",
      "epoch: 103,  batch step: 51, loss: 2.827963352203369\n",
      "epoch: 103,  batch step: 52, loss: 2.6159651279449463\n",
      "epoch: 103,  batch step: 53, loss: 3.9560861587524414\n",
      "epoch: 103,  batch step: 54, loss: 19.105213165283203\n",
      "epoch: 103,  batch step: 55, loss: 25.151554107666016\n",
      "epoch: 103,  batch step: 56, loss: 9.706164360046387\n",
      "epoch: 103,  batch step: 57, loss: 39.99092483520508\n",
      "epoch: 103,  batch step: 58, loss: 2.7820897102355957\n",
      "epoch: 103,  batch step: 59, loss: 18.840885162353516\n",
      "epoch: 103,  batch step: 60, loss: 23.4459228515625\n",
      "epoch: 103,  batch step: 61, loss: 2.357726573944092\n",
      "epoch: 103,  batch step: 62, loss: 23.07147789001465\n",
      "epoch: 103,  batch step: 63, loss: 7.975614070892334\n",
      "epoch: 103,  batch step: 64, loss: 3.0427653789520264\n",
      "epoch: 103,  batch step: 65, loss: 16.893213272094727\n",
      "epoch: 103,  batch step: 66, loss: 4.758193016052246\n",
      "epoch: 103,  batch step: 67, loss: 4.4150238037109375\n",
      "epoch: 103,  batch step: 68, loss: 14.173212051391602\n",
      "epoch: 103,  batch step: 69, loss: 6.957047462463379\n",
      "epoch: 103,  batch step: 70, loss: 9.308549880981445\n",
      "epoch: 103,  batch step: 71, loss: 2.150197982788086\n",
      "epoch: 103,  batch step: 72, loss: 34.365638732910156\n",
      "epoch: 103,  batch step: 73, loss: 17.203510284423828\n",
      "epoch: 103,  batch step: 74, loss: 2.3142480850219727\n",
      "epoch: 103,  batch step: 75, loss: 3.9123573303222656\n",
      "epoch: 103,  batch step: 76, loss: 2.258687734603882\n",
      "epoch: 103,  batch step: 77, loss: 19.569801330566406\n",
      "epoch: 103,  batch step: 78, loss: 2.1663291454315186\n",
      "epoch: 103,  batch step: 79, loss: 3.181915760040283\n",
      "epoch: 103,  batch step: 80, loss: 2.4440701007843018\n",
      "epoch: 103,  batch step: 81, loss: 2.4299614429473877\n",
      "epoch: 103,  batch step: 82, loss: 15.018922805786133\n",
      "epoch: 103,  batch step: 83, loss: 1.994092345237732\n",
      "epoch: 103,  batch step: 84, loss: 14.60118293762207\n",
      "epoch: 103,  batch step: 85, loss: 2.8689942359924316\n",
      "epoch: 103,  batch step: 86, loss: 2.350651741027832\n",
      "epoch: 103,  batch step: 87, loss: 1.9702775478363037\n",
      "epoch: 103,  batch step: 88, loss: 3.0954740047454834\n",
      "epoch: 103,  batch step: 89, loss: 18.360090255737305\n",
      "epoch: 103,  batch step: 90, loss: 2.234722852706909\n",
      "epoch: 103,  batch step: 91, loss: 2.7918052673339844\n",
      "epoch: 103,  batch step: 92, loss: 2.107825756072998\n",
      "epoch: 103,  batch step: 93, loss: 6.435437202453613\n",
      "epoch: 103,  batch step: 94, loss: 15.526275634765625\n",
      "epoch: 103,  batch step: 95, loss: 10.498947143554688\n",
      "epoch: 103,  batch step: 96, loss: 15.168146133422852\n",
      "epoch: 103,  batch step: 97, loss: 2.7412774562835693\n",
      "epoch: 103,  batch step: 98, loss: 3.584500312805176\n",
      "epoch: 103,  batch step: 99, loss: 2.825355291366577\n",
      "epoch: 103,  batch step: 100, loss: 11.320509910583496\n",
      "epoch: 103,  batch step: 101, loss: 10.929399490356445\n",
      "epoch: 103,  batch step: 102, loss: 1.8028335571289062\n",
      "epoch: 103,  batch step: 103, loss: 40.68330001831055\n",
      "epoch: 103,  batch step: 104, loss: 1.9834831953048706\n",
      "epoch: 103,  batch step: 105, loss: 2.5194602012634277\n",
      "epoch: 103,  batch step: 106, loss: 2.6171858310699463\n",
      "epoch: 103,  batch step: 107, loss: 46.670989990234375\n",
      "epoch: 103,  batch step: 108, loss: 28.576282501220703\n",
      "epoch: 103,  batch step: 109, loss: 1.858690619468689\n",
      "epoch: 103,  batch step: 110, loss: 2.5479044914245605\n",
      "epoch: 103,  batch step: 111, loss: 26.471715927124023\n",
      "epoch: 103,  batch step: 112, loss: 3.218782424926758\n",
      "epoch: 103,  batch step: 113, loss: 37.40141677856445\n",
      "epoch: 103,  batch step: 114, loss: 3.543307065963745\n",
      "epoch: 103,  batch step: 115, loss: 2.489063262939453\n",
      "epoch: 103,  batch step: 116, loss: 3.6577517986297607\n",
      "epoch: 103,  batch step: 117, loss: 8.684675216674805\n",
      "epoch: 103,  batch step: 118, loss: 5.289597511291504\n",
      "epoch: 103,  batch step: 119, loss: 5.207284927368164\n",
      "epoch: 103,  batch step: 120, loss: 2.4221906661987305\n",
      "epoch: 103,  batch step: 121, loss: 16.231788635253906\n",
      "epoch: 103,  batch step: 122, loss: 3.921210765838623\n",
      "epoch: 103,  batch step: 123, loss: 3.8204822540283203\n",
      "epoch: 103,  batch step: 124, loss: 93.37478637695312\n",
      "epoch: 103,  batch step: 125, loss: 4.493568420410156\n",
      "epoch: 103,  batch step: 126, loss: 12.383440971374512\n",
      "epoch: 103,  batch step: 127, loss: 2.2131290435791016\n",
      "epoch: 103,  batch step: 128, loss: 3.6977152824401855\n",
      "epoch: 103,  batch step: 129, loss: 27.901487350463867\n",
      "epoch: 103,  batch step: 130, loss: 50.46080017089844\n",
      "epoch: 103,  batch step: 131, loss: 6.8716325759887695\n",
      "epoch: 103,  batch step: 132, loss: 23.555646896362305\n",
      "epoch: 103,  batch step: 133, loss: 2.5972001552581787\n",
      "epoch: 103,  batch step: 134, loss: 5.686614990234375\n",
      "epoch: 103,  batch step: 135, loss: 2.334698438644409\n",
      "epoch: 103,  batch step: 136, loss: 16.045974731445312\n",
      "epoch: 103,  batch step: 137, loss: 2.490605115890503\n",
      "epoch: 103,  batch step: 138, loss: 4.493195056915283\n",
      "epoch: 103,  batch step: 139, loss: 2.5421972274780273\n",
      "epoch: 103,  batch step: 140, loss: 2.04787015914917\n",
      "epoch: 103,  batch step: 141, loss: 28.289403915405273\n",
      "epoch: 103,  batch step: 142, loss: 47.570587158203125\n",
      "epoch: 103,  batch step: 143, loss: 16.57666015625\n",
      "epoch: 103,  batch step: 144, loss: 50.95231246948242\n",
      "epoch: 103,  batch step: 145, loss: 4.382143020629883\n",
      "epoch: 103,  batch step: 146, loss: 31.07834243774414\n",
      "epoch: 103,  batch step: 147, loss: 2.3029513359069824\n",
      "epoch: 103,  batch step: 148, loss: 6.379115104675293\n",
      "epoch: 103,  batch step: 149, loss: 7.707198143005371\n",
      "epoch: 103,  batch step: 150, loss: 3.462286949157715\n",
      "epoch: 103,  batch step: 151, loss: 39.1607666015625\n",
      "epoch: 103,  batch step: 152, loss: 6.013450622558594\n",
      "epoch: 103,  batch step: 153, loss: 7.417356491088867\n",
      "epoch: 103,  batch step: 154, loss: 2.094989776611328\n",
      "epoch: 103,  batch step: 155, loss: 9.970344543457031\n",
      "epoch: 103,  batch step: 156, loss: 16.83384895324707\n",
      "epoch: 103,  batch step: 157, loss: 3.624380588531494\n",
      "epoch: 103,  batch step: 158, loss: 8.47740650177002\n",
      "epoch: 103,  batch step: 159, loss: 31.377517700195312\n",
      "epoch: 103,  batch step: 160, loss: 5.241402626037598\n",
      "epoch: 103,  batch step: 161, loss: 2.505801200866699\n",
      "epoch: 103,  batch step: 162, loss: 18.28778076171875\n",
      "epoch: 103,  batch step: 163, loss: 3.008505344390869\n",
      "epoch: 103,  batch step: 164, loss: 8.003779411315918\n",
      "epoch: 103,  batch step: 165, loss: 14.27286434173584\n",
      "epoch: 103,  batch step: 166, loss: 11.616786003112793\n",
      "epoch: 103,  batch step: 167, loss: 10.163208961486816\n",
      "epoch: 103,  batch step: 168, loss: 57.06510543823242\n",
      "epoch: 103,  batch step: 169, loss: 12.788127899169922\n",
      "epoch: 103,  batch step: 170, loss: 15.800230979919434\n",
      "epoch: 103,  batch step: 171, loss: 2.3354547023773193\n",
      "epoch: 103,  batch step: 172, loss: 32.14573287963867\n",
      "epoch: 103,  batch step: 173, loss: 6.903605937957764\n",
      "epoch: 103,  batch step: 174, loss: 5.437896251678467\n",
      "epoch: 103,  batch step: 175, loss: 21.526775360107422\n",
      "epoch: 103,  batch step: 176, loss: 2.746694326400757\n",
      "epoch: 103,  batch step: 177, loss: 12.083105087280273\n",
      "epoch: 103,  batch step: 178, loss: 2.9980111122131348\n",
      "epoch: 103,  batch step: 179, loss: 23.589916229248047\n",
      "epoch: 103,  batch step: 180, loss: 2.395406723022461\n",
      "epoch: 103,  batch step: 181, loss: 3.028346061706543\n",
      "epoch: 103,  batch step: 182, loss: 1.717007040977478\n",
      "epoch: 103,  batch step: 183, loss: 146.87461853027344\n",
      "epoch: 103,  batch step: 184, loss: 18.987667083740234\n",
      "epoch: 103,  batch step: 185, loss: 36.54003143310547\n",
      "epoch: 103,  batch step: 186, loss: 1.759017825126648\n",
      "epoch: 103,  batch step: 187, loss: 2.9841554164886475\n",
      "epoch: 103,  batch step: 188, loss: 56.8497428894043\n",
      "epoch: 103,  batch step: 189, loss: 2.1681811809539795\n",
      "epoch: 103,  batch step: 190, loss: 3.4727675914764404\n",
      "epoch: 103,  batch step: 191, loss: 38.64656066894531\n",
      "epoch: 103,  batch step: 192, loss: 7.862126350402832\n",
      "epoch: 103,  batch step: 193, loss: 13.293439865112305\n",
      "epoch: 103,  batch step: 194, loss: 3.209216594696045\n",
      "epoch: 103,  batch step: 195, loss: 7.369504928588867\n",
      "epoch: 103,  batch step: 196, loss: 2.081890821456909\n",
      "epoch: 103,  batch step: 197, loss: 40.08976745605469\n",
      "epoch: 103,  batch step: 198, loss: 7.361464500427246\n",
      "epoch: 103,  batch step: 199, loss: 26.720806121826172\n",
      "epoch: 103,  batch step: 200, loss: 14.474326133728027\n",
      "epoch: 103,  batch step: 201, loss: 8.154258728027344\n",
      "epoch: 103,  batch step: 202, loss: 40.9124641418457\n",
      "epoch: 103,  batch step: 203, loss: 4.371518135070801\n",
      "epoch: 103,  batch step: 204, loss: 2.245868444442749\n",
      "epoch: 103,  batch step: 205, loss: 2.8370280265808105\n",
      "epoch: 103,  batch step: 206, loss: 3.742187976837158\n",
      "epoch: 103,  batch step: 207, loss: 25.62491226196289\n",
      "epoch: 103,  batch step: 208, loss: 32.38331604003906\n",
      "epoch: 103,  batch step: 209, loss: 3.0931031703948975\n",
      "epoch: 103,  batch step: 210, loss: 53.16343688964844\n",
      "epoch: 103,  batch step: 211, loss: 2.496767997741699\n",
      "epoch: 103,  batch step: 212, loss: 21.884197235107422\n",
      "epoch: 103,  batch step: 213, loss: 3.9661102294921875\n",
      "epoch: 103,  batch step: 214, loss: 3.752955198287964\n",
      "epoch: 103,  batch step: 215, loss: 3.335371971130371\n",
      "epoch: 103,  batch step: 216, loss: 10.073534965515137\n",
      "epoch: 103,  batch step: 217, loss: 1.769864797592163\n",
      "epoch: 103,  batch step: 218, loss: 19.549999237060547\n",
      "epoch: 103,  batch step: 219, loss: 3.6213767528533936\n",
      "epoch: 103,  batch step: 220, loss: 14.753314971923828\n",
      "epoch: 103,  batch step: 221, loss: 18.060087203979492\n",
      "epoch: 103,  batch step: 222, loss: 3.5911993980407715\n",
      "epoch: 103,  batch step: 223, loss: 6.5687150955200195\n",
      "epoch: 103,  batch step: 224, loss: 3.8786721229553223\n",
      "epoch: 103,  batch step: 225, loss: 3.4648520946502686\n",
      "epoch: 103,  batch step: 226, loss: 2.7339110374450684\n",
      "epoch: 103,  batch step: 227, loss: 10.6644287109375\n",
      "epoch: 103,  batch step: 228, loss: 3.7159438133239746\n",
      "epoch: 103,  batch step: 229, loss: 20.536239624023438\n",
      "epoch: 103,  batch step: 230, loss: 2.8314666748046875\n",
      "epoch: 103,  batch step: 231, loss: 2.928893566131592\n",
      "epoch: 103,  batch step: 232, loss: 2.270991325378418\n",
      "epoch: 103,  batch step: 233, loss: 3.1612930297851562\n",
      "epoch: 103,  batch step: 234, loss: 79.52887725830078\n",
      "epoch: 103,  batch step: 235, loss: 9.115777969360352\n",
      "epoch: 103,  batch step: 236, loss: 37.21565246582031\n",
      "epoch: 103,  batch step: 237, loss: 19.960723876953125\n",
      "epoch: 103,  batch step: 238, loss: 1.5787341594696045\n",
      "epoch: 103,  batch step: 239, loss: 8.751354217529297\n",
      "epoch: 103,  batch step: 240, loss: 43.87628936767578\n",
      "epoch: 103,  batch step: 241, loss: 1.8159488439559937\n",
      "epoch: 103,  batch step: 242, loss: 2.106926918029785\n",
      "epoch: 103,  batch step: 243, loss: 2.955077648162842\n",
      "epoch: 103,  batch step: 244, loss: 9.123933792114258\n",
      "epoch: 103,  batch step: 245, loss: 31.58478355407715\n",
      "epoch: 103,  batch step: 246, loss: 2.7842793464660645\n",
      "epoch: 103,  batch step: 247, loss: 7.7377142906188965\n",
      "epoch: 103,  batch step: 248, loss: 1.5860776901245117\n",
      "epoch: 103,  batch step: 249, loss: 14.354743957519531\n",
      "epoch: 103,  batch step: 250, loss: 1.6659891605377197\n",
      "epoch: 103,  batch step: 251, loss: 296.18414306640625\n",
      "validation error epoch  103:    tensor(68.1494, device='cuda:0')\n",
      "316\n",
      "epoch: 104,  batch step: 0, loss: 2.568286418914795\n",
      "epoch: 104,  batch step: 1, loss: 22.80933952331543\n",
      "epoch: 104,  batch step: 2, loss: 15.113664627075195\n",
      "epoch: 104,  batch step: 3, loss: 36.14304733276367\n",
      "epoch: 104,  batch step: 4, loss: 10.441219329833984\n",
      "epoch: 104,  batch step: 5, loss: 7.814070701599121\n",
      "epoch: 104,  batch step: 6, loss: 10.905844688415527\n",
      "epoch: 104,  batch step: 7, loss: 11.620760917663574\n",
      "epoch: 104,  batch step: 8, loss: 232.59848022460938\n",
      "epoch: 104,  batch step: 9, loss: 177.20562744140625\n",
      "epoch: 104,  batch step: 10, loss: 107.20012664794922\n",
      "epoch: 104,  batch step: 11, loss: 6.836511135101318\n",
      "epoch: 104,  batch step: 12, loss: 10.601408004760742\n",
      "epoch: 104,  batch step: 13, loss: 326.8009948730469\n",
      "epoch: 104,  batch step: 14, loss: 19.310861587524414\n",
      "epoch: 104,  batch step: 15, loss: 15.493108749389648\n",
      "epoch: 104,  batch step: 16, loss: 86.02105712890625\n",
      "epoch: 104,  batch step: 17, loss: 16.586814880371094\n",
      "epoch: 104,  batch step: 18, loss: 21.141990661621094\n",
      "epoch: 104,  batch step: 19, loss: 23.40338897705078\n",
      "epoch: 104,  batch step: 20, loss: 7.553566932678223\n",
      "epoch: 104,  batch step: 21, loss: 87.33705139160156\n",
      "epoch: 104,  batch step: 22, loss: 50.11062240600586\n",
      "epoch: 104,  batch step: 23, loss: 53.656463623046875\n",
      "epoch: 104,  batch step: 24, loss: 37.611995697021484\n",
      "epoch: 104,  batch step: 25, loss: 14.877220153808594\n",
      "epoch: 104,  batch step: 26, loss: 73.11111450195312\n",
      "epoch: 104,  batch step: 27, loss: 13.871129989624023\n",
      "epoch: 104,  batch step: 28, loss: 26.0452880859375\n",
      "epoch: 104,  batch step: 29, loss: 29.136533737182617\n",
      "epoch: 104,  batch step: 30, loss: 12.674774169921875\n",
      "epoch: 104,  batch step: 31, loss: 53.08970642089844\n",
      "epoch: 104,  batch step: 32, loss: 7.419102668762207\n",
      "epoch: 104,  batch step: 33, loss: 28.5374755859375\n",
      "epoch: 104,  batch step: 34, loss: 4.171300411224365\n",
      "epoch: 104,  batch step: 35, loss: 6.137439250946045\n",
      "epoch: 104,  batch step: 36, loss: 44.989830017089844\n",
      "epoch: 104,  batch step: 37, loss: 4.3295440673828125\n",
      "epoch: 104,  batch step: 38, loss: 4.042071342468262\n",
      "epoch: 104,  batch step: 39, loss: 5.325355529785156\n",
      "epoch: 104,  batch step: 40, loss: 7.470571517944336\n",
      "epoch: 104,  batch step: 41, loss: 61.59052276611328\n",
      "epoch: 104,  batch step: 42, loss: 27.317245483398438\n",
      "epoch: 104,  batch step: 43, loss: 7.670537948608398\n",
      "epoch: 104,  batch step: 44, loss: 6.258260726928711\n",
      "epoch: 104,  batch step: 45, loss: 93.41085052490234\n",
      "epoch: 104,  batch step: 46, loss: 72.00007629394531\n",
      "epoch: 104,  batch step: 47, loss: 5.4020233154296875\n",
      "epoch: 104,  batch step: 48, loss: 11.84238052368164\n",
      "epoch: 104,  batch step: 49, loss: 29.624649047851562\n",
      "epoch: 104,  batch step: 50, loss: 7.5019989013671875\n",
      "epoch: 104,  batch step: 51, loss: 17.328380584716797\n",
      "epoch: 104,  batch step: 52, loss: 6.3141913414001465\n",
      "epoch: 104,  batch step: 53, loss: 33.73871994018555\n",
      "epoch: 104,  batch step: 54, loss: 17.11145782470703\n",
      "epoch: 104,  batch step: 55, loss: 24.375835418701172\n",
      "epoch: 104,  batch step: 56, loss: 9.173871994018555\n",
      "epoch: 104,  batch step: 57, loss: 15.06482219696045\n",
      "epoch: 104,  batch step: 58, loss: 31.79766082763672\n",
      "epoch: 104,  batch step: 59, loss: 7.213311195373535\n",
      "epoch: 104,  batch step: 60, loss: 3.2942380905151367\n",
      "epoch: 104,  batch step: 61, loss: 65.71327209472656\n",
      "epoch: 104,  batch step: 62, loss: 48.1663932800293\n",
      "epoch: 104,  batch step: 63, loss: 24.660812377929688\n",
      "epoch: 104,  batch step: 64, loss: 6.550642013549805\n",
      "epoch: 104,  batch step: 65, loss: 7.301003932952881\n",
      "epoch: 104,  batch step: 66, loss: 20.593303680419922\n",
      "epoch: 104,  batch step: 67, loss: 3.04947829246521\n",
      "epoch: 104,  batch step: 68, loss: 24.810575485229492\n",
      "epoch: 104,  batch step: 69, loss: 5.699556350708008\n",
      "epoch: 104,  batch step: 70, loss: 4.050774574279785\n",
      "epoch: 104,  batch step: 71, loss: 37.17718505859375\n",
      "epoch: 104,  batch step: 72, loss: 5.607433319091797\n",
      "epoch: 104,  batch step: 73, loss: 193.64401245117188\n",
      "epoch: 104,  batch step: 74, loss: 11.342458724975586\n",
      "epoch: 104,  batch step: 75, loss: 189.8462371826172\n",
      "epoch: 104,  batch step: 76, loss: 9.347618103027344\n",
      "epoch: 104,  batch step: 77, loss: 5.651144981384277\n",
      "epoch: 104,  batch step: 78, loss: 85.54588317871094\n",
      "epoch: 104,  batch step: 79, loss: 110.17805480957031\n",
      "epoch: 104,  batch step: 80, loss: 151.87408447265625\n",
      "epoch: 104,  batch step: 81, loss: 90.52287292480469\n",
      "epoch: 104,  batch step: 82, loss: 11.335403442382812\n",
      "epoch: 104,  batch step: 83, loss: 5.357480525970459\n",
      "epoch: 104,  batch step: 84, loss: 18.7473087310791\n",
      "epoch: 104,  batch step: 85, loss: 56.37187957763672\n",
      "epoch: 104,  batch step: 86, loss: 6.692964553833008\n",
      "epoch: 104,  batch step: 87, loss: 56.18933868408203\n",
      "epoch: 104,  batch step: 88, loss: 3.0770649909973145\n",
      "epoch: 104,  batch step: 89, loss: 41.95624923706055\n",
      "epoch: 104,  batch step: 90, loss: 8.816411972045898\n",
      "epoch: 104,  batch step: 91, loss: 8.838115692138672\n",
      "epoch: 104,  batch step: 92, loss: 29.931396484375\n",
      "epoch: 104,  batch step: 93, loss: 23.902301788330078\n",
      "epoch: 104,  batch step: 94, loss: 7.175631523132324\n",
      "epoch: 104,  batch step: 95, loss: 8.040367126464844\n",
      "epoch: 104,  batch step: 96, loss: 60.41294860839844\n",
      "epoch: 104,  batch step: 97, loss: 30.518110275268555\n",
      "epoch: 104,  batch step: 98, loss: 26.04698944091797\n",
      "epoch: 104,  batch step: 99, loss: 61.507564544677734\n",
      "epoch: 104,  batch step: 100, loss: 102.33401489257812\n",
      "epoch: 104,  batch step: 101, loss: 38.15363311767578\n",
      "epoch: 104,  batch step: 102, loss: 47.758758544921875\n",
      "epoch: 104,  batch step: 103, loss: 31.213104248046875\n",
      "epoch: 104,  batch step: 104, loss: 6.195667266845703\n",
      "epoch: 104,  batch step: 105, loss: 23.932889938354492\n",
      "epoch: 104,  batch step: 106, loss: 8.468337059020996\n",
      "epoch: 104,  batch step: 107, loss: 3.602900981903076\n",
      "epoch: 104,  batch step: 108, loss: 54.45563888549805\n",
      "epoch: 104,  batch step: 109, loss: 10.034420013427734\n",
      "epoch: 104,  batch step: 110, loss: 3.571876049041748\n",
      "epoch: 104,  batch step: 111, loss: 7.552397727966309\n",
      "epoch: 104,  batch step: 112, loss: 32.09721374511719\n",
      "epoch: 104,  batch step: 113, loss: 23.236135482788086\n",
      "epoch: 104,  batch step: 114, loss: 3.402883291244507\n",
      "epoch: 104,  batch step: 115, loss: 93.09024047851562\n",
      "epoch: 104,  batch step: 116, loss: 3.7271602153778076\n",
      "epoch: 104,  batch step: 117, loss: 14.441272735595703\n",
      "epoch: 104,  batch step: 118, loss: 3.015644073486328\n",
      "epoch: 104,  batch step: 119, loss: 3.287148952484131\n",
      "epoch: 104,  batch step: 120, loss: 7.172065734863281\n",
      "epoch: 104,  batch step: 121, loss: 20.147674560546875\n",
      "epoch: 104,  batch step: 122, loss: 97.06729125976562\n",
      "epoch: 104,  batch step: 123, loss: 3.9724678993225098\n",
      "epoch: 104,  batch step: 124, loss: 4.0611138343811035\n",
      "epoch: 104,  batch step: 125, loss: 5.26652717590332\n",
      "epoch: 104,  batch step: 126, loss: 6.930123805999756\n",
      "epoch: 104,  batch step: 127, loss: 26.521446228027344\n",
      "epoch: 104,  batch step: 128, loss: 3.994784355163574\n",
      "epoch: 104,  batch step: 129, loss: 5.446675777435303\n",
      "epoch: 104,  batch step: 130, loss: 16.49710464477539\n",
      "epoch: 104,  batch step: 131, loss: 15.30892562866211\n",
      "epoch: 104,  batch step: 132, loss: 17.285091400146484\n",
      "epoch: 104,  batch step: 133, loss: 88.6417465209961\n",
      "epoch: 104,  batch step: 134, loss: 90.40642547607422\n",
      "epoch: 104,  batch step: 135, loss: 22.479915618896484\n",
      "epoch: 104,  batch step: 136, loss: 10.801222801208496\n",
      "epoch: 104,  batch step: 137, loss: 3.354154586791992\n",
      "epoch: 104,  batch step: 138, loss: 49.559139251708984\n",
      "epoch: 104,  batch step: 139, loss: 7.117045879364014\n",
      "epoch: 104,  batch step: 140, loss: 18.566326141357422\n",
      "epoch: 104,  batch step: 141, loss: 140.4923095703125\n",
      "epoch: 104,  batch step: 142, loss: 7.158318996429443\n",
      "epoch: 104,  batch step: 143, loss: 135.71055603027344\n",
      "epoch: 104,  batch step: 144, loss: 3.8617210388183594\n",
      "epoch: 104,  batch step: 145, loss: 146.45066833496094\n",
      "epoch: 104,  batch step: 146, loss: 7.047618865966797\n",
      "epoch: 104,  batch step: 147, loss: 6.514071464538574\n",
      "epoch: 104,  batch step: 148, loss: 18.43549919128418\n",
      "epoch: 104,  batch step: 149, loss: 9.278079986572266\n",
      "epoch: 104,  batch step: 150, loss: 6.524383544921875\n",
      "epoch: 104,  batch step: 151, loss: 27.109844207763672\n",
      "epoch: 104,  batch step: 152, loss: 59.93000411987305\n",
      "epoch: 104,  batch step: 153, loss: 89.60931396484375\n",
      "epoch: 104,  batch step: 154, loss: 68.96819305419922\n",
      "epoch: 104,  batch step: 155, loss: 17.83608055114746\n",
      "epoch: 104,  batch step: 156, loss: 53.66218185424805\n",
      "epoch: 104,  batch step: 157, loss: 42.165245056152344\n",
      "epoch: 104,  batch step: 158, loss: 12.89353084564209\n",
      "epoch: 104,  batch step: 159, loss: 16.079267501831055\n",
      "epoch: 104,  batch step: 160, loss: 193.71942138671875\n",
      "epoch: 104,  batch step: 161, loss: 49.788299560546875\n",
      "epoch: 104,  batch step: 162, loss: 42.678489685058594\n",
      "epoch: 104,  batch step: 163, loss: 144.97244262695312\n",
      "epoch: 104,  batch step: 164, loss: 12.106346130371094\n",
      "epoch: 104,  batch step: 165, loss: 29.044841766357422\n",
      "epoch: 104,  batch step: 166, loss: 22.874408721923828\n",
      "epoch: 104,  batch step: 167, loss: 26.026790618896484\n",
      "epoch: 104,  batch step: 168, loss: 108.44100189208984\n",
      "epoch: 104,  batch step: 169, loss: 177.5912322998047\n",
      "epoch: 104,  batch step: 170, loss: 11.92785930633545\n",
      "epoch: 104,  batch step: 171, loss: 56.30919647216797\n",
      "epoch: 104,  batch step: 172, loss: 8.186824798583984\n",
      "epoch: 104,  batch step: 173, loss: 13.36553955078125\n",
      "epoch: 104,  batch step: 174, loss: 21.8461971282959\n",
      "epoch: 104,  batch step: 175, loss: 69.25523376464844\n",
      "epoch: 104,  batch step: 176, loss: 7.1980133056640625\n",
      "epoch: 104,  batch step: 177, loss: 36.82668685913086\n",
      "epoch: 104,  batch step: 178, loss: 13.643230438232422\n",
      "epoch: 104,  batch step: 179, loss: 23.627567291259766\n",
      "epoch: 104,  batch step: 180, loss: 8.036206245422363\n",
      "epoch: 104,  batch step: 181, loss: 105.07820892333984\n",
      "epoch: 104,  batch step: 182, loss: 34.7056999206543\n",
      "epoch: 104,  batch step: 183, loss: 156.15139770507812\n",
      "epoch: 104,  batch step: 184, loss: 40.98462677001953\n",
      "epoch: 104,  batch step: 185, loss: 21.264732360839844\n",
      "epoch: 104,  batch step: 186, loss: 141.6424560546875\n",
      "epoch: 104,  batch step: 187, loss: 5.203553676605225\n",
      "epoch: 104,  batch step: 188, loss: 4.89792013168335\n",
      "epoch: 104,  batch step: 189, loss: 26.754520416259766\n",
      "epoch: 104,  batch step: 190, loss: 55.59797286987305\n",
      "epoch: 104,  batch step: 191, loss: 191.6697235107422\n",
      "epoch: 104,  batch step: 192, loss: 8.740927696228027\n",
      "epoch: 104,  batch step: 193, loss: 18.22467803955078\n",
      "epoch: 104,  batch step: 194, loss: 6.615228652954102\n",
      "epoch: 104,  batch step: 195, loss: 7.429431915283203\n",
      "epoch: 104,  batch step: 196, loss: 28.025863647460938\n",
      "epoch: 104,  batch step: 197, loss: 63.841758728027344\n",
      "epoch: 104,  batch step: 198, loss: 13.693410873413086\n",
      "epoch: 104,  batch step: 199, loss: 3.4565303325653076\n",
      "epoch: 104,  batch step: 200, loss: 30.791210174560547\n",
      "epoch: 104,  batch step: 201, loss: 6.80352783203125\n",
      "epoch: 104,  batch step: 202, loss: 6.552457332611084\n",
      "epoch: 104,  batch step: 203, loss: 14.739913940429688\n",
      "epoch: 104,  batch step: 204, loss: 3.4379305839538574\n",
      "epoch: 104,  batch step: 205, loss: 22.51321792602539\n",
      "epoch: 104,  batch step: 206, loss: 4.727761268615723\n",
      "epoch: 104,  batch step: 207, loss: 117.16011047363281\n",
      "epoch: 104,  batch step: 208, loss: 61.903900146484375\n",
      "epoch: 104,  batch step: 209, loss: 4.554919719696045\n",
      "epoch: 104,  batch step: 210, loss: 9.003174781799316\n",
      "epoch: 104,  batch step: 211, loss: 31.630538940429688\n",
      "epoch: 104,  batch step: 212, loss: 6.126467704772949\n",
      "epoch: 104,  batch step: 213, loss: 5.4232497215271\n",
      "epoch: 104,  batch step: 214, loss: 5.711625576019287\n",
      "epoch: 104,  batch step: 215, loss: 59.31928634643555\n",
      "epoch: 104,  batch step: 216, loss: 9.638710021972656\n",
      "epoch: 104,  batch step: 217, loss: 4.796555995941162\n",
      "epoch: 104,  batch step: 218, loss: 3.9211907386779785\n",
      "epoch: 104,  batch step: 219, loss: 12.245634078979492\n",
      "epoch: 104,  batch step: 220, loss: 5.980467796325684\n",
      "epoch: 104,  batch step: 221, loss: 3.826897144317627\n",
      "epoch: 104,  batch step: 222, loss: 4.4796881675720215\n",
      "epoch: 104,  batch step: 223, loss: 3.9597184658050537\n",
      "epoch: 104,  batch step: 224, loss: 138.40414428710938\n",
      "epoch: 104,  batch step: 225, loss: 60.92863464355469\n",
      "epoch: 104,  batch step: 226, loss: 49.28633117675781\n",
      "epoch: 104,  batch step: 227, loss: 5.576657295227051\n",
      "epoch: 104,  batch step: 228, loss: 36.556087493896484\n",
      "epoch: 104,  batch step: 229, loss: 49.178131103515625\n",
      "epoch: 104,  batch step: 230, loss: 73.10942077636719\n",
      "epoch: 104,  batch step: 231, loss: 5.30608606338501\n",
      "epoch: 104,  batch step: 232, loss: 67.61598205566406\n",
      "epoch: 104,  batch step: 233, loss: 6.370009422302246\n",
      "epoch: 104,  batch step: 234, loss: 3.9962782859802246\n",
      "epoch: 104,  batch step: 235, loss: 19.46586036682129\n",
      "epoch: 104,  batch step: 236, loss: 6.580672264099121\n",
      "epoch: 104,  batch step: 237, loss: 18.166715621948242\n",
      "epoch: 104,  batch step: 238, loss: 31.365264892578125\n",
      "epoch: 104,  batch step: 239, loss: 4.145611763000488\n",
      "epoch: 104,  batch step: 240, loss: 12.145763397216797\n",
      "epoch: 104,  batch step: 241, loss: 66.60138702392578\n",
      "epoch: 104,  batch step: 242, loss: 4.916251182556152\n",
      "epoch: 104,  batch step: 243, loss: 4.912829399108887\n",
      "epoch: 104,  batch step: 244, loss: 31.05846405029297\n",
      "epoch: 104,  batch step: 245, loss: 39.865272521972656\n",
      "epoch: 104,  batch step: 246, loss: 3.2669267654418945\n",
      "epoch: 104,  batch step: 247, loss: 17.809473037719727\n",
      "epoch: 104,  batch step: 248, loss: 88.68025970458984\n",
      "epoch: 104,  batch step: 249, loss: 65.10450744628906\n",
      "epoch: 104,  batch step: 250, loss: 5.463077068328857\n",
      "epoch: 104,  batch step: 251, loss: 18.80385971069336\n",
      "validation error epoch  104:    tensor(72.5425, device='cuda:0')\n",
      "316\n",
      "epoch: 105,  batch step: 0, loss: 56.648826599121094\n",
      "epoch: 105,  batch step: 1, loss: 4.759921073913574\n",
      "epoch: 105,  batch step: 2, loss: 8.142330169677734\n",
      "epoch: 105,  batch step: 3, loss: 3.557676315307617\n",
      "epoch: 105,  batch step: 4, loss: 4.167183876037598\n",
      "epoch: 105,  batch step: 5, loss: 10.133749008178711\n",
      "epoch: 105,  batch step: 6, loss: 35.8729133605957\n",
      "epoch: 105,  batch step: 7, loss: 10.951495170593262\n",
      "epoch: 105,  batch step: 8, loss: 21.14599609375\n",
      "epoch: 105,  batch step: 9, loss: 35.42948913574219\n",
      "epoch: 105,  batch step: 10, loss: 3.2109761238098145\n",
      "epoch: 105,  batch step: 11, loss: 20.181629180908203\n",
      "epoch: 105,  batch step: 12, loss: 17.19283103942871\n",
      "epoch: 105,  batch step: 13, loss: 5.811888694763184\n",
      "epoch: 105,  batch step: 14, loss: 7.4274115562438965\n",
      "epoch: 105,  batch step: 15, loss: 2.2884998321533203\n",
      "epoch: 105,  batch step: 16, loss: 2.8076162338256836\n",
      "epoch: 105,  batch step: 17, loss: 15.734067916870117\n",
      "epoch: 105,  batch step: 18, loss: 51.6121826171875\n",
      "epoch: 105,  batch step: 19, loss: 60.779014587402344\n",
      "epoch: 105,  batch step: 20, loss: 26.782577514648438\n",
      "epoch: 105,  batch step: 21, loss: 7.890219688415527\n",
      "epoch: 105,  batch step: 22, loss: 12.314933776855469\n",
      "epoch: 105,  batch step: 23, loss: 42.29487228393555\n",
      "epoch: 105,  batch step: 24, loss: 6.070898056030273\n",
      "epoch: 105,  batch step: 25, loss: 4.081936836242676\n",
      "epoch: 105,  batch step: 26, loss: 3.5556888580322266\n",
      "epoch: 105,  batch step: 27, loss: 16.216758728027344\n",
      "epoch: 105,  batch step: 28, loss: 38.336265563964844\n",
      "epoch: 105,  batch step: 29, loss: 20.014060974121094\n",
      "epoch: 105,  batch step: 30, loss: 3.7658791542053223\n",
      "epoch: 105,  batch step: 31, loss: 32.0828971862793\n",
      "epoch: 105,  batch step: 32, loss: 3.467541456222534\n",
      "epoch: 105,  batch step: 33, loss: 4.5410871505737305\n",
      "epoch: 105,  batch step: 34, loss: 8.3760986328125\n",
      "epoch: 105,  batch step: 35, loss: 23.9971981048584\n",
      "epoch: 105,  batch step: 36, loss: 42.665523529052734\n",
      "epoch: 105,  batch step: 37, loss: 12.41629695892334\n",
      "epoch: 105,  batch step: 38, loss: 144.16656494140625\n",
      "epoch: 105,  batch step: 39, loss: 18.735767364501953\n",
      "epoch: 105,  batch step: 40, loss: 3.0296456813812256\n",
      "epoch: 105,  batch step: 41, loss: 48.070884704589844\n",
      "epoch: 105,  batch step: 42, loss: 3.211162567138672\n",
      "epoch: 105,  batch step: 43, loss: 27.194740295410156\n",
      "epoch: 105,  batch step: 44, loss: 4.643176078796387\n",
      "epoch: 105,  batch step: 45, loss: 11.074262619018555\n",
      "epoch: 105,  batch step: 46, loss: 16.16606330871582\n",
      "epoch: 105,  batch step: 47, loss: 2.884174346923828\n",
      "epoch: 105,  batch step: 48, loss: 3.113982677459717\n",
      "epoch: 105,  batch step: 49, loss: 5.834462642669678\n",
      "epoch: 105,  batch step: 50, loss: 30.424528121948242\n",
      "epoch: 105,  batch step: 51, loss: 17.728652954101562\n",
      "epoch: 105,  batch step: 52, loss: 29.40003776550293\n",
      "epoch: 105,  batch step: 53, loss: 5.305564880371094\n",
      "epoch: 105,  batch step: 54, loss: 12.477943420410156\n",
      "epoch: 105,  batch step: 55, loss: 4.237598419189453\n",
      "epoch: 105,  batch step: 56, loss: 8.739814758300781\n",
      "epoch: 105,  batch step: 57, loss: 14.239606857299805\n",
      "epoch: 105,  batch step: 58, loss: 4.497664928436279\n",
      "epoch: 105,  batch step: 59, loss: 2.675265073776245\n",
      "epoch: 105,  batch step: 60, loss: 26.620067596435547\n",
      "epoch: 105,  batch step: 61, loss: 5.167412757873535\n",
      "epoch: 105,  batch step: 62, loss: 4.9119181632995605\n",
      "epoch: 105,  batch step: 63, loss: 2.9324584007263184\n",
      "epoch: 105,  batch step: 64, loss: 3.0482025146484375\n",
      "epoch: 105,  batch step: 65, loss: 53.597755432128906\n",
      "epoch: 105,  batch step: 66, loss: 3.3307480812072754\n",
      "epoch: 105,  batch step: 67, loss: 4.337646484375\n",
      "epoch: 105,  batch step: 68, loss: 28.3851318359375\n",
      "epoch: 105,  batch step: 69, loss: 3.2633776664733887\n",
      "epoch: 105,  batch step: 70, loss: 2.5706629753112793\n",
      "epoch: 105,  batch step: 71, loss: 10.729358673095703\n",
      "epoch: 105,  batch step: 72, loss: 23.424732208251953\n",
      "epoch: 105,  batch step: 73, loss: 3.5690507888793945\n",
      "epoch: 105,  batch step: 74, loss: 18.25860595703125\n",
      "epoch: 105,  batch step: 75, loss: 3.1334662437438965\n",
      "epoch: 105,  batch step: 76, loss: 14.335429191589355\n",
      "epoch: 105,  batch step: 77, loss: 2.2628564834594727\n",
      "epoch: 105,  batch step: 78, loss: 21.61635971069336\n",
      "epoch: 105,  batch step: 79, loss: 5.059448719024658\n",
      "epoch: 105,  batch step: 80, loss: 39.58580780029297\n",
      "epoch: 105,  batch step: 81, loss: 2.97609806060791\n",
      "epoch: 105,  batch step: 82, loss: 3.603372573852539\n",
      "epoch: 105,  batch step: 83, loss: 4.516499042510986\n",
      "epoch: 105,  batch step: 84, loss: 58.08300018310547\n",
      "epoch: 105,  batch step: 85, loss: 2.2142090797424316\n",
      "epoch: 105,  batch step: 86, loss: 38.400901794433594\n",
      "epoch: 105,  batch step: 87, loss: 2.7376277446746826\n",
      "epoch: 105,  batch step: 88, loss: 3.3096909523010254\n",
      "epoch: 105,  batch step: 89, loss: 3.0579028129577637\n",
      "epoch: 105,  batch step: 90, loss: 10.002098083496094\n",
      "epoch: 105,  batch step: 91, loss: 2.6762075424194336\n",
      "epoch: 105,  batch step: 92, loss: 10.115080833435059\n",
      "epoch: 105,  batch step: 93, loss: 28.177200317382812\n",
      "epoch: 105,  batch step: 94, loss: 2.795685291290283\n",
      "epoch: 105,  batch step: 95, loss: 18.11773681640625\n",
      "epoch: 105,  batch step: 96, loss: 17.026540756225586\n",
      "epoch: 105,  batch step: 97, loss: 4.158960342407227\n",
      "epoch: 105,  batch step: 98, loss: 39.95923614501953\n",
      "epoch: 105,  batch step: 99, loss: 15.328350067138672\n",
      "epoch: 105,  batch step: 100, loss: 19.68974494934082\n",
      "epoch: 105,  batch step: 101, loss: 27.18197250366211\n",
      "epoch: 105,  batch step: 102, loss: 4.371865272521973\n",
      "epoch: 105,  batch step: 103, loss: 16.70199966430664\n",
      "epoch: 105,  batch step: 104, loss: 3.5253214836120605\n",
      "epoch: 105,  batch step: 105, loss: 3.2973599433898926\n",
      "epoch: 105,  batch step: 106, loss: 12.09391975402832\n",
      "epoch: 105,  batch step: 107, loss: 3.3673789501190186\n",
      "epoch: 105,  batch step: 108, loss: 29.80678367614746\n",
      "epoch: 105,  batch step: 109, loss: 53.965301513671875\n",
      "epoch: 105,  batch step: 110, loss: 7.206031799316406\n",
      "epoch: 105,  batch step: 111, loss: 21.30891227722168\n",
      "epoch: 105,  batch step: 112, loss: 16.094104766845703\n",
      "epoch: 105,  batch step: 113, loss: 9.456083297729492\n",
      "epoch: 105,  batch step: 114, loss: 39.33941650390625\n",
      "epoch: 105,  batch step: 115, loss: 22.480070114135742\n",
      "epoch: 105,  batch step: 116, loss: 9.992804527282715\n",
      "epoch: 105,  batch step: 117, loss: 1.6829164028167725\n",
      "epoch: 105,  batch step: 118, loss: 59.19902801513672\n",
      "epoch: 105,  batch step: 119, loss: 8.917142868041992\n",
      "epoch: 105,  batch step: 120, loss: 43.197078704833984\n",
      "epoch: 105,  batch step: 121, loss: 14.050307273864746\n",
      "epoch: 105,  batch step: 122, loss: 3.6536502838134766\n",
      "epoch: 105,  batch step: 123, loss: 3.0947704315185547\n",
      "epoch: 105,  batch step: 124, loss: 3.070197582244873\n",
      "epoch: 105,  batch step: 125, loss: 3.8001933097839355\n",
      "epoch: 105,  batch step: 126, loss: 3.99916410446167\n",
      "epoch: 105,  batch step: 127, loss: 6.978513240814209\n",
      "epoch: 105,  batch step: 128, loss: 28.44598388671875\n",
      "epoch: 105,  batch step: 129, loss: 4.931743621826172\n",
      "epoch: 105,  batch step: 130, loss: 3.027599334716797\n",
      "epoch: 105,  batch step: 131, loss: 2.780271530151367\n",
      "epoch: 105,  batch step: 132, loss: 3.3065481185913086\n",
      "epoch: 105,  batch step: 133, loss: 28.788345336914062\n",
      "epoch: 105,  batch step: 134, loss: 2.5178284645080566\n",
      "epoch: 105,  batch step: 135, loss: 4.901800155639648\n",
      "epoch: 105,  batch step: 136, loss: 48.781463623046875\n",
      "epoch: 105,  batch step: 137, loss: 23.120101928710938\n",
      "epoch: 105,  batch step: 138, loss: 40.944366455078125\n",
      "epoch: 105,  batch step: 139, loss: 2.9007625579833984\n",
      "epoch: 105,  batch step: 140, loss: 2.7364988327026367\n",
      "epoch: 105,  batch step: 141, loss: 3.158965587615967\n",
      "epoch: 105,  batch step: 142, loss: 7.685880661010742\n",
      "epoch: 105,  batch step: 143, loss: 14.476158142089844\n",
      "epoch: 105,  batch step: 144, loss: 67.83793640136719\n",
      "epoch: 105,  batch step: 145, loss: 3.793794631958008\n",
      "epoch: 105,  batch step: 146, loss: 18.553329467773438\n",
      "epoch: 105,  batch step: 147, loss: 5.752264976501465\n",
      "epoch: 105,  batch step: 148, loss: 8.324392318725586\n",
      "epoch: 105,  batch step: 149, loss: 7.607978820800781\n",
      "epoch: 105,  batch step: 150, loss: 2.861640453338623\n",
      "epoch: 105,  batch step: 151, loss: 2.7706642150878906\n",
      "epoch: 105,  batch step: 152, loss: 6.749910354614258\n",
      "epoch: 105,  batch step: 153, loss: 4.015387535095215\n",
      "epoch: 105,  batch step: 154, loss: 78.46830749511719\n",
      "epoch: 105,  batch step: 155, loss: 29.871685028076172\n",
      "epoch: 105,  batch step: 156, loss: 9.746397972106934\n",
      "epoch: 105,  batch step: 157, loss: 24.89205551147461\n",
      "epoch: 105,  batch step: 158, loss: 2.445830821990967\n",
      "epoch: 105,  batch step: 159, loss: 13.606013298034668\n",
      "epoch: 105,  batch step: 160, loss: 2.967017650604248\n",
      "epoch: 105,  batch step: 161, loss: 10.8343505859375\n",
      "epoch: 105,  batch step: 162, loss: 1.9768040180206299\n",
      "epoch: 105,  batch step: 163, loss: 2.0647902488708496\n",
      "epoch: 105,  batch step: 164, loss: 44.47083282470703\n",
      "epoch: 105,  batch step: 165, loss: 3.294898748397827\n",
      "epoch: 105,  batch step: 166, loss: 40.03262710571289\n",
      "epoch: 105,  batch step: 167, loss: 35.91206359863281\n",
      "epoch: 105,  batch step: 168, loss: 4.019227504730225\n",
      "epoch: 105,  batch step: 169, loss: 15.969791412353516\n",
      "epoch: 105,  batch step: 170, loss: 26.801673889160156\n",
      "epoch: 105,  batch step: 171, loss: 19.57321548461914\n",
      "epoch: 105,  batch step: 172, loss: 47.37513732910156\n",
      "epoch: 105,  batch step: 173, loss: 4.215028762817383\n",
      "epoch: 105,  batch step: 174, loss: 4.018283843994141\n",
      "epoch: 105,  batch step: 175, loss: 14.040010452270508\n",
      "epoch: 105,  batch step: 176, loss: 13.525862693786621\n",
      "epoch: 105,  batch step: 177, loss: 11.55012321472168\n",
      "epoch: 105,  batch step: 178, loss: 1.9486417770385742\n",
      "epoch: 105,  batch step: 179, loss: 2.2896275520324707\n",
      "epoch: 105,  batch step: 180, loss: 2.179936170578003\n",
      "epoch: 105,  batch step: 181, loss: 18.960140228271484\n",
      "epoch: 105,  batch step: 182, loss: 19.551095962524414\n",
      "epoch: 105,  batch step: 183, loss: 4.057373046875\n",
      "epoch: 105,  batch step: 184, loss: 44.807273864746094\n",
      "epoch: 105,  batch step: 185, loss: 14.531294822692871\n",
      "epoch: 105,  batch step: 186, loss: 25.002361297607422\n",
      "epoch: 105,  batch step: 187, loss: 2.798424482345581\n",
      "epoch: 105,  batch step: 188, loss: 4.631174087524414\n",
      "epoch: 105,  batch step: 189, loss: 2.038874626159668\n",
      "epoch: 105,  batch step: 190, loss: 2.5522892475128174\n",
      "epoch: 105,  batch step: 191, loss: 2.2887980937957764\n",
      "epoch: 105,  batch step: 192, loss: 13.914567947387695\n",
      "epoch: 105,  batch step: 193, loss: 2.8710124492645264\n",
      "epoch: 105,  batch step: 194, loss: 5.798083782196045\n",
      "epoch: 105,  batch step: 195, loss: 4.851175308227539\n",
      "epoch: 105,  batch step: 196, loss: 11.995162010192871\n",
      "epoch: 105,  batch step: 197, loss: 30.840978622436523\n",
      "epoch: 105,  batch step: 198, loss: 1.7552096843719482\n",
      "epoch: 105,  batch step: 199, loss: 2.71795654296875\n",
      "epoch: 105,  batch step: 200, loss: 3.0547256469726562\n",
      "epoch: 105,  batch step: 201, loss: 2.1748902797698975\n",
      "epoch: 105,  batch step: 202, loss: 4.325769424438477\n",
      "epoch: 105,  batch step: 203, loss: 2.205004930496216\n",
      "epoch: 105,  batch step: 204, loss: 9.237710952758789\n",
      "epoch: 105,  batch step: 205, loss: 1.7412879467010498\n",
      "epoch: 105,  batch step: 206, loss: 55.144378662109375\n",
      "epoch: 105,  batch step: 207, loss: 6.379563331604004\n",
      "epoch: 105,  batch step: 208, loss: 8.063554763793945\n",
      "epoch: 105,  batch step: 209, loss: 7.3902907371521\n",
      "epoch: 105,  batch step: 210, loss: 2.725449562072754\n",
      "epoch: 105,  batch step: 211, loss: 3.436405658721924\n",
      "epoch: 105,  batch step: 212, loss: 1.776815414428711\n",
      "epoch: 105,  batch step: 213, loss: 4.847137451171875\n",
      "epoch: 105,  batch step: 214, loss: 48.295589447021484\n",
      "epoch: 105,  batch step: 215, loss: 2.3041763305664062\n",
      "epoch: 105,  batch step: 216, loss: 8.775869369506836\n",
      "epoch: 105,  batch step: 217, loss: 27.28260040283203\n",
      "epoch: 105,  batch step: 218, loss: 2.73386812210083\n",
      "epoch: 105,  batch step: 219, loss: 75.25251770019531\n",
      "epoch: 105,  batch step: 220, loss: 14.986581802368164\n",
      "epoch: 105,  batch step: 221, loss: 4.771339416503906\n",
      "epoch: 105,  batch step: 222, loss: 22.089536666870117\n",
      "epoch: 105,  batch step: 223, loss: 3.8159871101379395\n",
      "epoch: 105,  batch step: 224, loss: 23.505918502807617\n",
      "epoch: 105,  batch step: 225, loss: 10.458938598632812\n",
      "epoch: 105,  batch step: 226, loss: 2.90138578414917\n",
      "epoch: 105,  batch step: 227, loss: 15.11209487915039\n",
      "epoch: 105,  batch step: 228, loss: 80.54657745361328\n",
      "epoch: 105,  batch step: 229, loss: 2.069895029067993\n",
      "epoch: 105,  batch step: 230, loss: 65.72209167480469\n",
      "epoch: 105,  batch step: 231, loss: 2.6944003105163574\n",
      "epoch: 105,  batch step: 232, loss: 3.1826252937316895\n",
      "epoch: 105,  batch step: 233, loss: 3.2094390392303467\n",
      "epoch: 105,  batch step: 234, loss: 31.1983642578125\n",
      "epoch: 105,  batch step: 235, loss: 3.4727745056152344\n",
      "epoch: 105,  batch step: 236, loss: 15.724109649658203\n",
      "epoch: 105,  batch step: 237, loss: 41.07920837402344\n",
      "epoch: 105,  batch step: 238, loss: 2.199751853942871\n",
      "epoch: 105,  batch step: 239, loss: 28.78229331970215\n",
      "epoch: 105,  batch step: 240, loss: 11.489128112792969\n",
      "epoch: 105,  batch step: 241, loss: 8.150739669799805\n",
      "epoch: 105,  batch step: 242, loss: 26.060562133789062\n",
      "epoch: 105,  batch step: 243, loss: 3.6160736083984375\n",
      "epoch: 105,  batch step: 244, loss: 23.173015594482422\n",
      "epoch: 105,  batch step: 245, loss: 2.7868716716766357\n",
      "epoch: 105,  batch step: 246, loss: 2.9402122497558594\n",
      "epoch: 105,  batch step: 247, loss: 11.037665367126465\n",
      "epoch: 105,  batch step: 248, loss: 2.481152057647705\n",
      "epoch: 105,  batch step: 249, loss: 3.6305136680603027\n",
      "epoch: 105,  batch step: 250, loss: 6.566041946411133\n",
      "epoch: 105,  batch step: 251, loss: 59.85391616821289\n",
      "validation error epoch  105:    tensor(68.0055, device='cuda:0')\n",
      "316\n",
      "epoch: 106,  batch step: 0, loss: 27.11805534362793\n",
      "epoch: 106,  batch step: 1, loss: 2.622607707977295\n",
      "epoch: 106,  batch step: 2, loss: 4.317169666290283\n",
      "epoch: 106,  batch step: 3, loss: 29.8016414642334\n",
      "epoch: 106,  batch step: 4, loss: 2.5485448837280273\n",
      "epoch: 106,  batch step: 5, loss: 40.13080596923828\n",
      "epoch: 106,  batch step: 6, loss: 7.274859428405762\n",
      "epoch: 106,  batch step: 7, loss: 3.3389089107513428\n",
      "epoch: 106,  batch step: 8, loss: 1.5433721542358398\n",
      "epoch: 106,  batch step: 9, loss: 2.5742902755737305\n",
      "epoch: 106,  batch step: 10, loss: 27.336393356323242\n",
      "epoch: 106,  batch step: 11, loss: 3.202874183654785\n",
      "epoch: 106,  batch step: 12, loss: 3.831415891647339\n",
      "epoch: 106,  batch step: 13, loss: 33.357505798339844\n",
      "epoch: 106,  batch step: 14, loss: 2.0944113731384277\n",
      "epoch: 106,  batch step: 15, loss: 37.622291564941406\n",
      "epoch: 106,  batch step: 16, loss: 21.643064498901367\n",
      "epoch: 106,  batch step: 17, loss: 15.014457702636719\n",
      "epoch: 106,  batch step: 18, loss: 7.733642101287842\n",
      "epoch: 106,  batch step: 19, loss: 2.1304187774658203\n",
      "epoch: 106,  batch step: 20, loss: 24.839508056640625\n",
      "epoch: 106,  batch step: 21, loss: 3.514343738555908\n",
      "epoch: 106,  batch step: 22, loss: 12.146055221557617\n",
      "epoch: 106,  batch step: 23, loss: 1.8873833417892456\n",
      "epoch: 106,  batch step: 24, loss: 9.543464660644531\n",
      "epoch: 106,  batch step: 25, loss: 2.0883865356445312\n",
      "epoch: 106,  batch step: 26, loss: 2.475201368331909\n",
      "epoch: 106,  batch step: 27, loss: 12.319583892822266\n",
      "epoch: 106,  batch step: 28, loss: 3.1344783306121826\n",
      "epoch: 106,  batch step: 29, loss: 5.05892276763916\n",
      "epoch: 106,  batch step: 30, loss: 4.734017372131348\n",
      "epoch: 106,  batch step: 31, loss: 3.011594772338867\n",
      "epoch: 106,  batch step: 32, loss: 3.0836424827575684\n",
      "epoch: 106,  batch step: 33, loss: 39.15824890136719\n",
      "epoch: 106,  batch step: 34, loss: 3.0258703231811523\n",
      "epoch: 106,  batch step: 35, loss: 2.2782387733459473\n",
      "epoch: 106,  batch step: 36, loss: 7.574172019958496\n",
      "epoch: 106,  batch step: 37, loss: 40.87422180175781\n",
      "epoch: 106,  batch step: 38, loss: 3.2231268882751465\n",
      "epoch: 106,  batch step: 39, loss: 76.48141479492188\n",
      "epoch: 106,  batch step: 40, loss: 2.222104787826538\n",
      "epoch: 106,  batch step: 41, loss: 11.828834533691406\n",
      "epoch: 106,  batch step: 42, loss: 3.8446080684661865\n",
      "epoch: 106,  batch step: 43, loss: 5.933276176452637\n",
      "epoch: 106,  batch step: 44, loss: 21.65945816040039\n",
      "epoch: 106,  batch step: 45, loss: 5.31449031829834\n",
      "epoch: 106,  batch step: 46, loss: 1.6572024822235107\n",
      "epoch: 106,  batch step: 47, loss: 2.066511869430542\n",
      "epoch: 106,  batch step: 48, loss: 3.3114306926727295\n",
      "epoch: 106,  batch step: 49, loss: 23.706531524658203\n",
      "epoch: 106,  batch step: 50, loss: 34.68891906738281\n",
      "epoch: 106,  batch step: 51, loss: 2.42486572265625\n",
      "epoch: 106,  batch step: 52, loss: 7.443797588348389\n",
      "epoch: 106,  batch step: 53, loss: 41.62571334838867\n",
      "epoch: 106,  batch step: 54, loss: 1.3820335865020752\n",
      "epoch: 106,  batch step: 55, loss: 15.574119567871094\n",
      "epoch: 106,  batch step: 56, loss: 2.0760245323181152\n",
      "epoch: 106,  batch step: 57, loss: 1.2200775146484375\n",
      "epoch: 106,  batch step: 58, loss: 13.266256332397461\n",
      "epoch: 106,  batch step: 59, loss: 1.98681640625\n",
      "epoch: 106,  batch step: 60, loss: 2.7703425884246826\n",
      "epoch: 106,  batch step: 61, loss: 16.175214767456055\n",
      "epoch: 106,  batch step: 62, loss: 61.99665832519531\n",
      "epoch: 106,  batch step: 63, loss: 13.505266189575195\n",
      "epoch: 106,  batch step: 64, loss: 3.706021785736084\n",
      "epoch: 106,  batch step: 65, loss: 4.241254806518555\n",
      "epoch: 106,  batch step: 66, loss: 4.549041271209717\n",
      "epoch: 106,  batch step: 67, loss: 1.5609482526779175\n",
      "epoch: 106,  batch step: 68, loss: 22.936635971069336\n",
      "epoch: 106,  batch step: 69, loss: 10.814624786376953\n",
      "epoch: 106,  batch step: 70, loss: 5.185615062713623\n",
      "epoch: 106,  batch step: 71, loss: 19.439441680908203\n",
      "epoch: 106,  batch step: 72, loss: 5.745326995849609\n",
      "epoch: 106,  batch step: 73, loss: 12.104106903076172\n",
      "epoch: 106,  batch step: 74, loss: 7.626251220703125\n",
      "epoch: 106,  batch step: 75, loss: 10.064764976501465\n",
      "epoch: 106,  batch step: 76, loss: 8.2164306640625\n",
      "epoch: 106,  batch step: 77, loss: 9.911697387695312\n",
      "epoch: 106,  batch step: 78, loss: 17.786344528198242\n",
      "epoch: 106,  batch step: 79, loss: 2.617084503173828\n",
      "epoch: 106,  batch step: 80, loss: 4.031814098358154\n",
      "epoch: 106,  batch step: 81, loss: 4.22382116317749\n",
      "epoch: 106,  batch step: 82, loss: 48.9217529296875\n",
      "epoch: 106,  batch step: 83, loss: 2.3656973838806152\n",
      "epoch: 106,  batch step: 84, loss: 7.3461432456970215\n",
      "epoch: 106,  batch step: 85, loss: 9.557989120483398\n",
      "epoch: 106,  batch step: 86, loss: 3.195375442504883\n",
      "epoch: 106,  batch step: 87, loss: 10.354288101196289\n",
      "epoch: 106,  batch step: 88, loss: 12.371831893920898\n",
      "epoch: 106,  batch step: 89, loss: 2.2078261375427246\n",
      "epoch: 106,  batch step: 90, loss: 2.1418633460998535\n",
      "epoch: 106,  batch step: 91, loss: 1.7097797393798828\n",
      "epoch: 106,  batch step: 92, loss: 11.362372398376465\n",
      "epoch: 106,  batch step: 93, loss: 10.448444366455078\n",
      "epoch: 106,  batch step: 94, loss: 11.044706344604492\n",
      "epoch: 106,  batch step: 95, loss: 4.3410491943359375\n",
      "epoch: 106,  batch step: 96, loss: 11.055793762207031\n",
      "epoch: 106,  batch step: 97, loss: 49.02289962768555\n",
      "epoch: 106,  batch step: 98, loss: 16.638757705688477\n",
      "epoch: 106,  batch step: 99, loss: 2.1028199195861816\n",
      "epoch: 106,  batch step: 100, loss: 11.342016220092773\n",
      "epoch: 106,  batch step: 101, loss: 2.9302940368652344\n",
      "epoch: 106,  batch step: 102, loss: 39.418235778808594\n",
      "epoch: 106,  batch step: 103, loss: 6.621079444885254\n",
      "epoch: 106,  batch step: 104, loss: 13.157546997070312\n",
      "epoch: 106,  batch step: 105, loss: 15.132669448852539\n",
      "epoch: 106,  batch step: 106, loss: 6.604691982269287\n",
      "epoch: 106,  batch step: 107, loss: 19.240623474121094\n",
      "epoch: 106,  batch step: 108, loss: 1.6399750709533691\n",
      "epoch: 106,  batch step: 109, loss: 2.3020505905151367\n",
      "epoch: 106,  batch step: 110, loss: 7.408407211303711\n",
      "epoch: 106,  batch step: 111, loss: 49.44781494140625\n",
      "epoch: 106,  batch step: 112, loss: 10.153302192687988\n",
      "epoch: 106,  batch step: 113, loss: 2.946991443634033\n",
      "epoch: 106,  batch step: 114, loss: 38.318992614746094\n",
      "epoch: 106,  batch step: 115, loss: 2.589261293411255\n",
      "epoch: 106,  batch step: 116, loss: 2.4111199378967285\n",
      "epoch: 106,  batch step: 117, loss: 2.5432660579681396\n",
      "epoch: 106,  batch step: 118, loss: 3.0547327995300293\n",
      "epoch: 106,  batch step: 119, loss: 2.2685489654541016\n",
      "epoch: 106,  batch step: 120, loss: 38.27573776245117\n",
      "epoch: 106,  batch step: 121, loss: 2.153970956802368\n",
      "epoch: 106,  batch step: 122, loss: 14.80237865447998\n",
      "epoch: 106,  batch step: 123, loss: 9.208932876586914\n",
      "epoch: 106,  batch step: 124, loss: 30.360488891601562\n",
      "epoch: 106,  batch step: 125, loss: 7.878963470458984\n",
      "epoch: 106,  batch step: 126, loss: 7.681754112243652\n",
      "epoch: 106,  batch step: 127, loss: 8.336732864379883\n",
      "epoch: 106,  batch step: 128, loss: 10.318521499633789\n",
      "epoch: 106,  batch step: 129, loss: 14.251917839050293\n",
      "epoch: 106,  batch step: 130, loss: 1.6070220470428467\n",
      "epoch: 106,  batch step: 131, loss: 12.817991256713867\n",
      "epoch: 106,  batch step: 132, loss: 1.2614073753356934\n",
      "epoch: 106,  batch step: 133, loss: 8.411539077758789\n",
      "epoch: 106,  batch step: 134, loss: 1.810063362121582\n",
      "epoch: 106,  batch step: 135, loss: 27.12152671813965\n",
      "epoch: 106,  batch step: 136, loss: 8.840967178344727\n",
      "epoch: 106,  batch step: 137, loss: 8.706132888793945\n",
      "epoch: 106,  batch step: 138, loss: 12.485754013061523\n",
      "epoch: 106,  batch step: 139, loss: 1.70192289352417\n",
      "epoch: 106,  batch step: 140, loss: 3.8584420680999756\n",
      "epoch: 106,  batch step: 141, loss: 2.1300292015075684\n",
      "epoch: 106,  batch step: 142, loss: 15.67888069152832\n",
      "epoch: 106,  batch step: 143, loss: 12.351103782653809\n",
      "epoch: 106,  batch step: 144, loss: 2.7565839290618896\n",
      "epoch: 106,  batch step: 145, loss: 27.967670440673828\n",
      "epoch: 106,  batch step: 146, loss: 3.505002498626709\n",
      "epoch: 106,  batch step: 147, loss: 2.6532440185546875\n",
      "epoch: 106,  batch step: 148, loss: 2.104342460632324\n",
      "epoch: 106,  batch step: 149, loss: 2.282824993133545\n",
      "epoch: 106,  batch step: 150, loss: 2.509786367416382\n",
      "epoch: 106,  batch step: 151, loss: 1.8404886722564697\n",
      "epoch: 106,  batch step: 152, loss: 14.05910873413086\n",
      "epoch: 106,  batch step: 153, loss: 13.269637107849121\n",
      "epoch: 106,  batch step: 154, loss: 3.617150068283081\n",
      "epoch: 106,  batch step: 155, loss: 3.16184663772583\n",
      "epoch: 106,  batch step: 156, loss: 8.752266883850098\n",
      "epoch: 106,  batch step: 157, loss: 4.758364677429199\n",
      "epoch: 106,  batch step: 158, loss: 2.3787107467651367\n",
      "epoch: 106,  batch step: 159, loss: 1.7974786758422852\n",
      "epoch: 106,  batch step: 160, loss: 4.038405895233154\n",
      "epoch: 106,  batch step: 161, loss: 1.7614638805389404\n",
      "epoch: 106,  batch step: 162, loss: 2.152665615081787\n",
      "epoch: 106,  batch step: 163, loss: 1.309045672416687\n",
      "epoch: 106,  batch step: 164, loss: 12.417887687683105\n",
      "epoch: 106,  batch step: 165, loss: 50.22034454345703\n",
      "epoch: 106,  batch step: 166, loss: 3.2742042541503906\n",
      "epoch: 106,  batch step: 167, loss: 3.316615581512451\n",
      "epoch: 106,  batch step: 168, loss: 2.2205848693847656\n",
      "epoch: 106,  batch step: 169, loss: 2.1711928844451904\n",
      "epoch: 106,  batch step: 170, loss: 2.7188174724578857\n",
      "epoch: 106,  batch step: 171, loss: 50.817291259765625\n",
      "epoch: 106,  batch step: 172, loss: 2.0059571266174316\n",
      "epoch: 106,  batch step: 173, loss: 1.968266248703003\n",
      "epoch: 106,  batch step: 174, loss: 7.488462448120117\n",
      "epoch: 106,  batch step: 175, loss: 1.804894208908081\n",
      "epoch: 106,  batch step: 176, loss: 9.487333297729492\n",
      "epoch: 106,  batch step: 177, loss: 5.549588203430176\n",
      "epoch: 106,  batch step: 178, loss: 12.660158157348633\n",
      "epoch: 106,  batch step: 179, loss: 20.535531997680664\n",
      "epoch: 106,  batch step: 180, loss: 13.842811584472656\n",
      "epoch: 106,  batch step: 181, loss: 1.7009732723236084\n",
      "epoch: 106,  batch step: 182, loss: 18.708641052246094\n",
      "epoch: 106,  batch step: 183, loss: 7.9864654541015625\n",
      "epoch: 106,  batch step: 184, loss: 66.88340759277344\n",
      "epoch: 106,  batch step: 185, loss: 17.740468978881836\n",
      "epoch: 106,  batch step: 186, loss: 28.770217895507812\n",
      "epoch: 106,  batch step: 187, loss: 21.320556640625\n",
      "epoch: 106,  batch step: 188, loss: 1.9738705158233643\n",
      "epoch: 106,  batch step: 189, loss: 4.2596282958984375\n",
      "epoch: 106,  batch step: 190, loss: 1.7988624572753906\n",
      "epoch: 106,  batch step: 191, loss: 16.92485809326172\n",
      "epoch: 106,  batch step: 192, loss: 31.142227172851562\n",
      "epoch: 106,  batch step: 193, loss: 5.252791404724121\n",
      "epoch: 106,  batch step: 194, loss: 1.7785334587097168\n",
      "epoch: 106,  batch step: 195, loss: 40.94572448730469\n",
      "epoch: 106,  batch step: 196, loss: 2.0469226837158203\n",
      "epoch: 106,  batch step: 197, loss: 21.907150268554688\n",
      "epoch: 106,  batch step: 198, loss: 6.003037452697754\n",
      "epoch: 106,  batch step: 199, loss: 11.234865188598633\n",
      "epoch: 106,  batch step: 200, loss: 1.5191619396209717\n",
      "epoch: 106,  batch step: 201, loss: 16.069665908813477\n",
      "epoch: 106,  batch step: 202, loss: 2.234924793243408\n",
      "epoch: 106,  batch step: 203, loss: 1.542786955833435\n",
      "epoch: 106,  batch step: 204, loss: 23.029935836791992\n",
      "epoch: 106,  batch step: 205, loss: 3.0585782527923584\n",
      "epoch: 106,  batch step: 206, loss: 2.0023231506347656\n",
      "epoch: 106,  batch step: 207, loss: 1.4977903366088867\n",
      "epoch: 106,  batch step: 208, loss: 3.319920063018799\n",
      "epoch: 106,  batch step: 209, loss: 2.335170030593872\n",
      "epoch: 106,  batch step: 210, loss: 4.1666460037231445\n",
      "epoch: 106,  batch step: 211, loss: 23.11522674560547\n",
      "epoch: 106,  batch step: 212, loss: 168.47750854492188\n",
      "epoch: 106,  batch step: 213, loss: 2.484555721282959\n",
      "epoch: 106,  batch step: 214, loss: 1.9802653789520264\n",
      "epoch: 106,  batch step: 215, loss: 1.993669033050537\n",
      "epoch: 106,  batch step: 216, loss: 2.43753981590271\n",
      "epoch: 106,  batch step: 217, loss: 6.928443908691406\n",
      "epoch: 106,  batch step: 218, loss: 2.2879695892333984\n",
      "epoch: 106,  batch step: 219, loss: 2.9597175121307373\n",
      "epoch: 106,  batch step: 220, loss: 14.16360855102539\n",
      "epoch: 106,  batch step: 221, loss: 7.17026424407959\n",
      "epoch: 106,  batch step: 222, loss: 9.02734661102295\n",
      "epoch: 106,  batch step: 223, loss: 4.713883399963379\n",
      "epoch: 106,  batch step: 224, loss: 12.45349407196045\n",
      "epoch: 106,  batch step: 225, loss: 2.3678598403930664\n",
      "epoch: 106,  batch step: 226, loss: 31.676532745361328\n",
      "epoch: 106,  batch step: 227, loss: 2.2706949710845947\n",
      "epoch: 106,  batch step: 228, loss: 1.9019321203231812\n",
      "epoch: 106,  batch step: 229, loss: 36.331886291503906\n",
      "epoch: 106,  batch step: 230, loss: 10.721803665161133\n",
      "epoch: 106,  batch step: 231, loss: 2.368180274963379\n",
      "epoch: 106,  batch step: 232, loss: 1.790128231048584\n",
      "epoch: 106,  batch step: 233, loss: 2.148233413696289\n",
      "epoch: 106,  batch step: 234, loss: 45.720516204833984\n",
      "epoch: 106,  batch step: 235, loss: 18.64044189453125\n",
      "epoch: 106,  batch step: 236, loss: 9.615032196044922\n",
      "epoch: 106,  batch step: 237, loss: 2.837491750717163\n",
      "epoch: 106,  batch step: 238, loss: 7.705336570739746\n",
      "epoch: 106,  batch step: 239, loss: 10.67664623260498\n",
      "epoch: 106,  batch step: 240, loss: 2.000030040740967\n",
      "epoch: 106,  batch step: 241, loss: 66.6880874633789\n",
      "epoch: 106,  batch step: 242, loss: 2.055065155029297\n",
      "epoch: 106,  batch step: 243, loss: 12.54532527923584\n",
      "epoch: 106,  batch step: 244, loss: 2.1167819499969482\n",
      "epoch: 106,  batch step: 245, loss: 24.606796264648438\n",
      "epoch: 106,  batch step: 246, loss: 29.360828399658203\n",
      "epoch: 106,  batch step: 247, loss: 9.253358840942383\n",
      "epoch: 106,  batch step: 248, loss: 1.9840949773788452\n",
      "epoch: 106,  batch step: 249, loss: 2.57777738571167\n",
      "epoch: 106,  batch step: 250, loss: 1.5547363758087158\n",
      "epoch: 106,  batch step: 251, loss: 219.815673828125\n",
      "validation error epoch  106:    tensor(69.4245, device='cuda:0')\n",
      "316\n",
      "epoch: 107,  batch step: 0, loss: 16.50897979736328\n",
      "epoch: 107,  batch step: 1, loss: 10.836603164672852\n",
      "epoch: 107,  batch step: 2, loss: 15.160881042480469\n",
      "epoch: 107,  batch step: 3, loss: 25.97370147705078\n",
      "epoch: 107,  batch step: 4, loss: 15.114042282104492\n",
      "epoch: 107,  batch step: 5, loss: 18.178382873535156\n",
      "epoch: 107,  batch step: 6, loss: 6.966350555419922\n",
      "epoch: 107,  batch step: 7, loss: 25.73859405517578\n",
      "epoch: 107,  batch step: 8, loss: 3.2626874446868896\n",
      "epoch: 107,  batch step: 9, loss: 50.66943359375\n",
      "epoch: 107,  batch step: 10, loss: 17.61896324157715\n",
      "epoch: 107,  batch step: 11, loss: 15.452560424804688\n",
      "epoch: 107,  batch step: 12, loss: 7.963946342468262\n",
      "epoch: 107,  batch step: 13, loss: 3.6060378551483154\n",
      "epoch: 107,  batch step: 14, loss: 8.554869651794434\n",
      "epoch: 107,  batch step: 15, loss: 6.58657169342041\n",
      "epoch: 107,  batch step: 16, loss: 10.340211868286133\n",
      "epoch: 107,  batch step: 17, loss: 3.7400290966033936\n",
      "epoch: 107,  batch step: 18, loss: 3.261514186859131\n",
      "epoch: 107,  batch step: 19, loss: 11.858561515808105\n",
      "epoch: 107,  batch step: 20, loss: 20.056135177612305\n",
      "epoch: 107,  batch step: 21, loss: 3.817624807357788\n",
      "epoch: 107,  batch step: 22, loss: 11.62098503112793\n",
      "epoch: 107,  batch step: 23, loss: 39.7413444519043\n",
      "epoch: 107,  batch step: 24, loss: 10.28052043914795\n",
      "epoch: 107,  batch step: 25, loss: 21.038997650146484\n",
      "epoch: 107,  batch step: 26, loss: 12.725465774536133\n",
      "epoch: 107,  batch step: 27, loss: 4.239953994750977\n",
      "epoch: 107,  batch step: 28, loss: 3.6229302883148193\n",
      "epoch: 107,  batch step: 29, loss: 4.604007720947266\n",
      "epoch: 107,  batch step: 30, loss: 3.2421367168426514\n",
      "epoch: 107,  batch step: 31, loss: 4.828190803527832\n",
      "epoch: 107,  batch step: 32, loss: 20.154949188232422\n",
      "epoch: 107,  batch step: 33, loss: 2.7526192665100098\n",
      "epoch: 107,  batch step: 34, loss: 9.262979507446289\n",
      "epoch: 107,  batch step: 35, loss: 9.752593994140625\n",
      "epoch: 107,  batch step: 36, loss: 13.247777938842773\n",
      "epoch: 107,  batch step: 37, loss: 5.733312606811523\n",
      "epoch: 107,  batch step: 38, loss: 13.996240615844727\n",
      "epoch: 107,  batch step: 39, loss: 11.589930534362793\n",
      "epoch: 107,  batch step: 40, loss: 3.108997344970703\n",
      "epoch: 107,  batch step: 41, loss: 22.095874786376953\n",
      "epoch: 107,  batch step: 42, loss: 2.5673797130584717\n",
      "epoch: 107,  batch step: 43, loss: 2.997344732284546\n",
      "epoch: 107,  batch step: 44, loss: 2.646360397338867\n",
      "epoch: 107,  batch step: 45, loss: 31.234479904174805\n",
      "epoch: 107,  batch step: 46, loss: 9.955995559692383\n",
      "epoch: 107,  batch step: 47, loss: 2.2351064682006836\n",
      "epoch: 107,  batch step: 48, loss: 2.860361099243164\n",
      "epoch: 107,  batch step: 49, loss: 15.405325889587402\n",
      "epoch: 107,  batch step: 50, loss: 2.040215492248535\n",
      "epoch: 107,  batch step: 51, loss: 17.680967330932617\n",
      "epoch: 107,  batch step: 52, loss: 4.436966896057129\n",
      "epoch: 107,  batch step: 53, loss: 3.7387938499450684\n",
      "epoch: 107,  batch step: 54, loss: 3.1593177318573\n",
      "epoch: 107,  batch step: 55, loss: 2.327275276184082\n",
      "epoch: 107,  batch step: 56, loss: 4.007190227508545\n",
      "epoch: 107,  batch step: 57, loss: 8.382608413696289\n",
      "epoch: 107,  batch step: 58, loss: 2.9043147563934326\n",
      "epoch: 107,  batch step: 59, loss: 7.857458114624023\n",
      "epoch: 107,  batch step: 60, loss: 24.009597778320312\n",
      "epoch: 107,  batch step: 61, loss: 2.2800440788269043\n",
      "epoch: 107,  batch step: 62, loss: 26.53096580505371\n",
      "epoch: 107,  batch step: 63, loss: 16.960453033447266\n",
      "epoch: 107,  batch step: 64, loss: 54.481529235839844\n",
      "epoch: 107,  batch step: 65, loss: 9.455077171325684\n",
      "epoch: 107,  batch step: 66, loss: 10.970160484313965\n",
      "epoch: 107,  batch step: 67, loss: 3.2659034729003906\n",
      "epoch: 107,  batch step: 68, loss: 10.472725868225098\n",
      "epoch: 107,  batch step: 69, loss: 33.201995849609375\n",
      "epoch: 107,  batch step: 70, loss: 3.075216293334961\n",
      "epoch: 107,  batch step: 71, loss: 49.07946014404297\n",
      "epoch: 107,  batch step: 72, loss: 3.0171427726745605\n",
      "epoch: 107,  batch step: 73, loss: 2.2661705017089844\n",
      "epoch: 107,  batch step: 74, loss: 13.672416687011719\n",
      "epoch: 107,  batch step: 75, loss: 29.8743896484375\n",
      "epoch: 107,  batch step: 76, loss: 3.795654058456421\n",
      "epoch: 107,  batch step: 77, loss: 4.873041152954102\n",
      "epoch: 107,  batch step: 78, loss: 16.63486671447754\n",
      "epoch: 107,  batch step: 79, loss: 1.8501265048980713\n",
      "epoch: 107,  batch step: 80, loss: 8.429327964782715\n",
      "epoch: 107,  batch step: 81, loss: 5.043853759765625\n",
      "epoch: 107,  batch step: 82, loss: 2.134230375289917\n",
      "epoch: 107,  batch step: 83, loss: 2.6323258876800537\n",
      "epoch: 107,  batch step: 84, loss: 9.153593063354492\n",
      "epoch: 107,  batch step: 85, loss: 3.816239833831787\n",
      "epoch: 107,  batch step: 86, loss: 11.678319931030273\n",
      "epoch: 107,  batch step: 87, loss: 157.58090209960938\n",
      "epoch: 107,  batch step: 88, loss: 23.606163024902344\n",
      "epoch: 107,  batch step: 89, loss: 16.537437438964844\n",
      "epoch: 107,  batch step: 90, loss: 12.066368103027344\n",
      "epoch: 107,  batch step: 91, loss: 6.067401885986328\n",
      "epoch: 107,  batch step: 92, loss: 57.73351287841797\n",
      "epoch: 107,  batch step: 93, loss: 2.9106826782226562\n",
      "epoch: 107,  batch step: 94, loss: 3.1632039546966553\n",
      "epoch: 107,  batch step: 95, loss: 5.115757942199707\n",
      "epoch: 107,  batch step: 96, loss: 22.050899505615234\n",
      "epoch: 107,  batch step: 97, loss: 1.3563010692596436\n",
      "epoch: 107,  batch step: 98, loss: 5.359367370605469\n",
      "epoch: 107,  batch step: 99, loss: 3.694470167160034\n",
      "epoch: 107,  batch step: 100, loss: 30.218441009521484\n",
      "epoch: 107,  batch step: 101, loss: 9.91482162475586\n",
      "epoch: 107,  batch step: 102, loss: 17.475677490234375\n",
      "epoch: 107,  batch step: 103, loss: 14.666614532470703\n",
      "epoch: 107,  batch step: 104, loss: 19.214458465576172\n",
      "epoch: 107,  batch step: 105, loss: 20.65774917602539\n",
      "epoch: 107,  batch step: 106, loss: 7.272538661956787\n",
      "epoch: 107,  batch step: 107, loss: 13.995079040527344\n",
      "epoch: 107,  batch step: 108, loss: 8.919599533081055\n",
      "epoch: 107,  batch step: 109, loss: 56.303627014160156\n",
      "epoch: 107,  batch step: 110, loss: 16.862850189208984\n",
      "epoch: 107,  batch step: 111, loss: 56.85377502441406\n",
      "epoch: 107,  batch step: 112, loss: 1.5511178970336914\n",
      "epoch: 107,  batch step: 113, loss: 2.24078106880188\n",
      "epoch: 107,  batch step: 114, loss: 13.783358573913574\n",
      "epoch: 107,  batch step: 115, loss: 1.750330924987793\n",
      "epoch: 107,  batch step: 116, loss: 2.7419848442077637\n",
      "epoch: 107,  batch step: 117, loss: 1.7729899883270264\n",
      "epoch: 107,  batch step: 118, loss: 5.171118259429932\n",
      "epoch: 107,  batch step: 119, loss: 5.956798076629639\n",
      "epoch: 107,  batch step: 120, loss: 17.66844940185547\n",
      "epoch: 107,  batch step: 121, loss: 23.532209396362305\n",
      "epoch: 107,  batch step: 122, loss: 2.5509462356567383\n",
      "epoch: 107,  batch step: 123, loss: 21.912433624267578\n",
      "epoch: 107,  batch step: 124, loss: 2.382016897201538\n",
      "epoch: 107,  batch step: 125, loss: 43.582550048828125\n",
      "epoch: 107,  batch step: 126, loss: 4.1930155754089355\n",
      "epoch: 107,  batch step: 127, loss: 18.72791290283203\n",
      "epoch: 107,  batch step: 128, loss: 1.7129428386688232\n",
      "epoch: 107,  batch step: 129, loss: 1.8080317974090576\n",
      "epoch: 107,  batch step: 130, loss: 1.7692439556121826\n",
      "epoch: 107,  batch step: 131, loss: 12.880300521850586\n",
      "epoch: 107,  batch step: 132, loss: 2.1091175079345703\n",
      "epoch: 107,  batch step: 133, loss: 5.400512218475342\n",
      "epoch: 107,  batch step: 134, loss: 13.417678833007812\n",
      "epoch: 107,  batch step: 135, loss: 2.260354995727539\n",
      "epoch: 107,  batch step: 136, loss: 3.075054883956909\n",
      "epoch: 107,  batch step: 137, loss: 2.092999219894409\n",
      "epoch: 107,  batch step: 138, loss: 2.300236225128174\n",
      "epoch: 107,  batch step: 139, loss: 17.988988876342773\n",
      "epoch: 107,  batch step: 140, loss: 5.945626258850098\n",
      "epoch: 107,  batch step: 141, loss: 12.47315502166748\n",
      "epoch: 107,  batch step: 142, loss: 17.823585510253906\n",
      "epoch: 107,  batch step: 143, loss: 2.2778689861297607\n",
      "epoch: 107,  batch step: 144, loss: 1.8842802047729492\n",
      "epoch: 107,  batch step: 145, loss: 11.043082237243652\n",
      "epoch: 107,  batch step: 146, loss: 3.226285934448242\n",
      "epoch: 107,  batch step: 147, loss: 2.7554128170013428\n",
      "epoch: 107,  batch step: 148, loss: 2.1519312858581543\n",
      "epoch: 107,  batch step: 149, loss: 2.8868331909179688\n",
      "epoch: 107,  batch step: 150, loss: 18.540292739868164\n",
      "epoch: 107,  batch step: 151, loss: 1.4555076360702515\n",
      "epoch: 107,  batch step: 152, loss: 27.65260887145996\n",
      "epoch: 107,  batch step: 153, loss: 22.71668815612793\n",
      "epoch: 107,  batch step: 154, loss: 4.56002950668335\n",
      "epoch: 107,  batch step: 155, loss: 11.59992504119873\n",
      "epoch: 107,  batch step: 156, loss: 3.483707904815674\n",
      "epoch: 107,  batch step: 157, loss: 3.6914870738983154\n",
      "epoch: 107,  batch step: 158, loss: 2.193497657775879\n",
      "epoch: 107,  batch step: 159, loss: 37.846900939941406\n",
      "epoch: 107,  batch step: 160, loss: 6.66031551361084\n",
      "epoch: 107,  batch step: 161, loss: 17.188718795776367\n",
      "epoch: 107,  batch step: 162, loss: 7.552840709686279\n",
      "epoch: 107,  batch step: 163, loss: 2.520970344543457\n",
      "epoch: 107,  batch step: 164, loss: 16.966339111328125\n",
      "epoch: 107,  batch step: 165, loss: 11.236616134643555\n",
      "epoch: 107,  batch step: 166, loss: 5.492818832397461\n",
      "epoch: 107,  batch step: 167, loss: 37.252986907958984\n",
      "epoch: 107,  batch step: 168, loss: 1.8904845714569092\n",
      "epoch: 107,  batch step: 169, loss: 1.592142105102539\n",
      "epoch: 107,  batch step: 170, loss: 3.59409499168396\n",
      "epoch: 107,  batch step: 171, loss: 3.5960605144500732\n",
      "epoch: 107,  batch step: 172, loss: 6.0645341873168945\n",
      "epoch: 107,  batch step: 173, loss: 27.617538452148438\n",
      "epoch: 107,  batch step: 174, loss: 3.2708606719970703\n",
      "epoch: 107,  batch step: 175, loss: 4.933773994445801\n",
      "epoch: 107,  batch step: 176, loss: 12.621391296386719\n",
      "epoch: 107,  batch step: 177, loss: 2.543869972229004\n",
      "epoch: 107,  batch step: 178, loss: 2.5172080993652344\n",
      "epoch: 107,  batch step: 179, loss: 2.314846992492676\n",
      "epoch: 107,  batch step: 180, loss: 2.1151671409606934\n",
      "epoch: 107,  batch step: 181, loss: 9.087571144104004\n",
      "epoch: 107,  batch step: 182, loss: 11.563361167907715\n",
      "epoch: 107,  batch step: 183, loss: 67.06877136230469\n",
      "epoch: 107,  batch step: 184, loss: 2.6460556983947754\n",
      "epoch: 107,  batch step: 185, loss: 2.969316244125366\n",
      "epoch: 107,  batch step: 186, loss: 1.561987280845642\n",
      "epoch: 107,  batch step: 187, loss: 16.340057373046875\n",
      "epoch: 107,  batch step: 188, loss: 2.782395124435425\n",
      "epoch: 107,  batch step: 189, loss: 3.389347791671753\n",
      "epoch: 107,  batch step: 190, loss: 3.5534846782684326\n",
      "epoch: 107,  batch step: 191, loss: 3.3879306316375732\n",
      "epoch: 107,  batch step: 192, loss: 50.32682800292969\n",
      "epoch: 107,  batch step: 193, loss: 8.610132217407227\n",
      "epoch: 107,  batch step: 194, loss: 7.285061836242676\n",
      "epoch: 107,  batch step: 195, loss: 4.361110687255859\n",
      "epoch: 107,  batch step: 196, loss: 2.206953525543213\n",
      "epoch: 107,  batch step: 197, loss: 4.176934242248535\n",
      "epoch: 107,  batch step: 198, loss: 11.124336242675781\n",
      "epoch: 107,  batch step: 199, loss: 3.7606191635131836\n",
      "epoch: 107,  batch step: 200, loss: 2.745961904525757\n",
      "epoch: 107,  batch step: 201, loss: 1.5285232067108154\n",
      "epoch: 107,  batch step: 202, loss: 9.196839332580566\n",
      "epoch: 107,  batch step: 203, loss: 2.282548189163208\n",
      "epoch: 107,  batch step: 204, loss: 2.2285585403442383\n",
      "epoch: 107,  batch step: 205, loss: 1.811840534210205\n",
      "epoch: 107,  batch step: 206, loss: 2.8083229064941406\n",
      "epoch: 107,  batch step: 207, loss: 4.889411449432373\n",
      "epoch: 107,  batch step: 208, loss: 29.691478729248047\n",
      "epoch: 107,  batch step: 209, loss: 29.0521240234375\n",
      "epoch: 107,  batch step: 210, loss: 2.5388131141662598\n",
      "epoch: 107,  batch step: 211, loss: 11.2312650680542\n",
      "epoch: 107,  batch step: 212, loss: 41.445430755615234\n",
      "epoch: 107,  batch step: 213, loss: 1.6495552062988281\n",
      "epoch: 107,  batch step: 214, loss: 8.598002433776855\n",
      "epoch: 107,  batch step: 215, loss: 5.137702941894531\n",
      "epoch: 107,  batch step: 216, loss: 2.1204071044921875\n",
      "epoch: 107,  batch step: 217, loss: 15.345316886901855\n",
      "epoch: 107,  batch step: 218, loss: 56.205528259277344\n",
      "epoch: 107,  batch step: 219, loss: 1.717576503753662\n",
      "epoch: 107,  batch step: 220, loss: 12.1533842086792\n",
      "epoch: 107,  batch step: 221, loss: 7.522452354431152\n",
      "epoch: 107,  batch step: 222, loss: 2.860276937484741\n",
      "epoch: 107,  batch step: 223, loss: 2.843484878540039\n",
      "epoch: 107,  batch step: 224, loss: 6.809300899505615\n",
      "epoch: 107,  batch step: 225, loss: 2.69435453414917\n",
      "epoch: 107,  batch step: 226, loss: 2.630241870880127\n",
      "epoch: 107,  batch step: 227, loss: 2.919771671295166\n",
      "epoch: 107,  batch step: 228, loss: 27.30731201171875\n",
      "epoch: 107,  batch step: 229, loss: 11.790946006774902\n",
      "epoch: 107,  batch step: 230, loss: 1.8313021659851074\n",
      "epoch: 107,  batch step: 231, loss: 2.8949496746063232\n",
      "epoch: 107,  batch step: 232, loss: 1.7889318466186523\n",
      "epoch: 107,  batch step: 233, loss: 13.388486862182617\n",
      "epoch: 107,  batch step: 234, loss: 76.98937225341797\n",
      "epoch: 107,  batch step: 235, loss: 1.9678977727890015\n",
      "epoch: 107,  batch step: 236, loss: 5.860114574432373\n",
      "epoch: 107,  batch step: 237, loss: 30.80963897705078\n",
      "epoch: 107,  batch step: 238, loss: 15.146048545837402\n",
      "epoch: 107,  batch step: 239, loss: 20.965782165527344\n",
      "epoch: 107,  batch step: 240, loss: 2.520362615585327\n",
      "epoch: 107,  batch step: 241, loss: 14.40717887878418\n",
      "epoch: 107,  batch step: 242, loss: 9.588153839111328\n",
      "epoch: 107,  batch step: 243, loss: 9.291975021362305\n",
      "epoch: 107,  batch step: 244, loss: 10.647684097290039\n",
      "epoch: 107,  batch step: 245, loss: 11.65060806274414\n",
      "epoch: 107,  batch step: 246, loss: 7.022582530975342\n",
      "epoch: 107,  batch step: 247, loss: 2.9271726608276367\n",
      "epoch: 107,  batch step: 248, loss: 2.846782684326172\n",
      "epoch: 107,  batch step: 249, loss: 7.97543478012085\n",
      "epoch: 107,  batch step: 250, loss: 12.709025382995605\n",
      "epoch: 107,  batch step: 251, loss: 684.3441772460938\n",
      "validation error epoch  107:    tensor(69.2002, device='cuda:0')\n",
      "316\n",
      "epoch: 108,  batch step: 0, loss: 2.0736122131347656\n",
      "epoch: 108,  batch step: 1, loss: 1.7933543920516968\n",
      "epoch: 108,  batch step: 2, loss: 5.852818965911865\n",
      "epoch: 108,  batch step: 3, loss: 11.075773239135742\n",
      "epoch: 108,  batch step: 4, loss: 66.9786605834961\n",
      "epoch: 108,  batch step: 5, loss: 10.430747985839844\n",
      "epoch: 108,  batch step: 6, loss: 26.42762565612793\n",
      "epoch: 108,  batch step: 7, loss: 11.135177612304688\n",
      "epoch: 108,  batch step: 8, loss: 13.387446403503418\n",
      "epoch: 108,  batch step: 9, loss: 7.606955528259277\n",
      "epoch: 108,  batch step: 10, loss: 58.885841369628906\n",
      "epoch: 108,  batch step: 11, loss: 3.3925235271453857\n",
      "epoch: 108,  batch step: 12, loss: 29.479084014892578\n",
      "epoch: 108,  batch step: 13, loss: 4.669619560241699\n",
      "epoch: 108,  batch step: 14, loss: 9.249921798706055\n",
      "epoch: 108,  batch step: 15, loss: 6.591519355773926\n",
      "epoch: 108,  batch step: 16, loss: 3.666090965270996\n",
      "epoch: 108,  batch step: 17, loss: 2.238008975982666\n",
      "epoch: 108,  batch step: 18, loss: 31.592937469482422\n",
      "epoch: 108,  batch step: 19, loss: 3.22589373588562\n",
      "epoch: 108,  batch step: 20, loss: 4.589360237121582\n",
      "epoch: 108,  batch step: 21, loss: 13.529708862304688\n",
      "epoch: 108,  batch step: 22, loss: 3.169217586517334\n",
      "epoch: 108,  batch step: 23, loss: 4.002488613128662\n",
      "epoch: 108,  batch step: 24, loss: 4.44464635848999\n",
      "epoch: 108,  batch step: 25, loss: 3.1675968170166016\n",
      "epoch: 108,  batch step: 26, loss: 8.370800018310547\n",
      "epoch: 108,  batch step: 27, loss: 13.366591453552246\n",
      "epoch: 108,  batch step: 28, loss: 48.66878890991211\n",
      "epoch: 108,  batch step: 29, loss: 4.193933486938477\n",
      "epoch: 108,  batch step: 30, loss: 26.010726928710938\n",
      "epoch: 108,  batch step: 31, loss: 36.55032730102539\n",
      "epoch: 108,  batch step: 32, loss: 4.191949367523193\n",
      "epoch: 108,  batch step: 33, loss: 3.0260236263275146\n",
      "epoch: 108,  batch step: 34, loss: 21.217849731445312\n",
      "epoch: 108,  batch step: 35, loss: 13.699466705322266\n",
      "epoch: 108,  batch step: 36, loss: 2.960972547531128\n",
      "epoch: 108,  batch step: 37, loss: 3.4373881816864014\n",
      "epoch: 108,  batch step: 38, loss: 13.659456253051758\n",
      "epoch: 108,  batch step: 39, loss: 6.58786153793335\n",
      "epoch: 108,  batch step: 40, loss: 18.752208709716797\n",
      "epoch: 108,  batch step: 41, loss: 20.068592071533203\n",
      "epoch: 108,  batch step: 42, loss: 61.70104217529297\n",
      "epoch: 108,  batch step: 43, loss: 13.388668060302734\n",
      "epoch: 108,  batch step: 44, loss: 3.0003342628479004\n",
      "epoch: 108,  batch step: 45, loss: 2.5729894638061523\n",
      "epoch: 108,  batch step: 46, loss: 17.259708404541016\n",
      "epoch: 108,  batch step: 47, loss: 31.320201873779297\n",
      "epoch: 108,  batch step: 48, loss: 33.15131759643555\n",
      "epoch: 108,  batch step: 49, loss: 10.912967681884766\n",
      "epoch: 108,  batch step: 50, loss: 2.4639880657196045\n",
      "epoch: 108,  batch step: 51, loss: 5.661233425140381\n",
      "epoch: 108,  batch step: 52, loss: 2.293706178665161\n",
      "epoch: 108,  batch step: 53, loss: 2.697253704071045\n",
      "epoch: 108,  batch step: 54, loss: 21.663131713867188\n",
      "epoch: 108,  batch step: 55, loss: 11.212634086608887\n",
      "epoch: 108,  batch step: 56, loss: 9.634608268737793\n",
      "epoch: 108,  batch step: 57, loss: 5.8029937744140625\n",
      "epoch: 108,  batch step: 58, loss: 46.772621154785156\n",
      "epoch: 108,  batch step: 59, loss: 2.785365104675293\n",
      "epoch: 108,  batch step: 60, loss: 10.625102043151855\n",
      "epoch: 108,  batch step: 61, loss: 9.328944206237793\n",
      "epoch: 108,  batch step: 62, loss: 2.4253406524658203\n",
      "epoch: 108,  batch step: 63, loss: 2.233534812927246\n",
      "epoch: 108,  batch step: 64, loss: 1.8925139904022217\n",
      "epoch: 108,  batch step: 65, loss: 11.355223655700684\n",
      "epoch: 108,  batch step: 66, loss: 2.585559606552124\n",
      "epoch: 108,  batch step: 67, loss: 22.695287704467773\n",
      "epoch: 108,  batch step: 68, loss: 9.684616088867188\n",
      "epoch: 108,  batch step: 69, loss: 2.4036383628845215\n",
      "epoch: 108,  batch step: 70, loss: 9.48554515838623\n",
      "epoch: 108,  batch step: 71, loss: 2.6733062267303467\n",
      "epoch: 108,  batch step: 72, loss: 2.0453615188598633\n",
      "epoch: 108,  batch step: 73, loss: 1.4967011213302612\n",
      "epoch: 108,  batch step: 74, loss: 3.448173999786377\n",
      "epoch: 108,  batch step: 75, loss: 47.59896469116211\n",
      "epoch: 108,  batch step: 76, loss: 135.29632568359375\n",
      "epoch: 108,  batch step: 77, loss: 1.7874993085861206\n",
      "epoch: 108,  batch step: 78, loss: 3.282404661178589\n",
      "epoch: 108,  batch step: 79, loss: 4.683544158935547\n",
      "epoch: 108,  batch step: 80, loss: 14.62402629852295\n",
      "epoch: 108,  batch step: 81, loss: 11.883491516113281\n",
      "epoch: 108,  batch step: 82, loss: 2.0695645809173584\n",
      "epoch: 108,  batch step: 83, loss: 1.9915168285369873\n",
      "epoch: 108,  batch step: 84, loss: 2.4424777030944824\n",
      "epoch: 108,  batch step: 85, loss: 23.168058395385742\n",
      "epoch: 108,  batch step: 86, loss: 50.544593811035156\n",
      "epoch: 108,  batch step: 87, loss: 4.060077667236328\n",
      "epoch: 108,  batch step: 88, loss: 1.7726516723632812\n",
      "epoch: 108,  batch step: 89, loss: 5.499391078948975\n",
      "epoch: 108,  batch step: 90, loss: 2.3899588584899902\n",
      "epoch: 108,  batch step: 91, loss: 2.8975417613983154\n",
      "epoch: 108,  batch step: 92, loss: 12.362383842468262\n",
      "epoch: 108,  batch step: 93, loss: 12.101249694824219\n",
      "epoch: 108,  batch step: 94, loss: 12.723959922790527\n",
      "epoch: 108,  batch step: 95, loss: 2.7724318504333496\n",
      "epoch: 108,  batch step: 96, loss: 33.10573196411133\n",
      "epoch: 108,  batch step: 97, loss: 22.78929901123047\n",
      "epoch: 108,  batch step: 98, loss: 3.59881854057312\n",
      "epoch: 108,  batch step: 99, loss: 10.85456371307373\n",
      "epoch: 108,  batch step: 100, loss: 11.098068237304688\n",
      "epoch: 108,  batch step: 101, loss: 4.12435245513916\n",
      "epoch: 108,  batch step: 102, loss: 22.264541625976562\n",
      "epoch: 108,  batch step: 103, loss: 2.734128475189209\n",
      "epoch: 108,  batch step: 104, loss: 21.960521697998047\n",
      "epoch: 108,  batch step: 105, loss: 4.812343120574951\n",
      "epoch: 108,  batch step: 106, loss: 8.32907485961914\n",
      "epoch: 108,  batch step: 107, loss: 2.4248013496398926\n",
      "epoch: 108,  batch step: 108, loss: 26.077163696289062\n",
      "epoch: 108,  batch step: 109, loss: 39.21149444580078\n",
      "epoch: 108,  batch step: 110, loss: 24.29416275024414\n",
      "epoch: 108,  batch step: 111, loss: 11.194750785827637\n",
      "epoch: 108,  batch step: 112, loss: 11.146224021911621\n",
      "epoch: 108,  batch step: 113, loss: 4.075891017913818\n",
      "epoch: 108,  batch step: 114, loss: 12.039623260498047\n",
      "epoch: 108,  batch step: 115, loss: 1.6490639448165894\n",
      "epoch: 108,  batch step: 116, loss: 11.442628860473633\n",
      "epoch: 108,  batch step: 117, loss: 32.645503997802734\n",
      "epoch: 108,  batch step: 118, loss: 14.31907844543457\n",
      "epoch: 108,  batch step: 119, loss: 2.3285341262817383\n",
      "epoch: 108,  batch step: 120, loss: 2.5303587913513184\n",
      "epoch: 108,  batch step: 121, loss: 7.029866695404053\n",
      "epoch: 108,  batch step: 122, loss: 26.71564292907715\n",
      "epoch: 108,  batch step: 123, loss: 40.57191467285156\n",
      "epoch: 108,  batch step: 124, loss: 10.575555801391602\n",
      "epoch: 108,  batch step: 125, loss: 2.5477051734924316\n",
      "epoch: 108,  batch step: 126, loss: 10.95966625213623\n",
      "epoch: 108,  batch step: 127, loss: 2.84329891204834\n",
      "epoch: 108,  batch step: 128, loss: 5.003665924072266\n",
      "epoch: 108,  batch step: 129, loss: 41.84775924682617\n",
      "epoch: 108,  batch step: 130, loss: 1.7355127334594727\n",
      "epoch: 108,  batch step: 131, loss: 6.5563764572143555\n",
      "epoch: 108,  batch step: 132, loss: 2.1294033527374268\n",
      "epoch: 108,  batch step: 133, loss: 2.892350196838379\n",
      "epoch: 108,  batch step: 134, loss: 2.195143699645996\n",
      "epoch: 108,  batch step: 135, loss: 8.683472633361816\n",
      "epoch: 108,  batch step: 136, loss: 22.114097595214844\n",
      "epoch: 108,  batch step: 137, loss: 1.9176406860351562\n",
      "epoch: 108,  batch step: 138, loss: 61.693511962890625\n",
      "epoch: 108,  batch step: 139, loss: 3.5572993755340576\n",
      "epoch: 108,  batch step: 140, loss: 2.966529607772827\n",
      "epoch: 108,  batch step: 141, loss: 1.946617603302002\n",
      "epoch: 108,  batch step: 142, loss: 57.6810417175293\n",
      "epoch: 108,  batch step: 143, loss: 12.251243591308594\n",
      "epoch: 108,  batch step: 144, loss: 6.111232757568359\n",
      "epoch: 108,  batch step: 145, loss: 14.652835845947266\n",
      "epoch: 108,  batch step: 146, loss: 33.19813919067383\n",
      "epoch: 108,  batch step: 147, loss: 38.4012451171875\n",
      "epoch: 108,  batch step: 148, loss: 2.4948909282684326\n",
      "epoch: 108,  batch step: 149, loss: 1.9531102180480957\n",
      "epoch: 108,  batch step: 150, loss: 1.6147701740264893\n",
      "epoch: 108,  batch step: 151, loss: 9.155380249023438\n",
      "epoch: 108,  batch step: 152, loss: 11.422720909118652\n",
      "epoch: 108,  batch step: 153, loss: 5.051936149597168\n",
      "epoch: 108,  batch step: 154, loss: 26.328689575195312\n",
      "epoch: 108,  batch step: 155, loss: 9.838436126708984\n",
      "epoch: 108,  batch step: 156, loss: 5.733257293701172\n",
      "epoch: 108,  batch step: 157, loss: 2.005451202392578\n",
      "epoch: 108,  batch step: 158, loss: 2.4183859825134277\n",
      "epoch: 108,  batch step: 159, loss: 2.3275508880615234\n",
      "epoch: 108,  batch step: 160, loss: 5.649272441864014\n",
      "epoch: 108,  batch step: 161, loss: 2.089111804962158\n",
      "epoch: 108,  batch step: 162, loss: 17.31355857849121\n",
      "epoch: 108,  batch step: 163, loss: 8.329184532165527\n",
      "epoch: 108,  batch step: 164, loss: 5.011988639831543\n",
      "epoch: 108,  batch step: 165, loss: 2.182145595550537\n",
      "epoch: 108,  batch step: 166, loss: 2.4680333137512207\n",
      "epoch: 108,  batch step: 167, loss: 40.697181701660156\n",
      "epoch: 108,  batch step: 168, loss: 5.859350204467773\n",
      "epoch: 108,  batch step: 169, loss: 2.6419951915740967\n",
      "epoch: 108,  batch step: 170, loss: 2.811505079269409\n",
      "epoch: 108,  batch step: 171, loss: 3.613161087036133\n",
      "epoch: 108,  batch step: 172, loss: 3.3894524574279785\n",
      "epoch: 108,  batch step: 173, loss: 1.940058708190918\n",
      "epoch: 108,  batch step: 174, loss: 3.256112575531006\n",
      "epoch: 108,  batch step: 175, loss: 5.238973617553711\n",
      "epoch: 108,  batch step: 176, loss: 15.315546989440918\n",
      "epoch: 108,  batch step: 177, loss: 14.993758201599121\n",
      "epoch: 108,  batch step: 178, loss: 3.0560708045959473\n",
      "epoch: 108,  batch step: 179, loss: 2.23148250579834\n",
      "epoch: 108,  batch step: 180, loss: 2.323549747467041\n",
      "epoch: 108,  batch step: 181, loss: 14.992270469665527\n",
      "epoch: 108,  batch step: 182, loss: 1.9643747806549072\n",
      "epoch: 108,  batch step: 183, loss: 1.7114307880401611\n",
      "epoch: 108,  batch step: 184, loss: 9.967778205871582\n",
      "epoch: 108,  batch step: 185, loss: 21.473995208740234\n",
      "epoch: 108,  batch step: 186, loss: 3.732174873352051\n",
      "epoch: 108,  batch step: 187, loss: 8.887738227844238\n",
      "epoch: 108,  batch step: 188, loss: 3.007549285888672\n",
      "epoch: 108,  batch step: 189, loss: 2.2394826412200928\n",
      "epoch: 108,  batch step: 190, loss: 11.464003562927246\n",
      "epoch: 108,  batch step: 191, loss: 2.8878326416015625\n",
      "epoch: 108,  batch step: 192, loss: 3.532762289047241\n",
      "epoch: 108,  batch step: 193, loss: 28.222225189208984\n",
      "epoch: 108,  batch step: 194, loss: 52.919830322265625\n",
      "epoch: 108,  batch step: 195, loss: 8.078083992004395\n",
      "epoch: 108,  batch step: 196, loss: 7.950620174407959\n",
      "epoch: 108,  batch step: 197, loss: 7.237954616546631\n",
      "epoch: 108,  batch step: 198, loss: 3.015476703643799\n",
      "epoch: 108,  batch step: 199, loss: 35.6268424987793\n",
      "epoch: 108,  batch step: 200, loss: 5.181499481201172\n",
      "epoch: 108,  batch step: 201, loss: 16.59261703491211\n",
      "epoch: 108,  batch step: 202, loss: 67.10855102539062\n",
      "epoch: 108,  batch step: 203, loss: 13.49522590637207\n",
      "epoch: 108,  batch step: 204, loss: 9.061738967895508\n",
      "epoch: 108,  batch step: 205, loss: 1.6268739700317383\n",
      "epoch: 108,  batch step: 206, loss: 39.61553192138672\n",
      "epoch: 108,  batch step: 207, loss: 2.526545524597168\n",
      "epoch: 108,  batch step: 208, loss: 10.472983360290527\n",
      "epoch: 108,  batch step: 209, loss: 15.651639938354492\n",
      "epoch: 108,  batch step: 210, loss: 51.227195739746094\n",
      "epoch: 108,  batch step: 211, loss: 2.1638355255126953\n",
      "epoch: 108,  batch step: 212, loss: 9.817096710205078\n",
      "epoch: 108,  batch step: 213, loss: 1.7899149656295776\n",
      "epoch: 108,  batch step: 214, loss: 1.9989832639694214\n",
      "epoch: 108,  batch step: 215, loss: 1.5168814659118652\n",
      "epoch: 108,  batch step: 216, loss: 15.625643730163574\n",
      "epoch: 108,  batch step: 217, loss: 11.501055717468262\n",
      "epoch: 108,  batch step: 218, loss: 9.26516056060791\n",
      "epoch: 108,  batch step: 219, loss: 70.17138671875\n",
      "epoch: 108,  batch step: 220, loss: 10.295843124389648\n",
      "epoch: 108,  batch step: 221, loss: 4.999212265014648\n",
      "epoch: 108,  batch step: 222, loss: 2.2904932498931885\n",
      "epoch: 108,  batch step: 223, loss: 67.59049987792969\n",
      "epoch: 108,  batch step: 224, loss: 20.30582046508789\n",
      "epoch: 108,  batch step: 225, loss: 1.3995997905731201\n",
      "epoch: 108,  batch step: 226, loss: 3.9433765411376953\n",
      "epoch: 108,  batch step: 227, loss: 2.2299890518188477\n",
      "epoch: 108,  batch step: 228, loss: 2.2648324966430664\n",
      "epoch: 108,  batch step: 229, loss: 1.7138208150863647\n",
      "epoch: 108,  batch step: 230, loss: 11.31051254272461\n",
      "epoch: 108,  batch step: 231, loss: 2.2766311168670654\n",
      "epoch: 108,  batch step: 232, loss: 2.3521242141723633\n",
      "epoch: 108,  batch step: 233, loss: 2.014188289642334\n",
      "epoch: 108,  batch step: 234, loss: 1.7986805438995361\n",
      "epoch: 108,  batch step: 235, loss: 22.93100357055664\n",
      "epoch: 108,  batch step: 236, loss: 8.454183578491211\n",
      "epoch: 108,  batch step: 237, loss: 26.9392147064209\n",
      "epoch: 108,  batch step: 238, loss: 8.273167610168457\n",
      "epoch: 108,  batch step: 239, loss: 7.360023021697998\n",
      "epoch: 108,  batch step: 240, loss: 7.8958234786987305\n",
      "epoch: 108,  batch step: 241, loss: 18.575244903564453\n",
      "epoch: 108,  batch step: 242, loss: 7.401012420654297\n",
      "epoch: 108,  batch step: 243, loss: 11.894176483154297\n",
      "epoch: 108,  batch step: 244, loss: 2.0476298332214355\n",
      "epoch: 108,  batch step: 245, loss: 36.631797790527344\n",
      "epoch: 108,  batch step: 246, loss: 15.197044372558594\n",
      "epoch: 108,  batch step: 247, loss: 9.43766975402832\n",
      "epoch: 108,  batch step: 248, loss: 18.34014892578125\n",
      "epoch: 108,  batch step: 249, loss: 11.782135009765625\n",
      "epoch: 108,  batch step: 250, loss: 2.488863468170166\n",
      "epoch: 108,  batch step: 251, loss: 69.03240966796875\n",
      "validation error epoch  108:    tensor(68.2318, device='cuda:0')\n",
      "316\n",
      "epoch: 109,  batch step: 0, loss: 5.802398204803467\n",
      "epoch: 109,  batch step: 1, loss: 36.064884185791016\n",
      "epoch: 109,  batch step: 2, loss: 9.312112808227539\n",
      "epoch: 109,  batch step: 3, loss: 18.96168327331543\n",
      "epoch: 109,  batch step: 4, loss: 1.850144624710083\n",
      "epoch: 109,  batch step: 5, loss: 34.2079963684082\n",
      "epoch: 109,  batch step: 6, loss: 4.057321548461914\n",
      "epoch: 109,  batch step: 7, loss: 14.343719482421875\n",
      "epoch: 109,  batch step: 8, loss: 3.4851574897766113\n",
      "epoch: 109,  batch step: 9, loss: 3.546722412109375\n",
      "epoch: 109,  batch step: 10, loss: 12.75507926940918\n",
      "epoch: 109,  batch step: 11, loss: 3.1835761070251465\n",
      "epoch: 109,  batch step: 12, loss: 3.2900867462158203\n",
      "epoch: 109,  batch step: 13, loss: 23.293827056884766\n",
      "epoch: 109,  batch step: 14, loss: 49.92927169799805\n",
      "epoch: 109,  batch step: 15, loss: 9.167779922485352\n",
      "epoch: 109,  batch step: 16, loss: 3.7270946502685547\n",
      "epoch: 109,  batch step: 17, loss: 9.151434898376465\n",
      "epoch: 109,  batch step: 18, loss: 1.9486274719238281\n",
      "epoch: 109,  batch step: 19, loss: 3.0465240478515625\n",
      "epoch: 109,  batch step: 20, loss: 12.911190032958984\n",
      "epoch: 109,  batch step: 21, loss: 13.106697082519531\n",
      "epoch: 109,  batch step: 22, loss: 3.4130477905273438\n",
      "epoch: 109,  batch step: 23, loss: 42.2332763671875\n",
      "epoch: 109,  batch step: 24, loss: 2.2901790142059326\n",
      "epoch: 109,  batch step: 25, loss: 33.350765228271484\n",
      "epoch: 109,  batch step: 26, loss: 4.126197814941406\n",
      "epoch: 109,  batch step: 27, loss: 2.168410301208496\n",
      "epoch: 109,  batch step: 28, loss: 7.608159065246582\n",
      "epoch: 109,  batch step: 29, loss: 67.09524536132812\n",
      "epoch: 109,  batch step: 30, loss: 12.7037992477417\n",
      "epoch: 109,  batch step: 31, loss: 10.115846633911133\n",
      "epoch: 109,  batch step: 32, loss: 2.3538451194763184\n",
      "epoch: 109,  batch step: 33, loss: 16.43997573852539\n",
      "epoch: 109,  batch step: 34, loss: 1.8207471370697021\n",
      "epoch: 109,  batch step: 35, loss: 78.1607666015625\n",
      "epoch: 109,  batch step: 36, loss: 4.014361381530762\n",
      "epoch: 109,  batch step: 37, loss: 2.6942224502563477\n",
      "epoch: 109,  batch step: 38, loss: 4.823869705200195\n",
      "epoch: 109,  batch step: 39, loss: 18.33390998840332\n",
      "epoch: 109,  batch step: 40, loss: 8.091148376464844\n",
      "epoch: 109,  batch step: 41, loss: 6.799443244934082\n",
      "epoch: 109,  batch step: 42, loss: 6.265281677246094\n",
      "epoch: 109,  batch step: 43, loss: 2.44195818901062\n",
      "epoch: 109,  batch step: 44, loss: 1.7986037731170654\n",
      "epoch: 109,  batch step: 45, loss: 1.9506502151489258\n",
      "epoch: 109,  batch step: 46, loss: 2.9427623748779297\n",
      "epoch: 109,  batch step: 47, loss: 8.920312881469727\n",
      "epoch: 109,  batch step: 48, loss: 22.91244888305664\n",
      "epoch: 109,  batch step: 49, loss: 23.683822631835938\n",
      "epoch: 109,  batch step: 50, loss: 27.021879196166992\n",
      "epoch: 109,  batch step: 51, loss: 2.797079563140869\n",
      "epoch: 109,  batch step: 52, loss: 10.527175903320312\n",
      "epoch: 109,  batch step: 53, loss: 2.3439228534698486\n",
      "epoch: 109,  batch step: 54, loss: 3.6950411796569824\n",
      "epoch: 109,  batch step: 55, loss: 5.577329158782959\n",
      "epoch: 109,  batch step: 56, loss: 12.752267837524414\n",
      "epoch: 109,  batch step: 57, loss: 1.8327983617782593\n",
      "epoch: 109,  batch step: 58, loss: 2.7058186531066895\n",
      "epoch: 109,  batch step: 59, loss: 17.150005340576172\n",
      "epoch: 109,  batch step: 60, loss: 3.1183314323425293\n",
      "epoch: 109,  batch step: 61, loss: 1.8100268840789795\n",
      "epoch: 109,  batch step: 62, loss: 15.837977409362793\n",
      "epoch: 109,  batch step: 63, loss: 5.294784069061279\n",
      "epoch: 109,  batch step: 64, loss: 46.65932846069336\n",
      "epoch: 109,  batch step: 65, loss: 2.843611717224121\n",
      "epoch: 109,  batch step: 66, loss: 9.114322662353516\n",
      "epoch: 109,  batch step: 67, loss: 5.5894975662231445\n",
      "epoch: 109,  batch step: 68, loss: 2.2454707622528076\n",
      "epoch: 109,  batch step: 69, loss: 3.547607898712158\n",
      "epoch: 109,  batch step: 70, loss: 14.611215591430664\n",
      "epoch: 109,  batch step: 71, loss: 12.079363822937012\n",
      "epoch: 109,  batch step: 72, loss: 3.9493508338928223\n",
      "epoch: 109,  batch step: 73, loss: 8.067386627197266\n",
      "epoch: 109,  batch step: 74, loss: 7.830310344696045\n",
      "epoch: 109,  batch step: 75, loss: 42.613182067871094\n",
      "epoch: 109,  batch step: 76, loss: 3.812498092651367\n",
      "epoch: 109,  batch step: 77, loss: 7.345598220825195\n",
      "epoch: 109,  batch step: 78, loss: 8.762478828430176\n",
      "epoch: 109,  batch step: 79, loss: 20.559310913085938\n",
      "epoch: 109,  batch step: 80, loss: 1.5973522663116455\n",
      "epoch: 109,  batch step: 81, loss: 6.268387794494629\n",
      "epoch: 109,  batch step: 82, loss: 11.141050338745117\n",
      "epoch: 109,  batch step: 83, loss: 2.7673490047454834\n",
      "epoch: 109,  batch step: 84, loss: 1.5634543895721436\n",
      "epoch: 109,  batch step: 85, loss: 1.9291267395019531\n",
      "epoch: 109,  batch step: 86, loss: 13.786707878112793\n",
      "epoch: 109,  batch step: 87, loss: 4.784912586212158\n",
      "epoch: 109,  batch step: 88, loss: 10.983981132507324\n",
      "epoch: 109,  batch step: 89, loss: 14.70118236541748\n",
      "epoch: 109,  batch step: 90, loss: 2.07289457321167\n",
      "epoch: 109,  batch step: 91, loss: 7.562493801116943\n",
      "epoch: 109,  batch step: 92, loss: 1.9770729541778564\n",
      "epoch: 109,  batch step: 93, loss: 3.8046774864196777\n",
      "epoch: 109,  batch step: 94, loss: 6.070596694946289\n",
      "epoch: 109,  batch step: 95, loss: 30.7452449798584\n",
      "epoch: 109,  batch step: 96, loss: 3.162158489227295\n",
      "epoch: 109,  batch step: 97, loss: 2.378385543823242\n",
      "epoch: 109,  batch step: 98, loss: 2.3597412109375\n",
      "epoch: 109,  batch step: 99, loss: 4.124547004699707\n",
      "epoch: 109,  batch step: 100, loss: 3.329834461212158\n",
      "epoch: 109,  batch step: 101, loss: 11.895004272460938\n",
      "epoch: 109,  batch step: 102, loss: 9.437150955200195\n",
      "epoch: 109,  batch step: 103, loss: 8.844269752502441\n",
      "epoch: 109,  batch step: 104, loss: 1.4184396266937256\n",
      "epoch: 109,  batch step: 105, loss: 9.266622543334961\n",
      "epoch: 109,  batch step: 106, loss: 5.811355113983154\n",
      "epoch: 109,  batch step: 107, loss: 45.03718185424805\n",
      "epoch: 109,  batch step: 108, loss: 2.306206226348877\n",
      "epoch: 109,  batch step: 109, loss: 1.4954924583435059\n",
      "epoch: 109,  batch step: 110, loss: 1.9057105779647827\n",
      "epoch: 109,  batch step: 111, loss: 2.142500638961792\n",
      "epoch: 109,  batch step: 112, loss: 11.496099472045898\n",
      "epoch: 109,  batch step: 113, loss: 8.135275840759277\n",
      "epoch: 109,  batch step: 114, loss: 1.7729820013046265\n",
      "epoch: 109,  batch step: 115, loss: 1.8928520679473877\n",
      "epoch: 109,  batch step: 116, loss: 2.5444483757019043\n",
      "epoch: 109,  batch step: 117, loss: 2.899074077606201\n",
      "epoch: 109,  batch step: 118, loss: 57.894187927246094\n",
      "epoch: 109,  batch step: 119, loss: 18.037120819091797\n",
      "epoch: 109,  batch step: 120, loss: 2.444890022277832\n",
      "epoch: 109,  batch step: 121, loss: 1.9586824178695679\n",
      "epoch: 109,  batch step: 122, loss: 37.38908767700195\n",
      "epoch: 109,  batch step: 123, loss: 13.88742446899414\n",
      "epoch: 109,  batch step: 124, loss: 1.253909945487976\n",
      "epoch: 109,  batch step: 125, loss: 1.7892200946807861\n",
      "epoch: 109,  batch step: 126, loss: 2.3453636169433594\n",
      "epoch: 109,  batch step: 127, loss: 65.93159484863281\n",
      "epoch: 109,  batch step: 128, loss: 12.278432846069336\n",
      "epoch: 109,  batch step: 129, loss: 16.684925079345703\n",
      "epoch: 109,  batch step: 130, loss: 11.466207504272461\n",
      "epoch: 109,  batch step: 131, loss: 3.075611114501953\n",
      "epoch: 109,  batch step: 132, loss: 2.191068172454834\n",
      "epoch: 109,  batch step: 133, loss: 11.887598037719727\n",
      "epoch: 109,  batch step: 134, loss: 1.6594985723495483\n",
      "epoch: 109,  batch step: 135, loss: 1.9045580625534058\n",
      "epoch: 109,  batch step: 136, loss: 2.427673101425171\n",
      "epoch: 109,  batch step: 137, loss: 1.93519926071167\n",
      "epoch: 109,  batch step: 138, loss: 11.19721508026123\n",
      "epoch: 109,  batch step: 139, loss: 50.82528305053711\n",
      "epoch: 109,  batch step: 140, loss: 6.615689754486084\n",
      "epoch: 109,  batch step: 141, loss: 10.383304595947266\n",
      "epoch: 109,  batch step: 142, loss: 10.18405818939209\n",
      "epoch: 109,  batch step: 143, loss: 2.6635425090789795\n",
      "epoch: 109,  batch step: 144, loss: 3.7979695796966553\n",
      "epoch: 109,  batch step: 145, loss: 11.620853424072266\n",
      "epoch: 109,  batch step: 146, loss: 4.646613121032715\n",
      "epoch: 109,  batch step: 147, loss: 12.32569694519043\n",
      "epoch: 109,  batch step: 148, loss: 3.079388380050659\n",
      "epoch: 109,  batch step: 149, loss: 4.0975446701049805\n",
      "epoch: 109,  batch step: 150, loss: 3.206183433532715\n",
      "epoch: 109,  batch step: 151, loss: 36.03987121582031\n",
      "epoch: 109,  batch step: 152, loss: 7.061094284057617\n",
      "epoch: 109,  batch step: 153, loss: 1.6774649620056152\n",
      "epoch: 109,  batch step: 154, loss: 1.635632038116455\n",
      "epoch: 109,  batch step: 155, loss: 2.8635730743408203\n",
      "epoch: 109,  batch step: 156, loss: 2.3667092323303223\n",
      "epoch: 109,  batch step: 157, loss: 3.2881431579589844\n",
      "epoch: 109,  batch step: 158, loss: 1.6665661334991455\n",
      "epoch: 109,  batch step: 159, loss: 28.948949813842773\n",
      "epoch: 109,  batch step: 160, loss: 12.395374298095703\n",
      "epoch: 109,  batch step: 161, loss: 1.9254834651947021\n",
      "epoch: 109,  batch step: 162, loss: 2.198563814163208\n",
      "epoch: 109,  batch step: 163, loss: 2.589923858642578\n",
      "epoch: 109,  batch step: 164, loss: 50.01919174194336\n",
      "epoch: 109,  batch step: 165, loss: 9.553582191467285\n",
      "epoch: 109,  batch step: 166, loss: 2.02762508392334\n",
      "epoch: 109,  batch step: 167, loss: 2.174900770187378\n",
      "epoch: 109,  batch step: 168, loss: 19.67462158203125\n",
      "epoch: 109,  batch step: 169, loss: 1.565164566040039\n",
      "epoch: 109,  batch step: 170, loss: 10.153341293334961\n",
      "epoch: 109,  batch step: 171, loss: 43.22895812988281\n",
      "epoch: 109,  batch step: 172, loss: 17.53301429748535\n",
      "epoch: 109,  batch step: 173, loss: 3.562182903289795\n",
      "epoch: 109,  batch step: 174, loss: 13.639601707458496\n",
      "epoch: 109,  batch step: 175, loss: 142.3490447998047\n",
      "epoch: 109,  batch step: 176, loss: 11.643172264099121\n",
      "epoch: 109,  batch step: 177, loss: 2.1356418132781982\n",
      "epoch: 109,  batch step: 178, loss: 1.6816362142562866\n",
      "epoch: 109,  batch step: 179, loss: 2.6423537731170654\n",
      "epoch: 109,  batch step: 180, loss: 29.174732208251953\n",
      "epoch: 109,  batch step: 181, loss: 13.246889114379883\n",
      "epoch: 109,  batch step: 182, loss: 20.95199203491211\n",
      "epoch: 109,  batch step: 183, loss: 2.710090160369873\n",
      "epoch: 109,  batch step: 184, loss: 1.5384458303451538\n",
      "epoch: 109,  batch step: 185, loss: 1.786247730255127\n",
      "epoch: 109,  batch step: 186, loss: 3.7148241996765137\n",
      "epoch: 109,  batch step: 187, loss: 3.4652934074401855\n",
      "epoch: 109,  batch step: 188, loss: 2.928154706954956\n",
      "epoch: 109,  batch step: 189, loss: 8.089452743530273\n",
      "epoch: 109,  batch step: 190, loss: 1.8542226552963257\n",
      "epoch: 109,  batch step: 191, loss: 2.132690191268921\n",
      "epoch: 109,  batch step: 192, loss: 1.7639482021331787\n",
      "epoch: 109,  batch step: 193, loss: 7.879961967468262\n",
      "epoch: 109,  batch step: 194, loss: 18.97191047668457\n",
      "epoch: 109,  batch step: 195, loss: 2.4506444931030273\n",
      "epoch: 109,  batch step: 196, loss: 19.93465805053711\n",
      "epoch: 109,  batch step: 197, loss: 3.843493938446045\n",
      "epoch: 109,  batch step: 198, loss: 2.9048497676849365\n",
      "epoch: 109,  batch step: 199, loss: 54.304931640625\n",
      "epoch: 109,  batch step: 200, loss: 1.967458963394165\n",
      "epoch: 109,  batch step: 201, loss: 1.722139596939087\n",
      "epoch: 109,  batch step: 202, loss: 1.9002633094787598\n",
      "epoch: 109,  batch step: 203, loss: 7.336066722869873\n",
      "epoch: 109,  batch step: 204, loss: 44.6697998046875\n",
      "epoch: 109,  batch step: 205, loss: 2.6715192794799805\n",
      "epoch: 109,  batch step: 206, loss: 5.545862197875977\n",
      "epoch: 109,  batch step: 207, loss: 2.7428364753723145\n",
      "epoch: 109,  batch step: 208, loss: 28.764366149902344\n",
      "epoch: 109,  batch step: 209, loss: 8.99057674407959\n",
      "epoch: 109,  batch step: 210, loss: 3.671868085861206\n",
      "epoch: 109,  batch step: 211, loss: 18.44147491455078\n",
      "epoch: 109,  batch step: 212, loss: 10.035507202148438\n",
      "epoch: 109,  batch step: 213, loss: 4.008416652679443\n",
      "epoch: 109,  batch step: 214, loss: 9.570133209228516\n",
      "epoch: 109,  batch step: 215, loss: 3.156881332397461\n",
      "epoch: 109,  batch step: 216, loss: 49.96221160888672\n",
      "epoch: 109,  batch step: 217, loss: 6.7121195793151855\n",
      "epoch: 109,  batch step: 218, loss: 1.9390885829925537\n",
      "epoch: 109,  batch step: 219, loss: 2.2531774044036865\n",
      "epoch: 109,  batch step: 220, loss: 10.79249095916748\n",
      "epoch: 109,  batch step: 221, loss: 1.6848864555358887\n",
      "epoch: 109,  batch step: 222, loss: 2.014284133911133\n",
      "epoch: 109,  batch step: 223, loss: 10.072266578674316\n",
      "epoch: 109,  batch step: 224, loss: 2.181706190109253\n",
      "epoch: 109,  batch step: 225, loss: 2.992354393005371\n",
      "epoch: 109,  batch step: 226, loss: 1.4513671398162842\n",
      "epoch: 109,  batch step: 227, loss: 1.5065877437591553\n",
      "epoch: 109,  batch step: 228, loss: 1.9882714748382568\n",
      "epoch: 109,  batch step: 229, loss: 13.586874008178711\n",
      "epoch: 109,  batch step: 230, loss: 14.398211479187012\n",
      "epoch: 109,  batch step: 231, loss: 1.4913400411605835\n",
      "epoch: 109,  batch step: 232, loss: 2.1998019218444824\n",
      "epoch: 109,  batch step: 233, loss: 2.5435471534729004\n",
      "epoch: 109,  batch step: 234, loss: 13.077654838562012\n",
      "epoch: 109,  batch step: 235, loss: 1.921057105064392\n",
      "epoch: 109,  batch step: 236, loss: 2.6448259353637695\n",
      "epoch: 109,  batch step: 237, loss: 1.618183970451355\n",
      "epoch: 109,  batch step: 238, loss: 92.38619995117188\n",
      "epoch: 109,  batch step: 239, loss: 17.187864303588867\n",
      "epoch: 109,  batch step: 240, loss: 27.482952117919922\n",
      "epoch: 109,  batch step: 241, loss: 14.560434341430664\n",
      "epoch: 109,  batch step: 242, loss: 2.5914342403411865\n",
      "epoch: 109,  batch step: 243, loss: 1.7909148931503296\n",
      "epoch: 109,  batch step: 244, loss: 2.1682727336883545\n",
      "epoch: 109,  batch step: 245, loss: 1.76409912109375\n",
      "epoch: 109,  batch step: 246, loss: 23.428808212280273\n",
      "epoch: 109,  batch step: 247, loss: 7.546849727630615\n",
      "epoch: 109,  batch step: 248, loss: 1.2321823835372925\n",
      "epoch: 109,  batch step: 249, loss: 11.111679077148438\n",
      "epoch: 109,  batch step: 250, loss: 2.5762319564819336\n",
      "epoch: 109,  batch step: 251, loss: 7.018988132476807\n",
      "finished saving checkpoints\n",
      "validation error epoch  109:    tensor(68.2186, device='cuda:0')\n",
      "316\n",
      "epoch: 110,  batch step: 0, loss: 19.02509117126465\n",
      "epoch: 110,  batch step: 1, loss: 5.335831642150879\n",
      "epoch: 110,  batch step: 2, loss: 9.3951416015625\n",
      "epoch: 110,  batch step: 3, loss: 3.343513011932373\n",
      "epoch: 110,  batch step: 4, loss: 8.573358535766602\n",
      "epoch: 110,  batch step: 5, loss: 1.5177793502807617\n",
      "epoch: 110,  batch step: 6, loss: 4.106398582458496\n",
      "epoch: 110,  batch step: 7, loss: 2.711737632751465\n",
      "epoch: 110,  batch step: 8, loss: 17.588321685791016\n",
      "epoch: 110,  batch step: 9, loss: 11.515666007995605\n",
      "epoch: 110,  batch step: 10, loss: 5.6592607498168945\n",
      "epoch: 110,  batch step: 11, loss: 7.139299392700195\n",
      "epoch: 110,  batch step: 12, loss: 1.9712989330291748\n",
      "epoch: 110,  batch step: 13, loss: 2.1362035274505615\n",
      "epoch: 110,  batch step: 14, loss: 2.166562557220459\n",
      "epoch: 110,  batch step: 15, loss: 2.0694727897644043\n",
      "epoch: 110,  batch step: 16, loss: 23.46228790283203\n",
      "epoch: 110,  batch step: 17, loss: 12.78907299041748\n",
      "epoch: 110,  batch step: 18, loss: 1.6452443599700928\n",
      "epoch: 110,  batch step: 19, loss: 2.2742106914520264\n",
      "epoch: 110,  batch step: 20, loss: 16.82508659362793\n",
      "epoch: 110,  batch step: 21, loss: 1.8399229049682617\n",
      "epoch: 110,  batch step: 22, loss: 21.31410789489746\n",
      "epoch: 110,  batch step: 23, loss: 2.8827552795410156\n",
      "epoch: 110,  batch step: 24, loss: 7.0616302490234375\n",
      "epoch: 110,  batch step: 25, loss: 14.007299423217773\n",
      "epoch: 110,  batch step: 26, loss: 2.2279365062713623\n",
      "epoch: 110,  batch step: 27, loss: 2.6196718215942383\n",
      "epoch: 110,  batch step: 28, loss: 1.922163486480713\n",
      "epoch: 110,  batch step: 29, loss: 10.588208198547363\n",
      "epoch: 110,  batch step: 30, loss: 4.055387020111084\n",
      "epoch: 110,  batch step: 31, loss: 2.6253631114959717\n",
      "epoch: 110,  batch step: 32, loss: 2.185289144515991\n",
      "epoch: 110,  batch step: 33, loss: 82.94235229492188\n",
      "epoch: 110,  batch step: 34, loss: 4.013243198394775\n",
      "epoch: 110,  batch step: 35, loss: 13.323843002319336\n",
      "epoch: 110,  batch step: 36, loss: 1.7655061483383179\n",
      "epoch: 110,  batch step: 37, loss: 30.790828704833984\n",
      "epoch: 110,  batch step: 38, loss: 5.802583694458008\n",
      "epoch: 110,  batch step: 39, loss: 8.34292221069336\n",
      "epoch: 110,  batch step: 40, loss: 30.95814323425293\n",
      "epoch: 110,  batch step: 41, loss: 1.373331069946289\n",
      "epoch: 110,  batch step: 42, loss: 63.629005432128906\n",
      "epoch: 110,  batch step: 43, loss: 2.382923126220703\n",
      "epoch: 110,  batch step: 44, loss: 2.0692074298858643\n",
      "epoch: 110,  batch step: 45, loss: 11.825797080993652\n",
      "epoch: 110,  batch step: 46, loss: 26.012451171875\n",
      "epoch: 110,  batch step: 47, loss: 2.298238754272461\n",
      "epoch: 110,  batch step: 48, loss: 3.5953426361083984\n",
      "epoch: 110,  batch step: 49, loss: 2.965137243270874\n",
      "epoch: 110,  batch step: 50, loss: 19.894460678100586\n",
      "epoch: 110,  batch step: 51, loss: 2.3053624629974365\n",
      "epoch: 110,  batch step: 52, loss: 2.8355712890625\n",
      "epoch: 110,  batch step: 53, loss: 77.75020599365234\n",
      "epoch: 110,  batch step: 54, loss: 11.580425262451172\n",
      "epoch: 110,  batch step: 55, loss: 12.832260131835938\n",
      "epoch: 110,  batch step: 56, loss: 1.5178523063659668\n",
      "epoch: 110,  batch step: 57, loss: 6.656810760498047\n",
      "epoch: 110,  batch step: 58, loss: 9.41142463684082\n",
      "epoch: 110,  batch step: 59, loss: 1.509648323059082\n",
      "epoch: 110,  batch step: 60, loss: 14.715307235717773\n",
      "epoch: 110,  batch step: 61, loss: 20.83008575439453\n",
      "epoch: 110,  batch step: 62, loss: 2.0927038192749023\n",
      "epoch: 110,  batch step: 63, loss: 2.341031789779663\n",
      "epoch: 110,  batch step: 64, loss: 9.66374683380127\n",
      "epoch: 110,  batch step: 65, loss: 2.0694942474365234\n",
      "epoch: 110,  batch step: 66, loss: 2.6019535064697266\n",
      "epoch: 110,  batch step: 67, loss: 34.49261474609375\n",
      "epoch: 110,  batch step: 68, loss: 7.541026592254639\n",
      "epoch: 110,  batch step: 69, loss: 1.319626808166504\n",
      "epoch: 110,  batch step: 70, loss: 1.5269378423690796\n",
      "epoch: 110,  batch step: 71, loss: 3.0463390350341797\n",
      "epoch: 110,  batch step: 72, loss: 8.242509841918945\n",
      "epoch: 110,  batch step: 73, loss: 2.695891857147217\n",
      "epoch: 110,  batch step: 74, loss: 8.844076156616211\n",
      "epoch: 110,  batch step: 75, loss: 48.59516525268555\n",
      "epoch: 110,  batch step: 76, loss: 7.937407970428467\n",
      "epoch: 110,  batch step: 77, loss: 4.967839241027832\n",
      "epoch: 110,  batch step: 78, loss: 7.882876873016357\n",
      "epoch: 110,  batch step: 79, loss: 9.089324951171875\n",
      "epoch: 110,  batch step: 80, loss: 16.041946411132812\n",
      "epoch: 110,  batch step: 81, loss: 11.897322654724121\n",
      "epoch: 110,  batch step: 82, loss: 14.974815368652344\n",
      "epoch: 110,  batch step: 83, loss: 6.423464775085449\n",
      "epoch: 110,  batch step: 84, loss: 10.493949890136719\n",
      "epoch: 110,  batch step: 85, loss: 15.005559921264648\n",
      "epoch: 110,  batch step: 86, loss: 3.847776412963867\n",
      "epoch: 110,  batch step: 87, loss: 1.8565855026245117\n",
      "epoch: 110,  batch step: 88, loss: 1.5721052885055542\n",
      "epoch: 110,  batch step: 89, loss: 11.541011810302734\n",
      "epoch: 110,  batch step: 90, loss: 2.3133373260498047\n",
      "epoch: 110,  batch step: 91, loss: 31.946884155273438\n",
      "epoch: 110,  batch step: 92, loss: 2.367792844772339\n",
      "epoch: 110,  batch step: 93, loss: 1.2788629531860352\n",
      "epoch: 110,  batch step: 94, loss: 1.799308180809021\n",
      "epoch: 110,  batch step: 95, loss: 28.388486862182617\n",
      "epoch: 110,  batch step: 96, loss: 1.5389251708984375\n",
      "epoch: 110,  batch step: 97, loss: 14.186308860778809\n",
      "epoch: 110,  batch step: 98, loss: 1.6929558515548706\n",
      "epoch: 110,  batch step: 99, loss: 70.76434326171875\n",
      "epoch: 110,  batch step: 100, loss: 9.089553833007812\n",
      "epoch: 110,  batch step: 101, loss: 6.623564720153809\n",
      "epoch: 110,  batch step: 102, loss: 4.638914108276367\n",
      "epoch: 110,  batch step: 103, loss: 15.247871398925781\n",
      "epoch: 110,  batch step: 104, loss: 13.132163047790527\n",
      "epoch: 110,  batch step: 105, loss: 3.442866802215576\n",
      "epoch: 110,  batch step: 106, loss: 9.849357604980469\n",
      "epoch: 110,  batch step: 107, loss: 2.016968011856079\n",
      "epoch: 110,  batch step: 108, loss: 30.062583923339844\n",
      "epoch: 110,  batch step: 109, loss: 15.269292831420898\n",
      "epoch: 110,  batch step: 110, loss: 10.542718887329102\n",
      "epoch: 110,  batch step: 111, loss: 2.5351130962371826\n",
      "epoch: 110,  batch step: 112, loss: 12.530941009521484\n",
      "epoch: 110,  batch step: 113, loss: 2.060272693634033\n",
      "epoch: 110,  batch step: 114, loss: 1.9184186458587646\n",
      "epoch: 110,  batch step: 115, loss: 35.01995849609375\n",
      "epoch: 110,  batch step: 116, loss: 1.5704187154769897\n",
      "epoch: 110,  batch step: 117, loss: 2.951498031616211\n",
      "epoch: 110,  batch step: 118, loss: 2.3790082931518555\n",
      "epoch: 110,  batch step: 119, loss: 15.034759521484375\n",
      "epoch: 110,  batch step: 120, loss: 10.426965713500977\n",
      "epoch: 110,  batch step: 121, loss: 2.213925838470459\n",
      "epoch: 110,  batch step: 122, loss: 10.93708324432373\n",
      "epoch: 110,  batch step: 123, loss: 1.72537362575531\n",
      "epoch: 110,  batch step: 124, loss: 28.261754989624023\n",
      "epoch: 110,  batch step: 125, loss: 1.3208577632904053\n",
      "epoch: 110,  batch step: 126, loss: 2.260592460632324\n",
      "epoch: 110,  batch step: 127, loss: 7.3666839599609375\n",
      "epoch: 110,  batch step: 128, loss: 2.135472059249878\n",
      "epoch: 110,  batch step: 129, loss: 8.501720428466797\n",
      "epoch: 110,  batch step: 130, loss: 21.229816436767578\n",
      "epoch: 110,  batch step: 131, loss: 8.84726333618164\n",
      "epoch: 110,  batch step: 132, loss: 6.56241512298584\n",
      "epoch: 110,  batch step: 133, loss: 1.9146533012390137\n",
      "epoch: 110,  batch step: 134, loss: 2.128751516342163\n",
      "epoch: 110,  batch step: 135, loss: 1.767455816268921\n",
      "epoch: 110,  batch step: 136, loss: 6.356651306152344\n",
      "epoch: 110,  batch step: 137, loss: 139.44154357910156\n",
      "epoch: 110,  batch step: 138, loss: 48.32778549194336\n",
      "epoch: 110,  batch step: 139, loss: 41.55634689331055\n",
      "epoch: 110,  batch step: 140, loss: 2.166051149368286\n",
      "epoch: 110,  batch step: 141, loss: 2.010942220687866\n",
      "epoch: 110,  batch step: 142, loss: 2.022068977355957\n",
      "epoch: 110,  batch step: 143, loss: 2.6740875244140625\n",
      "epoch: 110,  batch step: 144, loss: 6.8528056144714355\n",
      "epoch: 110,  batch step: 145, loss: 7.456299781799316\n",
      "epoch: 110,  batch step: 146, loss: 13.324392318725586\n",
      "epoch: 110,  batch step: 147, loss: 11.568095207214355\n",
      "epoch: 110,  batch step: 148, loss: 11.186807632446289\n",
      "epoch: 110,  batch step: 149, loss: 6.644434928894043\n",
      "epoch: 110,  batch step: 150, loss: 2.6660103797912598\n",
      "epoch: 110,  batch step: 151, loss: 14.745638847351074\n",
      "epoch: 110,  batch step: 152, loss: 3.182891845703125\n",
      "epoch: 110,  batch step: 153, loss: 48.15700912475586\n",
      "epoch: 110,  batch step: 154, loss: 10.046564102172852\n",
      "epoch: 110,  batch step: 155, loss: 4.437493324279785\n",
      "epoch: 110,  batch step: 156, loss: 8.22069263458252\n",
      "epoch: 110,  batch step: 157, loss: 4.41105842590332\n",
      "epoch: 110,  batch step: 158, loss: 1.2870807647705078\n",
      "epoch: 110,  batch step: 159, loss: 1.7479212284088135\n",
      "epoch: 110,  batch step: 160, loss: 2.0820062160491943\n",
      "epoch: 110,  batch step: 161, loss: 1.6929523944854736\n",
      "epoch: 110,  batch step: 162, loss: 13.453352928161621\n",
      "epoch: 110,  batch step: 163, loss: 2.361689329147339\n",
      "epoch: 110,  batch step: 164, loss: 51.5078239440918\n",
      "epoch: 110,  batch step: 165, loss: 8.06559944152832\n",
      "epoch: 110,  batch step: 166, loss: 1.8258647918701172\n",
      "epoch: 110,  batch step: 167, loss: 9.825164794921875\n",
      "epoch: 110,  batch step: 168, loss: 2.741178512573242\n",
      "epoch: 110,  batch step: 169, loss: 2.286670446395874\n",
      "epoch: 110,  batch step: 170, loss: 14.866728782653809\n",
      "epoch: 110,  batch step: 171, loss: 6.610744476318359\n",
      "epoch: 110,  batch step: 172, loss: 1.9130964279174805\n",
      "epoch: 110,  batch step: 173, loss: 1.6741021871566772\n",
      "epoch: 110,  batch step: 174, loss: 2.0213139057159424\n",
      "epoch: 110,  batch step: 175, loss: 2.7498016357421875\n",
      "epoch: 110,  batch step: 176, loss: 2.2770602703094482\n",
      "epoch: 110,  batch step: 177, loss: 12.260961532592773\n",
      "epoch: 110,  batch step: 178, loss: 8.978519439697266\n",
      "epoch: 110,  batch step: 179, loss: 4.897241592407227\n",
      "epoch: 110,  batch step: 180, loss: 1.5238559246063232\n",
      "epoch: 110,  batch step: 181, loss: 14.49963665008545\n",
      "epoch: 110,  batch step: 182, loss: 2.6651787757873535\n",
      "epoch: 110,  batch step: 183, loss: 13.339668273925781\n",
      "epoch: 110,  batch step: 184, loss: 4.922514915466309\n",
      "epoch: 110,  batch step: 185, loss: 12.357246398925781\n",
      "epoch: 110,  batch step: 186, loss: 9.312137603759766\n",
      "epoch: 110,  batch step: 187, loss: 10.679110527038574\n",
      "epoch: 110,  batch step: 188, loss: 34.536415100097656\n",
      "epoch: 110,  batch step: 189, loss: 2.4012680053710938\n",
      "epoch: 110,  batch step: 190, loss: 2.4244375228881836\n",
      "epoch: 110,  batch step: 191, loss: 2.900642156600952\n",
      "epoch: 110,  batch step: 192, loss: 8.178489685058594\n",
      "epoch: 110,  batch step: 193, loss: 10.872286796569824\n",
      "epoch: 110,  batch step: 194, loss: 4.792965888977051\n",
      "epoch: 110,  batch step: 195, loss: 8.414865493774414\n",
      "epoch: 110,  batch step: 196, loss: 2.681149959564209\n",
      "epoch: 110,  batch step: 197, loss: 4.760902404785156\n",
      "epoch: 110,  batch step: 198, loss: 3.5380687713623047\n",
      "epoch: 110,  batch step: 199, loss: 30.166486740112305\n",
      "epoch: 110,  batch step: 200, loss: 3.6658427715301514\n",
      "epoch: 110,  batch step: 201, loss: 9.275763511657715\n",
      "epoch: 110,  batch step: 202, loss: 4.971911430358887\n",
      "epoch: 110,  batch step: 203, loss: 7.5447187423706055\n",
      "epoch: 110,  batch step: 204, loss: 2.708709239959717\n",
      "epoch: 110,  batch step: 205, loss: 8.361898422241211\n",
      "epoch: 110,  batch step: 206, loss: 5.091745376586914\n",
      "epoch: 110,  batch step: 207, loss: 2.4295177459716797\n",
      "epoch: 110,  batch step: 208, loss: 17.10455322265625\n",
      "epoch: 110,  batch step: 209, loss: 2.2891006469726562\n",
      "epoch: 110,  batch step: 210, loss: 2.753110647201538\n",
      "epoch: 110,  batch step: 211, loss: 3.1525306701660156\n",
      "epoch: 110,  batch step: 212, loss: 4.247562885284424\n",
      "epoch: 110,  batch step: 213, loss: 3.007512092590332\n",
      "epoch: 110,  batch step: 214, loss: 2.4189274311065674\n",
      "epoch: 110,  batch step: 215, loss: 1.8169649839401245\n",
      "epoch: 110,  batch step: 216, loss: 7.487758159637451\n",
      "epoch: 110,  batch step: 217, loss: 8.038782119750977\n",
      "epoch: 110,  batch step: 218, loss: 2.769373893737793\n",
      "epoch: 110,  batch step: 219, loss: 2.471865177154541\n",
      "epoch: 110,  batch step: 220, loss: 12.067237854003906\n",
      "epoch: 110,  batch step: 221, loss: 27.907760620117188\n",
      "epoch: 110,  batch step: 222, loss: 2.540839910507202\n",
      "epoch: 110,  batch step: 223, loss: 6.0400495529174805\n",
      "epoch: 110,  batch step: 224, loss: 1.9801747798919678\n",
      "epoch: 110,  batch step: 225, loss: 10.322685241699219\n",
      "epoch: 110,  batch step: 226, loss: 3.9003190994262695\n",
      "epoch: 110,  batch step: 227, loss: 1.8515428304672241\n",
      "epoch: 110,  batch step: 228, loss: 10.167953491210938\n",
      "epoch: 110,  batch step: 229, loss: 2.1529951095581055\n",
      "epoch: 110,  batch step: 230, loss: 8.327513694763184\n",
      "epoch: 110,  batch step: 231, loss: 3.691138744354248\n",
      "epoch: 110,  batch step: 232, loss: 2.3190455436706543\n",
      "epoch: 110,  batch step: 233, loss: 8.608027458190918\n",
      "epoch: 110,  batch step: 234, loss: 1.4361512660980225\n",
      "epoch: 110,  batch step: 235, loss: 1.4284952878952026\n",
      "epoch: 110,  batch step: 236, loss: 57.0399284362793\n",
      "epoch: 110,  batch step: 237, loss: 2.396348476409912\n",
      "epoch: 110,  batch step: 238, loss: 2.4073007106781006\n",
      "epoch: 110,  batch step: 239, loss: 16.422683715820312\n",
      "epoch: 110,  batch step: 240, loss: 4.503817081451416\n",
      "epoch: 110,  batch step: 241, loss: 18.15057373046875\n",
      "epoch: 110,  batch step: 242, loss: 74.18059539794922\n",
      "epoch: 110,  batch step: 243, loss: 9.232343673706055\n",
      "epoch: 110,  batch step: 244, loss: 2.440232753753662\n",
      "epoch: 110,  batch step: 245, loss: 9.634256362915039\n",
      "epoch: 110,  batch step: 246, loss: 1.4568724632263184\n",
      "epoch: 110,  batch step: 247, loss: 1.666969656944275\n",
      "epoch: 110,  batch step: 248, loss: 1.4084327220916748\n",
      "epoch: 110,  batch step: 249, loss: 37.72758102416992\n",
      "epoch: 110,  batch step: 250, loss: 15.118094444274902\n",
      "epoch: 110,  batch step: 251, loss: 110.2255630493164\n",
      "validation error epoch  110:    tensor(67.3940, device='cuda:0')\n",
      "316\n",
      "epoch: 111,  batch step: 0, loss: 6.628993988037109\n",
      "epoch: 111,  batch step: 1, loss: 6.313467979431152\n",
      "epoch: 111,  batch step: 2, loss: 6.1407880783081055\n",
      "epoch: 111,  batch step: 3, loss: 35.899593353271484\n",
      "epoch: 111,  batch step: 4, loss: 6.60528564453125\n",
      "epoch: 111,  batch step: 5, loss: 7.981747627258301\n",
      "epoch: 111,  batch step: 6, loss: 3.600623607635498\n",
      "epoch: 111,  batch step: 7, loss: 2.598594903945923\n",
      "epoch: 111,  batch step: 8, loss: 3.1759347915649414\n",
      "epoch: 111,  batch step: 9, loss: 11.18012809753418\n",
      "epoch: 111,  batch step: 10, loss: 3.5592899322509766\n",
      "epoch: 111,  batch step: 11, loss: 10.477887153625488\n",
      "epoch: 111,  batch step: 12, loss: 5.472481727600098\n",
      "epoch: 111,  batch step: 13, loss: 45.801204681396484\n",
      "epoch: 111,  batch step: 14, loss: 9.956714630126953\n",
      "epoch: 111,  batch step: 15, loss: 3.295379400253296\n",
      "epoch: 111,  batch step: 16, loss: 22.459264755249023\n",
      "epoch: 111,  batch step: 17, loss: 7.815080642700195\n",
      "epoch: 111,  batch step: 18, loss: 27.248594284057617\n",
      "epoch: 111,  batch step: 19, loss: 7.264834403991699\n",
      "epoch: 111,  batch step: 20, loss: 3.1767711639404297\n",
      "epoch: 111,  batch step: 21, loss: 13.978075981140137\n",
      "epoch: 111,  batch step: 22, loss: 25.92299461364746\n",
      "epoch: 111,  batch step: 23, loss: 3.92299222946167\n",
      "epoch: 111,  batch step: 24, loss: 25.93560791015625\n",
      "epoch: 111,  batch step: 25, loss: 23.242488861083984\n",
      "epoch: 111,  batch step: 26, loss: 3.165985107421875\n",
      "epoch: 111,  batch step: 27, loss: 7.4598846435546875\n",
      "epoch: 111,  batch step: 28, loss: 4.712803840637207\n",
      "epoch: 111,  batch step: 29, loss: 6.976876258850098\n",
      "epoch: 111,  batch step: 30, loss: 6.629539489746094\n",
      "epoch: 111,  batch step: 31, loss: 16.219709396362305\n",
      "epoch: 111,  batch step: 32, loss: 2.171365261077881\n",
      "epoch: 111,  batch step: 33, loss: 50.586181640625\n",
      "epoch: 111,  batch step: 34, loss: 24.738849639892578\n",
      "epoch: 111,  batch step: 35, loss: 1.9908194541931152\n",
      "epoch: 111,  batch step: 36, loss: 3.5378966331481934\n",
      "epoch: 111,  batch step: 37, loss: 24.14552879333496\n",
      "epoch: 111,  batch step: 38, loss: 3.0066208839416504\n",
      "epoch: 111,  batch step: 39, loss: 5.72351598739624\n",
      "epoch: 111,  batch step: 40, loss: 2.7073915004730225\n",
      "epoch: 111,  batch step: 41, loss: 2.204853057861328\n",
      "epoch: 111,  batch step: 42, loss: 27.59145736694336\n",
      "epoch: 111,  batch step: 43, loss: 3.4620676040649414\n",
      "epoch: 111,  batch step: 44, loss: 19.72496795654297\n",
      "epoch: 111,  batch step: 45, loss: 5.2345805168151855\n",
      "epoch: 111,  batch step: 46, loss: 5.3814802169799805\n",
      "epoch: 111,  batch step: 47, loss: 1.7911962270736694\n",
      "epoch: 111,  batch step: 48, loss: 2.782519817352295\n",
      "epoch: 111,  batch step: 49, loss: 2.170910596847534\n",
      "epoch: 111,  batch step: 50, loss: 10.03061580657959\n",
      "epoch: 111,  batch step: 51, loss: 2.0921764373779297\n",
      "epoch: 111,  batch step: 52, loss: 14.027689933776855\n",
      "epoch: 111,  batch step: 53, loss: 3.9165573120117188\n",
      "epoch: 111,  batch step: 54, loss: 9.666486740112305\n",
      "epoch: 111,  batch step: 55, loss: 9.656784057617188\n",
      "epoch: 111,  batch step: 56, loss: 2.174229383468628\n",
      "epoch: 111,  batch step: 57, loss: 24.84282684326172\n",
      "epoch: 111,  batch step: 58, loss: 1.7356853485107422\n",
      "epoch: 111,  batch step: 59, loss: 52.509971618652344\n",
      "epoch: 111,  batch step: 60, loss: 13.119293212890625\n",
      "epoch: 111,  batch step: 61, loss: 2.2285943031311035\n",
      "epoch: 111,  batch step: 62, loss: 3.2507972717285156\n",
      "epoch: 111,  batch step: 63, loss: 2.4890732765197754\n",
      "epoch: 111,  batch step: 64, loss: 6.974924087524414\n",
      "epoch: 111,  batch step: 65, loss: 1.4857556819915771\n",
      "epoch: 111,  batch step: 66, loss: 2.4256083965301514\n",
      "epoch: 111,  batch step: 67, loss: 11.903182983398438\n",
      "epoch: 111,  batch step: 68, loss: 2.21718168258667\n",
      "epoch: 111,  batch step: 69, loss: 9.78474235534668\n",
      "epoch: 111,  batch step: 70, loss: 2.3234832286834717\n",
      "epoch: 111,  batch step: 71, loss: 9.316583633422852\n",
      "epoch: 111,  batch step: 72, loss: 67.59786224365234\n",
      "epoch: 111,  batch step: 73, loss: 6.727729797363281\n",
      "epoch: 111,  batch step: 74, loss: 2.6960370540618896\n",
      "epoch: 111,  batch step: 75, loss: 2.01406192779541\n",
      "epoch: 111,  batch step: 76, loss: 11.342536926269531\n",
      "epoch: 111,  batch step: 77, loss: 2.4450197219848633\n",
      "epoch: 111,  batch step: 78, loss: 2.0881404876708984\n",
      "epoch: 111,  batch step: 79, loss: 11.073469161987305\n",
      "epoch: 111,  batch step: 80, loss: 2.2334799766540527\n",
      "epoch: 111,  batch step: 81, loss: 9.161149978637695\n",
      "epoch: 111,  batch step: 82, loss: 13.95500373840332\n",
      "epoch: 111,  batch step: 83, loss: 3.7543838024139404\n",
      "epoch: 111,  batch step: 84, loss: 1.7756686210632324\n",
      "epoch: 111,  batch step: 85, loss: 32.83161163330078\n",
      "epoch: 111,  batch step: 86, loss: 14.562202453613281\n",
      "epoch: 111,  batch step: 87, loss: 49.65105438232422\n",
      "epoch: 111,  batch step: 88, loss: 5.618806838989258\n",
      "epoch: 111,  batch step: 89, loss: 2.4272851943969727\n",
      "epoch: 111,  batch step: 90, loss: 11.191095352172852\n",
      "epoch: 111,  batch step: 91, loss: 10.11008071899414\n",
      "epoch: 111,  batch step: 92, loss: 7.137401580810547\n",
      "epoch: 111,  batch step: 93, loss: 13.248435020446777\n",
      "epoch: 111,  batch step: 94, loss: 2.0836410522460938\n",
      "epoch: 111,  batch step: 95, loss: 1.9676960706710815\n",
      "epoch: 111,  batch step: 96, loss: 8.109358787536621\n",
      "epoch: 111,  batch step: 97, loss: 24.465301513671875\n",
      "epoch: 111,  batch step: 98, loss: 2.0564169883728027\n",
      "epoch: 111,  batch step: 99, loss: 1.7998905181884766\n",
      "epoch: 111,  batch step: 100, loss: 2.9885497093200684\n",
      "epoch: 111,  batch step: 101, loss: 1.6673638820648193\n",
      "epoch: 111,  batch step: 102, loss: 3.0724635124206543\n",
      "epoch: 111,  batch step: 103, loss: 3.0541586875915527\n",
      "epoch: 111,  batch step: 104, loss: 17.141193389892578\n",
      "epoch: 111,  batch step: 105, loss: 8.451711654663086\n",
      "epoch: 111,  batch step: 106, loss: 1.9714239835739136\n",
      "epoch: 111,  batch step: 107, loss: 1.4700253009796143\n",
      "epoch: 111,  batch step: 108, loss: 10.92835521697998\n",
      "epoch: 111,  batch step: 109, loss: 2.384747266769409\n",
      "epoch: 111,  batch step: 110, loss: 14.275638580322266\n",
      "epoch: 111,  batch step: 111, loss: 14.910200119018555\n",
      "epoch: 111,  batch step: 112, loss: 1.8211209774017334\n",
      "epoch: 111,  batch step: 113, loss: 3.130723476409912\n",
      "epoch: 111,  batch step: 114, loss: 2.011366367340088\n",
      "epoch: 111,  batch step: 115, loss: 8.730709075927734\n",
      "epoch: 111,  batch step: 116, loss: 1.811672568321228\n",
      "epoch: 111,  batch step: 117, loss: 6.939013481140137\n",
      "epoch: 111,  batch step: 118, loss: 1.6235483884811401\n",
      "epoch: 111,  batch step: 119, loss: 13.595335960388184\n",
      "epoch: 111,  batch step: 120, loss: 1.9398630857467651\n",
      "epoch: 111,  batch step: 121, loss: 16.19184684753418\n",
      "epoch: 111,  batch step: 122, loss: 2.0633723735809326\n",
      "epoch: 111,  batch step: 123, loss: 4.132421493530273\n",
      "epoch: 111,  batch step: 124, loss: 9.904566764831543\n",
      "epoch: 111,  batch step: 125, loss: 2.9895529747009277\n",
      "epoch: 111,  batch step: 126, loss: 1.9014959335327148\n",
      "epoch: 111,  batch step: 127, loss: 2.267582893371582\n",
      "epoch: 111,  batch step: 128, loss: 9.14142894744873\n",
      "epoch: 111,  batch step: 129, loss: 18.850479125976562\n",
      "epoch: 111,  batch step: 130, loss: 3.1372532844543457\n",
      "epoch: 111,  batch step: 131, loss: 8.586138725280762\n",
      "epoch: 111,  batch step: 132, loss: 8.508386611938477\n",
      "epoch: 111,  batch step: 133, loss: 1.6370071172714233\n",
      "epoch: 111,  batch step: 134, loss: 3.1485695838928223\n",
      "epoch: 111,  batch step: 135, loss: 26.98499298095703\n",
      "epoch: 111,  batch step: 136, loss: 54.56574249267578\n",
      "epoch: 111,  batch step: 137, loss: 6.391520977020264\n",
      "epoch: 111,  batch step: 138, loss: 2.7121715545654297\n",
      "epoch: 111,  batch step: 139, loss: 3.0812740325927734\n",
      "epoch: 111,  batch step: 140, loss: 17.40313720703125\n",
      "epoch: 111,  batch step: 141, loss: 14.407198905944824\n",
      "epoch: 111,  batch step: 142, loss: 2.420971632003784\n",
      "epoch: 111,  batch step: 143, loss: 2.4895355701446533\n",
      "epoch: 111,  batch step: 144, loss: 3.16474986076355\n",
      "epoch: 111,  batch step: 145, loss: 3.5662641525268555\n",
      "epoch: 111,  batch step: 146, loss: 4.0302629470825195\n",
      "epoch: 111,  batch step: 147, loss: 2.3804471492767334\n",
      "epoch: 111,  batch step: 148, loss: 13.929380416870117\n",
      "epoch: 111,  batch step: 149, loss: 10.846185684204102\n",
      "epoch: 111,  batch step: 150, loss: 4.774502277374268\n",
      "epoch: 111,  batch step: 151, loss: 4.923520565032959\n",
      "epoch: 111,  batch step: 152, loss: 5.181966304779053\n",
      "epoch: 111,  batch step: 153, loss: 8.183844566345215\n",
      "epoch: 111,  batch step: 154, loss: 2.210904598236084\n",
      "epoch: 111,  batch step: 155, loss: 13.84687328338623\n",
      "epoch: 111,  batch step: 156, loss: 3.2725400924682617\n",
      "epoch: 111,  batch step: 157, loss: 7.205679893493652\n",
      "epoch: 111,  batch step: 158, loss: 16.868175506591797\n",
      "epoch: 111,  batch step: 159, loss: 3.50360369682312\n",
      "epoch: 111,  batch step: 160, loss: 2.1649274826049805\n",
      "epoch: 111,  batch step: 161, loss: 2.60217547416687\n",
      "epoch: 111,  batch step: 162, loss: 2.072810649871826\n",
      "epoch: 111,  batch step: 163, loss: 13.066447257995605\n",
      "epoch: 111,  batch step: 164, loss: 2.0792784690856934\n",
      "epoch: 111,  batch step: 165, loss: 7.463583946228027\n",
      "epoch: 111,  batch step: 166, loss: 40.57311248779297\n",
      "epoch: 111,  batch step: 167, loss: 2.5961668491363525\n",
      "epoch: 111,  batch step: 168, loss: 4.352385520935059\n",
      "epoch: 111,  batch step: 169, loss: 1.4614537954330444\n",
      "epoch: 111,  batch step: 170, loss: 15.453261375427246\n",
      "epoch: 111,  batch step: 171, loss: 9.202376365661621\n",
      "epoch: 111,  batch step: 172, loss: 15.003219604492188\n",
      "epoch: 111,  batch step: 173, loss: 43.12983322143555\n",
      "epoch: 111,  batch step: 174, loss: 12.561022758483887\n",
      "epoch: 111,  batch step: 175, loss: 8.190696716308594\n",
      "epoch: 111,  batch step: 176, loss: 8.571362495422363\n",
      "epoch: 111,  batch step: 177, loss: 14.757270812988281\n",
      "epoch: 111,  batch step: 178, loss: 28.890743255615234\n",
      "epoch: 111,  batch step: 179, loss: 20.643627166748047\n",
      "epoch: 111,  batch step: 180, loss: 86.40109252929688\n",
      "epoch: 111,  batch step: 181, loss: 2.465989112854004\n",
      "epoch: 111,  batch step: 182, loss: 10.635232925415039\n",
      "epoch: 111,  batch step: 183, loss: 4.11793327331543\n",
      "epoch: 111,  batch step: 184, loss: 11.596308708190918\n",
      "epoch: 111,  batch step: 185, loss: 2.9153850078582764\n",
      "epoch: 111,  batch step: 186, loss: 2.2045488357543945\n",
      "epoch: 111,  batch step: 187, loss: 15.404912948608398\n",
      "epoch: 111,  batch step: 188, loss: 2.379486322402954\n",
      "epoch: 111,  batch step: 189, loss: 2.9833970069885254\n",
      "epoch: 111,  batch step: 190, loss: 4.6486077308654785\n",
      "epoch: 111,  batch step: 191, loss: 30.887035369873047\n",
      "epoch: 111,  batch step: 192, loss: 8.124017715454102\n",
      "epoch: 111,  batch step: 193, loss: 9.783880233764648\n",
      "epoch: 111,  batch step: 194, loss: 3.585491895675659\n",
      "epoch: 111,  batch step: 195, loss: 30.532852172851562\n",
      "epoch: 111,  batch step: 196, loss: 5.016583442687988\n",
      "epoch: 111,  batch step: 197, loss: 1.176750659942627\n",
      "epoch: 111,  batch step: 198, loss: 9.59571361541748\n",
      "epoch: 111,  batch step: 199, loss: 130.91729736328125\n",
      "epoch: 111,  batch step: 200, loss: 4.060737609863281\n",
      "epoch: 111,  batch step: 201, loss: 9.766265869140625\n",
      "epoch: 111,  batch step: 202, loss: 5.801436901092529\n",
      "epoch: 111,  batch step: 203, loss: 24.7120418548584\n",
      "epoch: 111,  batch step: 204, loss: 8.04798698425293\n",
      "epoch: 111,  batch step: 205, loss: 30.281217575073242\n",
      "epoch: 111,  batch step: 206, loss: 2.3003616333007812\n",
      "epoch: 111,  batch step: 207, loss: 6.19498348236084\n",
      "epoch: 111,  batch step: 208, loss: 8.907831192016602\n",
      "epoch: 111,  batch step: 209, loss: 2.864743709564209\n",
      "epoch: 111,  batch step: 210, loss: 1.8543227910995483\n",
      "epoch: 111,  batch step: 211, loss: 17.023418426513672\n",
      "epoch: 111,  batch step: 212, loss: 24.795997619628906\n",
      "epoch: 111,  batch step: 213, loss: 12.669676780700684\n",
      "epoch: 111,  batch step: 214, loss: 3.2068564891815186\n",
      "epoch: 111,  batch step: 215, loss: 1.8555619716644287\n",
      "epoch: 111,  batch step: 216, loss: 16.645370483398438\n",
      "epoch: 111,  batch step: 217, loss: 2.4062294960021973\n",
      "epoch: 111,  batch step: 218, loss: 50.22515869140625\n",
      "epoch: 111,  batch step: 219, loss: 6.748187065124512\n",
      "epoch: 111,  batch step: 220, loss: 2.5596203804016113\n",
      "epoch: 111,  batch step: 221, loss: 6.821475028991699\n",
      "epoch: 111,  batch step: 222, loss: 14.412349700927734\n",
      "epoch: 111,  batch step: 223, loss: 63.157005310058594\n",
      "epoch: 111,  batch step: 224, loss: 2.4262821674346924\n",
      "epoch: 111,  batch step: 225, loss: 37.820167541503906\n",
      "epoch: 111,  batch step: 226, loss: 5.812173366546631\n",
      "epoch: 111,  batch step: 227, loss: 4.471050262451172\n",
      "epoch: 111,  batch step: 228, loss: 34.901824951171875\n",
      "epoch: 111,  batch step: 229, loss: 4.201781749725342\n",
      "epoch: 111,  batch step: 230, loss: 6.492697715759277\n",
      "epoch: 111,  batch step: 231, loss: 5.810203552246094\n",
      "epoch: 111,  batch step: 232, loss: 2.8534297943115234\n",
      "epoch: 111,  batch step: 233, loss: 2.207883358001709\n",
      "epoch: 111,  batch step: 234, loss: 10.45782470703125\n",
      "epoch: 111,  batch step: 235, loss: 2.6729190349578857\n",
      "epoch: 111,  batch step: 236, loss: 1.6872973442077637\n",
      "epoch: 111,  batch step: 237, loss: 4.081154823303223\n",
      "epoch: 111,  batch step: 238, loss: 52.337890625\n",
      "epoch: 111,  batch step: 239, loss: 1.8220690488815308\n",
      "epoch: 111,  batch step: 240, loss: 2.2760093212127686\n",
      "epoch: 111,  batch step: 241, loss: 25.05956268310547\n",
      "epoch: 111,  batch step: 242, loss: 13.715660095214844\n",
      "epoch: 111,  batch step: 243, loss: 16.193599700927734\n",
      "epoch: 111,  batch step: 244, loss: 3.992219924926758\n",
      "epoch: 111,  batch step: 245, loss: 2.107354164123535\n",
      "epoch: 111,  batch step: 246, loss: 48.77779769897461\n",
      "epoch: 111,  batch step: 247, loss: 15.858512878417969\n",
      "epoch: 111,  batch step: 248, loss: 22.486038208007812\n",
      "epoch: 111,  batch step: 249, loss: 1.7511918544769287\n",
      "epoch: 111,  batch step: 250, loss: 2.3797850608825684\n",
      "epoch: 111,  batch step: 251, loss: 55.99882888793945\n",
      "validation error epoch  111:    tensor(66.9132, device='cuda:0')\n",
      "316\n",
      "epoch: 112,  batch step: 0, loss: 14.365821838378906\n",
      "epoch: 112,  batch step: 1, loss: 3.352842092514038\n",
      "epoch: 112,  batch step: 2, loss: 4.113275051116943\n",
      "epoch: 112,  batch step: 3, loss: 53.76166915893555\n",
      "epoch: 112,  batch step: 4, loss: 71.80465698242188\n",
      "epoch: 112,  batch step: 5, loss: 49.76854705810547\n",
      "epoch: 112,  batch step: 6, loss: 3.102289915084839\n",
      "epoch: 112,  batch step: 7, loss: 2.220564126968384\n",
      "epoch: 112,  batch step: 8, loss: 3.760458469390869\n",
      "epoch: 112,  batch step: 9, loss: 3.912614583969116\n",
      "epoch: 112,  batch step: 10, loss: 12.907737731933594\n",
      "epoch: 112,  batch step: 11, loss: 5.271059989929199\n",
      "epoch: 112,  batch step: 12, loss: 3.5724711418151855\n",
      "epoch: 112,  batch step: 13, loss: 11.488035202026367\n",
      "epoch: 112,  batch step: 14, loss: 4.556707859039307\n",
      "epoch: 112,  batch step: 15, loss: 4.917859077453613\n",
      "epoch: 112,  batch step: 16, loss: 3.058175563812256\n",
      "epoch: 112,  batch step: 17, loss: 3.609732151031494\n",
      "epoch: 112,  batch step: 18, loss: 2.651304244995117\n",
      "epoch: 112,  batch step: 19, loss: 20.135635375976562\n",
      "epoch: 112,  batch step: 20, loss: 3.2864654064178467\n",
      "epoch: 112,  batch step: 21, loss: 16.072071075439453\n",
      "epoch: 112,  batch step: 22, loss: 4.222064018249512\n",
      "epoch: 112,  batch step: 23, loss: 7.379857063293457\n",
      "epoch: 112,  batch step: 24, loss: 21.332286834716797\n",
      "epoch: 112,  batch step: 25, loss: 2.7499167919158936\n",
      "epoch: 112,  batch step: 26, loss: 19.43131446838379\n",
      "epoch: 112,  batch step: 27, loss: 6.227603912353516\n",
      "epoch: 112,  batch step: 28, loss: 10.136448860168457\n",
      "epoch: 112,  batch step: 29, loss: 2.677624464035034\n",
      "epoch: 112,  batch step: 30, loss: 15.860045433044434\n",
      "epoch: 112,  batch step: 31, loss: 5.231200695037842\n",
      "epoch: 112,  batch step: 32, loss: 55.81193161010742\n",
      "epoch: 112,  batch step: 33, loss: 2.2097859382629395\n",
      "epoch: 112,  batch step: 34, loss: 7.241045951843262\n",
      "epoch: 112,  batch step: 35, loss: 10.446146965026855\n",
      "epoch: 112,  batch step: 36, loss: 2.825010061264038\n",
      "epoch: 112,  batch step: 37, loss: 2.110753059387207\n",
      "epoch: 112,  batch step: 38, loss: 2.16806697845459\n",
      "epoch: 112,  batch step: 39, loss: 50.79039001464844\n",
      "epoch: 112,  batch step: 40, loss: 2.431931257247925\n",
      "epoch: 112,  batch step: 41, loss: 13.09464168548584\n",
      "epoch: 112,  batch step: 42, loss: 8.085256576538086\n",
      "epoch: 112,  batch step: 43, loss: 12.183996200561523\n",
      "epoch: 112,  batch step: 44, loss: 2.9235780239105225\n",
      "epoch: 112,  batch step: 45, loss: 21.891508102416992\n",
      "epoch: 112,  batch step: 46, loss: 49.261314392089844\n",
      "epoch: 112,  batch step: 47, loss: 47.97332763671875\n",
      "epoch: 112,  batch step: 48, loss: 4.9934401512146\n",
      "epoch: 112,  batch step: 49, loss: 3.7352664470672607\n",
      "epoch: 112,  batch step: 50, loss: 3.8704166412353516\n",
      "epoch: 112,  batch step: 51, loss: 2.336453437805176\n",
      "epoch: 112,  batch step: 52, loss: 13.87845516204834\n",
      "epoch: 112,  batch step: 53, loss: 15.339171409606934\n",
      "epoch: 112,  batch step: 54, loss: 16.159515380859375\n",
      "epoch: 112,  batch step: 55, loss: 7.893110275268555\n",
      "epoch: 112,  batch step: 56, loss: 2.734450101852417\n",
      "epoch: 112,  batch step: 57, loss: 3.018216133117676\n",
      "epoch: 112,  batch step: 58, loss: 2.9514951705932617\n",
      "epoch: 112,  batch step: 59, loss: 2.2425239086151123\n",
      "epoch: 112,  batch step: 60, loss: 15.71470832824707\n",
      "epoch: 112,  batch step: 61, loss: 6.361473083496094\n",
      "epoch: 112,  batch step: 62, loss: 17.440757751464844\n",
      "epoch: 112,  batch step: 63, loss: 1.6273021697998047\n",
      "epoch: 112,  batch step: 64, loss: 3.720686912536621\n",
      "epoch: 112,  batch step: 65, loss: 2.1949291229248047\n",
      "epoch: 112,  batch step: 66, loss: 3.28840708732605\n",
      "epoch: 112,  batch step: 67, loss: 8.552490234375\n",
      "epoch: 112,  batch step: 68, loss: 24.3310546875\n",
      "epoch: 112,  batch step: 69, loss: 10.489885330200195\n",
      "epoch: 112,  batch step: 70, loss: 3.8931338787078857\n",
      "epoch: 112,  batch step: 71, loss: 13.27743911743164\n",
      "epoch: 112,  batch step: 72, loss: 1.6332550048828125\n",
      "epoch: 112,  batch step: 73, loss: 38.205631256103516\n",
      "epoch: 112,  batch step: 74, loss: 1.548362135887146\n",
      "epoch: 112,  batch step: 75, loss: 138.4236297607422\n",
      "epoch: 112,  batch step: 76, loss: 19.02407455444336\n",
      "epoch: 112,  batch step: 77, loss: 1.470386028289795\n",
      "epoch: 112,  batch step: 78, loss: 4.349225997924805\n",
      "epoch: 112,  batch step: 79, loss: 8.310546875\n",
      "epoch: 112,  batch step: 80, loss: 1.3323338031768799\n",
      "epoch: 112,  batch step: 81, loss: 41.11248016357422\n",
      "epoch: 112,  batch step: 82, loss: 24.201454162597656\n",
      "epoch: 112,  batch step: 83, loss: 13.222646713256836\n",
      "epoch: 112,  batch step: 84, loss: 5.656765937805176\n",
      "epoch: 112,  batch step: 85, loss: 16.10175323486328\n",
      "epoch: 112,  batch step: 86, loss: 4.107968807220459\n",
      "epoch: 112,  batch step: 87, loss: 9.020408630371094\n",
      "epoch: 112,  batch step: 88, loss: 5.153755187988281\n",
      "epoch: 112,  batch step: 89, loss: 6.364282131195068\n",
      "epoch: 112,  batch step: 90, loss: 46.2281494140625\n",
      "epoch: 112,  batch step: 91, loss: 2.8199081420898438\n",
      "epoch: 112,  batch step: 92, loss: 11.84292984008789\n",
      "epoch: 112,  batch step: 93, loss: 1.8288581371307373\n",
      "epoch: 112,  batch step: 94, loss: 2.0608136653900146\n",
      "epoch: 112,  batch step: 95, loss: 47.43223571777344\n",
      "epoch: 112,  batch step: 96, loss: 3.1325817108154297\n",
      "epoch: 112,  batch step: 97, loss: 4.908872127532959\n",
      "epoch: 112,  batch step: 98, loss: 2.0996956825256348\n",
      "epoch: 112,  batch step: 99, loss: 2.167198419570923\n",
      "epoch: 112,  batch step: 100, loss: 2.168351411819458\n",
      "epoch: 112,  batch step: 101, loss: 1.5969562530517578\n",
      "epoch: 112,  batch step: 102, loss: 2.429245948791504\n",
      "epoch: 112,  batch step: 103, loss: 4.003011226654053\n",
      "epoch: 112,  batch step: 104, loss: 3.28629732131958\n",
      "epoch: 112,  batch step: 105, loss: 7.943902492523193\n",
      "epoch: 112,  batch step: 106, loss: 2.037409543991089\n",
      "epoch: 112,  batch step: 107, loss: 7.88960599899292\n",
      "epoch: 112,  batch step: 108, loss: 48.877220153808594\n",
      "epoch: 112,  batch step: 109, loss: 6.7236151695251465\n",
      "epoch: 112,  batch step: 110, loss: 1.8626575469970703\n",
      "epoch: 112,  batch step: 111, loss: 13.273008346557617\n",
      "epoch: 112,  batch step: 112, loss: 10.529081344604492\n",
      "epoch: 112,  batch step: 113, loss: 3.482443332672119\n",
      "epoch: 112,  batch step: 114, loss: 1.931947112083435\n",
      "epoch: 112,  batch step: 115, loss: 6.051286220550537\n",
      "epoch: 112,  batch step: 116, loss: 49.1116943359375\n",
      "epoch: 112,  batch step: 117, loss: 3.7237796783447266\n",
      "epoch: 112,  batch step: 118, loss: 29.0387020111084\n",
      "epoch: 112,  batch step: 119, loss: 6.93776798248291\n",
      "epoch: 112,  batch step: 120, loss: 9.546060562133789\n",
      "epoch: 112,  batch step: 121, loss: 2.641315460205078\n",
      "epoch: 112,  batch step: 122, loss: 11.135831832885742\n",
      "epoch: 112,  batch step: 123, loss: 2.185415029525757\n",
      "epoch: 112,  batch step: 124, loss: 1.9780091047286987\n",
      "epoch: 112,  batch step: 125, loss: 1.8316385746002197\n",
      "epoch: 112,  batch step: 126, loss: 1.7068513631820679\n",
      "epoch: 112,  batch step: 127, loss: 7.9161696434021\n",
      "epoch: 112,  batch step: 128, loss: 4.73810338973999\n",
      "epoch: 112,  batch step: 129, loss: 1.8967760801315308\n",
      "epoch: 112,  batch step: 130, loss: 5.7593793869018555\n",
      "epoch: 112,  batch step: 131, loss: 2.338024139404297\n",
      "epoch: 112,  batch step: 132, loss: 4.917235851287842\n",
      "epoch: 112,  batch step: 133, loss: 2.339782953262329\n",
      "epoch: 112,  batch step: 134, loss: 16.41602325439453\n",
      "epoch: 112,  batch step: 135, loss: 3.1077914237976074\n",
      "epoch: 112,  batch step: 136, loss: 7.1811442375183105\n",
      "epoch: 112,  batch step: 137, loss: 9.225027084350586\n",
      "epoch: 112,  batch step: 138, loss: 2.6964545249938965\n",
      "epoch: 112,  batch step: 139, loss: 12.668609619140625\n",
      "epoch: 112,  batch step: 140, loss: 2.0532474517822266\n",
      "epoch: 112,  batch step: 141, loss: 13.10966682434082\n",
      "epoch: 112,  batch step: 142, loss: 2.1991326808929443\n",
      "epoch: 112,  batch step: 143, loss: 9.936545372009277\n",
      "epoch: 112,  batch step: 144, loss: 1.7439372539520264\n",
      "epoch: 112,  batch step: 145, loss: 7.673247337341309\n",
      "epoch: 112,  batch step: 146, loss: 1.651786208152771\n",
      "epoch: 112,  batch step: 147, loss: 1.479084849357605\n",
      "epoch: 112,  batch step: 148, loss: 2.4221529960632324\n",
      "epoch: 112,  batch step: 149, loss: 10.53134536743164\n",
      "epoch: 112,  batch step: 150, loss: 7.678391933441162\n",
      "epoch: 112,  batch step: 151, loss: 1.4825246334075928\n",
      "epoch: 112,  batch step: 152, loss: 1.7876412868499756\n",
      "epoch: 112,  batch step: 153, loss: 7.3224897384643555\n",
      "epoch: 112,  batch step: 154, loss: 16.588241577148438\n",
      "epoch: 112,  batch step: 155, loss: 11.491573333740234\n",
      "epoch: 112,  batch step: 156, loss: 2.0734963417053223\n",
      "epoch: 112,  batch step: 157, loss: 1.4085334539413452\n",
      "epoch: 112,  batch step: 158, loss: 8.03189468383789\n",
      "epoch: 112,  batch step: 159, loss: 4.156772136688232\n",
      "epoch: 112,  batch step: 160, loss: 4.531070232391357\n",
      "epoch: 112,  batch step: 161, loss: 4.231031894683838\n",
      "epoch: 112,  batch step: 162, loss: 2.587808847427368\n",
      "epoch: 112,  batch step: 163, loss: 47.500343322753906\n",
      "epoch: 112,  batch step: 164, loss: 16.36132049560547\n",
      "epoch: 112,  batch step: 165, loss: 3.54902982711792\n",
      "epoch: 112,  batch step: 166, loss: 30.514541625976562\n",
      "epoch: 112,  batch step: 167, loss: 1.3367745876312256\n",
      "epoch: 112,  batch step: 168, loss: 6.232547760009766\n",
      "epoch: 112,  batch step: 169, loss: 37.01335906982422\n",
      "epoch: 112,  batch step: 170, loss: 2.9869823455810547\n",
      "epoch: 112,  batch step: 171, loss: 1.2496693134307861\n",
      "epoch: 112,  batch step: 172, loss: 1.6635241508483887\n",
      "epoch: 112,  batch step: 173, loss: 3.0225207805633545\n",
      "epoch: 112,  batch step: 174, loss: 10.165912628173828\n",
      "epoch: 112,  batch step: 175, loss: 3.475519895553589\n",
      "epoch: 112,  batch step: 176, loss: 26.217008590698242\n",
      "epoch: 112,  batch step: 177, loss: 2.4687752723693848\n",
      "epoch: 112,  batch step: 178, loss: 41.562931060791016\n",
      "epoch: 112,  batch step: 179, loss: 2.16641902923584\n",
      "epoch: 112,  batch step: 180, loss: 14.273001670837402\n",
      "epoch: 112,  batch step: 181, loss: 1.603493094444275\n",
      "epoch: 112,  batch step: 182, loss: 1.4596977233886719\n",
      "epoch: 112,  batch step: 183, loss: 8.343236923217773\n",
      "epoch: 112,  batch step: 184, loss: 11.172327995300293\n",
      "epoch: 112,  batch step: 185, loss: 25.62520408630371\n",
      "epoch: 112,  batch step: 186, loss: 13.971175193786621\n",
      "epoch: 112,  batch step: 187, loss: 10.554337501525879\n",
      "epoch: 112,  batch step: 188, loss: 1.6122726202011108\n",
      "epoch: 112,  batch step: 189, loss: 6.357948303222656\n",
      "epoch: 112,  batch step: 190, loss: 12.619051933288574\n",
      "epoch: 112,  batch step: 191, loss: 1.8731803894042969\n",
      "epoch: 112,  batch step: 192, loss: 9.045853614807129\n",
      "epoch: 112,  batch step: 193, loss: 2.468961715698242\n",
      "epoch: 112,  batch step: 194, loss: 2.869079113006592\n",
      "epoch: 112,  batch step: 195, loss: 1.4475958347320557\n",
      "epoch: 112,  batch step: 196, loss: 12.1350736618042\n",
      "epoch: 112,  batch step: 197, loss: 61.23802185058594\n",
      "epoch: 112,  batch step: 198, loss: 6.596356391906738\n",
      "epoch: 112,  batch step: 199, loss: 2.315986394882202\n",
      "epoch: 112,  batch step: 200, loss: 4.541937351226807\n",
      "epoch: 112,  batch step: 201, loss: 2.1217918395996094\n",
      "epoch: 112,  batch step: 202, loss: 6.764316558837891\n",
      "epoch: 112,  batch step: 203, loss: 2.862602949142456\n",
      "epoch: 112,  batch step: 204, loss: 5.322088241577148\n",
      "epoch: 112,  batch step: 205, loss: 30.440105438232422\n",
      "epoch: 112,  batch step: 206, loss: 7.031457424163818\n",
      "epoch: 112,  batch step: 207, loss: 9.862601280212402\n",
      "epoch: 112,  batch step: 208, loss: 2.134535789489746\n",
      "epoch: 112,  batch step: 209, loss: 13.156214714050293\n",
      "epoch: 112,  batch step: 210, loss: 7.838156700134277\n",
      "epoch: 112,  batch step: 211, loss: 2.0679032802581787\n",
      "epoch: 112,  batch step: 212, loss: 3.4960598945617676\n",
      "epoch: 112,  batch step: 213, loss: 2.6506240367889404\n",
      "epoch: 112,  batch step: 214, loss: 1.6465033292770386\n",
      "epoch: 112,  batch step: 215, loss: 8.635729789733887\n",
      "epoch: 112,  batch step: 216, loss: 7.404182434082031\n",
      "epoch: 112,  batch step: 217, loss: 10.43856430053711\n",
      "epoch: 112,  batch step: 218, loss: 66.75331115722656\n",
      "epoch: 112,  batch step: 219, loss: 2.151395559310913\n",
      "epoch: 112,  batch step: 220, loss: 2.0304629802703857\n",
      "epoch: 112,  batch step: 221, loss: 3.154484272003174\n",
      "epoch: 112,  batch step: 222, loss: 1.6100122928619385\n",
      "epoch: 112,  batch step: 223, loss: 7.807620525360107\n",
      "epoch: 112,  batch step: 224, loss: 10.639443397521973\n",
      "epoch: 112,  batch step: 225, loss: 16.518415451049805\n",
      "epoch: 112,  batch step: 226, loss: 16.71405601501465\n",
      "epoch: 112,  batch step: 227, loss: 16.5285587310791\n",
      "epoch: 112,  batch step: 228, loss: 2.9303736686706543\n",
      "epoch: 112,  batch step: 229, loss: 9.05858039855957\n",
      "epoch: 112,  batch step: 230, loss: 7.604880332946777\n",
      "epoch: 112,  batch step: 231, loss: 13.432111740112305\n",
      "epoch: 112,  batch step: 232, loss: 25.060806274414062\n",
      "epoch: 112,  batch step: 233, loss: 2.908886671066284\n",
      "epoch: 112,  batch step: 234, loss: 7.66341495513916\n",
      "epoch: 112,  batch step: 235, loss: 2.103708267211914\n",
      "epoch: 112,  batch step: 236, loss: 24.692264556884766\n",
      "epoch: 112,  batch step: 237, loss: 2.9256579875946045\n",
      "epoch: 112,  batch step: 238, loss: 5.22995662689209\n",
      "epoch: 112,  batch step: 239, loss: 65.8417739868164\n",
      "epoch: 112,  batch step: 240, loss: 5.606284141540527\n",
      "epoch: 112,  batch step: 241, loss: 2.622056722640991\n",
      "epoch: 112,  batch step: 242, loss: 2.9929628372192383\n",
      "epoch: 112,  batch step: 243, loss: 7.584877967834473\n",
      "epoch: 112,  batch step: 244, loss: 2.1813480854034424\n",
      "epoch: 112,  batch step: 245, loss: 7.256136894226074\n",
      "epoch: 112,  batch step: 246, loss: 2.2464990615844727\n",
      "epoch: 112,  batch step: 247, loss: 2.83231258392334\n",
      "epoch: 112,  batch step: 248, loss: 2.160487651824951\n",
      "epoch: 112,  batch step: 249, loss: 7.578860282897949\n",
      "epoch: 112,  batch step: 250, loss: 6.119863033294678\n",
      "epoch: 112,  batch step: 251, loss: 39.08393478393555\n",
      "validation error epoch  112:    tensor(68.9137, device='cuda:0')\n",
      "316\n",
      "epoch: 113,  batch step: 0, loss: 1.6789968013763428\n",
      "epoch: 113,  batch step: 1, loss: 2.37732195854187\n",
      "epoch: 113,  batch step: 2, loss: 78.1038818359375\n",
      "epoch: 113,  batch step: 3, loss: 2.805328369140625\n",
      "epoch: 113,  batch step: 4, loss: 31.90667724609375\n",
      "epoch: 113,  batch step: 5, loss: 2.682037115097046\n",
      "epoch: 113,  batch step: 6, loss: 41.29935836791992\n",
      "epoch: 113,  batch step: 7, loss: 9.234186172485352\n",
      "epoch: 113,  batch step: 8, loss: 8.200340270996094\n",
      "epoch: 113,  batch step: 9, loss: 1.6556880474090576\n",
      "epoch: 113,  batch step: 10, loss: 6.138852596282959\n",
      "epoch: 113,  batch step: 11, loss: 13.539286613464355\n",
      "epoch: 113,  batch step: 12, loss: 11.057247161865234\n",
      "epoch: 113,  batch step: 13, loss: 10.401436805725098\n",
      "epoch: 113,  batch step: 14, loss: 47.221519470214844\n",
      "epoch: 113,  batch step: 15, loss: 8.420882225036621\n",
      "epoch: 113,  batch step: 16, loss: 3.224992275238037\n",
      "epoch: 113,  batch step: 17, loss: 2.956608295440674\n",
      "epoch: 113,  batch step: 18, loss: 6.216405391693115\n",
      "epoch: 113,  batch step: 19, loss: 2.586920976638794\n",
      "epoch: 113,  batch step: 20, loss: 4.004375457763672\n",
      "epoch: 113,  batch step: 21, loss: 145.56906127929688\n",
      "epoch: 113,  batch step: 22, loss: 2.847188711166382\n",
      "epoch: 113,  batch step: 23, loss: 1.9669078588485718\n",
      "epoch: 113,  batch step: 24, loss: 10.849616050720215\n",
      "epoch: 113,  batch step: 25, loss: 1.4841229915618896\n",
      "epoch: 113,  batch step: 26, loss: 8.322517395019531\n",
      "epoch: 113,  batch step: 27, loss: 3.1058177947998047\n",
      "epoch: 113,  batch step: 28, loss: 4.7756500244140625\n",
      "epoch: 113,  batch step: 29, loss: 2.3011035919189453\n",
      "epoch: 113,  batch step: 30, loss: 7.565420150756836\n",
      "epoch: 113,  batch step: 31, loss: 2.655062675476074\n",
      "epoch: 113,  batch step: 32, loss: 27.18947982788086\n",
      "epoch: 113,  batch step: 33, loss: 5.953517436981201\n",
      "epoch: 113,  batch step: 34, loss: 7.5164923667907715\n",
      "epoch: 113,  batch step: 35, loss: 23.663055419921875\n",
      "epoch: 113,  batch step: 36, loss: 3.877150058746338\n",
      "epoch: 113,  batch step: 37, loss: 9.698554039001465\n",
      "epoch: 113,  batch step: 38, loss: 17.61495590209961\n",
      "epoch: 113,  batch step: 39, loss: 6.2702741622924805\n",
      "epoch: 113,  batch step: 40, loss: 2.979158878326416\n",
      "epoch: 113,  batch step: 41, loss: 8.831506729125977\n",
      "epoch: 113,  batch step: 42, loss: 2.016420602798462\n",
      "epoch: 113,  batch step: 43, loss: 15.709564208984375\n",
      "epoch: 113,  batch step: 44, loss: 1.7449495792388916\n",
      "epoch: 113,  batch step: 45, loss: 15.990802764892578\n",
      "epoch: 113,  batch step: 46, loss: 2.529249906539917\n",
      "epoch: 113,  batch step: 47, loss: 1.2863115072250366\n",
      "epoch: 113,  batch step: 48, loss: 50.44091796875\n",
      "epoch: 113,  batch step: 49, loss: 1.0777186155319214\n",
      "epoch: 113,  batch step: 50, loss: 2.588181495666504\n",
      "epoch: 113,  batch step: 51, loss: 6.265689373016357\n",
      "epoch: 113,  batch step: 52, loss: 12.39179801940918\n",
      "epoch: 113,  batch step: 53, loss: 2.0200107097625732\n",
      "epoch: 113,  batch step: 54, loss: 2.514626979827881\n",
      "epoch: 113,  batch step: 55, loss: 16.592275619506836\n",
      "epoch: 113,  batch step: 56, loss: 1.7504255771636963\n",
      "epoch: 113,  batch step: 57, loss: 2.216656446456909\n",
      "epoch: 113,  batch step: 58, loss: 2.843027114868164\n",
      "epoch: 113,  batch step: 59, loss: 50.338863372802734\n",
      "epoch: 113,  batch step: 60, loss: 14.985414505004883\n",
      "epoch: 113,  batch step: 61, loss: 6.972994804382324\n",
      "epoch: 113,  batch step: 62, loss: 2.2091283798217773\n",
      "epoch: 113,  batch step: 63, loss: 4.701991558074951\n",
      "epoch: 113,  batch step: 64, loss: 2.2749955654144287\n",
      "epoch: 113,  batch step: 65, loss: 5.215022087097168\n",
      "epoch: 113,  batch step: 66, loss: 12.609840393066406\n",
      "epoch: 113,  batch step: 67, loss: 34.097774505615234\n",
      "epoch: 113,  batch step: 68, loss: 7.277303218841553\n",
      "epoch: 113,  batch step: 69, loss: 2.754025459289551\n",
      "epoch: 113,  batch step: 70, loss: 27.531768798828125\n",
      "epoch: 113,  batch step: 71, loss: 8.26445484161377\n",
      "epoch: 113,  batch step: 72, loss: 9.842805862426758\n",
      "epoch: 113,  batch step: 73, loss: 18.259546279907227\n",
      "epoch: 113,  batch step: 74, loss: 2.019474983215332\n",
      "epoch: 113,  batch step: 75, loss: 6.3956618309021\n",
      "epoch: 113,  batch step: 76, loss: 8.246199607849121\n",
      "epoch: 113,  batch step: 77, loss: 7.644631862640381\n",
      "epoch: 113,  batch step: 78, loss: 17.845436096191406\n",
      "epoch: 113,  batch step: 79, loss: 1.1378458738327026\n",
      "epoch: 113,  batch step: 80, loss: 19.137859344482422\n",
      "epoch: 113,  batch step: 81, loss: 1.5936558246612549\n",
      "epoch: 113,  batch step: 82, loss: 11.660613059997559\n",
      "epoch: 113,  batch step: 83, loss: 1.2177103757858276\n",
      "epoch: 113,  batch step: 84, loss: 21.569374084472656\n",
      "epoch: 113,  batch step: 85, loss: 29.67798614501953\n",
      "epoch: 113,  batch step: 86, loss: 12.010885238647461\n",
      "epoch: 113,  batch step: 87, loss: 12.429122924804688\n",
      "epoch: 113,  batch step: 88, loss: 5.908763885498047\n",
      "epoch: 113,  batch step: 89, loss: 7.064539909362793\n",
      "epoch: 113,  batch step: 90, loss: 1.659298300743103\n",
      "epoch: 113,  batch step: 91, loss: 4.03288459777832\n",
      "epoch: 113,  batch step: 92, loss: 2.2338597774505615\n",
      "epoch: 113,  batch step: 93, loss: 6.937985420227051\n",
      "epoch: 113,  batch step: 94, loss: 6.367547035217285\n",
      "epoch: 113,  batch step: 95, loss: 14.726968765258789\n",
      "epoch: 113,  batch step: 96, loss: 6.233874797821045\n",
      "epoch: 113,  batch step: 97, loss: 4.085310935974121\n",
      "epoch: 113,  batch step: 98, loss: 12.658597946166992\n",
      "epoch: 113,  batch step: 99, loss: 9.02977466583252\n",
      "epoch: 113,  batch step: 100, loss: 2.8398046493530273\n",
      "epoch: 113,  batch step: 101, loss: 4.382037162780762\n",
      "epoch: 113,  batch step: 102, loss: 8.290260314941406\n",
      "epoch: 113,  batch step: 103, loss: 2.3575730323791504\n",
      "epoch: 113,  batch step: 104, loss: 2.4235024452209473\n",
      "epoch: 113,  batch step: 105, loss: 2.074782609939575\n",
      "epoch: 113,  batch step: 106, loss: 2.582869529724121\n",
      "epoch: 113,  batch step: 107, loss: 1.9862182140350342\n",
      "epoch: 113,  batch step: 108, loss: 6.514222621917725\n",
      "epoch: 113,  batch step: 109, loss: 12.582944869995117\n",
      "epoch: 113,  batch step: 110, loss: 6.249375820159912\n",
      "epoch: 113,  batch step: 111, loss: 2.131977081298828\n",
      "epoch: 113,  batch step: 112, loss: 5.559929847717285\n",
      "epoch: 113,  batch step: 113, loss: 4.398923873901367\n",
      "epoch: 113,  batch step: 114, loss: 16.002498626708984\n",
      "epoch: 113,  batch step: 115, loss: 2.8246192932128906\n",
      "epoch: 113,  batch step: 116, loss: 2.3443939685821533\n",
      "epoch: 113,  batch step: 117, loss: 3.012266159057617\n",
      "epoch: 113,  batch step: 118, loss: 4.6053571701049805\n",
      "epoch: 113,  batch step: 119, loss: 5.315191268920898\n",
      "epoch: 113,  batch step: 120, loss: 3.147608518600464\n",
      "epoch: 113,  batch step: 121, loss: 2.738786220550537\n",
      "epoch: 113,  batch step: 122, loss: 1.8935966491699219\n",
      "epoch: 113,  batch step: 123, loss: 13.77914047241211\n",
      "epoch: 113,  batch step: 124, loss: 1.779834508895874\n",
      "epoch: 113,  batch step: 125, loss: 49.93122863769531\n",
      "epoch: 113,  batch step: 126, loss: 1.3136093616485596\n",
      "epoch: 113,  batch step: 127, loss: 44.886287689208984\n",
      "epoch: 113,  batch step: 128, loss: 12.646095275878906\n",
      "epoch: 113,  batch step: 129, loss: 10.146957397460938\n",
      "epoch: 113,  batch step: 130, loss: 5.675639629364014\n",
      "epoch: 113,  batch step: 131, loss: 8.022848129272461\n",
      "epoch: 113,  batch step: 132, loss: 7.059099197387695\n",
      "epoch: 113,  batch step: 133, loss: 1.5669105052947998\n",
      "epoch: 113,  batch step: 134, loss: 1.1244926452636719\n",
      "epoch: 113,  batch step: 135, loss: 1.8349236249923706\n",
      "epoch: 113,  batch step: 136, loss: 6.349155426025391\n",
      "epoch: 113,  batch step: 137, loss: 8.23601245880127\n",
      "epoch: 113,  batch step: 138, loss: 3.6048507690429688\n",
      "epoch: 113,  batch step: 139, loss: 3.957091808319092\n",
      "epoch: 113,  batch step: 140, loss: 2.092268466949463\n",
      "epoch: 113,  batch step: 141, loss: 4.777629852294922\n",
      "epoch: 113,  batch step: 142, loss: 2.086547613143921\n",
      "epoch: 113,  batch step: 143, loss: 7.281949043273926\n",
      "epoch: 113,  batch step: 144, loss: 2.232459545135498\n",
      "epoch: 113,  batch step: 145, loss: 1.3450626134872437\n",
      "epoch: 113,  batch step: 146, loss: 1.8548434972763062\n",
      "epoch: 113,  batch step: 147, loss: 1.673694372177124\n",
      "epoch: 113,  batch step: 148, loss: 12.137736320495605\n",
      "epoch: 113,  batch step: 149, loss: 14.571653366088867\n",
      "epoch: 113,  batch step: 150, loss: 61.78596496582031\n",
      "epoch: 113,  batch step: 151, loss: 1.8542888164520264\n",
      "epoch: 113,  batch step: 152, loss: 17.541061401367188\n",
      "epoch: 113,  batch step: 153, loss: 2.4264848232269287\n",
      "epoch: 113,  batch step: 154, loss: 3.0587902069091797\n",
      "epoch: 113,  batch step: 155, loss: 39.360511779785156\n",
      "epoch: 113,  batch step: 156, loss: 4.799330234527588\n",
      "epoch: 113,  batch step: 157, loss: 11.423521995544434\n",
      "epoch: 113,  batch step: 158, loss: 1.7321275472640991\n",
      "epoch: 113,  batch step: 159, loss: 38.339378356933594\n",
      "epoch: 113,  batch step: 160, loss: 4.193062782287598\n",
      "epoch: 113,  batch step: 161, loss: 1.7212121486663818\n",
      "epoch: 113,  batch step: 162, loss: 1.451700210571289\n",
      "epoch: 113,  batch step: 163, loss: 5.684932708740234\n",
      "epoch: 113,  batch step: 164, loss: 2.6502485275268555\n",
      "epoch: 113,  batch step: 165, loss: 2.8397860527038574\n",
      "epoch: 113,  batch step: 166, loss: 34.7982177734375\n",
      "epoch: 113,  batch step: 167, loss: 4.961298942565918\n",
      "epoch: 113,  batch step: 168, loss: 1.9178798198699951\n",
      "epoch: 113,  batch step: 169, loss: 8.8602294921875\n",
      "epoch: 113,  batch step: 170, loss: 1.6686265468597412\n",
      "epoch: 113,  batch step: 171, loss: 1.361170768737793\n",
      "epoch: 113,  batch step: 172, loss: 5.425353050231934\n",
      "epoch: 113,  batch step: 173, loss: 1.9550261497497559\n",
      "epoch: 113,  batch step: 174, loss: 5.174350261688232\n",
      "epoch: 113,  batch step: 175, loss: 13.49832534790039\n",
      "epoch: 113,  batch step: 176, loss: 2.123870372772217\n",
      "epoch: 113,  batch step: 177, loss: 27.281980514526367\n",
      "epoch: 113,  batch step: 178, loss: 9.170985221862793\n",
      "epoch: 113,  batch step: 179, loss: 2.860673427581787\n",
      "epoch: 113,  batch step: 180, loss: 12.629897117614746\n",
      "epoch: 113,  batch step: 181, loss: 13.439471244812012\n",
      "epoch: 113,  batch step: 182, loss: 22.83784294128418\n",
      "epoch: 113,  batch step: 183, loss: 94.21300506591797\n",
      "epoch: 113,  batch step: 184, loss: 8.006048202514648\n",
      "epoch: 113,  batch step: 185, loss: 2.0072712898254395\n",
      "epoch: 113,  batch step: 186, loss: 9.311296463012695\n",
      "epoch: 113,  batch step: 187, loss: 2.0522916316986084\n",
      "epoch: 113,  batch step: 188, loss: 2.2286477088928223\n",
      "epoch: 113,  batch step: 189, loss: 2.3270750045776367\n",
      "epoch: 113,  batch step: 190, loss: 1.2105603218078613\n",
      "epoch: 113,  batch step: 191, loss: 3.0767600536346436\n",
      "epoch: 113,  batch step: 192, loss: 2.1436686515808105\n",
      "epoch: 113,  batch step: 193, loss: 5.052602767944336\n",
      "epoch: 113,  batch step: 194, loss: 84.29855346679688\n",
      "epoch: 113,  batch step: 195, loss: 1.9295635223388672\n",
      "epoch: 113,  batch step: 196, loss: 2.065901279449463\n",
      "epoch: 113,  batch step: 197, loss: 1.761599063873291\n",
      "epoch: 113,  batch step: 198, loss: 2.352752208709717\n",
      "epoch: 113,  batch step: 199, loss: 1.5706418752670288\n",
      "epoch: 113,  batch step: 200, loss: 10.629441261291504\n",
      "epoch: 113,  batch step: 201, loss: 56.577919006347656\n",
      "epoch: 113,  batch step: 202, loss: 31.51361656188965\n",
      "epoch: 113,  batch step: 203, loss: 1.8890361785888672\n",
      "epoch: 113,  batch step: 204, loss: 35.77684020996094\n",
      "epoch: 113,  batch step: 205, loss: 22.680858612060547\n",
      "epoch: 113,  batch step: 206, loss: 3.8114781379699707\n",
      "epoch: 113,  batch step: 207, loss: 3.0069499015808105\n",
      "epoch: 113,  batch step: 208, loss: 12.540901184082031\n",
      "epoch: 113,  batch step: 209, loss: 15.522138595581055\n",
      "epoch: 113,  batch step: 210, loss: 3.7523913383483887\n",
      "epoch: 113,  batch step: 211, loss: 29.437389373779297\n",
      "epoch: 113,  batch step: 212, loss: 6.522574424743652\n",
      "epoch: 113,  batch step: 213, loss: 21.87067222595215\n",
      "epoch: 113,  batch step: 214, loss: 3.0055432319641113\n",
      "epoch: 113,  batch step: 215, loss: 1.7817462682724\n",
      "epoch: 113,  batch step: 216, loss: 3.1124491691589355\n",
      "epoch: 113,  batch step: 217, loss: 13.347820281982422\n",
      "epoch: 113,  batch step: 218, loss: 1.8896106481552124\n",
      "epoch: 113,  batch step: 219, loss: 3.31555438041687\n",
      "epoch: 113,  batch step: 220, loss: 2.893692970275879\n",
      "epoch: 113,  batch step: 221, loss: 2.088862180709839\n",
      "epoch: 113,  batch step: 222, loss: 2.0325093269348145\n",
      "epoch: 113,  batch step: 223, loss: 3.6008710861206055\n",
      "epoch: 113,  batch step: 224, loss: 32.33280944824219\n",
      "epoch: 113,  batch step: 225, loss: 43.54210662841797\n",
      "epoch: 113,  batch step: 226, loss: 6.907907485961914\n",
      "epoch: 113,  batch step: 227, loss: 18.093801498413086\n",
      "epoch: 113,  batch step: 228, loss: 16.563983917236328\n",
      "epoch: 113,  batch step: 229, loss: 1.984366774559021\n",
      "epoch: 113,  batch step: 230, loss: 1.6011384725570679\n",
      "epoch: 113,  batch step: 231, loss: 31.458873748779297\n",
      "epoch: 113,  batch step: 232, loss: 2.041872978210449\n",
      "epoch: 113,  batch step: 233, loss: 3.485158681869507\n",
      "epoch: 113,  batch step: 234, loss: 2.062556743621826\n",
      "epoch: 113,  batch step: 235, loss: 20.195695877075195\n",
      "epoch: 113,  batch step: 236, loss: 11.77695083618164\n",
      "epoch: 113,  batch step: 237, loss: 4.714686393737793\n",
      "epoch: 113,  batch step: 238, loss: 1.3787753582000732\n",
      "epoch: 113,  batch step: 239, loss: 1.8394415378570557\n",
      "epoch: 113,  batch step: 240, loss: 4.5897321701049805\n",
      "epoch: 113,  batch step: 241, loss: 1.728420615196228\n",
      "epoch: 113,  batch step: 242, loss: 27.83544921875\n",
      "epoch: 113,  batch step: 243, loss: 2.5908875465393066\n",
      "epoch: 113,  batch step: 244, loss: 9.088165283203125\n",
      "epoch: 113,  batch step: 245, loss: 2.714618682861328\n",
      "epoch: 113,  batch step: 246, loss: 7.439265251159668\n",
      "epoch: 113,  batch step: 247, loss: 14.819571495056152\n",
      "epoch: 113,  batch step: 248, loss: 2.7026679515838623\n",
      "epoch: 113,  batch step: 249, loss: 14.890985488891602\n",
      "epoch: 113,  batch step: 250, loss: 1.9199550151824951\n",
      "epoch: 113,  batch step: 251, loss: 3.4919393062591553\n",
      "validation error epoch  113:    tensor(70.9623, device='cuda:0')\n",
      "316\n",
      "epoch: 114,  batch step: 0, loss: 2.491380453109741\n",
      "epoch: 114,  batch step: 1, loss: 30.680587768554688\n",
      "epoch: 114,  batch step: 2, loss: 2.2450852394104004\n",
      "epoch: 114,  batch step: 3, loss: 1.8669542074203491\n",
      "epoch: 114,  batch step: 4, loss: 1.6957911252975464\n",
      "epoch: 114,  batch step: 5, loss: 2.018380641937256\n",
      "epoch: 114,  batch step: 6, loss: 23.305927276611328\n",
      "epoch: 114,  batch step: 7, loss: 10.071057319641113\n",
      "epoch: 114,  batch step: 8, loss: 8.521710395812988\n",
      "epoch: 114,  batch step: 9, loss: 1.604191541671753\n",
      "epoch: 114,  batch step: 10, loss: 7.160821437835693\n",
      "epoch: 114,  batch step: 11, loss: 14.698145866394043\n",
      "epoch: 114,  batch step: 12, loss: 16.836088180541992\n",
      "epoch: 114,  batch step: 13, loss: 27.245147705078125\n",
      "epoch: 114,  batch step: 14, loss: 6.983563423156738\n",
      "epoch: 114,  batch step: 15, loss: 4.361363410949707\n",
      "epoch: 114,  batch step: 16, loss: 30.39797592163086\n",
      "epoch: 114,  batch step: 17, loss: 8.04771614074707\n",
      "epoch: 114,  batch step: 18, loss: 4.557427883148193\n",
      "epoch: 114,  batch step: 19, loss: 3.9998536109924316\n",
      "epoch: 114,  batch step: 20, loss: 6.349349498748779\n",
      "epoch: 114,  batch step: 21, loss: 1.7322287559509277\n",
      "epoch: 114,  batch step: 22, loss: 13.172052383422852\n",
      "epoch: 114,  batch step: 23, loss: 9.442959785461426\n",
      "epoch: 114,  batch step: 24, loss: 1.8262927532196045\n",
      "epoch: 114,  batch step: 25, loss: 12.211639404296875\n",
      "epoch: 114,  batch step: 26, loss: 1.766973853111267\n",
      "epoch: 114,  batch step: 27, loss: 7.686202049255371\n",
      "epoch: 114,  batch step: 28, loss: 1.4657410383224487\n",
      "epoch: 114,  batch step: 29, loss: 131.05242919921875\n",
      "epoch: 114,  batch step: 30, loss: 27.340322494506836\n",
      "epoch: 114,  batch step: 31, loss: 10.321163177490234\n",
      "epoch: 114,  batch step: 32, loss: 1.6861828565597534\n",
      "epoch: 114,  batch step: 33, loss: 10.202658653259277\n",
      "epoch: 114,  batch step: 34, loss: 8.384197235107422\n",
      "epoch: 114,  batch step: 35, loss: 3.6757450103759766\n",
      "epoch: 114,  batch step: 36, loss: 27.440120697021484\n",
      "epoch: 114,  batch step: 37, loss: 1.929123878479004\n",
      "epoch: 114,  batch step: 38, loss: 13.253881454467773\n",
      "epoch: 114,  batch step: 39, loss: 11.572669982910156\n",
      "epoch: 114,  batch step: 40, loss: 3.4824323654174805\n",
      "epoch: 114,  batch step: 41, loss: 8.007064819335938\n",
      "epoch: 114,  batch step: 42, loss: 17.916988372802734\n",
      "epoch: 114,  batch step: 43, loss: 1.828995704650879\n",
      "epoch: 114,  batch step: 44, loss: 13.910577774047852\n",
      "epoch: 114,  batch step: 45, loss: 10.059326171875\n",
      "epoch: 114,  batch step: 46, loss: 36.4241943359375\n",
      "epoch: 114,  batch step: 47, loss: 7.077105522155762\n",
      "epoch: 114,  batch step: 48, loss: 23.168418884277344\n",
      "epoch: 114,  batch step: 49, loss: 1.7852857112884521\n",
      "epoch: 114,  batch step: 50, loss: 3.3870952129364014\n",
      "epoch: 114,  batch step: 51, loss: 15.100044250488281\n",
      "epoch: 114,  batch step: 52, loss: 1.8541492223739624\n",
      "epoch: 114,  batch step: 53, loss: 1.9610332250595093\n",
      "epoch: 114,  batch step: 54, loss: 3.3729143142700195\n",
      "epoch: 114,  batch step: 55, loss: 23.594213485717773\n",
      "epoch: 114,  batch step: 56, loss: 4.058587074279785\n",
      "epoch: 114,  batch step: 57, loss: 10.414520263671875\n",
      "epoch: 114,  batch step: 58, loss: 3.201383590698242\n",
      "epoch: 114,  batch step: 59, loss: 14.763900756835938\n",
      "epoch: 114,  batch step: 60, loss: 2.393491506576538\n",
      "epoch: 114,  batch step: 61, loss: 7.516044616699219\n",
      "epoch: 114,  batch step: 62, loss: 6.985404968261719\n",
      "epoch: 114,  batch step: 63, loss: 15.005621910095215\n",
      "epoch: 114,  batch step: 64, loss: 1.3763103485107422\n",
      "epoch: 114,  batch step: 65, loss: 15.622554779052734\n",
      "epoch: 114,  batch step: 66, loss: 9.719514846801758\n",
      "epoch: 114,  batch step: 67, loss: 50.81028747558594\n",
      "epoch: 114,  batch step: 68, loss: 9.520870208740234\n",
      "epoch: 114,  batch step: 69, loss: 5.0048017501831055\n",
      "epoch: 114,  batch step: 70, loss: 57.332366943359375\n",
      "epoch: 114,  batch step: 71, loss: 16.058372497558594\n",
      "epoch: 114,  batch step: 72, loss: 2.657266616821289\n",
      "epoch: 114,  batch step: 73, loss: 2.5112144947052\n",
      "epoch: 114,  batch step: 74, loss: 2.331324577331543\n",
      "epoch: 114,  batch step: 75, loss: 6.858771324157715\n",
      "epoch: 114,  batch step: 76, loss: 2.5185790061950684\n",
      "epoch: 114,  batch step: 77, loss: 3.378518581390381\n",
      "epoch: 114,  batch step: 78, loss: 2.0065810680389404\n",
      "epoch: 114,  batch step: 79, loss: 22.325353622436523\n",
      "epoch: 114,  batch step: 80, loss: 6.8352370262146\n",
      "epoch: 114,  batch step: 81, loss: 3.5557708740234375\n",
      "epoch: 114,  batch step: 82, loss: 2.287881374359131\n",
      "epoch: 114,  batch step: 83, loss: 8.150091171264648\n",
      "epoch: 114,  batch step: 84, loss: 8.59908676147461\n",
      "epoch: 114,  batch step: 85, loss: 3.4699206352233887\n",
      "epoch: 114,  batch step: 86, loss: 2.6385769844055176\n",
      "epoch: 114,  batch step: 87, loss: 2.201211929321289\n",
      "epoch: 114,  batch step: 88, loss: 15.709803581237793\n",
      "epoch: 114,  batch step: 89, loss: 2.3899636268615723\n",
      "epoch: 114,  batch step: 90, loss: 1.646559476852417\n",
      "epoch: 114,  batch step: 91, loss: 46.13299560546875\n",
      "epoch: 114,  batch step: 92, loss: 22.021228790283203\n",
      "epoch: 114,  batch step: 93, loss: 1.358627200126648\n",
      "epoch: 114,  batch step: 94, loss: 7.016965389251709\n",
      "epoch: 114,  batch step: 95, loss: 6.841548919677734\n",
      "epoch: 114,  batch step: 96, loss: 1.9154384136199951\n",
      "epoch: 114,  batch step: 97, loss: 11.471083641052246\n",
      "epoch: 114,  batch step: 98, loss: 10.610820770263672\n",
      "epoch: 114,  batch step: 99, loss: 5.997102737426758\n",
      "epoch: 114,  batch step: 100, loss: 10.728788375854492\n",
      "epoch: 114,  batch step: 101, loss: 8.295174598693848\n",
      "epoch: 114,  batch step: 102, loss: 7.918254375457764\n",
      "epoch: 114,  batch step: 103, loss: 34.11933898925781\n",
      "epoch: 114,  batch step: 104, loss: 34.967079162597656\n",
      "epoch: 114,  batch step: 105, loss: 2.028365135192871\n",
      "epoch: 114,  batch step: 106, loss: 2.3103177547454834\n",
      "epoch: 114,  batch step: 107, loss: 2.7384002208709717\n",
      "epoch: 114,  batch step: 108, loss: 2.808013916015625\n",
      "epoch: 114,  batch step: 109, loss: 2.6185483932495117\n",
      "epoch: 114,  batch step: 110, loss: 16.842561721801758\n",
      "epoch: 114,  batch step: 111, loss: 2.4154484272003174\n",
      "epoch: 114,  batch step: 112, loss: 14.961185455322266\n",
      "epoch: 114,  batch step: 113, loss: 2.1711678504943848\n",
      "epoch: 114,  batch step: 114, loss: 1.9594429731369019\n",
      "epoch: 114,  batch step: 115, loss: 49.806331634521484\n",
      "epoch: 114,  batch step: 116, loss: 16.770374298095703\n",
      "epoch: 114,  batch step: 117, loss: 66.56315612792969\n",
      "epoch: 114,  batch step: 118, loss: 4.448174953460693\n",
      "epoch: 114,  batch step: 119, loss: 2.5481128692626953\n",
      "epoch: 114,  batch step: 120, loss: 1.8676583766937256\n",
      "epoch: 114,  batch step: 121, loss: 56.99227523803711\n",
      "epoch: 114,  batch step: 122, loss: 13.78622055053711\n",
      "epoch: 114,  batch step: 123, loss: 10.09625244140625\n",
      "epoch: 114,  batch step: 124, loss: 46.296409606933594\n",
      "epoch: 114,  batch step: 125, loss: 1.8568567037582397\n",
      "epoch: 114,  batch step: 126, loss: 46.899871826171875\n",
      "epoch: 114,  batch step: 127, loss: 9.270994186401367\n",
      "epoch: 114,  batch step: 128, loss: 2.388899326324463\n",
      "epoch: 114,  batch step: 129, loss: 6.656344413757324\n",
      "epoch: 114,  batch step: 130, loss: 2.030986785888672\n",
      "epoch: 114,  batch step: 131, loss: 1.7774887084960938\n",
      "epoch: 114,  batch step: 132, loss: 9.924792289733887\n",
      "epoch: 114,  batch step: 133, loss: 16.134204864501953\n",
      "epoch: 114,  batch step: 134, loss: 3.8790712356567383\n",
      "epoch: 114,  batch step: 135, loss: 1.9569759368896484\n",
      "epoch: 114,  batch step: 136, loss: 18.171493530273438\n",
      "epoch: 114,  batch step: 137, loss: 9.502914428710938\n",
      "epoch: 114,  batch step: 138, loss: 2.89262056350708\n",
      "epoch: 114,  batch step: 139, loss: 2.0565948486328125\n",
      "epoch: 114,  batch step: 140, loss: 39.118743896484375\n",
      "epoch: 114,  batch step: 141, loss: 8.65220832824707\n",
      "epoch: 114,  batch step: 142, loss: 2.4181134700775146\n",
      "epoch: 114,  batch step: 143, loss: 53.471893310546875\n",
      "epoch: 114,  batch step: 144, loss: 12.24398422241211\n",
      "epoch: 114,  batch step: 145, loss: 2.972501754760742\n",
      "epoch: 114,  batch step: 146, loss: 4.904418468475342\n",
      "epoch: 114,  batch step: 147, loss: 3.447073459625244\n",
      "epoch: 114,  batch step: 148, loss: 11.573653221130371\n",
      "epoch: 114,  batch step: 149, loss: 1.5914558172225952\n",
      "epoch: 114,  batch step: 150, loss: 2.9269015789031982\n",
      "epoch: 114,  batch step: 151, loss: 11.465131759643555\n",
      "epoch: 114,  batch step: 152, loss: 7.3620524406433105\n",
      "epoch: 114,  batch step: 153, loss: 15.55783748626709\n",
      "epoch: 114,  batch step: 154, loss: 4.803998947143555\n",
      "epoch: 114,  batch step: 155, loss: 1.8917759656906128\n",
      "epoch: 114,  batch step: 156, loss: 4.272416114807129\n",
      "epoch: 114,  batch step: 157, loss: 19.079336166381836\n",
      "epoch: 114,  batch step: 158, loss: 40.41743850708008\n",
      "epoch: 114,  batch step: 159, loss: 2.0302209854125977\n",
      "epoch: 114,  batch step: 160, loss: 31.43024253845215\n",
      "epoch: 114,  batch step: 161, loss: 2.1280932426452637\n",
      "epoch: 114,  batch step: 162, loss: 2.104825496673584\n",
      "epoch: 114,  batch step: 163, loss: 6.58899450302124\n",
      "epoch: 114,  batch step: 164, loss: 2.4518277645111084\n",
      "epoch: 114,  batch step: 165, loss: 18.712646484375\n",
      "epoch: 114,  batch step: 166, loss: 2.2178564071655273\n",
      "epoch: 114,  batch step: 167, loss: 8.237906455993652\n",
      "epoch: 114,  batch step: 168, loss: 11.210271835327148\n",
      "epoch: 114,  batch step: 169, loss: 13.443110466003418\n",
      "epoch: 114,  batch step: 170, loss: 6.814306259155273\n",
      "epoch: 114,  batch step: 171, loss: 3.3770456314086914\n",
      "epoch: 114,  batch step: 172, loss: 1.5829856395721436\n",
      "epoch: 114,  batch step: 173, loss: 8.185006141662598\n",
      "epoch: 114,  batch step: 174, loss: 39.19489288330078\n",
      "epoch: 114,  batch step: 175, loss: 7.9499831199646\n",
      "epoch: 114,  batch step: 176, loss: 7.670203685760498\n",
      "epoch: 114,  batch step: 177, loss: 1.7153514623641968\n",
      "epoch: 114,  batch step: 178, loss: 9.484416007995605\n",
      "epoch: 114,  batch step: 179, loss: 3.112220525741577\n",
      "epoch: 114,  batch step: 180, loss: 16.78403663635254\n",
      "epoch: 114,  batch step: 181, loss: 4.568772315979004\n",
      "epoch: 114,  batch step: 182, loss: 2.4611947536468506\n",
      "epoch: 114,  batch step: 183, loss: 4.732883453369141\n",
      "epoch: 114,  batch step: 184, loss: 47.59562301635742\n",
      "epoch: 114,  batch step: 185, loss: 17.24138641357422\n",
      "epoch: 114,  batch step: 186, loss: 15.667032241821289\n",
      "epoch: 114,  batch step: 187, loss: 21.50478172302246\n",
      "epoch: 114,  batch step: 188, loss: 41.404842376708984\n",
      "epoch: 114,  batch step: 189, loss: 2.3301053047180176\n",
      "epoch: 114,  batch step: 190, loss: 48.91510772705078\n",
      "epoch: 114,  batch step: 191, loss: 3.34077525138855\n",
      "epoch: 114,  batch step: 192, loss: 17.736019134521484\n",
      "epoch: 114,  batch step: 193, loss: 10.492664337158203\n",
      "epoch: 114,  batch step: 194, loss: 2.7865424156188965\n",
      "epoch: 114,  batch step: 195, loss: 9.400354385375977\n",
      "epoch: 114,  batch step: 196, loss: 1.3614530563354492\n",
      "epoch: 114,  batch step: 197, loss: 4.07100772857666\n",
      "epoch: 114,  batch step: 198, loss: 1.6074427366256714\n",
      "epoch: 114,  batch step: 199, loss: 2.1179616451263428\n",
      "epoch: 114,  batch step: 200, loss: 2.8905036449432373\n",
      "epoch: 114,  batch step: 201, loss: 2.723581314086914\n",
      "epoch: 114,  batch step: 202, loss: 2.9127631187438965\n",
      "epoch: 114,  batch step: 203, loss: 51.52935791015625\n",
      "epoch: 114,  batch step: 204, loss: 8.133627891540527\n",
      "epoch: 114,  batch step: 205, loss: 11.303492546081543\n",
      "epoch: 114,  batch step: 206, loss: 1.9317703247070312\n",
      "epoch: 114,  batch step: 207, loss: 2.2342424392700195\n",
      "epoch: 114,  batch step: 208, loss: 2.8215365409851074\n",
      "epoch: 114,  batch step: 209, loss: 18.87833023071289\n",
      "epoch: 114,  batch step: 210, loss: 4.180342674255371\n",
      "epoch: 114,  batch step: 211, loss: 15.403769493103027\n",
      "epoch: 114,  batch step: 212, loss: 3.815215587615967\n",
      "epoch: 114,  batch step: 213, loss: 26.090763092041016\n",
      "epoch: 114,  batch step: 214, loss: 6.861620903015137\n",
      "epoch: 114,  batch step: 215, loss: 12.280173301696777\n",
      "epoch: 114,  batch step: 216, loss: 19.170503616333008\n",
      "epoch: 114,  batch step: 217, loss: 2.9382262229919434\n",
      "epoch: 114,  batch step: 218, loss: 1.8783349990844727\n",
      "epoch: 114,  batch step: 219, loss: 6.646810054779053\n",
      "epoch: 114,  batch step: 220, loss: 17.98556137084961\n",
      "epoch: 114,  batch step: 221, loss: 16.739639282226562\n",
      "epoch: 114,  batch step: 222, loss: 35.56000518798828\n",
      "epoch: 114,  batch step: 223, loss: 19.585643768310547\n",
      "epoch: 114,  batch step: 224, loss: 3.54506778717041\n",
      "epoch: 114,  batch step: 225, loss: 12.64969539642334\n",
      "epoch: 114,  batch step: 226, loss: 5.21696662902832\n",
      "epoch: 114,  batch step: 227, loss: 3.9189095497131348\n",
      "epoch: 114,  batch step: 228, loss: 15.388023376464844\n",
      "epoch: 114,  batch step: 229, loss: 69.55219268798828\n",
      "epoch: 114,  batch step: 230, loss: 19.145339965820312\n",
      "epoch: 114,  batch step: 231, loss: 31.219898223876953\n",
      "epoch: 114,  batch step: 232, loss: 19.14516258239746\n",
      "epoch: 114,  batch step: 233, loss: 5.142870903015137\n",
      "epoch: 114,  batch step: 234, loss: 134.6537322998047\n",
      "epoch: 114,  batch step: 235, loss: 3.396270275115967\n",
      "epoch: 114,  batch step: 236, loss: 22.80788803100586\n",
      "epoch: 114,  batch step: 237, loss: 50.37386703491211\n",
      "epoch: 114,  batch step: 238, loss: 3.8368098735809326\n",
      "epoch: 114,  batch step: 239, loss: 5.509803771972656\n",
      "epoch: 114,  batch step: 240, loss: 5.110629558563232\n",
      "epoch: 114,  batch step: 241, loss: 30.16754150390625\n",
      "epoch: 114,  batch step: 242, loss: 12.775642395019531\n",
      "epoch: 114,  batch step: 243, loss: 54.917564392089844\n",
      "epoch: 114,  batch step: 244, loss: 5.421555995941162\n",
      "epoch: 114,  batch step: 245, loss: 3.1101808547973633\n",
      "epoch: 114,  batch step: 246, loss: 5.633416652679443\n",
      "epoch: 114,  batch step: 247, loss: 4.270118713378906\n",
      "epoch: 114,  batch step: 248, loss: 24.67731475830078\n",
      "epoch: 114,  batch step: 249, loss: 3.7816519737243652\n",
      "epoch: 114,  batch step: 250, loss: 13.992746353149414\n",
      "epoch: 114,  batch step: 251, loss: 78.93316650390625\n",
      "validation error epoch  114:    tensor(77.0313, device='cuda:0')\n",
      "316\n",
      "epoch: 115,  batch step: 0, loss: 11.581417083740234\n",
      "epoch: 115,  batch step: 1, loss: 48.5938720703125\n",
      "epoch: 115,  batch step: 2, loss: 18.831579208374023\n",
      "epoch: 115,  batch step: 3, loss: 11.486318588256836\n",
      "epoch: 115,  batch step: 4, loss: 22.72085189819336\n",
      "epoch: 115,  batch step: 5, loss: 20.309947967529297\n",
      "epoch: 115,  batch step: 6, loss: 6.216348171234131\n",
      "epoch: 115,  batch step: 7, loss: 4.03227424621582\n",
      "epoch: 115,  batch step: 8, loss: 17.401622772216797\n",
      "epoch: 115,  batch step: 9, loss: 12.637428283691406\n",
      "epoch: 115,  batch step: 10, loss: 20.952354431152344\n",
      "epoch: 115,  batch step: 11, loss: 9.925889015197754\n",
      "epoch: 115,  batch step: 12, loss: 20.15616226196289\n",
      "epoch: 115,  batch step: 13, loss: 13.850170135498047\n",
      "epoch: 115,  batch step: 14, loss: 29.929349899291992\n",
      "epoch: 115,  batch step: 15, loss: 3.141230344772339\n",
      "epoch: 115,  batch step: 16, loss: 159.079345703125\n",
      "epoch: 115,  batch step: 17, loss: 30.518951416015625\n",
      "epoch: 115,  batch step: 18, loss: 4.4722747802734375\n",
      "epoch: 115,  batch step: 19, loss: 5.44284200668335\n",
      "epoch: 115,  batch step: 20, loss: 12.074827194213867\n",
      "epoch: 115,  batch step: 21, loss: 14.399858474731445\n",
      "epoch: 115,  batch step: 22, loss: 4.607846736907959\n",
      "epoch: 115,  batch step: 23, loss: 4.886083602905273\n",
      "epoch: 115,  batch step: 24, loss: 2.894233465194702\n",
      "epoch: 115,  batch step: 25, loss: 4.135347843170166\n",
      "epoch: 115,  batch step: 26, loss: 3.3427557945251465\n",
      "epoch: 115,  batch step: 27, loss: 29.770566940307617\n",
      "epoch: 115,  batch step: 28, loss: 2.686143398284912\n",
      "epoch: 115,  batch step: 29, loss: 3.075777053833008\n",
      "epoch: 115,  batch step: 30, loss: 5.149450778961182\n",
      "epoch: 115,  batch step: 31, loss: 2.6660375595092773\n",
      "epoch: 115,  batch step: 32, loss: 45.08637237548828\n",
      "epoch: 115,  batch step: 33, loss: 2.534240245819092\n",
      "epoch: 115,  batch step: 34, loss: 16.822702407836914\n",
      "epoch: 115,  batch step: 35, loss: 6.759997367858887\n",
      "epoch: 115,  batch step: 36, loss: 2.331632137298584\n",
      "epoch: 115,  batch step: 37, loss: 2.395650863647461\n",
      "epoch: 115,  batch step: 38, loss: 2.6224989891052246\n",
      "epoch: 115,  batch step: 39, loss: 2.7944045066833496\n",
      "epoch: 115,  batch step: 40, loss: 7.065508842468262\n",
      "epoch: 115,  batch step: 41, loss: 51.172889709472656\n",
      "epoch: 115,  batch step: 42, loss: 4.6111159324646\n",
      "epoch: 115,  batch step: 43, loss: 2.7518234252929688\n",
      "epoch: 115,  batch step: 44, loss: 7.32528829574585\n",
      "epoch: 115,  batch step: 45, loss: 23.23525047302246\n",
      "epoch: 115,  batch step: 46, loss: 24.915481567382812\n",
      "epoch: 115,  batch step: 47, loss: 2.3589987754821777\n",
      "epoch: 115,  batch step: 48, loss: 26.094425201416016\n",
      "epoch: 115,  batch step: 49, loss: 46.67429733276367\n",
      "epoch: 115,  batch step: 50, loss: 3.3149526119232178\n",
      "epoch: 115,  batch step: 51, loss: 10.307483673095703\n",
      "epoch: 115,  batch step: 52, loss: 28.744558334350586\n",
      "epoch: 115,  batch step: 53, loss: 3.893159866333008\n",
      "epoch: 115,  batch step: 54, loss: 9.851530075073242\n",
      "epoch: 115,  batch step: 55, loss: 4.5756330490112305\n",
      "epoch: 115,  batch step: 56, loss: 13.52955436706543\n",
      "epoch: 115,  batch step: 57, loss: 9.795868873596191\n",
      "epoch: 115,  batch step: 58, loss: 10.93299388885498\n",
      "epoch: 115,  batch step: 59, loss: 10.599660873413086\n",
      "epoch: 115,  batch step: 60, loss: 16.434772491455078\n",
      "epoch: 115,  batch step: 61, loss: 3.238712787628174\n",
      "epoch: 115,  batch step: 62, loss: 11.04910659790039\n",
      "epoch: 115,  batch step: 63, loss: 19.928199768066406\n",
      "epoch: 115,  batch step: 64, loss: 28.918987274169922\n",
      "epoch: 115,  batch step: 65, loss: 3.129857063293457\n",
      "epoch: 115,  batch step: 66, loss: 24.51498794555664\n",
      "epoch: 115,  batch step: 67, loss: 2.247936725616455\n",
      "epoch: 115,  batch step: 68, loss: 16.994503021240234\n",
      "epoch: 115,  batch step: 69, loss: 22.76364517211914\n",
      "epoch: 115,  batch step: 70, loss: 15.029521942138672\n",
      "epoch: 115,  batch step: 71, loss: 18.610687255859375\n",
      "epoch: 115,  batch step: 72, loss: 2.820890426635742\n",
      "epoch: 115,  batch step: 73, loss: 2.618605852127075\n",
      "epoch: 115,  batch step: 74, loss: 2.4892752170562744\n",
      "epoch: 115,  batch step: 75, loss: 2.696394681930542\n",
      "epoch: 115,  batch step: 76, loss: 3.3458163738250732\n",
      "epoch: 115,  batch step: 77, loss: 31.763526916503906\n",
      "epoch: 115,  batch step: 78, loss: 4.6785569190979\n",
      "epoch: 115,  batch step: 79, loss: 2.991137981414795\n",
      "epoch: 115,  batch step: 80, loss: 8.141651153564453\n",
      "epoch: 115,  batch step: 81, loss: 25.948360443115234\n",
      "epoch: 115,  batch step: 82, loss: 2.043694257736206\n",
      "epoch: 115,  batch step: 83, loss: 15.23267650604248\n",
      "epoch: 115,  batch step: 84, loss: 50.55101776123047\n",
      "epoch: 115,  batch step: 85, loss: 6.587785243988037\n",
      "epoch: 115,  batch step: 86, loss: 5.801995754241943\n",
      "epoch: 115,  batch step: 87, loss: 3.060645818710327\n",
      "epoch: 115,  batch step: 88, loss: 12.9256591796875\n",
      "epoch: 115,  batch step: 89, loss: 33.08560562133789\n",
      "epoch: 115,  batch step: 90, loss: 3.8233132362365723\n",
      "epoch: 115,  batch step: 91, loss: 3.1993188858032227\n",
      "epoch: 115,  batch step: 92, loss: 2.0870745182037354\n",
      "epoch: 115,  batch step: 93, loss: 8.087003707885742\n",
      "epoch: 115,  batch step: 94, loss: 3.19868803024292\n",
      "epoch: 115,  batch step: 95, loss: 2.6019856929779053\n",
      "epoch: 115,  batch step: 96, loss: 4.868655204772949\n",
      "epoch: 115,  batch step: 97, loss: 2.1344223022460938\n",
      "epoch: 115,  batch step: 98, loss: 51.963409423828125\n",
      "epoch: 115,  batch step: 99, loss: 38.8554801940918\n",
      "epoch: 115,  batch step: 100, loss: 5.88486385345459\n",
      "epoch: 115,  batch step: 101, loss: 38.61946487426758\n",
      "epoch: 115,  batch step: 102, loss: 13.612590789794922\n",
      "epoch: 115,  batch step: 103, loss: 8.300399780273438\n",
      "epoch: 115,  batch step: 104, loss: 5.743661403656006\n",
      "epoch: 115,  batch step: 105, loss: 1.7687883377075195\n",
      "epoch: 115,  batch step: 106, loss: 4.033331871032715\n",
      "epoch: 115,  batch step: 107, loss: 4.195427894592285\n",
      "epoch: 115,  batch step: 108, loss: 1.9104669094085693\n",
      "epoch: 115,  batch step: 109, loss: 29.50646209716797\n",
      "epoch: 115,  batch step: 110, loss: 2.8152854442596436\n",
      "epoch: 115,  batch step: 111, loss: 47.94537353515625\n",
      "epoch: 115,  batch step: 112, loss: 3.6364781856536865\n",
      "epoch: 115,  batch step: 113, loss: 4.642871856689453\n",
      "epoch: 115,  batch step: 114, loss: 3.0693509578704834\n",
      "epoch: 115,  batch step: 115, loss: 5.234316825866699\n",
      "epoch: 115,  batch step: 116, loss: 12.526735305786133\n",
      "epoch: 115,  batch step: 117, loss: 1.7406620979309082\n",
      "epoch: 115,  batch step: 118, loss: 13.490655899047852\n",
      "epoch: 115,  batch step: 119, loss: 36.31104278564453\n",
      "epoch: 115,  batch step: 120, loss: 68.93203735351562\n",
      "epoch: 115,  batch step: 121, loss: 34.11090087890625\n",
      "epoch: 115,  batch step: 122, loss: 2.525454521179199\n",
      "epoch: 115,  batch step: 123, loss: 29.480552673339844\n",
      "epoch: 115,  batch step: 124, loss: 41.59156036376953\n",
      "epoch: 115,  batch step: 125, loss: 2.712548017501831\n",
      "epoch: 115,  batch step: 126, loss: 11.18536376953125\n",
      "epoch: 115,  batch step: 127, loss: 15.883509635925293\n",
      "epoch: 115,  batch step: 128, loss: 15.292303085327148\n",
      "epoch: 115,  batch step: 129, loss: 52.20347213745117\n",
      "epoch: 115,  batch step: 130, loss: 64.90630340576172\n",
      "epoch: 115,  batch step: 131, loss: 12.434423446655273\n",
      "epoch: 115,  batch step: 132, loss: 11.79994010925293\n",
      "epoch: 115,  batch step: 133, loss: 56.06745147705078\n",
      "epoch: 115,  batch step: 134, loss: 12.635601043701172\n",
      "epoch: 115,  batch step: 135, loss: 57.05509567260742\n",
      "epoch: 115,  batch step: 136, loss: 42.68517303466797\n",
      "epoch: 115,  batch step: 137, loss: 18.903766632080078\n",
      "epoch: 115,  batch step: 138, loss: 23.983144760131836\n",
      "epoch: 115,  batch step: 139, loss: 4.920534133911133\n",
      "epoch: 115,  batch step: 140, loss: 17.040678024291992\n",
      "epoch: 115,  batch step: 141, loss: 4.52822208404541\n",
      "epoch: 115,  batch step: 142, loss: 4.599191188812256\n",
      "epoch: 115,  batch step: 143, loss: 23.050872802734375\n",
      "epoch: 115,  batch step: 144, loss: 3.485642194747925\n",
      "epoch: 115,  batch step: 145, loss: 146.2433319091797\n",
      "epoch: 115,  batch step: 146, loss: 4.76874303817749\n",
      "epoch: 115,  batch step: 147, loss: 15.007670402526855\n",
      "epoch: 115,  batch step: 148, loss: 18.208858489990234\n",
      "epoch: 115,  batch step: 149, loss: 6.030357360839844\n",
      "epoch: 115,  batch step: 150, loss: 25.66523551940918\n",
      "epoch: 115,  batch step: 151, loss: 12.77717399597168\n",
      "epoch: 115,  batch step: 152, loss: 54.024932861328125\n",
      "epoch: 115,  batch step: 153, loss: 7.784663200378418\n",
      "epoch: 115,  batch step: 154, loss: 3.9798028469085693\n",
      "epoch: 115,  batch step: 155, loss: 3.892293691635132\n",
      "epoch: 115,  batch step: 156, loss: 12.017931938171387\n",
      "epoch: 115,  batch step: 157, loss: 42.18058395385742\n",
      "epoch: 115,  batch step: 158, loss: 6.083896160125732\n",
      "epoch: 115,  batch step: 159, loss: 11.112659454345703\n",
      "epoch: 115,  batch step: 160, loss: 20.226091384887695\n",
      "epoch: 115,  batch step: 161, loss: 5.140506267547607\n",
      "epoch: 115,  batch step: 162, loss: 9.868906021118164\n",
      "epoch: 115,  batch step: 163, loss: 2.9749016761779785\n",
      "epoch: 115,  batch step: 164, loss: 3.7599267959594727\n",
      "epoch: 115,  batch step: 165, loss: 25.835521697998047\n",
      "epoch: 115,  batch step: 166, loss: 10.225385665893555\n",
      "epoch: 115,  batch step: 167, loss: 31.962173461914062\n",
      "epoch: 115,  batch step: 168, loss: 6.015961647033691\n",
      "epoch: 115,  batch step: 169, loss: 82.77491760253906\n",
      "epoch: 115,  batch step: 170, loss: 6.872981071472168\n",
      "epoch: 115,  batch step: 171, loss: 21.951885223388672\n",
      "epoch: 115,  batch step: 172, loss: 3.7690277099609375\n",
      "epoch: 115,  batch step: 173, loss: 20.853124618530273\n",
      "epoch: 115,  batch step: 174, loss: 4.379543304443359\n",
      "epoch: 115,  batch step: 175, loss: 5.624051570892334\n",
      "epoch: 115,  batch step: 176, loss: 9.583174705505371\n",
      "epoch: 115,  batch step: 177, loss: 68.22811889648438\n",
      "epoch: 115,  batch step: 178, loss: 16.17355728149414\n",
      "epoch: 115,  batch step: 179, loss: 9.439123153686523\n",
      "epoch: 115,  batch step: 180, loss: 27.8515682220459\n",
      "epoch: 115,  batch step: 181, loss: 4.960339546203613\n",
      "epoch: 115,  batch step: 182, loss: 5.488753318786621\n",
      "epoch: 115,  batch step: 183, loss: 18.218868255615234\n",
      "epoch: 115,  batch step: 184, loss: 6.38705587387085\n",
      "epoch: 115,  batch step: 185, loss: 50.42026901245117\n",
      "epoch: 115,  batch step: 186, loss: 50.55229949951172\n",
      "epoch: 115,  batch step: 187, loss: 5.338151454925537\n",
      "epoch: 115,  batch step: 188, loss: 4.40692138671875\n",
      "epoch: 115,  batch step: 189, loss: 13.191652297973633\n",
      "epoch: 115,  batch step: 190, loss: 3.5000557899475098\n",
      "epoch: 115,  batch step: 191, loss: 26.901777267456055\n",
      "epoch: 115,  batch step: 192, loss: 4.521877288818359\n",
      "epoch: 115,  batch step: 193, loss: 86.14629364013672\n",
      "epoch: 115,  batch step: 194, loss: 13.020434379577637\n",
      "epoch: 115,  batch step: 195, loss: 51.651573181152344\n",
      "epoch: 115,  batch step: 196, loss: 21.261669158935547\n",
      "epoch: 115,  batch step: 197, loss: 24.724096298217773\n",
      "epoch: 115,  batch step: 198, loss: 51.68528747558594\n",
      "epoch: 115,  batch step: 199, loss: 14.814523696899414\n",
      "epoch: 115,  batch step: 200, loss: 13.26291275024414\n",
      "epoch: 115,  batch step: 201, loss: 15.255607604980469\n",
      "epoch: 115,  batch step: 202, loss: 6.596581935882568\n",
      "epoch: 115,  batch step: 203, loss: 2.8093554973602295\n",
      "epoch: 115,  batch step: 204, loss: 48.43583297729492\n",
      "epoch: 115,  batch step: 205, loss: 2.3311386108398438\n",
      "epoch: 115,  batch step: 206, loss: 34.297706604003906\n",
      "epoch: 115,  batch step: 207, loss: 6.161327362060547\n",
      "epoch: 115,  batch step: 208, loss: 33.459781646728516\n",
      "epoch: 115,  batch step: 209, loss: 48.590187072753906\n",
      "epoch: 115,  batch step: 210, loss: 3.2024686336517334\n",
      "epoch: 115,  batch step: 211, loss: 4.00046968460083\n",
      "epoch: 115,  batch step: 212, loss: 4.5109381675720215\n",
      "epoch: 115,  batch step: 213, loss: 7.764596939086914\n",
      "epoch: 115,  batch step: 214, loss: 2.4026036262512207\n",
      "epoch: 115,  batch step: 215, loss: 52.03229522705078\n",
      "epoch: 115,  batch step: 216, loss: 11.535947799682617\n",
      "epoch: 115,  batch step: 217, loss: 10.00102424621582\n",
      "epoch: 115,  batch step: 218, loss: 2.7718491554260254\n",
      "epoch: 115,  batch step: 219, loss: 2.2611114978790283\n",
      "epoch: 115,  batch step: 220, loss: 3.131499767303467\n",
      "epoch: 115,  batch step: 221, loss: 15.213676452636719\n",
      "epoch: 115,  batch step: 222, loss: 4.653049468994141\n",
      "epoch: 115,  batch step: 223, loss: 9.967144012451172\n",
      "epoch: 115,  batch step: 224, loss: 6.0754289627075195\n",
      "epoch: 115,  batch step: 225, loss: 33.22083282470703\n",
      "epoch: 115,  batch step: 226, loss: 13.9994535446167\n",
      "epoch: 115,  batch step: 227, loss: 5.145730495452881\n",
      "epoch: 115,  batch step: 228, loss: 3.1171693801879883\n",
      "epoch: 115,  batch step: 229, loss: 3.2433581352233887\n",
      "epoch: 115,  batch step: 230, loss: 14.018805503845215\n",
      "epoch: 115,  batch step: 231, loss: 15.974696159362793\n",
      "epoch: 115,  batch step: 232, loss: 3.177555799484253\n",
      "epoch: 115,  batch step: 233, loss: 3.4237895011901855\n",
      "epoch: 115,  batch step: 234, loss: 4.074887752532959\n",
      "epoch: 115,  batch step: 235, loss: 36.672115325927734\n",
      "epoch: 115,  batch step: 236, loss: 9.887613296508789\n",
      "epoch: 115,  batch step: 237, loss: 1.9267653226852417\n",
      "epoch: 115,  batch step: 238, loss: 16.263351440429688\n",
      "epoch: 115,  batch step: 239, loss: 29.082555770874023\n",
      "epoch: 115,  batch step: 240, loss: 55.884910583496094\n",
      "epoch: 115,  batch step: 241, loss: 4.2220354080200195\n",
      "epoch: 115,  batch step: 242, loss: 22.9312744140625\n",
      "epoch: 115,  batch step: 243, loss: 31.151905059814453\n",
      "epoch: 115,  batch step: 244, loss: 2.4001259803771973\n",
      "epoch: 115,  batch step: 245, loss: 3.3292014598846436\n",
      "epoch: 115,  batch step: 246, loss: 6.107816696166992\n",
      "epoch: 115,  batch step: 247, loss: 1.9111547470092773\n",
      "epoch: 115,  batch step: 248, loss: 3.2569055557250977\n",
      "epoch: 115,  batch step: 249, loss: 18.638280868530273\n",
      "epoch: 115,  batch step: 250, loss: 44.54646301269531\n",
      "epoch: 115,  batch step: 251, loss: 5.663774013519287\n",
      "validation error epoch  115:    tensor(69.8595, device='cuda:0')\n",
      "316\n",
      "epoch: 116,  batch step: 0, loss: 32.630584716796875\n",
      "epoch: 116,  batch step: 1, loss: 9.064475059509277\n",
      "epoch: 116,  batch step: 2, loss: 23.678070068359375\n",
      "epoch: 116,  batch step: 3, loss: 3.3379759788513184\n",
      "epoch: 116,  batch step: 4, loss: 1.9275907278060913\n",
      "epoch: 116,  batch step: 5, loss: 42.7403564453125\n",
      "epoch: 116,  batch step: 6, loss: 3.061433792114258\n",
      "epoch: 116,  batch step: 7, loss: 3.3269810676574707\n",
      "epoch: 116,  batch step: 8, loss: 15.99687385559082\n",
      "epoch: 116,  batch step: 9, loss: 2.512498617172241\n",
      "epoch: 116,  batch step: 10, loss: 15.536149978637695\n",
      "epoch: 116,  batch step: 11, loss: 4.354585647583008\n",
      "epoch: 116,  batch step: 12, loss: 13.840770721435547\n",
      "epoch: 116,  batch step: 13, loss: 8.5071439743042\n",
      "epoch: 116,  batch step: 14, loss: 7.649013519287109\n",
      "epoch: 116,  batch step: 15, loss: 2.7726173400878906\n",
      "epoch: 116,  batch step: 16, loss: 15.946906089782715\n",
      "epoch: 116,  batch step: 17, loss: 18.585830688476562\n",
      "epoch: 116,  batch step: 18, loss: 3.4448153972625732\n",
      "epoch: 116,  batch step: 19, loss: 11.261308670043945\n",
      "epoch: 116,  batch step: 20, loss: 51.108367919921875\n",
      "epoch: 116,  batch step: 21, loss: 3.2062602043151855\n",
      "epoch: 116,  batch step: 22, loss: 10.104087829589844\n",
      "epoch: 116,  batch step: 23, loss: 8.994985580444336\n",
      "epoch: 116,  batch step: 24, loss: 3.4501004219055176\n",
      "epoch: 116,  batch step: 25, loss: 6.004931449890137\n",
      "epoch: 116,  batch step: 26, loss: 15.192794799804688\n",
      "epoch: 116,  batch step: 27, loss: 2.4737861156463623\n",
      "epoch: 116,  batch step: 28, loss: 2.984851360321045\n",
      "epoch: 116,  batch step: 29, loss: 3.031998634338379\n",
      "epoch: 116,  batch step: 30, loss: 8.052764892578125\n",
      "epoch: 116,  batch step: 31, loss: 1.8367984294891357\n",
      "epoch: 116,  batch step: 32, loss: 7.2658538818359375\n",
      "epoch: 116,  batch step: 33, loss: 4.545452117919922\n",
      "epoch: 116,  batch step: 34, loss: 74.21990966796875\n",
      "epoch: 116,  batch step: 35, loss: 4.5649213790893555\n",
      "epoch: 116,  batch step: 36, loss: 10.357787132263184\n",
      "epoch: 116,  batch step: 37, loss: 15.364381790161133\n",
      "epoch: 116,  batch step: 38, loss: 1.992424488067627\n",
      "epoch: 116,  batch step: 39, loss: 2.716287136077881\n",
      "epoch: 116,  batch step: 40, loss: 3.292818784713745\n",
      "epoch: 116,  batch step: 41, loss: 30.284191131591797\n",
      "epoch: 116,  batch step: 42, loss: 39.00331115722656\n",
      "epoch: 116,  batch step: 43, loss: 10.064167022705078\n",
      "epoch: 116,  batch step: 44, loss: 3.407116651535034\n",
      "epoch: 116,  batch step: 45, loss: 13.376093864440918\n",
      "epoch: 116,  batch step: 46, loss: 8.03404712677002\n",
      "epoch: 116,  batch step: 47, loss: 46.021812438964844\n",
      "epoch: 116,  batch step: 48, loss: 15.273173332214355\n",
      "epoch: 116,  batch step: 49, loss: 5.53601598739624\n",
      "epoch: 116,  batch step: 50, loss: 6.194944381713867\n",
      "epoch: 116,  batch step: 51, loss: 3.869584560394287\n",
      "epoch: 116,  batch step: 52, loss: 30.120580673217773\n",
      "epoch: 116,  batch step: 53, loss: 3.2130839824676514\n",
      "epoch: 116,  batch step: 54, loss: 10.45908260345459\n",
      "epoch: 116,  batch step: 55, loss: 18.3060359954834\n",
      "epoch: 116,  batch step: 56, loss: 48.27887725830078\n",
      "epoch: 116,  batch step: 57, loss: 40.8865852355957\n",
      "epoch: 116,  batch step: 58, loss: 3.3228566646575928\n",
      "epoch: 116,  batch step: 59, loss: 7.072756767272949\n",
      "epoch: 116,  batch step: 60, loss: 2.164276361465454\n",
      "epoch: 116,  batch step: 61, loss: 21.82925796508789\n",
      "epoch: 116,  batch step: 62, loss: 6.152547359466553\n",
      "epoch: 116,  batch step: 63, loss: 132.74609375\n",
      "epoch: 116,  batch step: 64, loss: 57.68614959716797\n",
      "epoch: 116,  batch step: 65, loss: 23.725675582885742\n",
      "epoch: 116,  batch step: 66, loss: 107.83023071289062\n",
      "epoch: 116,  batch step: 67, loss: 76.60466003417969\n",
      "epoch: 116,  batch step: 68, loss: 59.62129211425781\n",
      "epoch: 116,  batch step: 69, loss: 15.916767120361328\n",
      "epoch: 116,  batch step: 70, loss: 7.702444076538086\n",
      "epoch: 116,  batch step: 71, loss: 91.57530212402344\n",
      "epoch: 116,  batch step: 72, loss: 11.077798843383789\n",
      "epoch: 116,  batch step: 73, loss: 2.7870421409606934\n",
      "epoch: 116,  batch step: 74, loss: 22.428733825683594\n",
      "epoch: 116,  batch step: 75, loss: 19.790721893310547\n",
      "epoch: 116,  batch step: 76, loss: 14.877253532409668\n",
      "epoch: 116,  batch step: 77, loss: 18.518756866455078\n",
      "epoch: 116,  batch step: 78, loss: 10.099422454833984\n",
      "epoch: 116,  batch step: 79, loss: 21.916202545166016\n",
      "epoch: 116,  batch step: 80, loss: 2.6013755798339844\n",
      "epoch: 116,  batch step: 81, loss: 4.187634468078613\n",
      "epoch: 116,  batch step: 82, loss: 21.779064178466797\n",
      "epoch: 116,  batch step: 83, loss: 4.03225040435791\n",
      "epoch: 116,  batch step: 84, loss: 11.511663436889648\n",
      "epoch: 116,  batch step: 85, loss: 4.587935447692871\n",
      "epoch: 116,  batch step: 86, loss: 2.208050489425659\n",
      "epoch: 116,  batch step: 87, loss: 12.727418899536133\n",
      "epoch: 116,  batch step: 88, loss: 3.1662516593933105\n",
      "epoch: 116,  batch step: 89, loss: 13.824068069458008\n",
      "epoch: 116,  batch step: 90, loss: 5.391442775726318\n",
      "epoch: 116,  batch step: 91, loss: 30.43796157836914\n",
      "epoch: 116,  batch step: 92, loss: 3.7718210220336914\n",
      "epoch: 116,  batch step: 93, loss: 2.0889806747436523\n",
      "epoch: 116,  batch step: 94, loss: 15.463109970092773\n",
      "epoch: 116,  batch step: 95, loss: 3.0836665630340576\n",
      "epoch: 116,  batch step: 96, loss: 20.61574363708496\n",
      "epoch: 116,  batch step: 97, loss: 3.841618537902832\n",
      "epoch: 116,  batch step: 98, loss: 31.578121185302734\n",
      "epoch: 116,  batch step: 99, loss: 6.943936824798584\n",
      "epoch: 116,  batch step: 100, loss: 3.202458143234253\n",
      "epoch: 116,  batch step: 101, loss: 2.244828701019287\n",
      "epoch: 116,  batch step: 102, loss: 13.903834342956543\n",
      "epoch: 116,  batch step: 103, loss: 4.408952713012695\n",
      "epoch: 116,  batch step: 104, loss: 2.391686201095581\n",
      "epoch: 116,  batch step: 105, loss: 7.617809295654297\n",
      "epoch: 116,  batch step: 106, loss: 2.4181125164031982\n",
      "epoch: 116,  batch step: 107, loss: 2.1757078170776367\n",
      "epoch: 116,  batch step: 108, loss: 24.958145141601562\n",
      "epoch: 116,  batch step: 109, loss: 2.2031126022338867\n",
      "epoch: 116,  batch step: 110, loss: 9.997715950012207\n",
      "epoch: 116,  batch step: 111, loss: 3.509768009185791\n",
      "epoch: 116,  batch step: 112, loss: 4.203402519226074\n",
      "epoch: 116,  batch step: 113, loss: 24.76936149597168\n",
      "epoch: 116,  batch step: 114, loss: 1.4992917776107788\n",
      "epoch: 116,  batch step: 115, loss: 9.040776252746582\n",
      "epoch: 116,  batch step: 116, loss: 17.559490203857422\n",
      "epoch: 116,  batch step: 117, loss: 20.82501220703125\n",
      "epoch: 116,  batch step: 118, loss: 3.2435317039489746\n",
      "epoch: 116,  batch step: 119, loss: 5.148934364318848\n",
      "epoch: 116,  batch step: 120, loss: 10.237868309020996\n",
      "epoch: 116,  batch step: 121, loss: 3.7331957817077637\n",
      "epoch: 116,  batch step: 122, loss: 4.585162162780762\n",
      "epoch: 116,  batch step: 123, loss: 2.6917476654052734\n",
      "epoch: 116,  batch step: 124, loss: 1.9674205780029297\n",
      "epoch: 116,  batch step: 125, loss: 8.834897994995117\n",
      "epoch: 116,  batch step: 126, loss: 30.848445892333984\n",
      "epoch: 116,  batch step: 127, loss: 16.46013641357422\n",
      "epoch: 116,  batch step: 128, loss: 11.998800277709961\n",
      "epoch: 116,  batch step: 129, loss: 2.5904626846313477\n",
      "epoch: 116,  batch step: 130, loss: 4.876092910766602\n",
      "epoch: 116,  batch step: 131, loss: 3.7642581462860107\n",
      "epoch: 116,  batch step: 132, loss: 14.861978530883789\n",
      "epoch: 116,  batch step: 133, loss: 5.135185241699219\n",
      "epoch: 116,  batch step: 134, loss: 13.903654098510742\n",
      "epoch: 116,  batch step: 135, loss: 2.4622600078582764\n",
      "epoch: 116,  batch step: 136, loss: 27.113086700439453\n",
      "epoch: 116,  batch step: 137, loss: 20.803138732910156\n",
      "epoch: 116,  batch step: 138, loss: 15.969717025756836\n",
      "epoch: 116,  batch step: 139, loss: 11.436591148376465\n",
      "epoch: 116,  batch step: 140, loss: 4.454717636108398\n",
      "epoch: 116,  batch step: 141, loss: 30.85712242126465\n",
      "epoch: 116,  batch step: 142, loss: 2.248441219329834\n",
      "epoch: 116,  batch step: 143, loss: 11.917445182800293\n",
      "epoch: 116,  batch step: 144, loss: 6.572999477386475\n",
      "epoch: 116,  batch step: 145, loss: 8.81407356262207\n",
      "epoch: 116,  batch step: 146, loss: 2.752686023712158\n",
      "epoch: 116,  batch step: 147, loss: 9.792664527893066\n",
      "epoch: 116,  batch step: 148, loss: 7.783374786376953\n",
      "epoch: 116,  batch step: 149, loss: 15.429763793945312\n",
      "epoch: 116,  batch step: 150, loss: 4.476103782653809\n",
      "epoch: 116,  batch step: 151, loss: 20.44894027709961\n",
      "epoch: 116,  batch step: 152, loss: 15.200900077819824\n",
      "epoch: 116,  batch step: 153, loss: 7.711592674255371\n",
      "epoch: 116,  batch step: 154, loss: 2.0212666988372803\n",
      "epoch: 116,  batch step: 155, loss: 6.800801753997803\n",
      "epoch: 116,  batch step: 156, loss: 9.804580688476562\n",
      "epoch: 116,  batch step: 157, loss: 8.47100830078125\n",
      "epoch: 116,  batch step: 158, loss: 1.991331934928894\n",
      "epoch: 116,  batch step: 159, loss: 5.801759243011475\n",
      "epoch: 116,  batch step: 160, loss: 13.06226921081543\n",
      "epoch: 116,  batch step: 161, loss: 2.3048312664031982\n",
      "epoch: 116,  batch step: 162, loss: 2.1248435974121094\n",
      "epoch: 116,  batch step: 163, loss: 4.3367719650268555\n",
      "epoch: 116,  batch step: 164, loss: 8.249308586120605\n",
      "epoch: 116,  batch step: 165, loss: 53.93811798095703\n",
      "epoch: 116,  batch step: 166, loss: 2.383859395980835\n",
      "epoch: 116,  batch step: 167, loss: 5.02970027923584\n",
      "epoch: 116,  batch step: 168, loss: 10.767134666442871\n",
      "epoch: 116,  batch step: 169, loss: 3.0466814041137695\n",
      "epoch: 116,  batch step: 170, loss: 2.096046209335327\n",
      "epoch: 116,  batch step: 171, loss: 2.679508924484253\n",
      "epoch: 116,  batch step: 172, loss: 2.41886043548584\n",
      "epoch: 116,  batch step: 173, loss: 12.463523864746094\n",
      "epoch: 116,  batch step: 174, loss: 3.728271484375\n",
      "epoch: 116,  batch step: 175, loss: 21.23051643371582\n",
      "epoch: 116,  batch step: 176, loss: 7.973670959472656\n",
      "epoch: 116,  batch step: 177, loss: 29.17544937133789\n",
      "epoch: 116,  batch step: 178, loss: 13.330989837646484\n",
      "epoch: 116,  batch step: 179, loss: 2.2293965816497803\n",
      "epoch: 116,  batch step: 180, loss: 10.216163635253906\n",
      "epoch: 116,  batch step: 181, loss: 20.872364044189453\n",
      "epoch: 116,  batch step: 182, loss: 1.7626152038574219\n",
      "epoch: 116,  batch step: 183, loss: 17.911409378051758\n",
      "epoch: 116,  batch step: 184, loss: 2.6457457542419434\n",
      "epoch: 116,  batch step: 185, loss: 52.921993255615234\n",
      "epoch: 116,  batch step: 186, loss: 4.026279449462891\n",
      "epoch: 116,  batch step: 187, loss: 30.595226287841797\n",
      "epoch: 116,  batch step: 188, loss: 23.66799545288086\n",
      "epoch: 116,  batch step: 189, loss: 55.693763732910156\n",
      "epoch: 116,  batch step: 190, loss: 12.285539627075195\n",
      "epoch: 116,  batch step: 191, loss: 11.430749893188477\n",
      "epoch: 116,  batch step: 192, loss: 13.565993309020996\n",
      "epoch: 116,  batch step: 193, loss: 11.073966979980469\n",
      "epoch: 116,  batch step: 194, loss: 4.2669525146484375\n",
      "epoch: 116,  batch step: 195, loss: 2.329407215118408\n",
      "epoch: 116,  batch step: 196, loss: 2.3394651412963867\n",
      "epoch: 116,  batch step: 197, loss: 2.937246322631836\n",
      "epoch: 116,  batch step: 198, loss: 1.5745444297790527\n",
      "epoch: 116,  batch step: 199, loss: 53.55137252807617\n",
      "epoch: 116,  batch step: 200, loss: 17.188405990600586\n",
      "epoch: 116,  batch step: 201, loss: 15.85540771484375\n",
      "epoch: 116,  batch step: 202, loss: 1.8521236181259155\n",
      "epoch: 116,  batch step: 203, loss: 13.397863388061523\n",
      "epoch: 116,  batch step: 204, loss: 5.147064685821533\n",
      "epoch: 116,  batch step: 205, loss: 1.8551161289215088\n",
      "epoch: 116,  batch step: 206, loss: 7.4001264572143555\n",
      "epoch: 116,  batch step: 207, loss: 1.9065132141113281\n",
      "epoch: 116,  batch step: 208, loss: 5.132538795471191\n",
      "epoch: 116,  batch step: 209, loss: 2.693136215209961\n",
      "epoch: 116,  batch step: 210, loss: 2.1991219520568848\n",
      "epoch: 116,  batch step: 211, loss: 3.097123146057129\n",
      "epoch: 116,  batch step: 212, loss: 11.552377700805664\n",
      "epoch: 116,  batch step: 213, loss: 40.97433853149414\n",
      "epoch: 116,  batch step: 214, loss: 9.908689498901367\n",
      "epoch: 116,  batch step: 215, loss: 1.5301965475082397\n",
      "epoch: 116,  batch step: 216, loss: 11.523338317871094\n",
      "epoch: 116,  batch step: 217, loss: 9.65999984741211\n",
      "epoch: 116,  batch step: 218, loss: 9.660322189331055\n",
      "epoch: 116,  batch step: 219, loss: 2.6829745769500732\n",
      "epoch: 116,  batch step: 220, loss: 5.153508186340332\n",
      "epoch: 116,  batch step: 221, loss: 2.9719817638397217\n",
      "epoch: 116,  batch step: 222, loss: 10.359070777893066\n",
      "epoch: 116,  batch step: 223, loss: 3.436952590942383\n",
      "epoch: 116,  batch step: 224, loss: 3.3162953853607178\n",
      "epoch: 116,  batch step: 225, loss: 2.1283907890319824\n",
      "epoch: 116,  batch step: 226, loss: 19.961498260498047\n",
      "epoch: 116,  batch step: 227, loss: 2.0559303760528564\n",
      "epoch: 116,  batch step: 228, loss: 2.5694456100463867\n",
      "epoch: 116,  batch step: 229, loss: 1.692806601524353\n",
      "epoch: 116,  batch step: 230, loss: 21.878173828125\n",
      "epoch: 116,  batch step: 231, loss: 3.0339438915252686\n",
      "epoch: 116,  batch step: 232, loss: 3.2765135765075684\n",
      "epoch: 116,  batch step: 233, loss: 9.36738395690918\n",
      "epoch: 116,  batch step: 234, loss: 27.293994903564453\n",
      "epoch: 116,  batch step: 235, loss: 23.18398094177246\n",
      "epoch: 116,  batch step: 236, loss: 1.7902531623840332\n",
      "epoch: 116,  batch step: 237, loss: 2.710155487060547\n",
      "epoch: 116,  batch step: 238, loss: 1.9264558553695679\n",
      "epoch: 116,  batch step: 239, loss: 5.917609214782715\n",
      "epoch: 116,  batch step: 240, loss: 3.2451395988464355\n",
      "epoch: 116,  batch step: 241, loss: 30.793006896972656\n",
      "epoch: 116,  batch step: 242, loss: 68.2166519165039\n",
      "epoch: 116,  batch step: 243, loss: 25.729503631591797\n",
      "epoch: 116,  batch step: 244, loss: 1.7783478498458862\n",
      "epoch: 116,  batch step: 245, loss: 2.5452921390533447\n",
      "epoch: 116,  batch step: 246, loss: 1.8901060819625854\n",
      "epoch: 116,  batch step: 247, loss: 1.4939048290252686\n",
      "epoch: 116,  batch step: 248, loss: 2.1101369857788086\n",
      "epoch: 116,  batch step: 249, loss: 1.903768539428711\n",
      "epoch: 116,  batch step: 250, loss: 49.69956588745117\n",
      "epoch: 116,  batch step: 251, loss: 6.264588356018066\n",
      "validation error epoch  116:    tensor(68.7133, device='cuda:0')\n",
      "316\n",
      "epoch: 117,  batch step: 0, loss: 16.335166931152344\n",
      "epoch: 117,  batch step: 1, loss: 2.098494291305542\n",
      "epoch: 117,  batch step: 2, loss: 2.762481451034546\n",
      "epoch: 117,  batch step: 3, loss: 10.831682205200195\n",
      "epoch: 117,  batch step: 4, loss: 10.615997314453125\n",
      "epoch: 117,  batch step: 5, loss: 7.223416805267334\n",
      "epoch: 117,  batch step: 6, loss: 2.4896347522735596\n",
      "epoch: 117,  batch step: 7, loss: 3.9506826400756836\n",
      "epoch: 117,  batch step: 8, loss: 36.43706512451172\n",
      "epoch: 117,  batch step: 9, loss: 3.0035500526428223\n",
      "epoch: 117,  batch step: 10, loss: 27.348243713378906\n",
      "epoch: 117,  batch step: 11, loss: 12.206085205078125\n",
      "epoch: 117,  batch step: 12, loss: 29.414220809936523\n",
      "epoch: 117,  batch step: 13, loss: 12.953786849975586\n",
      "epoch: 117,  batch step: 14, loss: 1.9248836040496826\n",
      "epoch: 117,  batch step: 15, loss: 142.31118774414062\n",
      "epoch: 117,  batch step: 16, loss: 9.315977096557617\n",
      "epoch: 117,  batch step: 17, loss: 2.3340909481048584\n",
      "epoch: 117,  batch step: 18, loss: 5.041223526000977\n",
      "epoch: 117,  batch step: 19, loss: 6.918351650238037\n",
      "epoch: 117,  batch step: 20, loss: 39.92151641845703\n",
      "epoch: 117,  batch step: 21, loss: 20.779714584350586\n",
      "epoch: 117,  batch step: 22, loss: 16.625417709350586\n",
      "epoch: 117,  batch step: 23, loss: 25.86420249938965\n",
      "epoch: 117,  batch step: 24, loss: 62.34056091308594\n",
      "epoch: 117,  batch step: 25, loss: 2.654275417327881\n",
      "epoch: 117,  batch step: 26, loss: 8.456001281738281\n",
      "epoch: 117,  batch step: 27, loss: 3.7215466499328613\n",
      "epoch: 117,  batch step: 28, loss: 6.550358772277832\n",
      "epoch: 117,  batch step: 29, loss: 7.758991241455078\n",
      "epoch: 117,  batch step: 30, loss: 1.819331407546997\n",
      "epoch: 117,  batch step: 31, loss: 3.012150764465332\n",
      "epoch: 117,  batch step: 32, loss: 29.579200744628906\n",
      "epoch: 117,  batch step: 33, loss: 9.057807922363281\n",
      "epoch: 117,  batch step: 34, loss: 30.649120330810547\n",
      "epoch: 117,  batch step: 35, loss: 4.150641918182373\n",
      "epoch: 117,  batch step: 36, loss: 10.862159729003906\n",
      "epoch: 117,  batch step: 37, loss: 30.656784057617188\n",
      "epoch: 117,  batch step: 38, loss: 1.543103575706482\n",
      "epoch: 117,  batch step: 39, loss: 3.0572469234466553\n",
      "epoch: 117,  batch step: 40, loss: 30.865995407104492\n",
      "epoch: 117,  batch step: 41, loss: 12.605299949645996\n",
      "epoch: 117,  batch step: 42, loss: 4.678642749786377\n",
      "epoch: 117,  batch step: 43, loss: 2.6948747634887695\n",
      "epoch: 117,  batch step: 44, loss: 22.73982048034668\n",
      "epoch: 117,  batch step: 45, loss: 11.32411003112793\n",
      "epoch: 117,  batch step: 46, loss: 5.21933126449585\n",
      "epoch: 117,  batch step: 47, loss: 10.000663757324219\n",
      "epoch: 117,  batch step: 48, loss: 28.322612762451172\n",
      "epoch: 117,  batch step: 49, loss: 11.83152961730957\n",
      "epoch: 117,  batch step: 50, loss: 36.686614990234375\n",
      "epoch: 117,  batch step: 51, loss: 2.5404186248779297\n",
      "epoch: 117,  batch step: 52, loss: 2.4860973358154297\n",
      "epoch: 117,  batch step: 53, loss: 1.551116943359375\n",
      "epoch: 117,  batch step: 54, loss: 5.246380805969238\n",
      "epoch: 117,  batch step: 55, loss: 3.4155311584472656\n",
      "epoch: 117,  batch step: 56, loss: 9.263446807861328\n",
      "epoch: 117,  batch step: 57, loss: 7.956209182739258\n",
      "epoch: 117,  batch step: 58, loss: 1.618478536605835\n",
      "epoch: 117,  batch step: 59, loss: 9.436426162719727\n",
      "epoch: 117,  batch step: 60, loss: 4.988945007324219\n",
      "epoch: 117,  batch step: 61, loss: 49.90362548828125\n",
      "epoch: 117,  batch step: 62, loss: 1.9158077239990234\n",
      "epoch: 117,  batch step: 63, loss: 12.964144706726074\n",
      "epoch: 117,  batch step: 64, loss: 2.8171775341033936\n",
      "epoch: 117,  batch step: 65, loss: 14.364143371582031\n",
      "epoch: 117,  batch step: 66, loss: 23.925640106201172\n",
      "epoch: 117,  batch step: 67, loss: 5.833361625671387\n",
      "epoch: 117,  batch step: 68, loss: 1.8404579162597656\n",
      "epoch: 117,  batch step: 69, loss: 1.4404895305633545\n",
      "epoch: 117,  batch step: 70, loss: 2.029953718185425\n",
      "epoch: 117,  batch step: 71, loss: 2.453138828277588\n",
      "epoch: 117,  batch step: 72, loss: 1.9396694898605347\n",
      "epoch: 117,  batch step: 73, loss: 7.573676586151123\n",
      "epoch: 117,  batch step: 74, loss: 13.00248908996582\n",
      "epoch: 117,  batch step: 75, loss: 7.29746150970459\n",
      "epoch: 117,  batch step: 76, loss: 7.356651306152344\n",
      "epoch: 117,  batch step: 77, loss: 9.202030181884766\n",
      "epoch: 117,  batch step: 78, loss: 15.286493301391602\n",
      "epoch: 117,  batch step: 79, loss: 7.112760543823242\n",
      "epoch: 117,  batch step: 80, loss: 68.4549560546875\n",
      "epoch: 117,  batch step: 81, loss: 12.534097671508789\n",
      "epoch: 117,  batch step: 82, loss: 3.034726619720459\n",
      "epoch: 117,  batch step: 83, loss: 1.6146211624145508\n",
      "epoch: 117,  batch step: 84, loss: 1.70163893699646\n",
      "epoch: 117,  batch step: 85, loss: 8.07870101928711\n",
      "epoch: 117,  batch step: 86, loss: 9.66779899597168\n",
      "epoch: 117,  batch step: 87, loss: 10.087778091430664\n",
      "epoch: 117,  batch step: 88, loss: 1.5155071020126343\n",
      "epoch: 117,  batch step: 89, loss: 19.9234619140625\n",
      "epoch: 117,  batch step: 90, loss: 2.7733335494995117\n",
      "epoch: 117,  batch step: 91, loss: 23.243892669677734\n",
      "epoch: 117,  batch step: 92, loss: 6.504222393035889\n",
      "epoch: 117,  batch step: 93, loss: 1.6419286727905273\n",
      "epoch: 117,  batch step: 94, loss: 3.5188090801239014\n",
      "epoch: 117,  batch step: 95, loss: 1.9192336797714233\n",
      "epoch: 117,  batch step: 96, loss: 19.599395751953125\n",
      "epoch: 117,  batch step: 97, loss: 6.711970329284668\n",
      "epoch: 117,  batch step: 98, loss: 13.341414451599121\n",
      "epoch: 117,  batch step: 99, loss: 4.149804592132568\n",
      "epoch: 117,  batch step: 100, loss: 6.338381767272949\n",
      "epoch: 117,  batch step: 101, loss: 1.8386783599853516\n",
      "epoch: 117,  batch step: 102, loss: 26.059293746948242\n",
      "epoch: 117,  batch step: 103, loss: 3.0942583084106445\n",
      "epoch: 117,  batch step: 104, loss: 21.613691329956055\n",
      "epoch: 117,  batch step: 105, loss: 2.945225477218628\n",
      "epoch: 117,  batch step: 106, loss: 14.964622497558594\n",
      "epoch: 117,  batch step: 107, loss: 7.79256534576416\n",
      "epoch: 117,  batch step: 108, loss: 2.351729393005371\n",
      "epoch: 117,  batch step: 109, loss: 2.7799601554870605\n",
      "epoch: 117,  batch step: 110, loss: 3.875917911529541\n",
      "epoch: 117,  batch step: 111, loss: 3.507611036300659\n",
      "epoch: 117,  batch step: 112, loss: 5.405251979827881\n",
      "epoch: 117,  batch step: 113, loss: 6.6054816246032715\n",
      "epoch: 117,  batch step: 114, loss: 15.639659881591797\n",
      "epoch: 117,  batch step: 115, loss: 3.595956325531006\n",
      "epoch: 117,  batch step: 116, loss: 1.9601730108261108\n",
      "epoch: 117,  batch step: 117, loss: 14.970396041870117\n",
      "epoch: 117,  batch step: 118, loss: 13.631193161010742\n",
      "epoch: 117,  batch step: 119, loss: 21.24262046813965\n",
      "epoch: 117,  batch step: 120, loss: 14.514002799987793\n",
      "epoch: 117,  batch step: 121, loss: 52.026851654052734\n",
      "epoch: 117,  batch step: 122, loss: 2.9714972972869873\n",
      "epoch: 117,  batch step: 123, loss: 1.7152214050292969\n",
      "epoch: 117,  batch step: 124, loss: 2.987696409225464\n",
      "epoch: 117,  batch step: 125, loss: 2.1842713356018066\n",
      "epoch: 117,  batch step: 126, loss: 2.4631848335266113\n",
      "epoch: 117,  batch step: 127, loss: 4.516904354095459\n",
      "epoch: 117,  batch step: 128, loss: 5.5771684646606445\n",
      "epoch: 117,  batch step: 129, loss: 8.065892219543457\n",
      "epoch: 117,  batch step: 130, loss: 21.09200668334961\n",
      "epoch: 117,  batch step: 131, loss: 35.4597282409668\n",
      "epoch: 117,  batch step: 132, loss: 18.076648712158203\n",
      "epoch: 117,  batch step: 133, loss: 5.5099005699157715\n",
      "epoch: 117,  batch step: 134, loss: 6.56258487701416\n",
      "epoch: 117,  batch step: 135, loss: 2.615412712097168\n",
      "epoch: 117,  batch step: 136, loss: 4.209603309631348\n",
      "epoch: 117,  batch step: 137, loss: 14.1295804977417\n",
      "epoch: 117,  batch step: 138, loss: 3.7531116008758545\n",
      "epoch: 117,  batch step: 139, loss: 4.051830291748047\n",
      "epoch: 117,  batch step: 140, loss: 6.584624767303467\n",
      "epoch: 117,  batch step: 141, loss: 28.23684310913086\n",
      "epoch: 117,  batch step: 142, loss: 1.8199079036712646\n",
      "epoch: 117,  batch step: 143, loss: 25.101165771484375\n",
      "epoch: 117,  batch step: 144, loss: 10.337939262390137\n",
      "epoch: 117,  batch step: 145, loss: 3.9328019618988037\n",
      "epoch: 117,  batch step: 146, loss: 3.601752281188965\n",
      "epoch: 117,  batch step: 147, loss: 20.429393768310547\n",
      "epoch: 117,  batch step: 148, loss: 3.9475855827331543\n",
      "epoch: 117,  batch step: 149, loss: 22.474721908569336\n",
      "epoch: 117,  batch step: 150, loss: 19.57740020751953\n",
      "epoch: 117,  batch step: 151, loss: 24.395048141479492\n",
      "epoch: 117,  batch step: 152, loss: 10.92807674407959\n",
      "epoch: 117,  batch step: 153, loss: 19.862016677856445\n",
      "epoch: 117,  batch step: 154, loss: 1.8157010078430176\n",
      "epoch: 117,  batch step: 155, loss: 4.774200439453125\n",
      "epoch: 117,  batch step: 156, loss: 6.441758155822754\n",
      "epoch: 117,  batch step: 157, loss: 3.369509696960449\n",
      "epoch: 117,  batch step: 158, loss: 56.51961898803711\n",
      "epoch: 117,  batch step: 159, loss: 1.6514174938201904\n",
      "epoch: 117,  batch step: 160, loss: 2.2046303749084473\n",
      "epoch: 117,  batch step: 161, loss: 3.0277938842773438\n",
      "epoch: 117,  batch step: 162, loss: 4.138040065765381\n",
      "epoch: 117,  batch step: 163, loss: 2.3310933113098145\n",
      "epoch: 117,  batch step: 164, loss: 5.7366533279418945\n",
      "epoch: 117,  batch step: 165, loss: 47.44001770019531\n",
      "epoch: 117,  batch step: 166, loss: 25.968002319335938\n",
      "epoch: 117,  batch step: 167, loss: 16.99587631225586\n",
      "epoch: 117,  batch step: 168, loss: 20.409530639648438\n",
      "epoch: 117,  batch step: 169, loss: 1.4446887969970703\n",
      "epoch: 117,  batch step: 170, loss: 49.14548873901367\n",
      "epoch: 117,  batch step: 171, loss: 4.874216556549072\n",
      "epoch: 117,  batch step: 172, loss: 2.1170222759246826\n",
      "epoch: 117,  batch step: 173, loss: 6.4624457359313965\n",
      "epoch: 117,  batch step: 174, loss: 8.928638458251953\n",
      "epoch: 117,  batch step: 175, loss: 26.34227752685547\n",
      "epoch: 117,  batch step: 176, loss: 3.6788222789764404\n",
      "epoch: 117,  batch step: 177, loss: 1.817448616027832\n",
      "epoch: 117,  batch step: 178, loss: 3.9374759197235107\n",
      "epoch: 117,  batch step: 179, loss: 12.317107200622559\n",
      "epoch: 117,  batch step: 180, loss: 1.630000114440918\n",
      "epoch: 117,  batch step: 181, loss: 34.93498611450195\n",
      "epoch: 117,  batch step: 182, loss: 2.1310667991638184\n",
      "epoch: 117,  batch step: 183, loss: 1.739039421081543\n",
      "epoch: 117,  batch step: 184, loss: 15.117300987243652\n",
      "epoch: 117,  batch step: 185, loss: 4.589901447296143\n",
      "epoch: 117,  batch step: 186, loss: 24.606796264648438\n",
      "epoch: 117,  batch step: 187, loss: 14.610542297363281\n",
      "epoch: 117,  batch step: 188, loss: 13.320680618286133\n",
      "epoch: 117,  batch step: 189, loss: 7.839101314544678\n",
      "epoch: 117,  batch step: 190, loss: 1.616843581199646\n",
      "epoch: 117,  batch step: 191, loss: 3.787606716156006\n",
      "epoch: 117,  batch step: 192, loss: 8.564237594604492\n",
      "epoch: 117,  batch step: 193, loss: 2.2329702377319336\n",
      "epoch: 117,  batch step: 194, loss: 52.57185363769531\n",
      "epoch: 117,  batch step: 195, loss: 9.669890403747559\n",
      "epoch: 117,  batch step: 196, loss: 1.5504263639450073\n",
      "epoch: 117,  batch step: 197, loss: 55.550071716308594\n",
      "epoch: 117,  batch step: 198, loss: 10.236244201660156\n",
      "epoch: 117,  batch step: 199, loss: 1.8817775249481201\n",
      "epoch: 117,  batch step: 200, loss: 8.635886192321777\n",
      "epoch: 117,  batch step: 201, loss: 2.2610931396484375\n",
      "epoch: 117,  batch step: 202, loss: 32.45795440673828\n",
      "epoch: 117,  batch step: 203, loss: 1.3416088819503784\n",
      "epoch: 117,  batch step: 204, loss: 2.274827718734741\n",
      "epoch: 117,  batch step: 205, loss: 2.195207118988037\n",
      "epoch: 117,  batch step: 206, loss: 2.163743734359741\n",
      "epoch: 117,  batch step: 207, loss: 1.4544105529785156\n",
      "epoch: 117,  batch step: 208, loss: 3.3534305095672607\n",
      "epoch: 117,  batch step: 209, loss: 1.6333574056625366\n",
      "epoch: 117,  batch step: 210, loss: 1.9197582006454468\n",
      "epoch: 117,  batch step: 211, loss: 10.752253532409668\n",
      "epoch: 117,  batch step: 212, loss: 82.40686798095703\n",
      "epoch: 117,  batch step: 213, loss: 1.9940879344940186\n",
      "epoch: 117,  batch step: 214, loss: 6.703298568725586\n",
      "epoch: 117,  batch step: 215, loss: 15.24588394165039\n",
      "epoch: 117,  batch step: 216, loss: 2.9282169342041016\n",
      "epoch: 117,  batch step: 217, loss: 1.9644427299499512\n",
      "epoch: 117,  batch step: 218, loss: 2.4682157039642334\n",
      "epoch: 117,  batch step: 219, loss: 2.0677943229675293\n",
      "epoch: 117,  batch step: 220, loss: 3.6321475505828857\n",
      "epoch: 117,  batch step: 221, loss: 8.237052917480469\n",
      "epoch: 117,  batch step: 222, loss: 2.3667051792144775\n",
      "epoch: 117,  batch step: 223, loss: 8.392915725708008\n",
      "epoch: 117,  batch step: 224, loss: 15.755163192749023\n",
      "epoch: 117,  batch step: 225, loss: 14.46997356414795\n",
      "epoch: 117,  batch step: 226, loss: 1.5164865255355835\n",
      "epoch: 117,  batch step: 227, loss: 9.312538146972656\n",
      "epoch: 117,  batch step: 228, loss: 6.3336181640625\n",
      "epoch: 117,  batch step: 229, loss: 6.255539894104004\n",
      "epoch: 117,  batch step: 230, loss: 24.055212020874023\n",
      "epoch: 117,  batch step: 231, loss: 10.501853942871094\n",
      "epoch: 117,  batch step: 232, loss: 1.0995844602584839\n",
      "epoch: 117,  batch step: 233, loss: 43.074066162109375\n",
      "epoch: 117,  batch step: 234, loss: 2.9880857467651367\n",
      "epoch: 117,  batch step: 235, loss: 2.8234317302703857\n",
      "epoch: 117,  batch step: 236, loss: 14.871376037597656\n",
      "epoch: 117,  batch step: 237, loss: 9.358217239379883\n",
      "epoch: 117,  batch step: 238, loss: 1.482391119003296\n",
      "epoch: 117,  batch step: 239, loss: 37.75856018066406\n",
      "epoch: 117,  batch step: 240, loss: 2.2786755561828613\n",
      "epoch: 117,  batch step: 241, loss: 13.853748321533203\n",
      "epoch: 117,  batch step: 242, loss: 2.7940444946289062\n",
      "epoch: 117,  batch step: 243, loss: 2.3843882083892822\n",
      "epoch: 117,  batch step: 244, loss: 46.220977783203125\n",
      "epoch: 117,  batch step: 245, loss: 1.4173692464828491\n",
      "epoch: 117,  batch step: 246, loss: 18.428241729736328\n",
      "epoch: 117,  batch step: 247, loss: 16.784244537353516\n",
      "epoch: 117,  batch step: 248, loss: 2.228572368621826\n",
      "epoch: 117,  batch step: 249, loss: 3.1923816204071045\n",
      "epoch: 117,  batch step: 250, loss: 3.970961332321167\n",
      "epoch: 117,  batch step: 251, loss: 199.22036743164062\n",
      "validation error epoch  117:    tensor(69.4116, device='cuda:0')\n",
      "316\n",
      "epoch: 118,  batch step: 0, loss: 5.888244152069092\n",
      "epoch: 118,  batch step: 1, loss: 15.813985824584961\n",
      "epoch: 118,  batch step: 2, loss: 21.69049644470215\n",
      "epoch: 118,  batch step: 3, loss: 23.069297790527344\n",
      "epoch: 118,  batch step: 4, loss: 10.14026165008545\n",
      "epoch: 118,  batch step: 5, loss: 23.939367294311523\n",
      "epoch: 118,  batch step: 6, loss: 23.345876693725586\n",
      "epoch: 118,  batch step: 7, loss: 2.901033878326416\n",
      "epoch: 118,  batch step: 8, loss: 26.849233627319336\n",
      "epoch: 118,  batch step: 9, loss: 10.264944076538086\n",
      "epoch: 118,  batch step: 10, loss: 7.362534999847412\n",
      "epoch: 118,  batch step: 11, loss: 18.835336685180664\n",
      "epoch: 118,  batch step: 12, loss: 8.924785614013672\n",
      "epoch: 118,  batch step: 13, loss: 19.73134422302246\n",
      "epoch: 118,  batch step: 14, loss: 10.155645370483398\n",
      "epoch: 118,  batch step: 15, loss: 4.137880325317383\n",
      "epoch: 118,  batch step: 16, loss: 3.5098628997802734\n",
      "epoch: 118,  batch step: 17, loss: 3.900494337081909\n",
      "epoch: 118,  batch step: 18, loss: 4.591280937194824\n",
      "epoch: 118,  batch step: 19, loss: 6.786303520202637\n",
      "epoch: 118,  batch step: 20, loss: 4.149147033691406\n",
      "epoch: 118,  batch step: 21, loss: 17.105710983276367\n",
      "epoch: 118,  batch step: 22, loss: 3.8801074028015137\n",
      "epoch: 118,  batch step: 23, loss: 3.564077854156494\n",
      "epoch: 118,  batch step: 24, loss: 3.1261680126190186\n",
      "epoch: 118,  batch step: 25, loss: 12.657122611999512\n",
      "epoch: 118,  batch step: 26, loss: 9.770313262939453\n",
      "epoch: 118,  batch step: 27, loss: 3.3168351650238037\n",
      "epoch: 118,  batch step: 28, loss: 3.0095314979553223\n",
      "epoch: 118,  batch step: 29, loss: 12.74817943572998\n",
      "epoch: 118,  batch step: 30, loss: 61.389801025390625\n",
      "epoch: 118,  batch step: 31, loss: 4.85589599609375\n",
      "epoch: 118,  batch step: 32, loss: 6.385101318359375\n",
      "epoch: 118,  batch step: 33, loss: 17.6318359375\n",
      "epoch: 118,  batch step: 34, loss: 4.434696197509766\n",
      "epoch: 118,  batch step: 35, loss: 3.5904407501220703\n",
      "epoch: 118,  batch step: 36, loss: 18.761898040771484\n",
      "epoch: 118,  batch step: 37, loss: 30.712583541870117\n",
      "epoch: 118,  batch step: 38, loss: 50.96918869018555\n",
      "epoch: 118,  batch step: 39, loss: 2.1658029556274414\n",
      "epoch: 118,  batch step: 40, loss: 10.416885375976562\n",
      "epoch: 118,  batch step: 41, loss: 48.254310607910156\n",
      "epoch: 118,  batch step: 42, loss: 3.8614137172698975\n",
      "epoch: 118,  batch step: 43, loss: 13.308845520019531\n",
      "epoch: 118,  batch step: 44, loss: 3.9979865550994873\n",
      "epoch: 118,  batch step: 45, loss: 7.847053050994873\n",
      "epoch: 118,  batch step: 46, loss: 14.777545928955078\n",
      "epoch: 118,  batch step: 47, loss: 13.947568893432617\n",
      "epoch: 118,  batch step: 48, loss: 4.204779148101807\n",
      "epoch: 118,  batch step: 49, loss: 28.804426193237305\n",
      "epoch: 118,  batch step: 50, loss: 3.6160361766815186\n",
      "epoch: 118,  batch step: 51, loss: 17.4477596282959\n",
      "epoch: 118,  batch step: 52, loss: 6.081210613250732\n",
      "epoch: 118,  batch step: 53, loss: 6.54178524017334\n",
      "epoch: 118,  batch step: 54, loss: 3.1563844680786133\n",
      "epoch: 118,  batch step: 55, loss: 16.120147705078125\n",
      "epoch: 118,  batch step: 56, loss: 18.807565689086914\n",
      "epoch: 118,  batch step: 57, loss: 3.2431368827819824\n",
      "epoch: 118,  batch step: 58, loss: 13.611593246459961\n",
      "epoch: 118,  batch step: 59, loss: 2.925002098083496\n",
      "epoch: 118,  batch step: 60, loss: 6.814824104309082\n",
      "epoch: 118,  batch step: 61, loss: 10.915250778198242\n",
      "epoch: 118,  batch step: 62, loss: 5.937009334564209\n",
      "epoch: 118,  batch step: 63, loss: 71.85256958007812\n",
      "epoch: 118,  batch step: 64, loss: 2.2671899795532227\n",
      "epoch: 118,  batch step: 65, loss: 6.202532768249512\n",
      "epoch: 118,  batch step: 66, loss: 4.686155319213867\n",
      "epoch: 118,  batch step: 67, loss: 9.062421798706055\n",
      "epoch: 118,  batch step: 68, loss: 16.096298217773438\n",
      "epoch: 118,  batch step: 69, loss: 21.94190216064453\n",
      "epoch: 118,  batch step: 70, loss: 2.507063627243042\n",
      "epoch: 118,  batch step: 71, loss: 2.908752918243408\n",
      "epoch: 118,  batch step: 72, loss: 11.108001708984375\n",
      "epoch: 118,  batch step: 73, loss: 3.1608693599700928\n",
      "epoch: 118,  batch step: 74, loss: 2.533435344696045\n",
      "epoch: 118,  batch step: 75, loss: 3.5211338996887207\n",
      "epoch: 118,  batch step: 76, loss: 9.008169174194336\n",
      "epoch: 118,  batch step: 77, loss: 23.007360458374023\n",
      "epoch: 118,  batch step: 78, loss: 3.9272098541259766\n",
      "epoch: 118,  batch step: 79, loss: 28.764265060424805\n",
      "epoch: 118,  batch step: 80, loss: 1.997466802597046\n",
      "epoch: 118,  batch step: 81, loss: 4.343830108642578\n",
      "epoch: 118,  batch step: 82, loss: 27.29071044921875\n",
      "epoch: 118,  batch step: 83, loss: 2.1863584518432617\n",
      "epoch: 118,  batch step: 84, loss: 42.03596878051758\n",
      "epoch: 118,  batch step: 85, loss: 3.5355308055877686\n",
      "epoch: 118,  batch step: 86, loss: 15.144678115844727\n",
      "epoch: 118,  batch step: 87, loss: 2.2073824405670166\n",
      "epoch: 118,  batch step: 88, loss: 2.064737319946289\n",
      "epoch: 118,  batch step: 89, loss: 15.122443199157715\n",
      "epoch: 118,  batch step: 90, loss: 4.2507758140563965\n",
      "epoch: 118,  batch step: 91, loss: 2.6584789752960205\n",
      "epoch: 118,  batch step: 92, loss: 1.8431735038757324\n",
      "epoch: 118,  batch step: 93, loss: 4.130870819091797\n",
      "epoch: 118,  batch step: 94, loss: 9.294426918029785\n",
      "epoch: 118,  batch step: 95, loss: 62.92671203613281\n",
      "epoch: 118,  batch step: 96, loss: 9.24629020690918\n",
      "epoch: 118,  batch step: 97, loss: 15.062426567077637\n",
      "epoch: 118,  batch step: 98, loss: 10.007109642028809\n",
      "epoch: 118,  batch step: 99, loss: 28.476665496826172\n",
      "epoch: 118,  batch step: 100, loss: 31.959836959838867\n",
      "epoch: 118,  batch step: 101, loss: 1.8983354568481445\n",
      "epoch: 118,  batch step: 102, loss: 27.334516525268555\n",
      "epoch: 118,  batch step: 103, loss: 4.027105331420898\n",
      "epoch: 118,  batch step: 104, loss: 42.130149841308594\n",
      "epoch: 118,  batch step: 105, loss: 7.498751640319824\n",
      "epoch: 118,  batch step: 106, loss: 2.8290047645568848\n",
      "epoch: 118,  batch step: 107, loss: 6.534543991088867\n",
      "epoch: 118,  batch step: 108, loss: 9.928426742553711\n",
      "epoch: 118,  batch step: 109, loss: 4.284351825714111\n",
      "epoch: 118,  batch step: 110, loss: 13.011442184448242\n",
      "epoch: 118,  batch step: 111, loss: 1.447007656097412\n",
      "epoch: 118,  batch step: 112, loss: 5.449058532714844\n",
      "epoch: 118,  batch step: 113, loss: 52.10477828979492\n",
      "epoch: 118,  batch step: 114, loss: 5.272235870361328\n",
      "epoch: 118,  batch step: 115, loss: 1.7712806463241577\n",
      "epoch: 118,  batch step: 116, loss: 4.629634857177734\n",
      "epoch: 118,  batch step: 117, loss: 6.076074123382568\n",
      "epoch: 118,  batch step: 118, loss: 23.031789779663086\n",
      "epoch: 118,  batch step: 119, loss: 8.304621696472168\n",
      "epoch: 118,  batch step: 120, loss: 12.57424545288086\n",
      "epoch: 118,  batch step: 121, loss: 2.198795795440674\n",
      "epoch: 118,  batch step: 122, loss: 4.262021541595459\n",
      "epoch: 118,  batch step: 123, loss: 8.134610176086426\n",
      "epoch: 118,  batch step: 124, loss: 5.661589622497559\n",
      "epoch: 118,  batch step: 125, loss: 8.391458511352539\n",
      "epoch: 118,  batch step: 126, loss: 17.49964141845703\n",
      "epoch: 118,  batch step: 127, loss: 5.944757461547852\n",
      "epoch: 118,  batch step: 128, loss: 2.168677806854248\n",
      "epoch: 118,  batch step: 129, loss: 8.437759399414062\n",
      "epoch: 118,  batch step: 130, loss: 2.629807949066162\n",
      "epoch: 118,  batch step: 131, loss: 6.933624267578125\n",
      "epoch: 118,  batch step: 132, loss: 33.41127395629883\n",
      "epoch: 118,  batch step: 133, loss: 9.071009635925293\n",
      "epoch: 118,  batch step: 134, loss: 1.888800024986267\n",
      "epoch: 118,  batch step: 135, loss: 4.866369247436523\n",
      "epoch: 118,  batch step: 136, loss: 12.468603134155273\n",
      "epoch: 118,  batch step: 137, loss: 2.7448477745056152\n",
      "epoch: 118,  batch step: 138, loss: 11.541540145874023\n",
      "epoch: 118,  batch step: 139, loss: 2.4317007064819336\n",
      "epoch: 118,  batch step: 140, loss: 12.354013442993164\n",
      "epoch: 118,  batch step: 141, loss: 4.316253185272217\n",
      "epoch: 118,  batch step: 142, loss: 2.1442954540252686\n",
      "epoch: 118,  batch step: 143, loss: 31.31066131591797\n",
      "epoch: 118,  batch step: 144, loss: 2.7733139991760254\n",
      "epoch: 118,  batch step: 145, loss: 3.711474895477295\n",
      "epoch: 118,  batch step: 146, loss: 23.658931732177734\n",
      "epoch: 118,  batch step: 147, loss: 2.899677276611328\n",
      "epoch: 118,  batch step: 148, loss: 16.225421905517578\n",
      "epoch: 118,  batch step: 149, loss: 28.60494041442871\n",
      "epoch: 118,  batch step: 150, loss: 3.190000295639038\n",
      "epoch: 118,  batch step: 151, loss: 8.521954536437988\n",
      "epoch: 118,  batch step: 152, loss: 2.6856048107147217\n",
      "epoch: 118,  batch step: 153, loss: 20.035736083984375\n",
      "epoch: 118,  batch step: 154, loss: 3.357320547103882\n",
      "epoch: 118,  batch step: 155, loss: 5.077098369598389\n",
      "epoch: 118,  batch step: 156, loss: 9.679132461547852\n",
      "epoch: 118,  batch step: 157, loss: 2.777266025543213\n",
      "epoch: 118,  batch step: 158, loss: 3.4008846282958984\n",
      "epoch: 118,  batch step: 159, loss: 18.012948989868164\n",
      "epoch: 118,  batch step: 160, loss: 11.839869499206543\n",
      "epoch: 118,  batch step: 161, loss: 18.657072067260742\n",
      "epoch: 118,  batch step: 162, loss: 8.888079643249512\n",
      "epoch: 118,  batch step: 163, loss: 12.816634178161621\n",
      "epoch: 118,  batch step: 164, loss: 46.48912811279297\n",
      "epoch: 118,  batch step: 165, loss: 2.015082359313965\n",
      "epoch: 118,  batch step: 166, loss: 12.537853240966797\n",
      "epoch: 118,  batch step: 167, loss: 2.336376190185547\n",
      "epoch: 118,  batch step: 168, loss: 4.993198394775391\n",
      "epoch: 118,  batch step: 169, loss: 34.49338150024414\n",
      "epoch: 118,  batch step: 170, loss: 37.25202941894531\n",
      "epoch: 118,  batch step: 171, loss: 2.220834255218506\n",
      "epoch: 118,  batch step: 172, loss: 2.131894111633301\n",
      "epoch: 118,  batch step: 173, loss: 13.319872856140137\n",
      "epoch: 118,  batch step: 174, loss: 10.080000877380371\n",
      "epoch: 118,  batch step: 175, loss: 9.72476577758789\n",
      "epoch: 118,  batch step: 176, loss: 8.43564510345459\n",
      "epoch: 118,  batch step: 177, loss: 7.061710357666016\n",
      "epoch: 118,  batch step: 178, loss: 4.877922534942627\n",
      "epoch: 118,  batch step: 179, loss: 19.983327865600586\n",
      "epoch: 118,  batch step: 180, loss: 9.115944862365723\n",
      "epoch: 118,  batch step: 181, loss: 3.3321709632873535\n",
      "epoch: 118,  batch step: 182, loss: 3.368431806564331\n",
      "epoch: 118,  batch step: 183, loss: 26.02021598815918\n",
      "epoch: 118,  batch step: 184, loss: 17.098064422607422\n",
      "epoch: 118,  batch step: 185, loss: 7.7175188064575195\n",
      "epoch: 118,  batch step: 186, loss: 13.9252347946167\n",
      "epoch: 118,  batch step: 187, loss: 3.4908995628356934\n",
      "epoch: 118,  batch step: 188, loss: 15.709625244140625\n",
      "epoch: 118,  batch step: 189, loss: 2.4506676197052\n",
      "epoch: 118,  batch step: 190, loss: 2.162709951400757\n",
      "epoch: 118,  batch step: 191, loss: 8.641449928283691\n",
      "epoch: 118,  batch step: 192, loss: 3.468484878540039\n",
      "epoch: 118,  batch step: 193, loss: 11.8527250289917\n",
      "epoch: 118,  batch step: 194, loss: 3.8749232292175293\n",
      "epoch: 118,  batch step: 195, loss: 28.175642013549805\n",
      "epoch: 118,  batch step: 196, loss: 48.88444519042969\n",
      "epoch: 118,  batch step: 197, loss: 50.15160369873047\n",
      "epoch: 118,  batch step: 198, loss: 2.2752203941345215\n",
      "epoch: 118,  batch step: 199, loss: 3.6186892986297607\n",
      "epoch: 118,  batch step: 200, loss: 5.002509593963623\n",
      "epoch: 118,  batch step: 201, loss: 2.6741273403167725\n",
      "epoch: 118,  batch step: 202, loss: 3.7379040718078613\n",
      "epoch: 118,  batch step: 203, loss: 27.8287353515625\n",
      "epoch: 118,  batch step: 204, loss: 2.133957624435425\n",
      "epoch: 118,  batch step: 205, loss: 4.9835100173950195\n",
      "epoch: 118,  batch step: 206, loss: 4.430297374725342\n",
      "epoch: 118,  batch step: 207, loss: 2.0406367778778076\n",
      "epoch: 118,  batch step: 208, loss: 24.947864532470703\n",
      "epoch: 118,  batch step: 209, loss: 4.8215131759643555\n",
      "epoch: 118,  batch step: 210, loss: 17.466739654541016\n",
      "epoch: 118,  batch step: 211, loss: 3.9330804347991943\n",
      "epoch: 118,  batch step: 212, loss: 9.455486297607422\n",
      "epoch: 118,  batch step: 213, loss: 2.9472625255584717\n",
      "epoch: 118,  batch step: 214, loss: 31.58600425720215\n",
      "epoch: 118,  batch step: 215, loss: 1.7311185598373413\n",
      "epoch: 118,  batch step: 216, loss: 22.32439422607422\n",
      "epoch: 118,  batch step: 217, loss: 2.8398489952087402\n",
      "epoch: 118,  batch step: 218, loss: 8.467150688171387\n",
      "epoch: 118,  batch step: 219, loss: 22.177188873291016\n",
      "epoch: 118,  batch step: 220, loss: 46.53052520751953\n",
      "epoch: 118,  batch step: 221, loss: 12.234682083129883\n",
      "epoch: 118,  batch step: 222, loss: 2.8529272079467773\n",
      "epoch: 118,  batch step: 223, loss: 139.833740234375\n",
      "epoch: 118,  batch step: 224, loss: 6.345374584197998\n",
      "epoch: 118,  batch step: 225, loss: 28.202411651611328\n",
      "epoch: 118,  batch step: 226, loss: 3.337381362915039\n",
      "epoch: 118,  batch step: 227, loss: 12.85281753540039\n",
      "epoch: 118,  batch step: 228, loss: 12.361517906188965\n",
      "epoch: 118,  batch step: 229, loss: 3.2054145336151123\n",
      "epoch: 118,  batch step: 230, loss: 2.5592851638793945\n",
      "epoch: 118,  batch step: 231, loss: 2.334164619445801\n",
      "epoch: 118,  batch step: 232, loss: 3.2088623046875\n",
      "epoch: 118,  batch step: 233, loss: 2.9762961864471436\n",
      "epoch: 118,  batch step: 234, loss: 7.611326217651367\n",
      "epoch: 118,  batch step: 235, loss: 4.603463649749756\n",
      "epoch: 118,  batch step: 236, loss: 11.109881401062012\n",
      "epoch: 118,  batch step: 237, loss: 10.762413024902344\n",
      "epoch: 118,  batch step: 238, loss: 1.9909934997558594\n",
      "epoch: 118,  batch step: 239, loss: 2.4732611179351807\n",
      "epoch: 118,  batch step: 240, loss: 25.169456481933594\n",
      "epoch: 118,  batch step: 241, loss: 9.441110610961914\n",
      "epoch: 118,  batch step: 242, loss: 8.453216552734375\n",
      "epoch: 118,  batch step: 243, loss: 3.0644702911376953\n",
      "epoch: 118,  batch step: 244, loss: 2.3936851024627686\n",
      "epoch: 118,  batch step: 245, loss: 3.4176576137542725\n",
      "epoch: 118,  batch step: 246, loss: 85.46660614013672\n",
      "epoch: 118,  batch step: 247, loss: 2.168926954269409\n",
      "epoch: 118,  batch step: 248, loss: 8.648134231567383\n",
      "epoch: 118,  batch step: 249, loss: 1.9273741245269775\n",
      "epoch: 118,  batch step: 250, loss: 7.007176876068115\n",
      "epoch: 118,  batch step: 251, loss: 54.64849090576172\n",
      "validation error epoch  118:    tensor(67.8790, device='cuda:0')\n",
      "316\n",
      "epoch: 119,  batch step: 0, loss: 7.937707424163818\n",
      "epoch: 119,  batch step: 1, loss: 4.016183376312256\n",
      "epoch: 119,  batch step: 2, loss: 3.1567516326904297\n",
      "epoch: 119,  batch step: 3, loss: 17.241294860839844\n",
      "epoch: 119,  batch step: 4, loss: 19.089757919311523\n",
      "epoch: 119,  batch step: 5, loss: 11.442325592041016\n",
      "epoch: 119,  batch step: 6, loss: 17.926959991455078\n",
      "epoch: 119,  batch step: 7, loss: 8.042275428771973\n",
      "epoch: 119,  batch step: 8, loss: 4.017021179199219\n",
      "epoch: 119,  batch step: 9, loss: 4.098500728607178\n",
      "epoch: 119,  batch step: 10, loss: 32.85414123535156\n",
      "epoch: 119,  batch step: 11, loss: 3.3643670082092285\n",
      "epoch: 119,  batch step: 12, loss: 6.106935024261475\n",
      "epoch: 119,  batch step: 13, loss: 11.225147247314453\n",
      "epoch: 119,  batch step: 14, loss: 24.184782028198242\n",
      "epoch: 119,  batch step: 15, loss: 28.263927459716797\n",
      "epoch: 119,  batch step: 16, loss: 45.885223388671875\n",
      "epoch: 119,  batch step: 17, loss: 10.782958984375\n",
      "epoch: 119,  batch step: 18, loss: 3.017012596130371\n",
      "epoch: 119,  batch step: 19, loss: 6.269930839538574\n",
      "epoch: 119,  batch step: 20, loss: 22.20388412475586\n",
      "epoch: 119,  batch step: 21, loss: 8.061445236206055\n",
      "epoch: 119,  batch step: 22, loss: 7.757302284240723\n",
      "epoch: 119,  batch step: 23, loss: 43.06948471069336\n",
      "epoch: 119,  batch step: 24, loss: 10.708571434020996\n",
      "epoch: 119,  batch step: 25, loss: 4.353865623474121\n",
      "epoch: 119,  batch step: 26, loss: 3.9100875854492188\n",
      "epoch: 119,  batch step: 27, loss: 3.784780502319336\n",
      "epoch: 119,  batch step: 28, loss: 3.1042733192443848\n",
      "epoch: 119,  batch step: 29, loss: 10.68738079071045\n",
      "epoch: 119,  batch step: 30, loss: 22.151952743530273\n",
      "epoch: 119,  batch step: 31, loss: 3.873878002166748\n",
      "epoch: 119,  batch step: 32, loss: 3.5158698558807373\n",
      "epoch: 119,  batch step: 33, loss: 4.077785491943359\n",
      "epoch: 119,  batch step: 34, loss: 2.8687424659729004\n",
      "epoch: 119,  batch step: 35, loss: 12.9539213180542\n",
      "epoch: 119,  batch step: 36, loss: 4.471749305725098\n",
      "epoch: 119,  batch step: 37, loss: 7.587419509887695\n",
      "epoch: 119,  batch step: 38, loss: 3.146665573120117\n",
      "epoch: 119,  batch step: 39, loss: 3.565488338470459\n",
      "epoch: 119,  batch step: 40, loss: 2.418027400970459\n",
      "epoch: 119,  batch step: 41, loss: 3.881519079208374\n",
      "epoch: 119,  batch step: 42, loss: 54.10719299316406\n",
      "epoch: 119,  batch step: 43, loss: 5.458248138427734\n",
      "epoch: 119,  batch step: 44, loss: 3.925133466720581\n",
      "epoch: 119,  batch step: 45, loss: 2.956660032272339\n",
      "epoch: 119,  batch step: 46, loss: 2.910228729248047\n",
      "epoch: 119,  batch step: 47, loss: 41.31724166870117\n",
      "epoch: 119,  batch step: 48, loss: 6.39011287689209\n",
      "epoch: 119,  batch step: 49, loss: 131.256103515625\n",
      "epoch: 119,  batch step: 50, loss: 2.5943992137908936\n",
      "epoch: 119,  batch step: 51, loss: 17.06621551513672\n",
      "epoch: 119,  batch step: 52, loss: 2.750192165374756\n",
      "epoch: 119,  batch step: 53, loss: 11.385643005371094\n",
      "epoch: 119,  batch step: 54, loss: 4.36488676071167\n",
      "epoch: 119,  batch step: 55, loss: 24.999797821044922\n",
      "epoch: 119,  batch step: 56, loss: 3.6950788497924805\n",
      "epoch: 119,  batch step: 57, loss: 6.499568462371826\n",
      "epoch: 119,  batch step: 58, loss: 13.353785514831543\n",
      "epoch: 119,  batch step: 59, loss: 21.447490692138672\n",
      "epoch: 119,  batch step: 60, loss: 10.2254638671875\n",
      "epoch: 119,  batch step: 61, loss: 2.7198123931884766\n",
      "epoch: 119,  batch step: 62, loss: 2.952967643737793\n",
      "epoch: 119,  batch step: 63, loss: 2.527416944503784\n",
      "epoch: 119,  batch step: 64, loss: 2.8499789237976074\n",
      "epoch: 119,  batch step: 65, loss: 7.821261405944824\n",
      "epoch: 119,  batch step: 66, loss: 92.5582504272461\n",
      "epoch: 119,  batch step: 67, loss: 15.05055046081543\n",
      "epoch: 119,  batch step: 68, loss: 65.48469543457031\n",
      "epoch: 119,  batch step: 69, loss: 2.3959805965423584\n",
      "epoch: 119,  batch step: 70, loss: 14.168703079223633\n",
      "epoch: 119,  batch step: 71, loss: 3.318342685699463\n",
      "epoch: 119,  batch step: 72, loss: 13.920611381530762\n",
      "epoch: 119,  batch step: 73, loss: 10.009672164916992\n",
      "epoch: 119,  batch step: 74, loss: 3.2366387844085693\n",
      "epoch: 119,  batch step: 75, loss: 9.772741317749023\n",
      "epoch: 119,  batch step: 76, loss: 12.29642105102539\n",
      "epoch: 119,  batch step: 77, loss: 17.064464569091797\n",
      "epoch: 119,  batch step: 78, loss: 14.695816040039062\n",
      "epoch: 119,  batch step: 79, loss: 3.0645594596862793\n",
      "epoch: 119,  batch step: 80, loss: 2.3787689208984375\n",
      "epoch: 119,  batch step: 81, loss: 2.0653138160705566\n",
      "epoch: 119,  batch step: 82, loss: 29.566938400268555\n",
      "epoch: 119,  batch step: 83, loss: 2.295121669769287\n",
      "epoch: 119,  batch step: 84, loss: 46.86646270751953\n",
      "epoch: 119,  batch step: 85, loss: 14.067306518554688\n",
      "epoch: 119,  batch step: 86, loss: 3.969975471496582\n",
      "epoch: 119,  batch step: 87, loss: 2.7673964500427246\n",
      "epoch: 119,  batch step: 88, loss: 4.982203006744385\n",
      "epoch: 119,  batch step: 89, loss: 77.76097106933594\n",
      "epoch: 119,  batch step: 90, loss: 2.140169143676758\n",
      "epoch: 119,  batch step: 91, loss: 9.472417831420898\n",
      "epoch: 119,  batch step: 92, loss: 1.6537036895751953\n",
      "epoch: 119,  batch step: 93, loss: 2.746058225631714\n",
      "epoch: 119,  batch step: 94, loss: 2.07735013961792\n",
      "epoch: 119,  batch step: 95, loss: 1.6729100942611694\n",
      "epoch: 119,  batch step: 96, loss: 32.39696502685547\n",
      "epoch: 119,  batch step: 97, loss: 2.6237382888793945\n",
      "epoch: 119,  batch step: 98, loss: 13.281661987304688\n",
      "epoch: 119,  batch step: 99, loss: 2.3978326320648193\n",
      "epoch: 119,  batch step: 100, loss: 7.195714473724365\n",
      "epoch: 119,  batch step: 101, loss: 5.077375411987305\n",
      "epoch: 119,  batch step: 102, loss: 5.699716091156006\n",
      "epoch: 119,  batch step: 103, loss: 2.0683774948120117\n",
      "epoch: 119,  batch step: 104, loss: 2.0071444511413574\n",
      "epoch: 119,  batch step: 105, loss: 1.5558929443359375\n",
      "epoch: 119,  batch step: 106, loss: 8.485721588134766\n",
      "epoch: 119,  batch step: 107, loss: 12.63297176361084\n",
      "epoch: 119,  batch step: 108, loss: 2.638112783432007\n",
      "epoch: 119,  batch step: 109, loss: 12.729662895202637\n",
      "epoch: 119,  batch step: 110, loss: 3.891141891479492\n",
      "epoch: 119,  batch step: 111, loss: 3.2501540184020996\n",
      "epoch: 119,  batch step: 112, loss: 18.14651107788086\n",
      "epoch: 119,  batch step: 113, loss: 5.956946849822998\n",
      "epoch: 119,  batch step: 114, loss: 4.252784729003906\n",
      "epoch: 119,  batch step: 115, loss: 3.041604995727539\n",
      "epoch: 119,  batch step: 116, loss: 6.794955730438232\n",
      "epoch: 119,  batch step: 117, loss: 1.8528932332992554\n",
      "epoch: 119,  batch step: 118, loss: 1.8844919204711914\n",
      "epoch: 119,  batch step: 119, loss: 1.52701997756958\n",
      "epoch: 119,  batch step: 120, loss: 39.43754959106445\n",
      "epoch: 119,  batch step: 121, loss: 3.5544166564941406\n",
      "epoch: 119,  batch step: 122, loss: 6.9429731369018555\n",
      "epoch: 119,  batch step: 123, loss: 4.637514114379883\n",
      "epoch: 119,  batch step: 124, loss: 7.2000017166137695\n",
      "epoch: 119,  batch step: 125, loss: 5.271939277648926\n",
      "epoch: 119,  batch step: 126, loss: 2.128812074661255\n",
      "epoch: 119,  batch step: 127, loss: 12.955007553100586\n",
      "epoch: 119,  batch step: 128, loss: 2.8037447929382324\n",
      "epoch: 119,  batch step: 129, loss: 1.40716552734375\n",
      "epoch: 119,  batch step: 130, loss: 1.4133739471435547\n",
      "epoch: 119,  batch step: 131, loss: 5.635936260223389\n",
      "epoch: 119,  batch step: 132, loss: 12.27592658996582\n",
      "epoch: 119,  batch step: 133, loss: 2.4772825241088867\n",
      "epoch: 119,  batch step: 134, loss: 92.4736328125\n",
      "epoch: 119,  batch step: 135, loss: 12.043261528015137\n",
      "epoch: 119,  batch step: 136, loss: 49.56643295288086\n",
      "epoch: 119,  batch step: 137, loss: 19.881898880004883\n",
      "epoch: 119,  batch step: 138, loss: 10.604007720947266\n",
      "epoch: 119,  batch step: 139, loss: 6.260383605957031\n",
      "epoch: 119,  batch step: 140, loss: 2.3520278930664062\n",
      "epoch: 119,  batch step: 141, loss: 7.415317535400391\n",
      "epoch: 119,  batch step: 142, loss: 6.446602821350098\n",
      "epoch: 119,  batch step: 143, loss: 9.733890533447266\n",
      "epoch: 119,  batch step: 144, loss: 58.64199447631836\n",
      "epoch: 119,  batch step: 145, loss: 14.252649307250977\n",
      "epoch: 119,  batch step: 146, loss: 1.9812002182006836\n",
      "epoch: 119,  batch step: 147, loss: 5.204799652099609\n",
      "epoch: 119,  batch step: 148, loss: 70.45877075195312\n",
      "epoch: 119,  batch step: 149, loss: 26.082136154174805\n",
      "epoch: 119,  batch step: 150, loss: 2.018070936203003\n",
      "epoch: 119,  batch step: 151, loss: 1.9672693014144897\n",
      "epoch: 119,  batch step: 152, loss: 1.9268745183944702\n",
      "epoch: 119,  batch step: 153, loss: 9.599658966064453\n",
      "epoch: 119,  batch step: 154, loss: 2.93891978263855\n",
      "epoch: 119,  batch step: 155, loss: 2.847749710083008\n",
      "epoch: 119,  batch step: 156, loss: 1.4663575887680054\n",
      "epoch: 119,  batch step: 157, loss: 7.624940395355225\n",
      "epoch: 119,  batch step: 158, loss: 12.665910720825195\n",
      "epoch: 119,  batch step: 159, loss: 2.154329776763916\n",
      "epoch: 119,  batch step: 160, loss: 2.6679978370666504\n",
      "epoch: 119,  batch step: 161, loss: 4.638105392456055\n",
      "epoch: 119,  batch step: 162, loss: 5.931386947631836\n",
      "epoch: 119,  batch step: 163, loss: 2.4188456535339355\n",
      "epoch: 119,  batch step: 164, loss: 2.561462163925171\n",
      "epoch: 119,  batch step: 165, loss: 2.014345645904541\n",
      "epoch: 119,  batch step: 166, loss: 33.325950622558594\n",
      "epoch: 119,  batch step: 167, loss: 3.217834234237671\n",
      "epoch: 119,  batch step: 168, loss: 11.907161712646484\n",
      "epoch: 119,  batch step: 169, loss: 18.803787231445312\n",
      "epoch: 119,  batch step: 170, loss: 1.8165395259857178\n",
      "epoch: 119,  batch step: 171, loss: 3.256493330001831\n",
      "epoch: 119,  batch step: 172, loss: 27.44142723083496\n",
      "epoch: 119,  batch step: 173, loss: 10.625528335571289\n",
      "epoch: 119,  batch step: 174, loss: 2.4769673347473145\n",
      "epoch: 119,  batch step: 175, loss: 7.839992523193359\n",
      "epoch: 119,  batch step: 176, loss: 2.020603895187378\n",
      "epoch: 119,  batch step: 177, loss: 12.034505844116211\n",
      "epoch: 119,  batch step: 178, loss: 3.1407341957092285\n",
      "epoch: 119,  batch step: 179, loss: 2.097257137298584\n",
      "epoch: 119,  batch step: 180, loss: 15.994516372680664\n",
      "epoch: 119,  batch step: 181, loss: 3.081395149230957\n",
      "epoch: 119,  batch step: 182, loss: 15.211337089538574\n",
      "epoch: 119,  batch step: 183, loss: 8.392000198364258\n",
      "epoch: 119,  batch step: 184, loss: 1.8674566745758057\n",
      "epoch: 119,  batch step: 185, loss: 6.674394130706787\n",
      "epoch: 119,  batch step: 186, loss: 1.3094576597213745\n",
      "epoch: 119,  batch step: 187, loss: 2.461704730987549\n",
      "epoch: 119,  batch step: 188, loss: 31.85812759399414\n",
      "epoch: 119,  batch step: 189, loss: 28.944807052612305\n",
      "epoch: 119,  batch step: 190, loss: 2.0323739051818848\n",
      "epoch: 119,  batch step: 191, loss: 3.317974805831909\n",
      "epoch: 119,  batch step: 192, loss: 36.74053955078125\n",
      "epoch: 119,  batch step: 193, loss: 11.316058158874512\n",
      "epoch: 119,  batch step: 194, loss: 2.5985288619995117\n",
      "epoch: 119,  batch step: 195, loss: 12.868121147155762\n",
      "epoch: 119,  batch step: 196, loss: 2.583631992340088\n",
      "epoch: 119,  batch step: 197, loss: 1.613478660583496\n",
      "epoch: 119,  batch step: 198, loss: 1.3722410202026367\n",
      "epoch: 119,  batch step: 199, loss: 10.75471305847168\n",
      "epoch: 119,  batch step: 200, loss: 4.689846515655518\n",
      "epoch: 119,  batch step: 201, loss: 3.962482213973999\n",
      "epoch: 119,  batch step: 202, loss: 2.391939401626587\n",
      "epoch: 119,  batch step: 203, loss: 14.044920921325684\n",
      "epoch: 119,  batch step: 204, loss: 7.702233791351318\n",
      "epoch: 119,  batch step: 205, loss: 3.563807249069214\n",
      "epoch: 119,  batch step: 206, loss: 11.819522857666016\n",
      "epoch: 119,  batch step: 207, loss: 22.535526275634766\n",
      "epoch: 119,  batch step: 208, loss: 1.97793710231781\n",
      "epoch: 119,  batch step: 209, loss: 1.7501814365386963\n",
      "epoch: 119,  batch step: 210, loss: 8.647981643676758\n",
      "epoch: 119,  batch step: 211, loss: 34.312599182128906\n",
      "epoch: 119,  batch step: 212, loss: 28.69649314880371\n",
      "epoch: 119,  batch step: 213, loss: 5.178228378295898\n",
      "epoch: 119,  batch step: 214, loss: 6.422582149505615\n",
      "epoch: 119,  batch step: 215, loss: 2.0627918243408203\n",
      "epoch: 119,  batch step: 216, loss: 7.060126781463623\n",
      "epoch: 119,  batch step: 217, loss: 9.925607681274414\n",
      "epoch: 119,  batch step: 218, loss: 2.169365882873535\n",
      "epoch: 119,  batch step: 219, loss: 2.1194190979003906\n",
      "epoch: 119,  batch step: 220, loss: 2.3072969913482666\n",
      "epoch: 119,  batch step: 221, loss: 6.7282233238220215\n",
      "epoch: 119,  batch step: 222, loss: 1.784477949142456\n",
      "epoch: 119,  batch step: 223, loss: 2.317056655883789\n",
      "epoch: 119,  batch step: 224, loss: 1.94316565990448\n",
      "epoch: 119,  batch step: 225, loss: 16.057575225830078\n",
      "epoch: 119,  batch step: 226, loss: 48.70819854736328\n",
      "epoch: 119,  batch step: 227, loss: 7.007060527801514\n",
      "epoch: 119,  batch step: 228, loss: 7.878530502319336\n",
      "epoch: 119,  batch step: 229, loss: 1.4414680004119873\n",
      "epoch: 119,  batch step: 230, loss: 8.795604705810547\n",
      "epoch: 119,  batch step: 231, loss: 2.2341341972351074\n",
      "epoch: 119,  batch step: 232, loss: 9.759679794311523\n",
      "epoch: 119,  batch step: 233, loss: 1.5731265544891357\n",
      "epoch: 119,  batch step: 234, loss: 2.6572155952453613\n",
      "epoch: 119,  batch step: 235, loss: 3.5558505058288574\n",
      "epoch: 119,  batch step: 236, loss: 2.080770492553711\n",
      "epoch: 119,  batch step: 237, loss: 6.635318756103516\n",
      "epoch: 119,  batch step: 238, loss: 1.5548350811004639\n",
      "epoch: 119,  batch step: 239, loss: 2.231541395187378\n",
      "epoch: 119,  batch step: 240, loss: 9.748512268066406\n",
      "epoch: 119,  batch step: 241, loss: 3.4672391414642334\n",
      "epoch: 119,  batch step: 242, loss: 21.306438446044922\n",
      "epoch: 119,  batch step: 243, loss: 2.7292323112487793\n",
      "epoch: 119,  batch step: 244, loss: 10.39918041229248\n",
      "epoch: 119,  batch step: 245, loss: 2.119586706161499\n",
      "epoch: 119,  batch step: 246, loss: 2.41776704788208\n",
      "epoch: 119,  batch step: 247, loss: 7.569213390350342\n",
      "epoch: 119,  batch step: 248, loss: 3.9421043395996094\n",
      "epoch: 119,  batch step: 249, loss: 1.9037692546844482\n",
      "epoch: 119,  batch step: 250, loss: 27.485136032104492\n",
      "epoch: 119,  batch step: 251, loss: 3.698713779449463\n",
      "finished saving checkpoints\n",
      "validation error epoch  119:    tensor(69.3107, device='cuda:0')\n",
      "316\n",
      "epoch: 120,  batch step: 0, loss: 2.7005250453948975\n",
      "epoch: 120,  batch step: 1, loss: 2.570516347885132\n",
      "epoch: 120,  batch step: 2, loss: 2.8777072429656982\n",
      "epoch: 120,  batch step: 3, loss: 3.0998668670654297\n",
      "epoch: 120,  batch step: 4, loss: 41.32999801635742\n",
      "epoch: 120,  batch step: 5, loss: 2.045959234237671\n",
      "epoch: 120,  batch step: 6, loss: 2.067091941833496\n",
      "epoch: 120,  batch step: 7, loss: 2.34769606590271\n",
      "epoch: 120,  batch step: 8, loss: 6.5674872398376465\n",
      "epoch: 120,  batch step: 9, loss: 6.0332794189453125\n",
      "epoch: 120,  batch step: 10, loss: 1.5388848781585693\n",
      "epoch: 120,  batch step: 11, loss: 3.5484073162078857\n",
      "epoch: 120,  batch step: 12, loss: 2.131347417831421\n",
      "epoch: 120,  batch step: 13, loss: 1.4891490936279297\n",
      "epoch: 120,  batch step: 14, loss: 2.3298373222351074\n",
      "epoch: 120,  batch step: 15, loss: 15.144878387451172\n",
      "epoch: 120,  batch step: 16, loss: 10.285472869873047\n",
      "epoch: 120,  batch step: 17, loss: 1.5034575462341309\n",
      "epoch: 120,  batch step: 18, loss: 2.164844512939453\n",
      "epoch: 120,  batch step: 19, loss: 23.361278533935547\n",
      "epoch: 120,  batch step: 20, loss: 9.171463966369629\n",
      "epoch: 120,  batch step: 21, loss: 4.50266695022583\n",
      "epoch: 120,  batch step: 22, loss: 2.2007014751434326\n",
      "epoch: 120,  batch step: 23, loss: 1.8318918943405151\n",
      "epoch: 120,  batch step: 24, loss: 10.844141960144043\n",
      "epoch: 120,  batch step: 25, loss: 40.309417724609375\n",
      "epoch: 120,  batch step: 26, loss: 9.50196647644043\n",
      "epoch: 120,  batch step: 27, loss: 55.5823974609375\n",
      "epoch: 120,  batch step: 28, loss: 2.1685702800750732\n",
      "epoch: 120,  batch step: 29, loss: 18.20372772216797\n",
      "epoch: 120,  batch step: 30, loss: 8.735662460327148\n",
      "epoch: 120,  batch step: 31, loss: 1.7628921270370483\n",
      "epoch: 120,  batch step: 32, loss: 10.18558120727539\n",
      "epoch: 120,  batch step: 33, loss: 1.3960638046264648\n",
      "epoch: 120,  batch step: 34, loss: 29.85439109802246\n",
      "epoch: 120,  batch step: 35, loss: 9.249150276184082\n",
      "epoch: 120,  batch step: 36, loss: 1.6567481756210327\n",
      "epoch: 120,  batch step: 37, loss: 2.7273988723754883\n",
      "epoch: 120,  batch step: 38, loss: 1.5013501644134521\n",
      "epoch: 120,  batch step: 39, loss: 1.8533339500427246\n",
      "epoch: 120,  batch step: 40, loss: 11.287951469421387\n",
      "epoch: 120,  batch step: 41, loss: 2.6956746578216553\n",
      "epoch: 120,  batch step: 42, loss: 1.889059066772461\n",
      "epoch: 120,  batch step: 43, loss: 15.536982536315918\n",
      "epoch: 120,  batch step: 44, loss: 2.807614326477051\n",
      "epoch: 120,  batch step: 45, loss: 1.8630995750427246\n",
      "epoch: 120,  batch step: 46, loss: 14.652003288269043\n",
      "epoch: 120,  batch step: 47, loss: 2.5630102157592773\n",
      "epoch: 120,  batch step: 48, loss: 2.2259669303894043\n",
      "epoch: 120,  batch step: 49, loss: 2.0316286087036133\n",
      "epoch: 120,  batch step: 50, loss: 7.8513898849487305\n",
      "epoch: 120,  batch step: 51, loss: 49.91965866088867\n",
      "epoch: 120,  batch step: 52, loss: 2.5902247428894043\n",
      "epoch: 120,  batch step: 53, loss: 1.485440731048584\n",
      "epoch: 120,  batch step: 54, loss: 3.5748772621154785\n",
      "epoch: 120,  batch step: 55, loss: 8.747020721435547\n",
      "epoch: 120,  batch step: 56, loss: 7.601428985595703\n",
      "epoch: 120,  batch step: 57, loss: 71.25723266601562\n",
      "epoch: 120,  batch step: 58, loss: 27.646385192871094\n",
      "epoch: 120,  batch step: 59, loss: 11.624700546264648\n",
      "epoch: 120,  batch step: 60, loss: 30.53838348388672\n",
      "epoch: 120,  batch step: 61, loss: 19.510845184326172\n",
      "epoch: 120,  batch step: 62, loss: 8.165373802185059\n",
      "epoch: 120,  batch step: 63, loss: 2.6866726875305176\n",
      "epoch: 120,  batch step: 64, loss: 13.509420394897461\n",
      "epoch: 120,  batch step: 65, loss: 18.66854476928711\n",
      "epoch: 120,  batch step: 66, loss: 55.42442321777344\n",
      "epoch: 120,  batch step: 67, loss: 3.2093753814697266\n",
      "epoch: 120,  batch step: 68, loss: 2.4221363067626953\n",
      "epoch: 120,  batch step: 69, loss: 17.97818946838379\n",
      "epoch: 120,  batch step: 70, loss: 13.004297256469727\n",
      "epoch: 120,  batch step: 71, loss: 37.84741973876953\n",
      "epoch: 120,  batch step: 72, loss: 2.388247013092041\n",
      "epoch: 120,  batch step: 73, loss: 8.831258773803711\n",
      "epoch: 120,  batch step: 74, loss: 37.984519958496094\n",
      "epoch: 120,  batch step: 75, loss: 38.96129608154297\n",
      "epoch: 120,  batch step: 76, loss: 45.96903610229492\n",
      "epoch: 120,  batch step: 77, loss: 6.508240699768066\n",
      "epoch: 120,  batch step: 78, loss: 6.373165130615234\n",
      "epoch: 120,  batch step: 79, loss: 2.1046969890594482\n",
      "epoch: 120,  batch step: 80, loss: 11.774378776550293\n",
      "epoch: 120,  batch step: 81, loss: 2.4588372707366943\n",
      "epoch: 120,  batch step: 82, loss: 7.193720817565918\n",
      "epoch: 120,  batch step: 83, loss: 10.201608657836914\n",
      "epoch: 120,  batch step: 84, loss: 1.6770204305648804\n",
      "epoch: 120,  batch step: 85, loss: 5.410449981689453\n",
      "epoch: 120,  batch step: 86, loss: 1.2319697141647339\n",
      "epoch: 120,  batch step: 87, loss: 1.5017247200012207\n",
      "epoch: 120,  batch step: 88, loss: 3.155567169189453\n",
      "epoch: 120,  batch step: 89, loss: 4.393161296844482\n",
      "epoch: 120,  batch step: 90, loss: 6.692852020263672\n",
      "epoch: 120,  batch step: 91, loss: 7.3725690841674805\n",
      "epoch: 120,  batch step: 92, loss: 34.976280212402344\n",
      "epoch: 120,  batch step: 93, loss: 2.7416629791259766\n",
      "epoch: 120,  batch step: 94, loss: 37.2617301940918\n",
      "epoch: 120,  batch step: 95, loss: 2.3917317390441895\n",
      "epoch: 120,  batch step: 96, loss: 2.4578118324279785\n",
      "epoch: 120,  batch step: 97, loss: 8.601669311523438\n",
      "epoch: 120,  batch step: 98, loss: 2.132112503051758\n",
      "epoch: 120,  batch step: 99, loss: 8.68376636505127\n",
      "epoch: 120,  batch step: 100, loss: 6.8271002769470215\n",
      "epoch: 120,  batch step: 101, loss: 1.6666860580444336\n",
      "epoch: 120,  batch step: 102, loss: 7.03870153427124\n",
      "epoch: 120,  batch step: 103, loss: 3.2734832763671875\n",
      "epoch: 120,  batch step: 104, loss: 3.4871184825897217\n",
      "epoch: 120,  batch step: 105, loss: 21.424652099609375\n",
      "epoch: 120,  batch step: 106, loss: 30.543149948120117\n",
      "epoch: 120,  batch step: 107, loss: 8.952449798583984\n",
      "epoch: 120,  batch step: 108, loss: 2.2140510082244873\n",
      "epoch: 120,  batch step: 109, loss: 1.8176298141479492\n",
      "epoch: 120,  batch step: 110, loss: 3.7611780166625977\n",
      "epoch: 120,  batch step: 111, loss: 41.99858093261719\n",
      "epoch: 120,  batch step: 112, loss: 3.061854839324951\n",
      "epoch: 120,  batch step: 113, loss: 7.328547954559326\n",
      "epoch: 120,  batch step: 114, loss: 2.562746047973633\n",
      "epoch: 120,  batch step: 115, loss: 3.844804525375366\n",
      "epoch: 120,  batch step: 116, loss: 26.48482322692871\n",
      "epoch: 120,  batch step: 117, loss: 2.0018563270568848\n",
      "epoch: 120,  batch step: 118, loss: 1.6994495391845703\n",
      "epoch: 120,  batch step: 119, loss: 2.1274213790893555\n",
      "epoch: 120,  batch step: 120, loss: 11.184236526489258\n",
      "epoch: 120,  batch step: 121, loss: 5.261833667755127\n",
      "epoch: 120,  batch step: 122, loss: 2.070610523223877\n",
      "epoch: 120,  batch step: 123, loss: 2.3875746726989746\n",
      "epoch: 120,  batch step: 124, loss: 1.7011480331420898\n",
      "epoch: 120,  batch step: 125, loss: 1.6552257537841797\n",
      "epoch: 120,  batch step: 126, loss: 13.985129356384277\n",
      "epoch: 120,  batch step: 127, loss: 7.025368690490723\n",
      "epoch: 120,  batch step: 128, loss: 16.359973907470703\n",
      "epoch: 120,  batch step: 129, loss: 1.4155302047729492\n",
      "epoch: 120,  batch step: 130, loss: 31.720458984375\n",
      "epoch: 120,  batch step: 131, loss: 2.305126190185547\n",
      "epoch: 120,  batch step: 132, loss: 5.374108791351318\n",
      "epoch: 120,  batch step: 133, loss: 1.960371971130371\n",
      "epoch: 120,  batch step: 134, loss: 2.4247307777404785\n",
      "epoch: 120,  batch step: 135, loss: 26.612850189208984\n",
      "epoch: 120,  batch step: 136, loss: 2.450080394744873\n",
      "epoch: 120,  batch step: 137, loss: 1.6895413398742676\n",
      "epoch: 120,  batch step: 138, loss: 1.364084005355835\n",
      "epoch: 120,  batch step: 139, loss: 13.021829605102539\n",
      "epoch: 120,  batch step: 140, loss: 4.21338415145874\n",
      "epoch: 120,  batch step: 141, loss: 3.150648832321167\n",
      "epoch: 120,  batch step: 142, loss: 49.812583923339844\n",
      "epoch: 120,  batch step: 143, loss: 2.585601329803467\n",
      "epoch: 120,  batch step: 144, loss: 10.404481887817383\n",
      "epoch: 120,  batch step: 145, loss: 2.033020257949829\n",
      "epoch: 120,  batch step: 146, loss: 2.1172304153442383\n",
      "epoch: 120,  batch step: 147, loss: 9.621512413024902\n",
      "epoch: 120,  batch step: 148, loss: 13.226800918579102\n",
      "epoch: 120,  batch step: 149, loss: 8.719943046569824\n",
      "epoch: 120,  batch step: 150, loss: 1.216300368309021\n",
      "epoch: 120,  batch step: 151, loss: 4.632296562194824\n",
      "epoch: 120,  batch step: 152, loss: 16.96712875366211\n",
      "epoch: 120,  batch step: 153, loss: 23.023788452148438\n",
      "epoch: 120,  batch step: 154, loss: 25.270750045776367\n",
      "epoch: 120,  batch step: 155, loss: 4.552175998687744\n",
      "epoch: 120,  batch step: 156, loss: 1.7381929159164429\n",
      "epoch: 120,  batch step: 157, loss: 2.8771982192993164\n",
      "epoch: 120,  batch step: 158, loss: 2.4355907440185547\n",
      "epoch: 120,  batch step: 159, loss: 9.064729690551758\n",
      "epoch: 120,  batch step: 160, loss: 11.808073043823242\n",
      "epoch: 120,  batch step: 161, loss: 1.5351552963256836\n",
      "epoch: 120,  batch step: 162, loss: 3.0675580501556396\n",
      "epoch: 120,  batch step: 163, loss: 62.148765563964844\n",
      "epoch: 120,  batch step: 164, loss: 12.951501846313477\n",
      "epoch: 120,  batch step: 165, loss: 18.136016845703125\n",
      "epoch: 120,  batch step: 166, loss: 13.143102645874023\n",
      "epoch: 120,  batch step: 167, loss: 3.1570630073547363\n",
      "epoch: 120,  batch step: 168, loss: 10.780951499938965\n",
      "epoch: 120,  batch step: 169, loss: 2.8347835540771484\n",
      "epoch: 120,  batch step: 170, loss: 1.5264348983764648\n",
      "epoch: 120,  batch step: 171, loss: 8.610121726989746\n",
      "epoch: 120,  batch step: 172, loss: 30.80939483642578\n",
      "epoch: 120,  batch step: 173, loss: 7.958038330078125\n",
      "epoch: 120,  batch step: 174, loss: 3.5776290893554688\n",
      "epoch: 120,  batch step: 175, loss: 2.9906492233276367\n",
      "epoch: 120,  batch step: 176, loss: 66.46741485595703\n",
      "epoch: 120,  batch step: 177, loss: 1.2898130416870117\n",
      "epoch: 120,  batch step: 178, loss: 5.361241817474365\n",
      "epoch: 120,  batch step: 179, loss: 55.368465423583984\n",
      "epoch: 120,  batch step: 180, loss: 1.4276084899902344\n",
      "epoch: 120,  batch step: 181, loss: 1.5571478605270386\n",
      "epoch: 120,  batch step: 182, loss: 1.6598809957504272\n",
      "epoch: 120,  batch step: 183, loss: 2.2278571128845215\n",
      "epoch: 120,  batch step: 184, loss: 2.532303810119629\n",
      "epoch: 120,  batch step: 185, loss: 5.368592262268066\n",
      "epoch: 120,  batch step: 186, loss: 5.274167537689209\n",
      "epoch: 120,  batch step: 187, loss: 1.7053792476654053\n",
      "epoch: 120,  batch step: 188, loss: 1.3636430501937866\n",
      "epoch: 120,  batch step: 189, loss: 5.666614532470703\n",
      "epoch: 120,  batch step: 190, loss: 1.5445102453231812\n",
      "epoch: 120,  batch step: 191, loss: 2.3909835815429688\n",
      "epoch: 120,  batch step: 192, loss: 1.6646636724472046\n",
      "epoch: 120,  batch step: 193, loss: 1.6325972080230713\n",
      "epoch: 120,  batch step: 194, loss: 2.1618475914001465\n",
      "epoch: 120,  batch step: 195, loss: 9.578065872192383\n",
      "epoch: 120,  batch step: 196, loss: 1.6899851560592651\n",
      "epoch: 120,  batch step: 197, loss: 6.647706508636475\n",
      "epoch: 120,  batch step: 198, loss: 1.9186575412750244\n",
      "epoch: 120,  batch step: 199, loss: 6.668084144592285\n",
      "epoch: 120,  batch step: 200, loss: 7.122600555419922\n",
      "epoch: 120,  batch step: 201, loss: 8.985579490661621\n",
      "epoch: 120,  batch step: 202, loss: 7.628844738006592\n",
      "epoch: 120,  batch step: 203, loss: 1.6483230590820312\n",
      "epoch: 120,  batch step: 204, loss: 2.3233728408813477\n",
      "epoch: 120,  batch step: 205, loss: 1.9527835845947266\n",
      "epoch: 120,  batch step: 206, loss: 2.060086727142334\n",
      "epoch: 120,  batch step: 207, loss: 6.979346752166748\n",
      "epoch: 120,  batch step: 208, loss: 131.99215698242188\n",
      "epoch: 120,  batch step: 209, loss: 2.8880507946014404\n",
      "epoch: 120,  batch step: 210, loss: 2.269721031188965\n",
      "epoch: 120,  batch step: 211, loss: 6.580638885498047\n",
      "epoch: 120,  batch step: 212, loss: 2.2820205688476562\n",
      "epoch: 120,  batch step: 213, loss: 2.652351140975952\n",
      "epoch: 120,  batch step: 214, loss: 1.8773231506347656\n",
      "epoch: 120,  batch step: 215, loss: 2.0862646102905273\n",
      "epoch: 120,  batch step: 216, loss: 2.776604652404785\n",
      "epoch: 120,  batch step: 217, loss: 4.4727277755737305\n",
      "epoch: 120,  batch step: 218, loss: 9.38647747039795\n",
      "epoch: 120,  batch step: 219, loss: 5.34601354598999\n",
      "epoch: 120,  batch step: 220, loss: 1.926193118095398\n",
      "epoch: 120,  batch step: 221, loss: 1.785037875175476\n",
      "epoch: 120,  batch step: 222, loss: 1.2806546688079834\n",
      "epoch: 120,  batch step: 223, loss: 3.82204008102417\n",
      "epoch: 120,  batch step: 224, loss: 1.6368902921676636\n",
      "epoch: 120,  batch step: 225, loss: 5.570697784423828\n",
      "epoch: 120,  batch step: 226, loss: 1.6338393688201904\n",
      "epoch: 120,  batch step: 227, loss: 10.466743469238281\n",
      "epoch: 120,  batch step: 228, loss: 3.292052745819092\n",
      "epoch: 120,  batch step: 229, loss: 1.756202220916748\n",
      "epoch: 120,  batch step: 230, loss: 9.383857727050781\n",
      "epoch: 120,  batch step: 231, loss: 10.305294036865234\n",
      "epoch: 120,  batch step: 232, loss: 48.35331344604492\n",
      "epoch: 120,  batch step: 233, loss: 11.485777854919434\n",
      "epoch: 120,  batch step: 234, loss: 1.6641170978546143\n",
      "epoch: 120,  batch step: 235, loss: 15.432607650756836\n",
      "epoch: 120,  batch step: 236, loss: 1.847071647644043\n",
      "epoch: 120,  batch step: 237, loss: 25.528121948242188\n",
      "epoch: 120,  batch step: 238, loss: 9.522749900817871\n",
      "epoch: 120,  batch step: 239, loss: 14.175750732421875\n",
      "epoch: 120,  batch step: 240, loss: 1.1971909999847412\n",
      "epoch: 120,  batch step: 241, loss: 14.131162643432617\n",
      "epoch: 120,  batch step: 242, loss: 2.3826560974121094\n",
      "epoch: 120,  batch step: 243, loss: 1.6862976551055908\n",
      "epoch: 120,  batch step: 244, loss: 1.8177225589752197\n",
      "epoch: 120,  batch step: 245, loss: 5.10324764251709\n",
      "epoch: 120,  batch step: 246, loss: 2.249919891357422\n",
      "epoch: 120,  batch step: 247, loss: 2.0846376419067383\n",
      "epoch: 120,  batch step: 248, loss: 7.927431106567383\n",
      "epoch: 120,  batch step: 249, loss: 17.284530639648438\n",
      "epoch: 120,  batch step: 250, loss: 6.025721549987793\n",
      "epoch: 120,  batch step: 251, loss: 44.55569839477539\n",
      "validation error epoch  120:    tensor(70.5645, device='cuda:0')\n",
      "316\n",
      "epoch: 121,  batch step: 0, loss: 9.681194305419922\n",
      "epoch: 121,  batch step: 1, loss: 58.697505950927734\n",
      "epoch: 121,  batch step: 2, loss: 3.9708077907562256\n",
      "epoch: 121,  batch step: 3, loss: 7.545262813568115\n",
      "epoch: 121,  batch step: 4, loss: 16.511127471923828\n",
      "epoch: 121,  batch step: 5, loss: 4.542579650878906\n",
      "epoch: 121,  batch step: 6, loss: 7.354313850402832\n",
      "epoch: 121,  batch step: 7, loss: 10.619422912597656\n",
      "epoch: 121,  batch step: 8, loss: 2.8468940258026123\n",
      "epoch: 121,  batch step: 9, loss: 34.4822998046875\n",
      "epoch: 121,  batch step: 10, loss: 4.607784748077393\n",
      "epoch: 121,  batch step: 11, loss: 7.611758708953857\n",
      "epoch: 121,  batch step: 12, loss: 7.27225399017334\n",
      "epoch: 121,  batch step: 13, loss: 3.788445472717285\n",
      "epoch: 121,  batch step: 14, loss: 28.0703182220459\n",
      "epoch: 121,  batch step: 15, loss: 16.468053817749023\n",
      "epoch: 121,  batch step: 16, loss: 4.934237957000732\n",
      "epoch: 121,  batch step: 17, loss: 2.7503135204315186\n",
      "epoch: 121,  batch step: 18, loss: 2.3369481563568115\n",
      "epoch: 121,  batch step: 19, loss: 2.2239456176757812\n",
      "epoch: 121,  batch step: 20, loss: 6.804841041564941\n",
      "epoch: 121,  batch step: 21, loss: 23.042675018310547\n",
      "epoch: 121,  batch step: 22, loss: 2.3335044384002686\n",
      "epoch: 121,  batch step: 23, loss: 10.911757469177246\n",
      "epoch: 121,  batch step: 24, loss: 8.76623821258545\n",
      "epoch: 121,  batch step: 25, loss: 7.868720054626465\n",
      "epoch: 121,  batch step: 26, loss: 3.4632468223571777\n",
      "epoch: 121,  batch step: 27, loss: 2.917543411254883\n",
      "epoch: 121,  batch step: 28, loss: 5.955029487609863\n",
      "epoch: 121,  batch step: 29, loss: 3.5665230751037598\n",
      "epoch: 121,  batch step: 30, loss: 4.462587356567383\n",
      "epoch: 121,  batch step: 31, loss: 1.8086144924163818\n",
      "epoch: 121,  batch step: 32, loss: 10.861442565917969\n",
      "epoch: 121,  batch step: 33, loss: 66.60066223144531\n",
      "epoch: 121,  batch step: 34, loss: 19.724348068237305\n",
      "epoch: 121,  batch step: 35, loss: 36.59849166870117\n",
      "epoch: 121,  batch step: 36, loss: 10.939628601074219\n",
      "epoch: 121,  batch step: 37, loss: 2.2415518760681152\n",
      "epoch: 121,  batch step: 38, loss: 5.3827009201049805\n",
      "epoch: 121,  batch step: 39, loss: 10.469621658325195\n",
      "epoch: 121,  batch step: 40, loss: 1.8052176237106323\n",
      "epoch: 121,  batch step: 41, loss: 12.767539978027344\n",
      "epoch: 121,  batch step: 42, loss: 5.0383758544921875\n",
      "epoch: 121,  batch step: 43, loss: 4.117275238037109\n",
      "epoch: 121,  batch step: 44, loss: 2.4973134994506836\n",
      "epoch: 121,  batch step: 45, loss: 2.921487808227539\n",
      "epoch: 121,  batch step: 46, loss: 12.636870384216309\n",
      "epoch: 121,  batch step: 47, loss: 5.848494529724121\n",
      "epoch: 121,  batch step: 48, loss: 35.1680908203125\n",
      "epoch: 121,  batch step: 49, loss: 5.262799263000488\n",
      "epoch: 121,  batch step: 50, loss: 10.310996055603027\n",
      "epoch: 121,  batch step: 51, loss: 2.312851667404175\n",
      "epoch: 121,  batch step: 52, loss: 5.127044200897217\n",
      "epoch: 121,  batch step: 53, loss: 3.675886631011963\n",
      "epoch: 121,  batch step: 54, loss: 10.2029390335083\n",
      "epoch: 121,  batch step: 55, loss: 2.2210183143615723\n",
      "epoch: 121,  batch step: 56, loss: 10.615405082702637\n",
      "epoch: 121,  batch step: 57, loss: 76.92478942871094\n",
      "epoch: 121,  batch step: 58, loss: 6.0288801193237305\n",
      "epoch: 121,  batch step: 59, loss: 12.646955490112305\n",
      "epoch: 121,  batch step: 60, loss: 3.029439687728882\n",
      "epoch: 121,  batch step: 61, loss: 6.9012250900268555\n",
      "epoch: 121,  batch step: 62, loss: 3.179417610168457\n",
      "epoch: 121,  batch step: 63, loss: 9.199705123901367\n",
      "epoch: 121,  batch step: 64, loss: 2.30605411529541\n",
      "epoch: 121,  batch step: 65, loss: 5.958808898925781\n",
      "epoch: 121,  batch step: 66, loss: 2.5897340774536133\n",
      "epoch: 121,  batch step: 67, loss: 4.114184379577637\n",
      "epoch: 121,  batch step: 68, loss: 1.1454534530639648\n",
      "epoch: 121,  batch step: 69, loss: 3.2212042808532715\n",
      "epoch: 121,  batch step: 70, loss: 3.0957820415496826\n",
      "epoch: 121,  batch step: 71, loss: 1.4150002002716064\n",
      "epoch: 121,  batch step: 72, loss: 6.236590385437012\n",
      "epoch: 121,  batch step: 73, loss: 50.314422607421875\n",
      "epoch: 121,  batch step: 74, loss: 3.6621484756469727\n",
      "epoch: 121,  batch step: 75, loss: 13.963531494140625\n",
      "epoch: 121,  batch step: 76, loss: 2.3236937522888184\n",
      "epoch: 121,  batch step: 77, loss: 9.854192733764648\n",
      "epoch: 121,  batch step: 78, loss: 2.9159247875213623\n",
      "epoch: 121,  batch step: 79, loss: 3.3103702068328857\n",
      "epoch: 121,  batch step: 80, loss: 2.398099899291992\n",
      "epoch: 121,  batch step: 81, loss: 5.807961940765381\n",
      "epoch: 121,  batch step: 82, loss: 12.704597473144531\n",
      "epoch: 121,  batch step: 83, loss: 2.0535972118377686\n",
      "epoch: 121,  batch step: 84, loss: 1.7190040349960327\n",
      "epoch: 121,  batch step: 85, loss: 5.680320739746094\n",
      "epoch: 121,  batch step: 86, loss: 1.4182486534118652\n",
      "epoch: 121,  batch step: 87, loss: 34.482566833496094\n",
      "epoch: 121,  batch step: 88, loss: 1.636629581451416\n",
      "epoch: 121,  batch step: 89, loss: 6.411057949066162\n",
      "epoch: 121,  batch step: 90, loss: 2.5941386222839355\n",
      "epoch: 121,  batch step: 91, loss: 1.1010934114456177\n",
      "epoch: 121,  batch step: 92, loss: 2.334291458129883\n",
      "epoch: 121,  batch step: 93, loss: 2.2265431880950928\n",
      "epoch: 121,  batch step: 94, loss: 1.4947524070739746\n",
      "epoch: 121,  batch step: 95, loss: 13.579989433288574\n",
      "epoch: 121,  batch step: 96, loss: 5.834126949310303\n",
      "epoch: 121,  batch step: 97, loss: 1.4078633785247803\n",
      "epoch: 121,  batch step: 98, loss: 8.35232925415039\n",
      "epoch: 121,  batch step: 99, loss: 1.62576425075531\n",
      "epoch: 121,  batch step: 100, loss: 1.6651586294174194\n",
      "epoch: 121,  batch step: 101, loss: 43.98044967651367\n",
      "epoch: 121,  batch step: 102, loss: 51.247352600097656\n",
      "epoch: 121,  batch step: 103, loss: 6.396932601928711\n",
      "epoch: 121,  batch step: 104, loss: 1.1644842624664307\n",
      "epoch: 121,  batch step: 105, loss: 5.617104530334473\n",
      "epoch: 121,  batch step: 106, loss: 1.5675694942474365\n",
      "epoch: 121,  batch step: 107, loss: 14.546394348144531\n",
      "epoch: 121,  batch step: 108, loss: 2.460108518600464\n",
      "epoch: 121,  batch step: 109, loss: 6.959592342376709\n",
      "epoch: 121,  batch step: 110, loss: 4.060948371887207\n",
      "epoch: 121,  batch step: 111, loss: 1.9163340330123901\n",
      "epoch: 121,  batch step: 112, loss: 20.291309356689453\n",
      "epoch: 121,  batch step: 113, loss: 1.4486660957336426\n",
      "epoch: 121,  batch step: 114, loss: 7.771903991699219\n",
      "epoch: 121,  batch step: 115, loss: 1.850632667541504\n",
      "epoch: 121,  batch step: 116, loss: 11.942209243774414\n",
      "epoch: 121,  batch step: 117, loss: 10.750019073486328\n",
      "epoch: 121,  batch step: 118, loss: 3.0778675079345703\n",
      "epoch: 121,  batch step: 119, loss: 11.933771133422852\n",
      "epoch: 121,  batch step: 120, loss: 4.400491714477539\n",
      "epoch: 121,  batch step: 121, loss: 2.144641876220703\n",
      "epoch: 121,  batch step: 122, loss: 20.21673583984375\n",
      "epoch: 121,  batch step: 123, loss: 25.587890625\n",
      "epoch: 121,  batch step: 124, loss: 7.991321563720703\n",
      "epoch: 121,  batch step: 125, loss: 6.919521331787109\n",
      "epoch: 121,  batch step: 126, loss: 31.949626922607422\n",
      "epoch: 121,  batch step: 127, loss: 2.607572317123413\n",
      "epoch: 121,  batch step: 128, loss: 10.869209289550781\n",
      "epoch: 121,  batch step: 129, loss: 12.719200134277344\n",
      "epoch: 121,  batch step: 130, loss: 2.273083448410034\n",
      "epoch: 121,  batch step: 131, loss: 17.144027709960938\n",
      "epoch: 121,  batch step: 132, loss: 1.6906999349594116\n",
      "epoch: 121,  batch step: 133, loss: 2.5534777641296387\n",
      "epoch: 121,  batch step: 134, loss: 1.4606125354766846\n",
      "epoch: 121,  batch step: 135, loss: 14.321979522705078\n",
      "epoch: 121,  batch step: 136, loss: 2.242238998413086\n",
      "epoch: 121,  batch step: 137, loss: 2.4345808029174805\n",
      "epoch: 121,  batch step: 138, loss: 12.639318466186523\n",
      "epoch: 121,  batch step: 139, loss: 4.262346267700195\n",
      "epoch: 121,  batch step: 140, loss: 29.571128845214844\n",
      "epoch: 121,  batch step: 141, loss: 3.358705520629883\n",
      "epoch: 121,  batch step: 142, loss: 3.037278652191162\n",
      "epoch: 121,  batch step: 143, loss: 7.153180122375488\n",
      "epoch: 121,  batch step: 144, loss: 1.605674147605896\n",
      "epoch: 121,  batch step: 145, loss: 1.6012868881225586\n",
      "epoch: 121,  batch step: 146, loss: 8.17247200012207\n",
      "epoch: 121,  batch step: 147, loss: 18.833425521850586\n",
      "epoch: 121,  batch step: 148, loss: 1.4328200817108154\n",
      "epoch: 121,  batch step: 149, loss: 3.9824252128601074\n",
      "epoch: 121,  batch step: 150, loss: 4.614286422729492\n",
      "epoch: 121,  batch step: 151, loss: 2.1356863975524902\n",
      "epoch: 121,  batch step: 152, loss: 1.8955925703048706\n",
      "epoch: 121,  batch step: 153, loss: 6.192250728607178\n",
      "epoch: 121,  batch step: 154, loss: 1.2257850170135498\n",
      "epoch: 121,  batch step: 155, loss: 6.7507781982421875\n",
      "epoch: 121,  batch step: 156, loss: 6.404946327209473\n",
      "epoch: 121,  batch step: 157, loss: 1.6043901443481445\n",
      "epoch: 121,  batch step: 158, loss: 2.252819776535034\n",
      "epoch: 121,  batch step: 159, loss: 2.302537441253662\n",
      "epoch: 121,  batch step: 160, loss: 2.9387269020080566\n",
      "epoch: 121,  batch step: 161, loss: 1.8316371440887451\n",
      "epoch: 121,  batch step: 162, loss: 3.3884735107421875\n",
      "epoch: 121,  batch step: 163, loss: 13.491660118103027\n",
      "epoch: 121,  batch step: 164, loss: 3.0272374153137207\n",
      "epoch: 121,  batch step: 165, loss: 2.2583510875701904\n",
      "epoch: 121,  batch step: 166, loss: 6.087548732757568\n",
      "epoch: 121,  batch step: 167, loss: 3.634932041168213\n",
      "epoch: 121,  batch step: 168, loss: 2.296677589416504\n",
      "epoch: 121,  batch step: 169, loss: 13.307134628295898\n",
      "epoch: 121,  batch step: 170, loss: 2.453153371810913\n",
      "epoch: 121,  batch step: 171, loss: 1.9810266494750977\n",
      "epoch: 121,  batch step: 172, loss: 4.295865058898926\n",
      "epoch: 121,  batch step: 173, loss: 1.477874517440796\n",
      "epoch: 121,  batch step: 174, loss: 1.8649723529815674\n",
      "epoch: 121,  batch step: 175, loss: 3.7654941082000732\n",
      "epoch: 121,  batch step: 176, loss: 2.1181888580322266\n",
      "epoch: 121,  batch step: 177, loss: 2.0593314170837402\n",
      "epoch: 121,  batch step: 178, loss: 2.2737529277801514\n",
      "epoch: 121,  batch step: 179, loss: 1.7962074279785156\n",
      "epoch: 121,  batch step: 180, loss: 6.359979152679443\n",
      "epoch: 121,  batch step: 181, loss: 6.722408294677734\n",
      "epoch: 121,  batch step: 182, loss: 2.2205629348754883\n",
      "epoch: 121,  batch step: 183, loss: 2.0516061782836914\n",
      "epoch: 121,  batch step: 184, loss: 1.21194589138031\n",
      "epoch: 121,  batch step: 185, loss: 1.9711575508117676\n",
      "epoch: 121,  batch step: 186, loss: 16.5674991607666\n",
      "epoch: 121,  batch step: 187, loss: 46.981292724609375\n",
      "epoch: 121,  batch step: 188, loss: 9.103767395019531\n",
      "epoch: 121,  batch step: 189, loss: 1.9026010036468506\n",
      "epoch: 121,  batch step: 190, loss: 7.021245002746582\n",
      "epoch: 121,  batch step: 191, loss: 8.869048118591309\n",
      "epoch: 121,  batch step: 192, loss: 1.6515711545944214\n",
      "epoch: 121,  batch step: 193, loss: 4.661659240722656\n",
      "epoch: 121,  batch step: 194, loss: 20.378978729248047\n",
      "epoch: 121,  batch step: 195, loss: 16.89321517944336\n",
      "epoch: 121,  batch step: 196, loss: 3.573545217514038\n",
      "epoch: 121,  batch step: 197, loss: 1.5020989179611206\n",
      "epoch: 121,  batch step: 198, loss: 1.3983711004257202\n",
      "epoch: 121,  batch step: 199, loss: 1.6450607776641846\n",
      "epoch: 121,  batch step: 200, loss: 7.663922309875488\n",
      "epoch: 121,  batch step: 201, loss: 7.723659515380859\n",
      "epoch: 121,  batch step: 202, loss: 29.447593688964844\n",
      "epoch: 121,  batch step: 203, loss: 12.244482040405273\n",
      "epoch: 121,  batch step: 204, loss: 27.331802368164062\n",
      "epoch: 121,  batch step: 205, loss: 15.517078399658203\n",
      "epoch: 121,  batch step: 206, loss: 5.326661109924316\n",
      "epoch: 121,  batch step: 207, loss: 2.0112504959106445\n",
      "epoch: 121,  batch step: 208, loss: 36.48107147216797\n",
      "epoch: 121,  batch step: 209, loss: 1.4315537214279175\n",
      "epoch: 121,  batch step: 210, loss: 1.8040764331817627\n",
      "epoch: 121,  batch step: 211, loss: 4.449175834655762\n",
      "epoch: 121,  batch step: 212, loss: 131.5488739013672\n",
      "epoch: 121,  batch step: 213, loss: 1.4198226928710938\n",
      "epoch: 121,  batch step: 214, loss: 1.6940540075302124\n",
      "epoch: 121,  batch step: 215, loss: 3.223461151123047\n",
      "epoch: 121,  batch step: 216, loss: 6.748382568359375\n",
      "epoch: 121,  batch step: 217, loss: 5.972815990447998\n",
      "epoch: 121,  batch step: 218, loss: 15.53253173828125\n",
      "epoch: 121,  batch step: 219, loss: 1.7537908554077148\n",
      "epoch: 121,  batch step: 220, loss: 27.781417846679688\n",
      "epoch: 121,  batch step: 221, loss: 2.1877923011779785\n",
      "epoch: 121,  batch step: 222, loss: 3.021956443786621\n",
      "epoch: 121,  batch step: 223, loss: 2.8824851512908936\n",
      "epoch: 121,  batch step: 224, loss: 2.655515670776367\n",
      "epoch: 121,  batch step: 225, loss: 6.18369197845459\n",
      "epoch: 121,  batch step: 226, loss: 17.839130401611328\n",
      "epoch: 121,  batch step: 227, loss: 10.616668701171875\n",
      "epoch: 121,  batch step: 228, loss: 15.992725372314453\n",
      "epoch: 121,  batch step: 229, loss: 1.9631872177124023\n",
      "epoch: 121,  batch step: 230, loss: 2.3546109199523926\n",
      "epoch: 121,  batch step: 231, loss: 17.151168823242188\n",
      "epoch: 121,  batch step: 232, loss: 2.4280471801757812\n",
      "epoch: 121,  batch step: 233, loss: 37.77191162109375\n",
      "epoch: 121,  batch step: 234, loss: 2.078834056854248\n",
      "epoch: 121,  batch step: 235, loss: 10.785577774047852\n",
      "epoch: 121,  batch step: 236, loss: 7.362303733825684\n",
      "epoch: 121,  batch step: 237, loss: 48.326454162597656\n",
      "epoch: 121,  batch step: 238, loss: 1.3098605871200562\n",
      "epoch: 121,  batch step: 239, loss: 62.65355682373047\n",
      "epoch: 121,  batch step: 240, loss: 1.6188411712646484\n",
      "epoch: 121,  batch step: 241, loss: 1.9306795597076416\n",
      "epoch: 121,  batch step: 242, loss: 2.1647536754608154\n",
      "epoch: 121,  batch step: 243, loss: 50.22903060913086\n",
      "epoch: 121,  batch step: 244, loss: 5.691888332366943\n",
      "epoch: 121,  batch step: 245, loss: 9.624093055725098\n",
      "epoch: 121,  batch step: 246, loss: 7.314680099487305\n",
      "epoch: 121,  batch step: 247, loss: 2.918416738510132\n",
      "epoch: 121,  batch step: 248, loss: 8.976555824279785\n",
      "epoch: 121,  batch step: 249, loss: 5.807773590087891\n",
      "epoch: 121,  batch step: 250, loss: 27.261808395385742\n",
      "epoch: 121,  batch step: 251, loss: 136.22592163085938\n",
      "validation error epoch  121:    tensor(69.4861, device='cuda:0')\n",
      "316\n",
      "epoch: 122,  batch step: 0, loss: 12.017087936401367\n",
      "epoch: 122,  batch step: 1, loss: 5.871242523193359\n",
      "epoch: 122,  batch step: 2, loss: 9.413524627685547\n",
      "epoch: 122,  batch step: 3, loss: 15.042922019958496\n",
      "epoch: 122,  batch step: 4, loss: 28.253400802612305\n",
      "epoch: 122,  batch step: 5, loss: 7.641158103942871\n",
      "epoch: 122,  batch step: 6, loss: 39.782386779785156\n",
      "epoch: 122,  batch step: 7, loss: 5.593148708343506\n",
      "epoch: 122,  batch step: 8, loss: 8.623065948486328\n",
      "epoch: 122,  batch step: 9, loss: 6.631341934204102\n",
      "epoch: 122,  batch step: 10, loss: 15.090633392333984\n",
      "epoch: 122,  batch step: 11, loss: 6.331890106201172\n",
      "epoch: 122,  batch step: 12, loss: 32.88385772705078\n",
      "epoch: 122,  batch step: 13, loss: 33.38484573364258\n",
      "epoch: 122,  batch step: 14, loss: 10.103729248046875\n",
      "epoch: 122,  batch step: 15, loss: 7.317927360534668\n",
      "epoch: 122,  batch step: 16, loss: 5.522542953491211\n",
      "epoch: 122,  batch step: 17, loss: 2.9163355827331543\n",
      "epoch: 122,  batch step: 18, loss: 5.286888122558594\n",
      "epoch: 122,  batch step: 19, loss: 6.228206634521484\n",
      "epoch: 122,  batch step: 20, loss: 7.290797233581543\n",
      "epoch: 122,  batch step: 21, loss: 29.94636344909668\n",
      "epoch: 122,  batch step: 22, loss: 5.6868743896484375\n",
      "epoch: 122,  batch step: 23, loss: 8.658258438110352\n",
      "epoch: 122,  batch step: 24, loss: 3.2862091064453125\n",
      "epoch: 122,  batch step: 25, loss: 10.684394836425781\n",
      "epoch: 122,  batch step: 26, loss: 66.60992431640625\n",
      "epoch: 122,  batch step: 27, loss: 14.983930587768555\n",
      "epoch: 122,  batch step: 28, loss: 8.983685493469238\n",
      "epoch: 122,  batch step: 29, loss: 3.5970458984375\n",
      "epoch: 122,  batch step: 30, loss: 5.873096466064453\n",
      "epoch: 122,  batch step: 31, loss: 2.465458869934082\n",
      "epoch: 122,  batch step: 32, loss: 11.64460563659668\n",
      "epoch: 122,  batch step: 33, loss: 17.568578720092773\n",
      "epoch: 122,  batch step: 34, loss: 3.758138656616211\n",
      "epoch: 122,  batch step: 35, loss: 8.173988342285156\n",
      "epoch: 122,  batch step: 36, loss: 7.572460174560547\n",
      "epoch: 122,  batch step: 37, loss: 8.947456359863281\n",
      "epoch: 122,  batch step: 38, loss: 15.89344596862793\n",
      "epoch: 122,  batch step: 39, loss: 33.300968170166016\n",
      "epoch: 122,  batch step: 40, loss: 2.9689910411834717\n",
      "epoch: 122,  batch step: 41, loss: 2.7220377922058105\n",
      "epoch: 122,  batch step: 42, loss: 7.444192886352539\n",
      "epoch: 122,  batch step: 43, loss: 59.53034591674805\n",
      "epoch: 122,  batch step: 44, loss: 2.109522819519043\n",
      "epoch: 122,  batch step: 45, loss: 4.218347549438477\n",
      "epoch: 122,  batch step: 46, loss: 29.734243392944336\n",
      "epoch: 122,  batch step: 47, loss: 4.472825527191162\n",
      "epoch: 122,  batch step: 48, loss: 3.490995168685913\n",
      "epoch: 122,  batch step: 49, loss: 1.8865773677825928\n",
      "epoch: 122,  batch step: 50, loss: 3.139543294906616\n",
      "epoch: 122,  batch step: 51, loss: 2.4570188522338867\n",
      "epoch: 122,  batch step: 52, loss: 3.0914273262023926\n",
      "epoch: 122,  batch step: 53, loss: 2.5496435165405273\n",
      "epoch: 122,  batch step: 54, loss: 12.394704818725586\n",
      "epoch: 122,  batch step: 55, loss: 2.777228832244873\n",
      "epoch: 122,  batch step: 56, loss: 1.9654443264007568\n",
      "epoch: 122,  batch step: 57, loss: 9.44391918182373\n",
      "epoch: 122,  batch step: 58, loss: 10.656990051269531\n",
      "epoch: 122,  batch step: 59, loss: 10.37419319152832\n",
      "epoch: 122,  batch step: 60, loss: 1.9598463773727417\n",
      "epoch: 122,  batch step: 61, loss: 4.595617771148682\n",
      "epoch: 122,  batch step: 62, loss: 12.587371826171875\n",
      "epoch: 122,  batch step: 63, loss: 4.298125267028809\n",
      "epoch: 122,  batch step: 64, loss: 3.305171012878418\n",
      "epoch: 122,  batch step: 65, loss: 2.1902523040771484\n",
      "epoch: 122,  batch step: 66, loss: 16.50836181640625\n",
      "epoch: 122,  batch step: 67, loss: 4.498749732971191\n",
      "epoch: 122,  batch step: 68, loss: 2.103574275970459\n",
      "epoch: 122,  batch step: 69, loss: 1.9364874362945557\n",
      "epoch: 122,  batch step: 70, loss: 2.3012657165527344\n",
      "epoch: 122,  batch step: 71, loss: 13.803507804870605\n",
      "epoch: 122,  batch step: 72, loss: 13.744918823242188\n",
      "epoch: 122,  batch step: 73, loss: 3.8441226482391357\n",
      "epoch: 122,  batch step: 74, loss: 10.407259941101074\n",
      "epoch: 122,  batch step: 75, loss: 7.279391288757324\n",
      "epoch: 122,  batch step: 76, loss: 2.17996883392334\n",
      "epoch: 122,  batch step: 77, loss: 3.858566999435425\n",
      "epoch: 122,  batch step: 78, loss: 2.4280929565429688\n",
      "epoch: 122,  batch step: 79, loss: 2.620816707611084\n",
      "epoch: 122,  batch step: 80, loss: 4.931685447692871\n",
      "epoch: 122,  batch step: 81, loss: 13.74347972869873\n",
      "epoch: 122,  batch step: 82, loss: 85.96890258789062\n",
      "epoch: 122,  batch step: 83, loss: 33.32444763183594\n",
      "epoch: 122,  batch step: 84, loss: 7.5984954833984375\n",
      "epoch: 122,  batch step: 85, loss: 3.5366220474243164\n",
      "epoch: 122,  batch step: 86, loss: 3.544781446456909\n",
      "epoch: 122,  batch step: 87, loss: 3.828535795211792\n",
      "epoch: 122,  batch step: 88, loss: 97.64605712890625\n",
      "epoch: 122,  batch step: 89, loss: 7.544239044189453\n",
      "epoch: 122,  batch step: 90, loss: 2.11903715133667\n",
      "epoch: 122,  batch step: 91, loss: 4.951080322265625\n",
      "epoch: 122,  batch step: 92, loss: 8.988181114196777\n",
      "epoch: 122,  batch step: 93, loss: 8.982067108154297\n",
      "epoch: 122,  batch step: 94, loss: 14.771449089050293\n",
      "epoch: 122,  batch step: 95, loss: 2.072730541229248\n",
      "epoch: 122,  batch step: 96, loss: 27.2918758392334\n",
      "epoch: 122,  batch step: 97, loss: 5.966963768005371\n",
      "epoch: 122,  batch step: 98, loss: 4.003725051879883\n",
      "epoch: 122,  batch step: 99, loss: 2.4873690605163574\n",
      "epoch: 122,  batch step: 100, loss: 11.312949180603027\n",
      "epoch: 122,  batch step: 101, loss: 4.171930313110352\n",
      "epoch: 122,  batch step: 102, loss: 2.3617019653320312\n",
      "epoch: 122,  batch step: 103, loss: 5.438545227050781\n",
      "epoch: 122,  batch step: 104, loss: 2.285738945007324\n",
      "epoch: 122,  batch step: 105, loss: 8.51557445526123\n",
      "epoch: 122,  batch step: 106, loss: 16.11849594116211\n",
      "epoch: 122,  batch step: 107, loss: 3.683032989501953\n",
      "epoch: 122,  batch step: 108, loss: 2.202164888381958\n",
      "epoch: 122,  batch step: 109, loss: 1.8118455410003662\n",
      "epoch: 122,  batch step: 110, loss: 2.098788261413574\n",
      "epoch: 122,  batch step: 111, loss: 4.14787483215332\n",
      "epoch: 122,  batch step: 112, loss: 7.259687423706055\n",
      "epoch: 122,  batch step: 113, loss: 8.076155662536621\n",
      "epoch: 122,  batch step: 114, loss: 9.24206829071045\n",
      "epoch: 122,  batch step: 115, loss: 2.295625686645508\n",
      "epoch: 122,  batch step: 116, loss: 4.8752641677856445\n",
      "epoch: 122,  batch step: 117, loss: 5.7979960441589355\n",
      "epoch: 122,  batch step: 118, loss: 7.721330642700195\n",
      "epoch: 122,  batch step: 119, loss: 3.939831495285034\n",
      "epoch: 122,  batch step: 120, loss: 2.4378457069396973\n",
      "epoch: 122,  batch step: 121, loss: 1.5140514373779297\n",
      "epoch: 122,  batch step: 122, loss: 14.818479537963867\n",
      "epoch: 122,  batch step: 123, loss: 1.7662891149520874\n",
      "epoch: 122,  batch step: 124, loss: 2.6842200756073\n",
      "epoch: 122,  batch step: 125, loss: 3.4636282920837402\n",
      "epoch: 122,  batch step: 126, loss: 11.203265190124512\n",
      "epoch: 122,  batch step: 127, loss: 1.9896557331085205\n",
      "epoch: 122,  batch step: 128, loss: 2.388500213623047\n",
      "epoch: 122,  batch step: 129, loss: 1.9206998348236084\n",
      "epoch: 122,  batch step: 130, loss: 3.1751184463500977\n",
      "epoch: 122,  batch step: 131, loss: 1.8731529712677002\n",
      "epoch: 122,  batch step: 132, loss: 27.80524444580078\n",
      "epoch: 122,  batch step: 133, loss: 1.4488186836242676\n",
      "epoch: 122,  batch step: 134, loss: 1.4915177822113037\n",
      "epoch: 122,  batch step: 135, loss: 2.353048086166382\n",
      "epoch: 122,  batch step: 136, loss: 14.347372055053711\n",
      "epoch: 122,  batch step: 137, loss: 6.471660614013672\n",
      "epoch: 122,  batch step: 138, loss: 3.447387456893921\n",
      "epoch: 122,  batch step: 139, loss: 7.086947441101074\n",
      "epoch: 122,  batch step: 140, loss: 34.62480163574219\n",
      "epoch: 122,  batch step: 141, loss: 15.308420181274414\n",
      "epoch: 122,  batch step: 142, loss: 28.346141815185547\n",
      "epoch: 122,  batch step: 143, loss: 11.349868774414062\n",
      "epoch: 122,  batch step: 144, loss: 2.8510615825653076\n",
      "epoch: 122,  batch step: 145, loss: 2.3940844535827637\n",
      "epoch: 122,  batch step: 146, loss: 1.9314143657684326\n",
      "epoch: 122,  batch step: 147, loss: 1.898934245109558\n",
      "epoch: 122,  batch step: 148, loss: 4.990511894226074\n",
      "epoch: 122,  batch step: 149, loss: 10.93476390838623\n",
      "epoch: 122,  batch step: 150, loss: 23.36501121520996\n",
      "epoch: 122,  batch step: 151, loss: 54.45929718017578\n",
      "epoch: 122,  batch step: 152, loss: 5.379430770874023\n",
      "epoch: 122,  batch step: 153, loss: 18.557395935058594\n",
      "epoch: 122,  batch step: 154, loss: 1.8880760669708252\n",
      "epoch: 122,  batch step: 155, loss: 8.01136589050293\n",
      "epoch: 122,  batch step: 156, loss: 2.629058361053467\n",
      "epoch: 122,  batch step: 157, loss: 2.3238611221313477\n",
      "epoch: 122,  batch step: 158, loss: 8.045145988464355\n",
      "epoch: 122,  batch step: 159, loss: 5.679298400878906\n",
      "epoch: 122,  batch step: 160, loss: 12.641119003295898\n",
      "epoch: 122,  batch step: 161, loss: 10.343489646911621\n",
      "epoch: 122,  batch step: 162, loss: 2.1264476776123047\n",
      "epoch: 122,  batch step: 163, loss: 22.217987060546875\n",
      "epoch: 122,  batch step: 164, loss: 18.849849700927734\n",
      "epoch: 122,  batch step: 165, loss: 1.7570165395736694\n",
      "epoch: 122,  batch step: 166, loss: 127.1163330078125\n",
      "epoch: 122,  batch step: 167, loss: 3.115128993988037\n",
      "epoch: 122,  batch step: 168, loss: 7.744002819061279\n",
      "epoch: 122,  batch step: 169, loss: 11.937252044677734\n",
      "epoch: 122,  batch step: 170, loss: 8.862422943115234\n",
      "epoch: 122,  batch step: 171, loss: 2.494445562362671\n",
      "epoch: 122,  batch step: 172, loss: 5.08330774307251\n",
      "epoch: 122,  batch step: 173, loss: 5.253291606903076\n",
      "epoch: 122,  batch step: 174, loss: 2.2275373935699463\n",
      "epoch: 122,  batch step: 175, loss: 11.240123748779297\n",
      "epoch: 122,  batch step: 176, loss: 15.824691772460938\n",
      "epoch: 122,  batch step: 177, loss: 9.514472007751465\n",
      "epoch: 122,  batch step: 178, loss: 2.123865842819214\n",
      "epoch: 122,  batch step: 179, loss: 9.143268585205078\n",
      "epoch: 122,  batch step: 180, loss: 6.360654354095459\n",
      "epoch: 122,  batch step: 181, loss: 1.924184799194336\n",
      "epoch: 122,  batch step: 182, loss: 1.4251811504364014\n",
      "epoch: 122,  batch step: 183, loss: 179.20211791992188\n",
      "epoch: 122,  batch step: 184, loss: 1.9072022438049316\n",
      "epoch: 122,  batch step: 185, loss: 2.784654140472412\n",
      "epoch: 122,  batch step: 186, loss: 11.202723503112793\n",
      "epoch: 122,  batch step: 187, loss: 2.794524669647217\n",
      "epoch: 122,  batch step: 188, loss: 7.217897415161133\n",
      "epoch: 122,  batch step: 189, loss: 2.290334701538086\n",
      "epoch: 122,  batch step: 190, loss: 1.8892779350280762\n",
      "epoch: 122,  batch step: 191, loss: 31.650320053100586\n",
      "epoch: 122,  batch step: 192, loss: 7.482826232910156\n",
      "epoch: 122,  batch step: 193, loss: 52.708900451660156\n",
      "epoch: 122,  batch step: 194, loss: 10.39205265045166\n",
      "epoch: 122,  batch step: 195, loss: 10.466522216796875\n",
      "epoch: 122,  batch step: 196, loss: 3.903825283050537\n",
      "epoch: 122,  batch step: 197, loss: 8.746360778808594\n",
      "epoch: 122,  batch step: 198, loss: 3.0696749687194824\n",
      "epoch: 122,  batch step: 199, loss: 2.5522923469543457\n",
      "epoch: 122,  batch step: 200, loss: 6.029107093811035\n",
      "epoch: 122,  batch step: 201, loss: 6.835428237915039\n",
      "epoch: 122,  batch step: 202, loss: 2.7446041107177734\n",
      "epoch: 122,  batch step: 203, loss: 2.828352689743042\n",
      "epoch: 122,  batch step: 204, loss: 9.897184371948242\n",
      "epoch: 122,  batch step: 205, loss: 1.7164591550827026\n",
      "epoch: 122,  batch step: 206, loss: 11.257768630981445\n",
      "epoch: 122,  batch step: 207, loss: 1.5052239894866943\n",
      "epoch: 122,  batch step: 208, loss: 2.4996933937072754\n",
      "epoch: 122,  batch step: 209, loss: 1.2256097793579102\n",
      "epoch: 122,  batch step: 210, loss: 5.160256385803223\n",
      "epoch: 122,  batch step: 211, loss: 1.6880226135253906\n",
      "epoch: 122,  batch step: 212, loss: 16.138282775878906\n",
      "epoch: 122,  batch step: 213, loss: 4.881712436676025\n",
      "epoch: 122,  batch step: 214, loss: 1.9085592031478882\n",
      "epoch: 122,  batch step: 215, loss: 1.4978392124176025\n",
      "epoch: 122,  batch step: 216, loss: 7.766499996185303\n",
      "epoch: 122,  batch step: 217, loss: 1.0851787328720093\n",
      "epoch: 122,  batch step: 218, loss: 6.293954849243164\n",
      "epoch: 122,  batch step: 219, loss: 1.7261896133422852\n",
      "epoch: 122,  batch step: 220, loss: 26.177175521850586\n",
      "epoch: 122,  batch step: 221, loss: 4.82077169418335\n",
      "epoch: 122,  batch step: 222, loss: 1.6237162351608276\n",
      "epoch: 122,  batch step: 223, loss: 9.696134567260742\n",
      "epoch: 122,  batch step: 224, loss: 0.9782359600067139\n",
      "epoch: 122,  batch step: 225, loss: 18.972003936767578\n",
      "epoch: 122,  batch step: 226, loss: 4.5926947593688965\n",
      "epoch: 122,  batch step: 227, loss: 9.558309555053711\n",
      "epoch: 122,  batch step: 228, loss: 9.049474716186523\n",
      "epoch: 122,  batch step: 229, loss: 1.6381462812423706\n",
      "epoch: 122,  batch step: 230, loss: 9.73727798461914\n",
      "epoch: 122,  batch step: 231, loss: 48.40003967285156\n",
      "epoch: 122,  batch step: 232, loss: 1.417402744293213\n",
      "epoch: 122,  batch step: 233, loss: 1.778725028038025\n",
      "epoch: 122,  batch step: 234, loss: 12.998300552368164\n",
      "epoch: 122,  batch step: 235, loss: 3.165299415588379\n",
      "epoch: 122,  batch step: 236, loss: 1.7648825645446777\n",
      "epoch: 122,  batch step: 237, loss: 1.4925179481506348\n",
      "epoch: 122,  batch step: 238, loss: 3.4278712272644043\n",
      "epoch: 122,  batch step: 239, loss: 42.273292541503906\n",
      "epoch: 122,  batch step: 240, loss: 9.454910278320312\n",
      "epoch: 122,  batch step: 241, loss: 2.3131837844848633\n",
      "epoch: 122,  batch step: 242, loss: 12.622505187988281\n",
      "epoch: 122,  batch step: 243, loss: 2.8636765480041504\n",
      "epoch: 122,  batch step: 244, loss: 2.266309976577759\n",
      "epoch: 122,  batch step: 245, loss: 2.2008845806121826\n",
      "epoch: 122,  batch step: 246, loss: 7.860269546508789\n",
      "epoch: 122,  batch step: 247, loss: 16.677459716796875\n",
      "epoch: 122,  batch step: 248, loss: 17.060508728027344\n",
      "epoch: 122,  batch step: 249, loss: 3.113276243209839\n",
      "epoch: 122,  batch step: 250, loss: 1.4037150144577026\n",
      "epoch: 122,  batch step: 251, loss: 7.607132434844971\n",
      "validation error epoch  122:    tensor(70.0412, device='cuda:0')\n",
      "316\n",
      "epoch: 123,  batch step: 0, loss: 3.537024974822998\n",
      "epoch: 123,  batch step: 1, loss: 14.593608856201172\n",
      "epoch: 123,  batch step: 2, loss: 2.3613998889923096\n",
      "epoch: 123,  batch step: 3, loss: 1.8992317914962769\n",
      "epoch: 123,  batch step: 4, loss: 3.4317665100097656\n",
      "epoch: 123,  batch step: 5, loss: 64.69673919677734\n",
      "epoch: 123,  batch step: 6, loss: 41.620418548583984\n",
      "epoch: 123,  batch step: 7, loss: 13.995795249938965\n",
      "epoch: 123,  batch step: 8, loss: 13.087122917175293\n",
      "epoch: 123,  batch step: 9, loss: 7.524627685546875\n",
      "epoch: 123,  batch step: 10, loss: 2.8653182983398438\n",
      "epoch: 123,  batch step: 11, loss: 2.9116554260253906\n",
      "epoch: 123,  batch step: 12, loss: 9.868036270141602\n",
      "epoch: 123,  batch step: 13, loss: 2.4590306282043457\n",
      "epoch: 123,  batch step: 14, loss: 2.1076459884643555\n",
      "epoch: 123,  batch step: 15, loss: 3.5095367431640625\n",
      "epoch: 123,  batch step: 16, loss: 2.567335605621338\n",
      "epoch: 123,  batch step: 17, loss: 7.085940361022949\n",
      "epoch: 123,  batch step: 18, loss: 2.4427006244659424\n",
      "epoch: 123,  batch step: 19, loss: 6.8770294189453125\n",
      "epoch: 123,  batch step: 20, loss: 3.5666568279266357\n",
      "epoch: 123,  batch step: 21, loss: 7.960615158081055\n",
      "epoch: 123,  batch step: 22, loss: 2.2027177810668945\n",
      "epoch: 123,  batch step: 23, loss: 7.825507640838623\n",
      "epoch: 123,  batch step: 24, loss: 2.1196930408477783\n",
      "epoch: 123,  batch step: 25, loss: 5.873043060302734\n",
      "epoch: 123,  batch step: 26, loss: 52.32442855834961\n",
      "epoch: 123,  batch step: 27, loss: 10.152382850646973\n",
      "epoch: 123,  batch step: 28, loss: 2.241791248321533\n",
      "epoch: 123,  batch step: 29, loss: 3.2321510314941406\n",
      "epoch: 123,  batch step: 30, loss: 2.274447441101074\n",
      "epoch: 123,  batch step: 31, loss: 2.6966376304626465\n",
      "epoch: 123,  batch step: 32, loss: 16.31045150756836\n",
      "epoch: 123,  batch step: 33, loss: 2.739692211151123\n",
      "epoch: 123,  batch step: 34, loss: 16.003406524658203\n",
      "epoch: 123,  batch step: 35, loss: 48.30712127685547\n",
      "epoch: 123,  batch step: 36, loss: 3.0211455821990967\n",
      "epoch: 123,  batch step: 37, loss: 13.513519287109375\n",
      "epoch: 123,  batch step: 38, loss: 3.71742582321167\n",
      "epoch: 123,  batch step: 39, loss: 28.861209869384766\n",
      "epoch: 123,  batch step: 40, loss: 1.949127197265625\n",
      "epoch: 123,  batch step: 41, loss: 14.000089645385742\n",
      "epoch: 123,  batch step: 42, loss: 17.08499526977539\n",
      "epoch: 123,  batch step: 43, loss: 2.1373159885406494\n",
      "epoch: 123,  batch step: 44, loss: 3.550018787384033\n",
      "epoch: 123,  batch step: 45, loss: 2.5107133388519287\n",
      "epoch: 123,  batch step: 46, loss: 15.887165069580078\n",
      "epoch: 123,  batch step: 47, loss: 7.711084365844727\n",
      "epoch: 123,  batch step: 48, loss: 6.4868974685668945\n",
      "epoch: 123,  batch step: 49, loss: 6.85587215423584\n",
      "epoch: 123,  batch step: 50, loss: 7.096475601196289\n",
      "epoch: 123,  batch step: 51, loss: 2.5371298789978027\n",
      "epoch: 123,  batch step: 52, loss: 2.241009473800659\n",
      "epoch: 123,  batch step: 53, loss: 52.63264465332031\n",
      "epoch: 123,  batch step: 54, loss: 8.73206901550293\n",
      "epoch: 123,  batch step: 55, loss: 2.624405860900879\n",
      "epoch: 123,  batch step: 56, loss: 28.327354431152344\n",
      "epoch: 123,  batch step: 57, loss: 24.76314353942871\n",
      "epoch: 123,  batch step: 58, loss: 4.862252235412598\n",
      "epoch: 123,  batch step: 59, loss: 1.8790277242660522\n",
      "epoch: 123,  batch step: 60, loss: 3.5746963024139404\n",
      "epoch: 123,  batch step: 61, loss: 6.63449764251709\n",
      "epoch: 123,  batch step: 62, loss: 2.2181320190429688\n",
      "epoch: 123,  batch step: 63, loss: 2.2086284160614014\n",
      "epoch: 123,  batch step: 64, loss: 5.261128902435303\n",
      "epoch: 123,  batch step: 65, loss: 17.850419998168945\n",
      "epoch: 123,  batch step: 66, loss: 55.14330291748047\n",
      "epoch: 123,  batch step: 67, loss: 2.0680339336395264\n",
      "epoch: 123,  batch step: 68, loss: 19.79205322265625\n",
      "epoch: 123,  batch step: 69, loss: 24.60102653503418\n",
      "epoch: 123,  batch step: 70, loss: 5.966793060302734\n",
      "epoch: 123,  batch step: 71, loss: 2.780730724334717\n",
      "epoch: 123,  batch step: 72, loss: 6.879199504852295\n",
      "epoch: 123,  batch step: 73, loss: 1.4580717086791992\n",
      "epoch: 123,  batch step: 74, loss: 8.685065269470215\n",
      "epoch: 123,  batch step: 75, loss: 55.5285758972168\n",
      "epoch: 123,  batch step: 76, loss: 7.463753700256348\n",
      "epoch: 123,  batch step: 77, loss: 2.126255989074707\n",
      "epoch: 123,  batch step: 78, loss: 1.8274459838867188\n",
      "epoch: 123,  batch step: 79, loss: 1.772969126701355\n",
      "epoch: 123,  batch step: 80, loss: 11.837677001953125\n",
      "epoch: 123,  batch step: 81, loss: 66.25540161132812\n",
      "epoch: 123,  batch step: 82, loss: 4.477057933807373\n",
      "epoch: 123,  batch step: 83, loss: 3.016927719116211\n",
      "epoch: 123,  batch step: 84, loss: 7.625736236572266\n",
      "epoch: 123,  batch step: 85, loss: 1.5092809200286865\n",
      "epoch: 123,  batch step: 86, loss: 10.196125984191895\n",
      "epoch: 123,  batch step: 87, loss: 6.808133125305176\n",
      "epoch: 123,  batch step: 88, loss: 2.7966418266296387\n",
      "epoch: 123,  batch step: 89, loss: 23.324810028076172\n",
      "epoch: 123,  batch step: 90, loss: 9.393228530883789\n",
      "epoch: 123,  batch step: 91, loss: 1.2919334173202515\n",
      "epoch: 123,  batch step: 92, loss: 11.707860946655273\n",
      "epoch: 123,  batch step: 93, loss: 2.096672534942627\n",
      "epoch: 123,  batch step: 94, loss: 1.601436972618103\n",
      "epoch: 123,  batch step: 95, loss: 3.3775973320007324\n",
      "epoch: 123,  batch step: 96, loss: 1.6221660375595093\n",
      "epoch: 123,  batch step: 97, loss: 9.646007537841797\n",
      "epoch: 123,  batch step: 98, loss: 2.658977508544922\n",
      "epoch: 123,  batch step: 99, loss: 1.5286424160003662\n",
      "epoch: 123,  batch step: 100, loss: 17.02289390563965\n",
      "epoch: 123,  batch step: 101, loss: 2.261259078979492\n",
      "epoch: 123,  batch step: 102, loss: 28.364585876464844\n",
      "epoch: 123,  batch step: 103, loss: 4.319188117980957\n",
      "epoch: 123,  batch step: 104, loss: 8.804174423217773\n",
      "epoch: 123,  batch step: 105, loss: 2.1222329139709473\n",
      "epoch: 123,  batch step: 106, loss: 37.96824645996094\n",
      "epoch: 123,  batch step: 107, loss: 11.866310119628906\n",
      "epoch: 123,  batch step: 108, loss: 26.939311981201172\n",
      "epoch: 123,  batch step: 109, loss: 3.1843981742858887\n",
      "epoch: 123,  batch step: 110, loss: 5.959125518798828\n",
      "epoch: 123,  batch step: 111, loss: 1.7781493663787842\n",
      "epoch: 123,  batch step: 112, loss: 3.140479564666748\n",
      "epoch: 123,  batch step: 113, loss: 2.6786391735076904\n",
      "epoch: 123,  batch step: 114, loss: 2.5001332759857178\n",
      "epoch: 123,  batch step: 115, loss: 5.888961315155029\n",
      "epoch: 123,  batch step: 116, loss: 17.499940872192383\n",
      "epoch: 123,  batch step: 117, loss: 1.5714194774627686\n",
      "epoch: 123,  batch step: 118, loss: 5.108046531677246\n",
      "epoch: 123,  batch step: 119, loss: 2.5494472980499268\n",
      "epoch: 123,  batch step: 120, loss: 1.7045586109161377\n",
      "epoch: 123,  batch step: 121, loss: 1.8760733604431152\n",
      "epoch: 123,  batch step: 122, loss: 26.40127944946289\n",
      "epoch: 123,  batch step: 123, loss: 2.58105731010437\n",
      "epoch: 123,  batch step: 124, loss: 2.201087236404419\n",
      "epoch: 123,  batch step: 125, loss: 22.496963500976562\n",
      "epoch: 123,  batch step: 126, loss: 5.24896240234375\n",
      "epoch: 123,  batch step: 127, loss: 2.208362340927124\n",
      "epoch: 123,  batch step: 128, loss: 33.658531188964844\n",
      "epoch: 123,  batch step: 129, loss: 46.61012268066406\n",
      "epoch: 123,  batch step: 130, loss: 13.545722007751465\n",
      "epoch: 123,  batch step: 131, loss: 6.576507091522217\n",
      "epoch: 123,  batch step: 132, loss: 28.479312896728516\n",
      "epoch: 123,  batch step: 133, loss: 23.144628524780273\n",
      "epoch: 123,  batch step: 134, loss: 9.875551223754883\n",
      "epoch: 123,  batch step: 135, loss: 8.790637969970703\n",
      "epoch: 123,  batch step: 136, loss: 9.614248275756836\n",
      "epoch: 123,  batch step: 137, loss: 1.8344504833221436\n",
      "epoch: 123,  batch step: 138, loss: 10.683525085449219\n",
      "epoch: 123,  batch step: 139, loss: 2.6528100967407227\n",
      "epoch: 123,  batch step: 140, loss: 8.303799629211426\n",
      "epoch: 123,  batch step: 141, loss: 5.922510147094727\n",
      "epoch: 123,  batch step: 142, loss: 1.3208203315734863\n",
      "epoch: 123,  batch step: 143, loss: 1.7965519428253174\n",
      "epoch: 123,  batch step: 144, loss: 2.1987695693969727\n",
      "epoch: 123,  batch step: 145, loss: 6.004961967468262\n",
      "epoch: 123,  batch step: 146, loss: 13.449053764343262\n",
      "epoch: 123,  batch step: 147, loss: 1.5485419034957886\n",
      "epoch: 123,  batch step: 148, loss: 2.7340831756591797\n",
      "epoch: 123,  batch step: 149, loss: 1.5898830890655518\n",
      "epoch: 123,  batch step: 150, loss: 18.315269470214844\n",
      "epoch: 123,  batch step: 151, loss: 6.712303161621094\n",
      "epoch: 123,  batch step: 152, loss: 1.3657045364379883\n",
      "epoch: 123,  batch step: 153, loss: 6.235456466674805\n",
      "epoch: 123,  batch step: 154, loss: 52.98286056518555\n",
      "epoch: 123,  batch step: 155, loss: 7.385776519775391\n",
      "epoch: 123,  batch step: 156, loss: 45.56256103515625\n",
      "epoch: 123,  batch step: 157, loss: 7.6711745262146\n",
      "epoch: 123,  batch step: 158, loss: 6.411157131195068\n",
      "epoch: 123,  batch step: 159, loss: 6.96835994720459\n",
      "epoch: 123,  batch step: 160, loss: 2.8665428161621094\n",
      "epoch: 123,  batch step: 161, loss: 3.64927077293396\n",
      "epoch: 123,  batch step: 162, loss: 26.185842514038086\n",
      "epoch: 123,  batch step: 163, loss: 36.79631423950195\n",
      "epoch: 123,  batch step: 164, loss: 7.52419376373291\n",
      "epoch: 123,  batch step: 165, loss: 12.109545707702637\n",
      "epoch: 123,  batch step: 166, loss: 12.545539855957031\n",
      "epoch: 123,  batch step: 167, loss: 12.23684310913086\n",
      "epoch: 123,  batch step: 168, loss: 3.5523290634155273\n",
      "epoch: 123,  batch step: 169, loss: 1.3358596563339233\n",
      "epoch: 123,  batch step: 170, loss: 3.5612025260925293\n",
      "epoch: 123,  batch step: 171, loss: 2.8384625911712646\n",
      "epoch: 123,  batch step: 172, loss: 9.159124374389648\n",
      "epoch: 123,  batch step: 173, loss: 1.770880937576294\n",
      "epoch: 123,  batch step: 174, loss: 1.8598072528839111\n",
      "epoch: 123,  batch step: 175, loss: 1.9798381328582764\n",
      "epoch: 123,  batch step: 176, loss: 8.373756408691406\n",
      "epoch: 123,  batch step: 177, loss: 1.5518983602523804\n",
      "epoch: 123,  batch step: 178, loss: 4.6586408615112305\n",
      "epoch: 123,  batch step: 179, loss: 2.1000163555145264\n",
      "epoch: 123,  batch step: 180, loss: 2.3428761959075928\n",
      "epoch: 123,  batch step: 181, loss: 1.8708966970443726\n",
      "epoch: 123,  batch step: 182, loss: 2.1989495754241943\n",
      "epoch: 123,  batch step: 183, loss: 7.440641403198242\n",
      "epoch: 123,  batch step: 184, loss: 35.743988037109375\n",
      "epoch: 123,  batch step: 185, loss: 2.290262222290039\n",
      "epoch: 123,  batch step: 186, loss: 10.980151176452637\n",
      "epoch: 123,  batch step: 187, loss: 7.3159589767456055\n",
      "epoch: 123,  batch step: 188, loss: 1.3668769598007202\n",
      "epoch: 123,  batch step: 189, loss: 9.470504760742188\n",
      "epoch: 123,  batch step: 190, loss: 25.718704223632812\n",
      "epoch: 123,  batch step: 191, loss: 1.8455798625946045\n",
      "epoch: 123,  batch step: 192, loss: 1.4988527297973633\n",
      "epoch: 123,  batch step: 193, loss: 4.49419641494751\n",
      "epoch: 123,  batch step: 194, loss: 2.2833995819091797\n",
      "epoch: 123,  batch step: 195, loss: 9.32119369506836\n",
      "epoch: 123,  batch step: 196, loss: 5.909950256347656\n",
      "epoch: 123,  batch step: 197, loss: 2.1430623531341553\n",
      "epoch: 123,  batch step: 198, loss: 1.428297996520996\n",
      "epoch: 123,  batch step: 199, loss: 16.184246063232422\n",
      "epoch: 123,  batch step: 200, loss: 4.331385612487793\n",
      "epoch: 123,  batch step: 201, loss: 4.318918704986572\n",
      "epoch: 123,  batch step: 202, loss: 6.3743486404418945\n",
      "epoch: 123,  batch step: 203, loss: 3.0889270305633545\n",
      "epoch: 123,  batch step: 204, loss: 2.994987964630127\n",
      "epoch: 123,  batch step: 205, loss: 2.510166645050049\n",
      "epoch: 123,  batch step: 206, loss: 2.6644320487976074\n",
      "epoch: 123,  batch step: 207, loss: 3.267897129058838\n",
      "epoch: 123,  batch step: 208, loss: 2.2152957916259766\n",
      "epoch: 123,  batch step: 209, loss: 45.12676239013672\n",
      "epoch: 123,  batch step: 210, loss: 2.055396795272827\n",
      "epoch: 123,  batch step: 211, loss: 5.900473594665527\n",
      "epoch: 123,  batch step: 212, loss: 17.07257843017578\n",
      "epoch: 123,  batch step: 213, loss: 10.741218566894531\n",
      "epoch: 123,  batch step: 214, loss: 1.781368613243103\n",
      "epoch: 123,  batch step: 215, loss: 1.933786392211914\n",
      "epoch: 123,  batch step: 216, loss: 3.635758876800537\n",
      "epoch: 123,  batch step: 217, loss: 1.7032091617584229\n",
      "epoch: 123,  batch step: 218, loss: 132.7935333251953\n",
      "epoch: 123,  batch step: 219, loss: 2.1452698707580566\n",
      "epoch: 123,  batch step: 220, loss: 1.0946736335754395\n",
      "epoch: 123,  batch step: 221, loss: 1.7873973846435547\n",
      "epoch: 123,  batch step: 222, loss: 2.28161883354187\n",
      "epoch: 123,  batch step: 223, loss: 29.995725631713867\n",
      "epoch: 123,  batch step: 224, loss: 2.5108642578125\n",
      "epoch: 123,  batch step: 225, loss: 13.589799880981445\n",
      "epoch: 123,  batch step: 226, loss: 14.797122955322266\n",
      "epoch: 123,  batch step: 227, loss: 26.512203216552734\n",
      "epoch: 123,  batch step: 228, loss: 29.369140625\n",
      "epoch: 123,  batch step: 229, loss: 33.225528717041016\n",
      "epoch: 123,  batch step: 230, loss: 1.8295978307724\n",
      "epoch: 123,  batch step: 231, loss: 15.704277038574219\n",
      "epoch: 123,  batch step: 232, loss: 41.21027755737305\n",
      "epoch: 123,  batch step: 233, loss: 2.452115058898926\n",
      "epoch: 123,  batch step: 234, loss: 8.386343002319336\n",
      "epoch: 123,  batch step: 235, loss: 6.124016761779785\n",
      "epoch: 123,  batch step: 236, loss: 1.4035592079162598\n",
      "epoch: 123,  batch step: 237, loss: 1.9198942184448242\n",
      "epoch: 123,  batch step: 238, loss: 11.308052062988281\n",
      "epoch: 123,  batch step: 239, loss: 2.5541226863861084\n",
      "epoch: 123,  batch step: 240, loss: 2.403435230255127\n",
      "epoch: 123,  batch step: 241, loss: 22.93122100830078\n",
      "epoch: 123,  batch step: 242, loss: 1.412413477897644\n",
      "epoch: 123,  batch step: 243, loss: 8.091550827026367\n",
      "epoch: 123,  batch step: 244, loss: 2.0431747436523438\n",
      "epoch: 123,  batch step: 245, loss: 15.865042686462402\n",
      "epoch: 123,  batch step: 246, loss: 1.8319482803344727\n",
      "epoch: 123,  batch step: 247, loss: 1.9781551361083984\n",
      "epoch: 123,  batch step: 248, loss: 2.269207000732422\n",
      "epoch: 123,  batch step: 249, loss: 7.30865478515625\n",
      "epoch: 123,  batch step: 250, loss: 6.797935485839844\n",
      "epoch: 123,  batch step: 251, loss: 19.931072235107422\n",
      "validation error epoch  123:    tensor(69.3764, device='cuda:0')\n",
      "316\n",
      "epoch: 124,  batch step: 0, loss: 4.512496471405029\n",
      "epoch: 124,  batch step: 1, loss: 3.582692861557007\n",
      "epoch: 124,  batch step: 2, loss: 3.7708823680877686\n",
      "epoch: 124,  batch step: 3, loss: 10.58790397644043\n",
      "epoch: 124,  batch step: 4, loss: 2.311922073364258\n",
      "epoch: 124,  batch step: 5, loss: 2.1393394470214844\n",
      "epoch: 124,  batch step: 6, loss: 13.56165599822998\n",
      "epoch: 124,  batch step: 7, loss: 7.592830657958984\n",
      "epoch: 124,  batch step: 8, loss: 1.634521245956421\n",
      "epoch: 124,  batch step: 9, loss: 13.369277000427246\n",
      "epoch: 124,  batch step: 10, loss: 6.051779747009277\n",
      "epoch: 124,  batch step: 11, loss: 15.632278442382812\n",
      "epoch: 124,  batch step: 12, loss: 2.115560293197632\n",
      "epoch: 124,  batch step: 13, loss: 33.930213928222656\n",
      "epoch: 124,  batch step: 14, loss: 10.47749137878418\n",
      "epoch: 124,  batch step: 15, loss: 20.34197235107422\n",
      "epoch: 124,  batch step: 16, loss: 5.698535442352295\n",
      "epoch: 124,  batch step: 17, loss: 3.1124846935272217\n",
      "epoch: 124,  batch step: 18, loss: 3.4648053646087646\n",
      "epoch: 124,  batch step: 19, loss: 3.695451259613037\n",
      "epoch: 124,  batch step: 20, loss: 53.17228698730469\n",
      "epoch: 124,  batch step: 21, loss: 2.818549633026123\n",
      "epoch: 124,  batch step: 22, loss: 51.90937805175781\n",
      "epoch: 124,  batch step: 23, loss: 8.520610809326172\n",
      "epoch: 124,  batch step: 24, loss: 10.305594444274902\n",
      "epoch: 124,  batch step: 25, loss: 3.797755002975464\n",
      "epoch: 124,  batch step: 26, loss: 16.508708953857422\n",
      "epoch: 124,  batch step: 27, loss: 1.9452667236328125\n",
      "epoch: 124,  batch step: 28, loss: 12.82636547088623\n",
      "epoch: 124,  batch step: 29, loss: 12.163302421569824\n",
      "epoch: 124,  batch step: 30, loss: 6.146541595458984\n",
      "epoch: 124,  batch step: 31, loss: 14.911779403686523\n",
      "epoch: 124,  batch step: 32, loss: 2.9897570610046387\n",
      "epoch: 124,  batch step: 33, loss: 22.794536590576172\n",
      "epoch: 124,  batch step: 34, loss: 20.63836669921875\n",
      "epoch: 124,  batch step: 35, loss: 1.9623960256576538\n",
      "epoch: 124,  batch step: 36, loss: 1.8091024160385132\n",
      "epoch: 124,  batch step: 37, loss: 18.42564582824707\n",
      "epoch: 124,  batch step: 38, loss: 2.3962395191192627\n",
      "epoch: 124,  batch step: 39, loss: 11.01732063293457\n",
      "epoch: 124,  batch step: 40, loss: 1.4089922904968262\n",
      "epoch: 124,  batch step: 41, loss: 3.9412689208984375\n",
      "epoch: 124,  batch step: 42, loss: 61.06403732299805\n",
      "epoch: 124,  batch step: 43, loss: 2.756516933441162\n",
      "epoch: 124,  batch step: 44, loss: 2.5859267711639404\n",
      "epoch: 124,  batch step: 45, loss: 2.1678545475006104\n",
      "epoch: 124,  batch step: 46, loss: 6.741560935974121\n",
      "epoch: 124,  batch step: 47, loss: 1.4452667236328125\n",
      "epoch: 124,  batch step: 48, loss: 12.409728050231934\n",
      "epoch: 124,  batch step: 49, loss: 1.9247573614120483\n",
      "epoch: 124,  batch step: 50, loss: 2.003004550933838\n",
      "epoch: 124,  batch step: 51, loss: 17.04568862915039\n",
      "epoch: 124,  batch step: 52, loss: 21.49542236328125\n",
      "epoch: 124,  batch step: 53, loss: 10.770992279052734\n",
      "epoch: 124,  batch step: 54, loss: 12.517112731933594\n",
      "epoch: 124,  batch step: 55, loss: 1.3863716125488281\n",
      "epoch: 124,  batch step: 56, loss: 13.791934967041016\n",
      "epoch: 124,  batch step: 57, loss: 7.446745872497559\n",
      "epoch: 124,  batch step: 58, loss: 12.843679428100586\n",
      "epoch: 124,  batch step: 59, loss: 7.916404724121094\n",
      "epoch: 124,  batch step: 60, loss: 11.239374160766602\n",
      "epoch: 124,  batch step: 61, loss: 28.519811630249023\n",
      "epoch: 124,  batch step: 62, loss: 2.3166162967681885\n",
      "epoch: 124,  batch step: 63, loss: 2.119396686553955\n",
      "epoch: 124,  batch step: 64, loss: 9.416385650634766\n",
      "epoch: 124,  batch step: 65, loss: 6.116539001464844\n",
      "epoch: 124,  batch step: 66, loss: 2.462656021118164\n",
      "epoch: 124,  batch step: 67, loss: 2.690885543823242\n",
      "epoch: 124,  batch step: 68, loss: 3.0491397380828857\n",
      "epoch: 124,  batch step: 69, loss: 7.446847438812256\n",
      "epoch: 124,  batch step: 70, loss: 13.690593719482422\n",
      "epoch: 124,  batch step: 71, loss: 1.8121201992034912\n",
      "epoch: 124,  batch step: 72, loss: 36.081871032714844\n",
      "epoch: 124,  batch step: 73, loss: 6.203483581542969\n",
      "epoch: 124,  batch step: 74, loss: 36.17304992675781\n",
      "epoch: 124,  batch step: 75, loss: 1.7900643348693848\n",
      "epoch: 124,  batch step: 76, loss: 8.366376876831055\n",
      "epoch: 124,  batch step: 77, loss: 131.23397827148438\n",
      "epoch: 124,  batch step: 78, loss: 7.903308868408203\n",
      "epoch: 124,  batch step: 79, loss: 8.795063018798828\n",
      "epoch: 124,  batch step: 80, loss: 7.2912821769714355\n",
      "epoch: 124,  batch step: 81, loss: 70.55819702148438\n",
      "epoch: 124,  batch step: 82, loss: 8.627331733703613\n",
      "epoch: 124,  batch step: 83, loss: 3.400299549102783\n",
      "epoch: 124,  batch step: 84, loss: 1.6341514587402344\n",
      "epoch: 124,  batch step: 85, loss: 5.169883728027344\n",
      "epoch: 124,  batch step: 86, loss: 35.184471130371094\n",
      "epoch: 124,  batch step: 87, loss: 8.865501403808594\n",
      "epoch: 124,  batch step: 88, loss: 20.270721435546875\n",
      "epoch: 124,  batch step: 89, loss: 3.3668699264526367\n",
      "epoch: 124,  batch step: 90, loss: 2.247683525085449\n",
      "epoch: 124,  batch step: 91, loss: 8.483356475830078\n",
      "epoch: 124,  batch step: 92, loss: 4.5983052253723145\n",
      "epoch: 124,  batch step: 93, loss: 2.943333148956299\n",
      "epoch: 124,  batch step: 94, loss: 1.8009270429611206\n",
      "epoch: 124,  batch step: 95, loss: 2.2359657287597656\n",
      "epoch: 124,  batch step: 96, loss: 1.3045580387115479\n",
      "epoch: 124,  batch step: 97, loss: 1.71735417842865\n",
      "epoch: 124,  batch step: 98, loss: 3.4295852184295654\n",
      "epoch: 124,  batch step: 99, loss: 1.6441588401794434\n",
      "epoch: 124,  batch step: 100, loss: 2.0730109214782715\n",
      "epoch: 124,  batch step: 101, loss: 15.457544326782227\n",
      "epoch: 124,  batch step: 102, loss: 2.0358636379241943\n",
      "epoch: 124,  batch step: 103, loss: 10.378480911254883\n",
      "epoch: 124,  batch step: 104, loss: 9.044852256774902\n",
      "epoch: 124,  batch step: 105, loss: 2.5165443420410156\n",
      "epoch: 124,  batch step: 106, loss: 3.002408027648926\n",
      "epoch: 124,  batch step: 107, loss: 22.46839714050293\n",
      "epoch: 124,  batch step: 108, loss: 4.61511754989624\n",
      "epoch: 124,  batch step: 109, loss: 26.017589569091797\n",
      "epoch: 124,  batch step: 110, loss: 2.6159555912017822\n",
      "epoch: 124,  batch step: 111, loss: 2.7021756172180176\n",
      "epoch: 124,  batch step: 112, loss: 2.279599666595459\n",
      "epoch: 124,  batch step: 113, loss: 6.783540725708008\n",
      "epoch: 124,  batch step: 114, loss: 5.816831588745117\n",
      "epoch: 124,  batch step: 115, loss: 2.120922088623047\n",
      "epoch: 124,  batch step: 116, loss: 2.460056781768799\n",
      "epoch: 124,  batch step: 117, loss: 1.998155951499939\n",
      "epoch: 124,  batch step: 118, loss: 20.498130798339844\n",
      "epoch: 124,  batch step: 119, loss: 4.2486066818237305\n",
      "epoch: 124,  batch step: 120, loss: 7.826322078704834\n",
      "epoch: 124,  batch step: 121, loss: 7.875888347625732\n",
      "epoch: 124,  batch step: 122, loss: 3.25569224357605\n",
      "epoch: 124,  batch step: 123, loss: 9.547826766967773\n",
      "epoch: 124,  batch step: 124, loss: 1.6564258337020874\n",
      "epoch: 124,  batch step: 125, loss: 4.397151947021484\n",
      "epoch: 124,  batch step: 126, loss: 2.9162299633026123\n",
      "epoch: 124,  batch step: 127, loss: 1.0090970993041992\n",
      "epoch: 124,  batch step: 128, loss: 9.63624095916748\n",
      "epoch: 124,  batch step: 129, loss: 10.97693920135498\n",
      "epoch: 124,  batch step: 130, loss: 15.36109447479248\n",
      "epoch: 124,  batch step: 131, loss: 2.023045539855957\n",
      "epoch: 124,  batch step: 132, loss: 9.539666175842285\n",
      "epoch: 124,  batch step: 133, loss: 9.953214645385742\n",
      "epoch: 124,  batch step: 134, loss: 5.230536937713623\n",
      "epoch: 124,  batch step: 135, loss: 1.244261622428894\n",
      "epoch: 124,  batch step: 136, loss: 40.87273025512695\n",
      "epoch: 124,  batch step: 137, loss: 1.3726046085357666\n",
      "epoch: 124,  batch step: 138, loss: 2.761105537414551\n",
      "epoch: 124,  batch step: 139, loss: 28.1339168548584\n",
      "epoch: 124,  batch step: 140, loss: 5.898223876953125\n",
      "epoch: 124,  batch step: 141, loss: 2.0443854331970215\n",
      "epoch: 124,  batch step: 142, loss: 22.855087280273438\n",
      "epoch: 124,  batch step: 143, loss: 1.729162573814392\n",
      "epoch: 124,  batch step: 144, loss: 4.907358169555664\n",
      "epoch: 124,  batch step: 145, loss: 1.3362507820129395\n",
      "epoch: 124,  batch step: 146, loss: 4.285269737243652\n",
      "epoch: 124,  batch step: 147, loss: 10.661701202392578\n",
      "epoch: 124,  batch step: 148, loss: 0.9891325831413269\n",
      "epoch: 124,  batch step: 149, loss: 30.00123405456543\n",
      "epoch: 124,  batch step: 150, loss: 13.994434356689453\n",
      "epoch: 124,  batch step: 151, loss: 3.98352313041687\n",
      "epoch: 124,  batch step: 152, loss: 1.6625442504882812\n",
      "epoch: 124,  batch step: 153, loss: 3.4413981437683105\n",
      "epoch: 124,  batch step: 154, loss: 2.104473829269409\n",
      "epoch: 124,  batch step: 155, loss: 3.191534996032715\n",
      "epoch: 124,  batch step: 156, loss: 1.9984195232391357\n",
      "epoch: 124,  batch step: 157, loss: 1.771050214767456\n",
      "epoch: 124,  batch step: 158, loss: 4.902886867523193\n",
      "epoch: 124,  batch step: 159, loss: 16.45174217224121\n",
      "epoch: 124,  batch step: 160, loss: 50.1705322265625\n",
      "epoch: 124,  batch step: 161, loss: 1.8196227550506592\n",
      "epoch: 124,  batch step: 162, loss: 2.1074366569519043\n",
      "epoch: 124,  batch step: 163, loss: 1.9515985250473022\n",
      "epoch: 124,  batch step: 164, loss: 10.099726676940918\n",
      "epoch: 124,  batch step: 165, loss: 1.315800666809082\n",
      "epoch: 124,  batch step: 166, loss: 10.295414924621582\n",
      "epoch: 124,  batch step: 167, loss: 12.775009155273438\n",
      "epoch: 124,  batch step: 168, loss: 18.39679718017578\n",
      "epoch: 124,  batch step: 169, loss: 6.411953926086426\n",
      "epoch: 124,  batch step: 170, loss: 4.692994594573975\n",
      "epoch: 124,  batch step: 171, loss: 3.775318145751953\n",
      "epoch: 124,  batch step: 172, loss: 1.350356936454773\n",
      "epoch: 124,  batch step: 173, loss: 2.683335065841675\n",
      "epoch: 124,  batch step: 174, loss: 37.951751708984375\n",
      "epoch: 124,  batch step: 175, loss: 6.896212577819824\n",
      "epoch: 124,  batch step: 176, loss: 8.15870189666748\n",
      "epoch: 124,  batch step: 177, loss: 1.624680519104004\n",
      "epoch: 124,  batch step: 178, loss: 16.30799674987793\n",
      "epoch: 124,  batch step: 179, loss: 1.8915061950683594\n",
      "epoch: 124,  batch step: 180, loss: 8.756895065307617\n",
      "epoch: 124,  batch step: 181, loss: 1.6388356685638428\n",
      "epoch: 124,  batch step: 182, loss: 1.9679925441741943\n",
      "epoch: 124,  batch step: 183, loss: 6.661769866943359\n",
      "epoch: 124,  batch step: 184, loss: 10.20323657989502\n",
      "epoch: 124,  batch step: 185, loss: 5.250596046447754\n",
      "epoch: 124,  batch step: 186, loss: 9.221549987792969\n",
      "epoch: 124,  batch step: 187, loss: 3.4146878719329834\n",
      "epoch: 124,  batch step: 188, loss: 5.394844055175781\n",
      "epoch: 124,  batch step: 189, loss: 55.532691955566406\n",
      "epoch: 124,  batch step: 190, loss: 9.551948547363281\n",
      "epoch: 124,  batch step: 191, loss: 6.33149528503418\n",
      "epoch: 124,  batch step: 192, loss: 25.271438598632812\n",
      "epoch: 124,  batch step: 193, loss: 5.715729713439941\n",
      "epoch: 124,  batch step: 194, loss: 1.885480284690857\n",
      "epoch: 124,  batch step: 195, loss: 7.300014495849609\n",
      "epoch: 124,  batch step: 196, loss: 12.874113082885742\n",
      "epoch: 124,  batch step: 197, loss: 3.291746139526367\n",
      "epoch: 124,  batch step: 198, loss: 4.604375839233398\n",
      "epoch: 124,  batch step: 199, loss: 10.83203411102295\n",
      "epoch: 124,  batch step: 200, loss: 1.756446123123169\n",
      "epoch: 124,  batch step: 201, loss: 1.427126169204712\n",
      "epoch: 124,  batch step: 202, loss: 6.704636573791504\n",
      "epoch: 124,  batch step: 203, loss: 6.306161403656006\n",
      "epoch: 124,  batch step: 204, loss: 2.5608344078063965\n",
      "epoch: 124,  batch step: 205, loss: 7.079773902893066\n",
      "epoch: 124,  batch step: 206, loss: 9.462251663208008\n",
      "epoch: 124,  batch step: 207, loss: 11.113219261169434\n",
      "epoch: 124,  batch step: 208, loss: 3.4861559867858887\n",
      "epoch: 124,  batch step: 209, loss: 1.7584741115570068\n",
      "epoch: 124,  batch step: 210, loss: 79.7579345703125\n",
      "epoch: 124,  batch step: 211, loss: 66.79911804199219\n",
      "epoch: 124,  batch step: 212, loss: 2.123790979385376\n",
      "epoch: 124,  batch step: 213, loss: 7.575863838195801\n",
      "epoch: 124,  batch step: 214, loss: 8.002791404724121\n",
      "epoch: 124,  batch step: 215, loss: 7.062327861785889\n",
      "epoch: 124,  batch step: 216, loss: 7.021298408508301\n",
      "epoch: 124,  batch step: 217, loss: 5.797274589538574\n",
      "epoch: 124,  batch step: 218, loss: 1.9921687841415405\n",
      "epoch: 124,  batch step: 219, loss: 1.9890565872192383\n",
      "epoch: 124,  batch step: 220, loss: 18.331012725830078\n",
      "epoch: 124,  batch step: 221, loss: 29.979312896728516\n",
      "epoch: 124,  batch step: 222, loss: 9.999439239501953\n",
      "epoch: 124,  batch step: 223, loss: 23.4335994720459\n",
      "epoch: 124,  batch step: 224, loss: 6.560965538024902\n",
      "epoch: 124,  batch step: 225, loss: 3.3548598289489746\n",
      "epoch: 124,  batch step: 226, loss: 1.3289189338684082\n",
      "epoch: 124,  batch step: 227, loss: 1.66389799118042\n",
      "epoch: 124,  batch step: 228, loss: 1.6522274017333984\n",
      "epoch: 124,  batch step: 229, loss: 9.720415115356445\n",
      "epoch: 124,  batch step: 230, loss: 10.320055961608887\n",
      "epoch: 124,  batch step: 231, loss: 2.04999041557312\n",
      "epoch: 124,  batch step: 232, loss: 5.835836410522461\n",
      "epoch: 124,  batch step: 233, loss: 2.36396861076355\n",
      "epoch: 124,  batch step: 234, loss: 2.1615841388702393\n",
      "epoch: 124,  batch step: 235, loss: 1.854959487915039\n",
      "epoch: 124,  batch step: 236, loss: 2.9853901863098145\n",
      "epoch: 124,  batch step: 237, loss: 6.886926174163818\n",
      "epoch: 124,  batch step: 238, loss: 29.67842674255371\n",
      "epoch: 124,  batch step: 239, loss: 2.5781967639923096\n",
      "epoch: 124,  batch step: 240, loss: 2.3024895191192627\n",
      "epoch: 124,  batch step: 241, loss: 1.9401905536651611\n",
      "epoch: 124,  batch step: 242, loss: 8.510747909545898\n",
      "epoch: 124,  batch step: 243, loss: 2.0422677993774414\n",
      "epoch: 124,  batch step: 244, loss: 4.406105041503906\n",
      "epoch: 124,  batch step: 245, loss: 50.70063781738281\n",
      "epoch: 124,  batch step: 246, loss: 2.3814332485198975\n",
      "epoch: 124,  batch step: 247, loss: 52.567283630371094\n",
      "epoch: 124,  batch step: 248, loss: 19.752058029174805\n",
      "epoch: 124,  batch step: 249, loss: 1.7960870265960693\n",
      "epoch: 124,  batch step: 250, loss: 5.8559112548828125\n",
      "epoch: 124,  batch step: 251, loss: 13.37907600402832\n",
      "validation error epoch  124:    tensor(70.2367, device='cuda:0')\n",
      "316\n",
      "epoch: 125,  batch step: 0, loss: 24.328140258789062\n",
      "epoch: 125,  batch step: 1, loss: 12.395164489746094\n",
      "epoch: 125,  batch step: 2, loss: 53.2603874206543\n",
      "epoch: 125,  batch step: 3, loss: 5.635068893432617\n",
      "epoch: 125,  batch step: 4, loss: 3.224817991256714\n",
      "epoch: 125,  batch step: 5, loss: 1.5294175148010254\n",
      "epoch: 125,  batch step: 6, loss: 20.94396209716797\n",
      "epoch: 125,  batch step: 7, loss: 2.5203170776367188\n",
      "epoch: 125,  batch step: 8, loss: 2.7089173793792725\n",
      "epoch: 125,  batch step: 9, loss: 1.9884252548217773\n",
      "epoch: 125,  batch step: 10, loss: 1.3808581829071045\n",
      "epoch: 125,  batch step: 11, loss: 8.923088073730469\n",
      "epoch: 125,  batch step: 12, loss: 2.169572114944458\n",
      "epoch: 125,  batch step: 13, loss: 15.135747909545898\n",
      "epoch: 125,  batch step: 14, loss: 1.9253549575805664\n",
      "epoch: 125,  batch step: 15, loss: 3.3462367057800293\n",
      "epoch: 125,  batch step: 16, loss: 6.266325950622559\n",
      "epoch: 125,  batch step: 17, loss: 1.2653467655181885\n",
      "epoch: 125,  batch step: 18, loss: 2.3195347785949707\n",
      "epoch: 125,  batch step: 19, loss: 4.627986431121826\n",
      "epoch: 125,  batch step: 20, loss: 4.778441429138184\n",
      "epoch: 125,  batch step: 21, loss: 8.546072959899902\n",
      "epoch: 125,  batch step: 22, loss: 2.1538102626800537\n",
      "epoch: 125,  batch step: 23, loss: 8.734926223754883\n",
      "epoch: 125,  batch step: 24, loss: 2.5729033946990967\n",
      "epoch: 125,  batch step: 25, loss: 1.8016672134399414\n",
      "epoch: 125,  batch step: 26, loss: 1.8949143886566162\n",
      "epoch: 125,  batch step: 27, loss: 6.022869110107422\n",
      "epoch: 125,  batch step: 28, loss: 1.7190589904785156\n",
      "epoch: 125,  batch step: 29, loss: 1.3808506727218628\n",
      "epoch: 125,  batch step: 30, loss: 1.4987201690673828\n",
      "epoch: 125,  batch step: 31, loss: 2.848381757736206\n",
      "epoch: 125,  batch step: 32, loss: 45.879356384277344\n",
      "epoch: 125,  batch step: 33, loss: 1.8423724174499512\n",
      "epoch: 125,  batch step: 34, loss: 4.659862995147705\n",
      "epoch: 125,  batch step: 35, loss: 2.289649724960327\n",
      "epoch: 125,  batch step: 36, loss: 1.4362618923187256\n",
      "epoch: 125,  batch step: 37, loss: 3.6535115242004395\n",
      "epoch: 125,  batch step: 38, loss: 2.1885695457458496\n",
      "epoch: 125,  batch step: 39, loss: 51.544307708740234\n",
      "epoch: 125,  batch step: 40, loss: 1.6446703672409058\n",
      "epoch: 125,  batch step: 41, loss: 13.152670860290527\n",
      "epoch: 125,  batch step: 42, loss: 30.11854362487793\n",
      "epoch: 125,  batch step: 43, loss: 1.4880025386810303\n",
      "epoch: 125,  batch step: 44, loss: 9.731742858886719\n",
      "epoch: 125,  batch step: 45, loss: 5.649015426635742\n",
      "epoch: 125,  batch step: 46, loss: 7.646699905395508\n",
      "epoch: 125,  batch step: 47, loss: 53.39289474487305\n",
      "epoch: 125,  batch step: 48, loss: 2.488292694091797\n",
      "epoch: 125,  batch step: 49, loss: 1.8553636074066162\n",
      "epoch: 125,  batch step: 50, loss: 2.1788015365600586\n",
      "epoch: 125,  batch step: 51, loss: 1.635824203491211\n",
      "epoch: 125,  batch step: 52, loss: 62.208534240722656\n",
      "epoch: 125,  batch step: 53, loss: 5.699607849121094\n",
      "epoch: 125,  batch step: 54, loss: 9.474882125854492\n",
      "epoch: 125,  batch step: 55, loss: 9.37128734588623\n",
      "epoch: 125,  batch step: 56, loss: 2.2049272060394287\n",
      "epoch: 125,  batch step: 57, loss: 5.765411376953125\n",
      "epoch: 125,  batch step: 58, loss: 2.2788050174713135\n",
      "epoch: 125,  batch step: 59, loss: 9.466079711914062\n",
      "epoch: 125,  batch step: 60, loss: 50.16123580932617\n",
      "epoch: 125,  batch step: 61, loss: 1.99220871925354\n",
      "epoch: 125,  batch step: 62, loss: 25.15228271484375\n",
      "epoch: 125,  batch step: 63, loss: 1.3974854946136475\n",
      "epoch: 125,  batch step: 64, loss: 7.22135066986084\n",
      "epoch: 125,  batch step: 65, loss: 1.7896974086761475\n",
      "epoch: 125,  batch step: 66, loss: 2.03878116607666\n",
      "epoch: 125,  batch step: 67, loss: 44.965065002441406\n",
      "epoch: 125,  batch step: 68, loss: 10.55064868927002\n",
      "epoch: 125,  batch step: 69, loss: 14.118968963623047\n",
      "epoch: 125,  batch step: 70, loss: 7.8205695152282715\n",
      "epoch: 125,  batch step: 71, loss: 1.9105563163757324\n",
      "epoch: 125,  batch step: 72, loss: 2.7740302085876465\n",
      "epoch: 125,  batch step: 73, loss: 19.78787612915039\n",
      "epoch: 125,  batch step: 74, loss: 6.102450847625732\n",
      "epoch: 125,  batch step: 75, loss: 14.522246360778809\n",
      "epoch: 125,  batch step: 76, loss: 21.697399139404297\n",
      "epoch: 125,  batch step: 77, loss: 9.996219635009766\n",
      "epoch: 125,  batch step: 78, loss: 3.2337560653686523\n",
      "epoch: 125,  batch step: 79, loss: 2.7623391151428223\n",
      "epoch: 125,  batch step: 80, loss: 2.7699315547943115\n",
      "epoch: 125,  batch step: 81, loss: 6.346381187438965\n",
      "epoch: 125,  batch step: 82, loss: 12.026605606079102\n",
      "epoch: 125,  batch step: 83, loss: 3.089634895324707\n",
      "epoch: 125,  batch step: 84, loss: 2.4504055976867676\n",
      "epoch: 125,  batch step: 85, loss: 2.5768184661865234\n",
      "epoch: 125,  batch step: 86, loss: 2.2806825637817383\n",
      "epoch: 125,  batch step: 87, loss: 18.094650268554688\n",
      "epoch: 125,  batch step: 88, loss: 11.436185836791992\n",
      "epoch: 125,  batch step: 89, loss: 4.702672004699707\n",
      "epoch: 125,  batch step: 90, loss: 3.488497495651245\n",
      "epoch: 125,  batch step: 91, loss: 28.90277099609375\n",
      "epoch: 125,  batch step: 92, loss: 4.272096633911133\n",
      "epoch: 125,  batch step: 93, loss: 9.938697814941406\n",
      "epoch: 125,  batch step: 94, loss: 15.064269065856934\n",
      "epoch: 125,  batch step: 95, loss: 6.109470367431641\n",
      "epoch: 125,  batch step: 96, loss: 2.035421133041382\n",
      "epoch: 125,  batch step: 97, loss: 70.9505615234375\n",
      "epoch: 125,  batch step: 98, loss: 2.4652490615844727\n",
      "epoch: 125,  batch step: 99, loss: 4.207097053527832\n",
      "epoch: 125,  batch step: 100, loss: 2.167787551879883\n",
      "epoch: 125,  batch step: 101, loss: 7.4655866622924805\n",
      "epoch: 125,  batch step: 102, loss: 2.63900089263916\n",
      "epoch: 125,  batch step: 103, loss: 4.14879035949707\n",
      "epoch: 125,  batch step: 104, loss: 1.8657960891723633\n",
      "epoch: 125,  batch step: 105, loss: 4.975892066955566\n",
      "epoch: 125,  batch step: 106, loss: 3.257397413253784\n",
      "epoch: 125,  batch step: 107, loss: 7.46149206161499\n",
      "epoch: 125,  batch step: 108, loss: 2.674866199493408\n",
      "epoch: 125,  batch step: 109, loss: 1.808538794517517\n",
      "epoch: 125,  batch step: 110, loss: 9.074702262878418\n",
      "epoch: 125,  batch step: 111, loss: 1.2612378597259521\n",
      "epoch: 125,  batch step: 112, loss: 1.8690447807312012\n",
      "epoch: 125,  batch step: 113, loss: 41.090538024902344\n",
      "epoch: 125,  batch step: 114, loss: 10.444838523864746\n",
      "epoch: 125,  batch step: 115, loss: 1.3980889320373535\n",
      "epoch: 125,  batch step: 116, loss: 3.2042434215545654\n",
      "epoch: 125,  batch step: 117, loss: 1.9226772785186768\n",
      "epoch: 125,  batch step: 118, loss: 11.400102615356445\n",
      "epoch: 125,  batch step: 119, loss: 7.923272609710693\n",
      "epoch: 125,  batch step: 120, loss: 16.92935562133789\n",
      "epoch: 125,  batch step: 121, loss: 2.239985942840576\n",
      "epoch: 125,  batch step: 122, loss: 2.111067771911621\n",
      "epoch: 125,  batch step: 123, loss: 6.571131706237793\n",
      "epoch: 125,  batch step: 124, loss: 1.6601684093475342\n",
      "epoch: 125,  batch step: 125, loss: 1.8492004871368408\n",
      "epoch: 125,  batch step: 126, loss: 2.0657577514648438\n",
      "epoch: 125,  batch step: 127, loss: 19.966873168945312\n",
      "epoch: 125,  batch step: 128, loss: 10.636496543884277\n",
      "epoch: 125,  batch step: 129, loss: 1.9763978719711304\n",
      "epoch: 125,  batch step: 130, loss: 9.872861862182617\n",
      "epoch: 125,  batch step: 131, loss: 18.744007110595703\n",
      "epoch: 125,  batch step: 132, loss: 5.966083526611328\n",
      "epoch: 125,  batch step: 133, loss: 4.52195405960083\n",
      "epoch: 125,  batch step: 134, loss: 11.560491561889648\n",
      "epoch: 125,  batch step: 135, loss: 198.78363037109375\n",
      "epoch: 125,  batch step: 136, loss: 1.4135017395019531\n",
      "epoch: 125,  batch step: 137, loss: 5.851138591766357\n",
      "epoch: 125,  batch step: 138, loss: 1.5673269033432007\n",
      "epoch: 125,  batch step: 139, loss: 2.4526708126068115\n",
      "epoch: 125,  batch step: 140, loss: 1.4859530925750732\n",
      "epoch: 125,  batch step: 141, loss: 4.448357105255127\n",
      "epoch: 125,  batch step: 142, loss: 9.92837142944336\n",
      "epoch: 125,  batch step: 143, loss: 16.499765396118164\n",
      "epoch: 125,  batch step: 144, loss: 56.095516204833984\n",
      "epoch: 125,  batch step: 145, loss: 2.2048633098602295\n",
      "epoch: 125,  batch step: 146, loss: 13.308825492858887\n",
      "epoch: 125,  batch step: 147, loss: 2.105828285217285\n",
      "epoch: 125,  batch step: 148, loss: 1.4207433462142944\n",
      "epoch: 125,  batch step: 149, loss: 11.098780632019043\n",
      "epoch: 125,  batch step: 150, loss: 8.837913513183594\n",
      "epoch: 125,  batch step: 151, loss: 6.291501998901367\n",
      "epoch: 125,  batch step: 152, loss: 1.8064706325531006\n",
      "epoch: 125,  batch step: 153, loss: 1.500737190246582\n",
      "epoch: 125,  batch step: 154, loss: 4.028493404388428\n",
      "epoch: 125,  batch step: 155, loss: 1.577323317527771\n",
      "epoch: 125,  batch step: 156, loss: 3.610522747039795\n",
      "epoch: 125,  batch step: 157, loss: 59.33475875854492\n",
      "epoch: 125,  batch step: 158, loss: 3.4866533279418945\n",
      "epoch: 125,  batch step: 159, loss: 1.676733374595642\n",
      "epoch: 125,  batch step: 160, loss: 14.19541072845459\n",
      "epoch: 125,  batch step: 161, loss: 9.722436904907227\n",
      "epoch: 125,  batch step: 162, loss: 11.948420524597168\n",
      "epoch: 125,  batch step: 163, loss: 25.535335540771484\n",
      "epoch: 125,  batch step: 164, loss: 2.357670545578003\n",
      "epoch: 125,  batch step: 165, loss: 8.111505508422852\n",
      "epoch: 125,  batch step: 166, loss: 11.220556259155273\n",
      "epoch: 125,  batch step: 167, loss: 1.3047760725021362\n",
      "epoch: 125,  batch step: 168, loss: 10.882762908935547\n",
      "epoch: 125,  batch step: 169, loss: 70.10696411132812\n",
      "epoch: 125,  batch step: 170, loss: 35.68632507324219\n",
      "epoch: 125,  batch step: 171, loss: 1.8421454429626465\n",
      "epoch: 125,  batch step: 172, loss: 9.149614334106445\n",
      "epoch: 125,  batch step: 173, loss: 2.6628518104553223\n",
      "epoch: 125,  batch step: 174, loss: 9.374676704406738\n",
      "epoch: 125,  batch step: 175, loss: 3.458529233932495\n",
      "epoch: 125,  batch step: 176, loss: 2.0715887546539307\n",
      "epoch: 125,  batch step: 177, loss: 1.3538212776184082\n",
      "epoch: 125,  batch step: 178, loss: 25.91010284423828\n",
      "epoch: 125,  batch step: 179, loss: 2.6725902557373047\n",
      "epoch: 125,  batch step: 180, loss: 1.5541720390319824\n",
      "epoch: 125,  batch step: 181, loss: 4.473901748657227\n",
      "epoch: 125,  batch step: 182, loss: 7.95145320892334\n",
      "epoch: 125,  batch step: 183, loss: 50.05181121826172\n",
      "epoch: 125,  batch step: 184, loss: 10.819433212280273\n",
      "epoch: 125,  batch step: 185, loss: 1.4560506343841553\n",
      "epoch: 125,  batch step: 186, loss: 2.7927987575531006\n",
      "epoch: 125,  batch step: 187, loss: 8.599691390991211\n",
      "epoch: 125,  batch step: 188, loss: 9.889732360839844\n",
      "epoch: 125,  batch step: 189, loss: 25.48299217224121\n",
      "epoch: 125,  batch step: 190, loss: 8.483332633972168\n",
      "epoch: 125,  batch step: 191, loss: 1.3777297735214233\n",
      "epoch: 125,  batch step: 192, loss: 1.3050389289855957\n",
      "epoch: 125,  batch step: 193, loss: 9.108183860778809\n",
      "epoch: 125,  batch step: 194, loss: 37.85188674926758\n",
      "epoch: 125,  batch step: 195, loss: 10.543869018554688\n",
      "epoch: 125,  batch step: 196, loss: 1.8398091793060303\n",
      "epoch: 125,  batch step: 197, loss: 6.272677421569824\n",
      "epoch: 125,  batch step: 198, loss: 1.8079102039337158\n",
      "epoch: 125,  batch step: 199, loss: 2.531114339828491\n",
      "epoch: 125,  batch step: 200, loss: 1.3672800064086914\n",
      "epoch: 125,  batch step: 201, loss: 1.6449817419052124\n",
      "epoch: 125,  batch step: 202, loss: 11.111174583435059\n",
      "epoch: 125,  batch step: 203, loss: 3.546886444091797\n",
      "epoch: 125,  batch step: 204, loss: 1.3078409433364868\n",
      "epoch: 125,  batch step: 205, loss: 31.451805114746094\n",
      "epoch: 125,  batch step: 206, loss: 3.139045238494873\n",
      "epoch: 125,  batch step: 207, loss: 1.72422456741333\n",
      "epoch: 125,  batch step: 208, loss: 1.8381478786468506\n",
      "epoch: 125,  batch step: 209, loss: 2.4834742546081543\n",
      "epoch: 125,  batch step: 210, loss: 9.536887168884277\n",
      "epoch: 125,  batch step: 211, loss: 12.823284149169922\n",
      "epoch: 125,  batch step: 212, loss: 4.501018047332764\n",
      "epoch: 125,  batch step: 213, loss: 2.4542930126190186\n",
      "epoch: 125,  batch step: 214, loss: 11.726581573486328\n",
      "epoch: 125,  batch step: 215, loss: 1.761504054069519\n",
      "epoch: 125,  batch step: 216, loss: 3.6267471313476562\n",
      "epoch: 125,  batch step: 217, loss: 31.11507225036621\n",
      "epoch: 125,  batch step: 218, loss: 13.578646659851074\n",
      "epoch: 125,  batch step: 219, loss: 7.4858012199401855\n",
      "epoch: 125,  batch step: 220, loss: 3.3372275829315186\n",
      "epoch: 125,  batch step: 221, loss: 41.2769775390625\n",
      "epoch: 125,  batch step: 222, loss: 2.3353946208953857\n",
      "epoch: 125,  batch step: 223, loss: 1.9636605978012085\n",
      "epoch: 125,  batch step: 224, loss: 13.862642288208008\n",
      "epoch: 125,  batch step: 225, loss: 1.9936994314193726\n",
      "epoch: 125,  batch step: 226, loss: 2.8828036785125732\n",
      "epoch: 125,  batch step: 227, loss: 33.576141357421875\n",
      "epoch: 125,  batch step: 228, loss: 7.350539207458496\n",
      "epoch: 125,  batch step: 229, loss: 2.884355306625366\n",
      "epoch: 125,  batch step: 230, loss: 30.608524322509766\n",
      "epoch: 125,  batch step: 231, loss: 3.87457537651062\n",
      "epoch: 125,  batch step: 232, loss: 45.91084289550781\n",
      "epoch: 125,  batch step: 233, loss: 3.0074994564056396\n",
      "epoch: 125,  batch step: 234, loss: 21.614011764526367\n",
      "epoch: 125,  batch step: 235, loss: 30.10556411743164\n",
      "epoch: 125,  batch step: 236, loss: 2.904326915740967\n",
      "epoch: 125,  batch step: 237, loss: 1.718338966369629\n",
      "epoch: 125,  batch step: 238, loss: 34.6893424987793\n",
      "epoch: 125,  batch step: 239, loss: 2.391714572906494\n",
      "epoch: 125,  batch step: 240, loss: 3.1778132915496826\n",
      "epoch: 125,  batch step: 241, loss: 10.339323043823242\n",
      "epoch: 125,  batch step: 242, loss: 2.163079261779785\n",
      "epoch: 125,  batch step: 243, loss: 7.583209991455078\n",
      "epoch: 125,  batch step: 244, loss: 1.6162288188934326\n",
      "epoch: 125,  batch step: 245, loss: 10.098897933959961\n",
      "epoch: 125,  batch step: 246, loss: 7.135077476501465\n",
      "epoch: 125,  batch step: 247, loss: 2.2743334770202637\n",
      "epoch: 125,  batch step: 248, loss: 2.811784505844116\n",
      "epoch: 125,  batch step: 249, loss: 1.7143714427947998\n",
      "epoch: 125,  batch step: 250, loss: 3.256758451461792\n",
      "epoch: 125,  batch step: 251, loss: 8.917251586914062\n",
      "validation error epoch  125:    tensor(68.9641, device='cuda:0')\n",
      "316\n",
      "epoch: 126,  batch step: 0, loss: 2.280909776687622\n",
      "epoch: 126,  batch step: 1, loss: 1.8401845693588257\n",
      "epoch: 126,  batch step: 2, loss: 3.5469861030578613\n",
      "epoch: 126,  batch step: 3, loss: 11.890823364257812\n",
      "epoch: 126,  batch step: 4, loss: 55.56315231323242\n",
      "epoch: 126,  batch step: 5, loss: 1.8729827404022217\n",
      "epoch: 126,  batch step: 6, loss: 13.213645935058594\n",
      "epoch: 126,  batch step: 7, loss: 2.7340962886810303\n",
      "epoch: 126,  batch step: 8, loss: 7.148812770843506\n",
      "epoch: 126,  batch step: 9, loss: 5.850742340087891\n",
      "epoch: 126,  batch step: 10, loss: 1.9898808002471924\n",
      "epoch: 126,  batch step: 11, loss: 4.742947578430176\n",
      "epoch: 126,  batch step: 12, loss: 2.3264689445495605\n",
      "epoch: 126,  batch step: 13, loss: 8.346728324890137\n",
      "epoch: 126,  batch step: 14, loss: 12.34710693359375\n",
      "epoch: 126,  batch step: 15, loss: 10.282814979553223\n",
      "epoch: 126,  batch step: 16, loss: 2.843775510787964\n",
      "epoch: 126,  batch step: 17, loss: 13.409876823425293\n",
      "epoch: 126,  batch step: 18, loss: 6.488528728485107\n",
      "epoch: 126,  batch step: 19, loss: 1.9548442363739014\n",
      "epoch: 126,  batch step: 20, loss: 5.119355201721191\n",
      "epoch: 126,  batch step: 21, loss: 44.80747604370117\n",
      "epoch: 126,  batch step: 22, loss: 1.8667936325073242\n",
      "epoch: 126,  batch step: 23, loss: 1.4912060499191284\n",
      "epoch: 126,  batch step: 24, loss: 27.06244468688965\n",
      "epoch: 126,  batch step: 25, loss: 7.478945732116699\n",
      "epoch: 126,  batch step: 26, loss: 2.49648380279541\n",
      "epoch: 126,  batch step: 27, loss: 4.527887344360352\n",
      "epoch: 126,  batch step: 28, loss: 1.628645896911621\n",
      "epoch: 126,  batch step: 29, loss: 8.571516036987305\n",
      "epoch: 126,  batch step: 30, loss: 1.7746784687042236\n",
      "epoch: 126,  batch step: 31, loss: 2.8063464164733887\n",
      "epoch: 126,  batch step: 32, loss: 2.0254275798797607\n",
      "epoch: 126,  batch step: 33, loss: 1.9373254776000977\n",
      "epoch: 126,  batch step: 34, loss: 2.2302494049072266\n",
      "epoch: 126,  batch step: 35, loss: 2.7996487617492676\n",
      "epoch: 126,  batch step: 36, loss: 11.32986068725586\n",
      "epoch: 126,  batch step: 37, loss: 15.268783569335938\n",
      "epoch: 126,  batch step: 38, loss: 10.712677001953125\n",
      "epoch: 126,  batch step: 39, loss: 20.462539672851562\n",
      "epoch: 126,  batch step: 40, loss: 7.068817138671875\n",
      "epoch: 126,  batch step: 41, loss: 12.671056747436523\n",
      "epoch: 126,  batch step: 42, loss: 11.529464721679688\n",
      "epoch: 126,  batch step: 43, loss: 20.86040496826172\n",
      "epoch: 126,  batch step: 44, loss: 5.114643096923828\n",
      "epoch: 126,  batch step: 45, loss: 50.57433319091797\n",
      "epoch: 126,  batch step: 46, loss: 15.974878311157227\n",
      "epoch: 126,  batch step: 47, loss: 1.6033014059066772\n",
      "epoch: 126,  batch step: 48, loss: 3.2447400093078613\n",
      "epoch: 126,  batch step: 49, loss: 1.672966718673706\n",
      "epoch: 126,  batch step: 50, loss: 1.697609305381775\n",
      "epoch: 126,  batch step: 51, loss: 1.4438791275024414\n",
      "epoch: 126,  batch step: 52, loss: 4.463005542755127\n",
      "epoch: 126,  batch step: 53, loss: 37.510826110839844\n",
      "epoch: 126,  batch step: 54, loss: 46.679161071777344\n",
      "epoch: 126,  batch step: 55, loss: 2.4958178997039795\n",
      "epoch: 126,  batch step: 56, loss: 27.134193420410156\n",
      "epoch: 126,  batch step: 57, loss: 3.9603476524353027\n",
      "epoch: 126,  batch step: 58, loss: 3.0872225761413574\n",
      "epoch: 126,  batch step: 59, loss: 6.913395881652832\n",
      "epoch: 126,  batch step: 60, loss: 1.7869091033935547\n",
      "epoch: 126,  batch step: 61, loss: 11.071401596069336\n",
      "epoch: 126,  batch step: 62, loss: 4.404341697692871\n",
      "epoch: 126,  batch step: 63, loss: 31.482343673706055\n",
      "epoch: 126,  batch step: 64, loss: 1.7505472898483276\n",
      "epoch: 126,  batch step: 65, loss: 7.19007682800293\n",
      "epoch: 126,  batch step: 66, loss: 15.962112426757812\n",
      "epoch: 126,  batch step: 67, loss: 9.192601203918457\n",
      "epoch: 126,  batch step: 68, loss: 2.6229424476623535\n",
      "epoch: 126,  batch step: 69, loss: 66.61736297607422\n",
      "epoch: 126,  batch step: 70, loss: 8.477039337158203\n",
      "epoch: 126,  batch step: 71, loss: 2.187453269958496\n",
      "epoch: 126,  batch step: 72, loss: 2.298971652984619\n",
      "epoch: 126,  batch step: 73, loss: 5.337367534637451\n",
      "epoch: 126,  batch step: 74, loss: 5.366433620452881\n",
      "epoch: 126,  batch step: 75, loss: 2.3206400871276855\n",
      "epoch: 126,  batch step: 76, loss: 2.2243618965148926\n",
      "epoch: 126,  batch step: 77, loss: 1.5487337112426758\n",
      "epoch: 126,  batch step: 78, loss: 5.381511211395264\n",
      "epoch: 126,  batch step: 79, loss: 1.8594038486480713\n",
      "epoch: 126,  batch step: 80, loss: 9.820343971252441\n",
      "epoch: 126,  batch step: 81, loss: 3.3504714965820312\n",
      "epoch: 126,  batch step: 82, loss: 1.4084994792938232\n",
      "epoch: 126,  batch step: 83, loss: 4.5458221435546875\n",
      "epoch: 126,  batch step: 84, loss: 4.704747200012207\n",
      "epoch: 126,  batch step: 85, loss: 3.1742568016052246\n",
      "epoch: 126,  batch step: 86, loss: 13.49731731414795\n",
      "epoch: 126,  batch step: 87, loss: 5.806272029876709\n",
      "epoch: 126,  batch step: 88, loss: 1.5391435623168945\n",
      "epoch: 126,  batch step: 89, loss: 1.7590720653533936\n",
      "epoch: 126,  batch step: 90, loss: 14.655086517333984\n",
      "epoch: 126,  batch step: 91, loss: 11.219664573669434\n",
      "epoch: 126,  batch step: 92, loss: 3.7090024948120117\n",
      "epoch: 126,  batch step: 93, loss: 31.81940460205078\n",
      "epoch: 126,  batch step: 94, loss: 6.326026439666748\n",
      "epoch: 126,  batch step: 95, loss: 32.29964828491211\n",
      "epoch: 126,  batch step: 96, loss: 33.34123992919922\n",
      "epoch: 126,  batch step: 97, loss: 1.9354562759399414\n",
      "epoch: 126,  batch step: 98, loss: 4.637324333190918\n",
      "epoch: 126,  batch step: 99, loss: 1.3619365692138672\n",
      "epoch: 126,  batch step: 100, loss: 4.753236770629883\n",
      "epoch: 126,  batch step: 101, loss: 10.396086692810059\n",
      "epoch: 126,  batch step: 102, loss: 1.862633466720581\n",
      "epoch: 126,  batch step: 103, loss: 47.63543701171875\n",
      "epoch: 126,  batch step: 104, loss: 52.59461975097656\n",
      "epoch: 126,  batch step: 105, loss: 11.354891777038574\n",
      "epoch: 126,  batch step: 106, loss: 15.536410331726074\n",
      "epoch: 126,  batch step: 107, loss: 10.620952606201172\n",
      "epoch: 126,  batch step: 108, loss: 10.442253112792969\n",
      "epoch: 126,  batch step: 109, loss: 26.58743667602539\n",
      "epoch: 126,  batch step: 110, loss: 3.4273269176483154\n",
      "epoch: 126,  batch step: 111, loss: 1.5023236274719238\n",
      "epoch: 126,  batch step: 112, loss: 2.308565139770508\n",
      "epoch: 126,  batch step: 113, loss: 2.0873703956604004\n",
      "epoch: 126,  batch step: 114, loss: 4.062476634979248\n",
      "epoch: 126,  batch step: 115, loss: 1.9716061353683472\n",
      "epoch: 126,  batch step: 116, loss: 2.6216771602630615\n",
      "epoch: 126,  batch step: 117, loss: 3.019037961959839\n",
      "epoch: 126,  batch step: 118, loss: 2.10892653465271\n",
      "epoch: 126,  batch step: 119, loss: 1.865072250366211\n",
      "epoch: 126,  batch step: 120, loss: 2.334286689758301\n",
      "epoch: 126,  batch step: 121, loss: 1.9374027252197266\n",
      "epoch: 126,  batch step: 122, loss: 9.512701988220215\n",
      "epoch: 126,  batch step: 123, loss: 7.393953323364258\n",
      "epoch: 126,  batch step: 124, loss: 2.348557233810425\n",
      "epoch: 126,  batch step: 125, loss: 36.28373336791992\n",
      "epoch: 126,  batch step: 126, loss: 11.779809951782227\n",
      "epoch: 126,  batch step: 127, loss: 2.116081953048706\n",
      "epoch: 126,  batch step: 128, loss: 2.933403730392456\n",
      "epoch: 126,  batch step: 129, loss: 16.00693702697754\n",
      "epoch: 126,  batch step: 130, loss: 1.837181806564331\n",
      "epoch: 126,  batch step: 131, loss: 2.982725143432617\n",
      "epoch: 126,  batch step: 132, loss: 44.36773681640625\n",
      "epoch: 126,  batch step: 133, loss: 6.673606872558594\n",
      "epoch: 126,  batch step: 134, loss: 9.493470191955566\n",
      "epoch: 126,  batch step: 135, loss: 6.8109540939331055\n",
      "epoch: 126,  batch step: 136, loss: 21.204757690429688\n",
      "epoch: 126,  batch step: 137, loss: 2.0832700729370117\n",
      "epoch: 126,  batch step: 138, loss: 6.830538749694824\n",
      "epoch: 126,  batch step: 139, loss: 2.130500316619873\n",
      "epoch: 126,  batch step: 140, loss: 6.048562049865723\n",
      "epoch: 126,  batch step: 141, loss: 2.1378867626190186\n",
      "epoch: 126,  batch step: 142, loss: 1.401310682296753\n",
      "epoch: 126,  batch step: 143, loss: 7.480722427368164\n",
      "epoch: 126,  batch step: 144, loss: 3.7853832244873047\n",
      "epoch: 126,  batch step: 145, loss: 2.0859875679016113\n",
      "epoch: 126,  batch step: 146, loss: 1.7569315433502197\n",
      "epoch: 126,  batch step: 147, loss: 4.73692512512207\n",
      "epoch: 126,  batch step: 148, loss: 1.965846061706543\n",
      "epoch: 126,  batch step: 149, loss: 7.6053924560546875\n",
      "epoch: 126,  batch step: 150, loss: 35.52915573120117\n",
      "epoch: 126,  batch step: 151, loss: 13.920382499694824\n",
      "epoch: 126,  batch step: 152, loss: 1.503156065940857\n",
      "epoch: 126,  batch step: 153, loss: 26.420211791992188\n",
      "epoch: 126,  batch step: 154, loss: 57.31829833984375\n",
      "epoch: 126,  batch step: 155, loss: 2.6377434730529785\n",
      "epoch: 126,  batch step: 156, loss: 10.743448257446289\n",
      "epoch: 126,  batch step: 157, loss: 4.923576354980469\n",
      "epoch: 126,  batch step: 158, loss: 7.345223426818848\n",
      "epoch: 126,  batch step: 159, loss: 1.7754290103912354\n",
      "epoch: 126,  batch step: 160, loss: 20.673599243164062\n",
      "epoch: 126,  batch step: 161, loss: 6.849416255950928\n",
      "epoch: 126,  batch step: 162, loss: 1.561287760734558\n",
      "epoch: 126,  batch step: 163, loss: 22.989744186401367\n",
      "epoch: 126,  batch step: 164, loss: 1.7325176000595093\n",
      "epoch: 126,  batch step: 165, loss: 10.46804428100586\n",
      "epoch: 126,  batch step: 166, loss: 17.347415924072266\n",
      "epoch: 126,  batch step: 167, loss: 26.58266830444336\n",
      "epoch: 126,  batch step: 168, loss: 8.81716537475586\n",
      "epoch: 126,  batch step: 169, loss: 9.957897186279297\n",
      "epoch: 126,  batch step: 170, loss: 1.6090598106384277\n",
      "epoch: 126,  batch step: 171, loss: 2.545531749725342\n",
      "epoch: 126,  batch step: 172, loss: 5.299566745758057\n",
      "epoch: 126,  batch step: 173, loss: 1.8373130559921265\n",
      "epoch: 126,  batch step: 174, loss: 130.13380432128906\n",
      "epoch: 126,  batch step: 175, loss: 3.955441951751709\n",
      "epoch: 126,  batch step: 176, loss: 12.938762664794922\n",
      "epoch: 126,  batch step: 177, loss: 1.830657720565796\n",
      "epoch: 126,  batch step: 178, loss: 1.8472604751586914\n",
      "epoch: 126,  batch step: 179, loss: 34.7095947265625\n",
      "epoch: 126,  batch step: 180, loss: 1.4827051162719727\n",
      "epoch: 126,  batch step: 181, loss: 1.681187391281128\n",
      "epoch: 126,  batch step: 182, loss: 2.883143186569214\n",
      "epoch: 126,  batch step: 183, loss: 8.822259902954102\n",
      "epoch: 126,  batch step: 184, loss: 1.5199540853500366\n",
      "epoch: 126,  batch step: 185, loss: 6.335060119628906\n",
      "epoch: 126,  batch step: 186, loss: 6.016909599304199\n",
      "epoch: 126,  batch step: 187, loss: 10.098994255065918\n",
      "epoch: 126,  batch step: 188, loss: 57.36124801635742\n",
      "epoch: 126,  batch step: 189, loss: 21.618637084960938\n",
      "epoch: 126,  batch step: 190, loss: 1.8133786916732788\n",
      "epoch: 126,  batch step: 191, loss: 1.9751074314117432\n",
      "epoch: 126,  batch step: 192, loss: 4.099886894226074\n",
      "epoch: 126,  batch step: 193, loss: 19.682769775390625\n",
      "epoch: 126,  batch step: 194, loss: 9.87034797668457\n",
      "epoch: 126,  batch step: 195, loss: 6.015114784240723\n",
      "epoch: 126,  batch step: 196, loss: 40.893470764160156\n",
      "epoch: 126,  batch step: 197, loss: 2.854713201522827\n",
      "epoch: 126,  batch step: 198, loss: 1.7541263103485107\n",
      "epoch: 126,  batch step: 199, loss: 3.011763334274292\n",
      "epoch: 126,  batch step: 200, loss: 1.4965274333953857\n",
      "epoch: 126,  batch step: 201, loss: 1.6834516525268555\n",
      "epoch: 126,  batch step: 202, loss: 14.541780471801758\n",
      "epoch: 126,  batch step: 203, loss: 2.0414371490478516\n",
      "epoch: 126,  batch step: 204, loss: 1.5793942213058472\n",
      "epoch: 126,  batch step: 205, loss: 30.720733642578125\n",
      "epoch: 126,  batch step: 206, loss: 2.1851420402526855\n",
      "epoch: 126,  batch step: 207, loss: 12.554586410522461\n",
      "epoch: 126,  batch step: 208, loss: 25.81135368347168\n",
      "epoch: 126,  batch step: 209, loss: 2.198037624359131\n",
      "epoch: 126,  batch step: 210, loss: 5.871480464935303\n",
      "epoch: 126,  batch step: 211, loss: 3.880577564239502\n",
      "epoch: 126,  batch step: 212, loss: 4.654618263244629\n",
      "epoch: 126,  batch step: 213, loss: 2.912256956100464\n",
      "epoch: 126,  batch step: 214, loss: 55.722225189208984\n",
      "epoch: 126,  batch step: 215, loss: 7.515082359313965\n",
      "epoch: 126,  batch step: 216, loss: 8.762167930603027\n",
      "epoch: 126,  batch step: 217, loss: 2.6354222297668457\n",
      "epoch: 126,  batch step: 218, loss: 10.609127044677734\n",
      "epoch: 126,  batch step: 219, loss: 2.4252991676330566\n",
      "epoch: 126,  batch step: 220, loss: 2.3157451152801514\n",
      "epoch: 126,  batch step: 221, loss: 2.401798725128174\n",
      "epoch: 126,  batch step: 222, loss: 11.034626007080078\n",
      "epoch: 126,  batch step: 223, loss: 2.834627151489258\n",
      "epoch: 126,  batch step: 224, loss: 18.117374420166016\n",
      "epoch: 126,  batch step: 225, loss: 1.7249935865402222\n",
      "epoch: 126,  batch step: 226, loss: 1.3529431819915771\n",
      "epoch: 126,  batch step: 227, loss: 1.4858510494232178\n",
      "epoch: 126,  batch step: 228, loss: 7.837141990661621\n",
      "epoch: 126,  batch step: 229, loss: 56.829254150390625\n",
      "epoch: 126,  batch step: 230, loss: 3.9335246086120605\n",
      "epoch: 126,  batch step: 231, loss: 4.430397033691406\n",
      "epoch: 126,  batch step: 232, loss: 2.615192413330078\n",
      "epoch: 126,  batch step: 233, loss: 3.8238906860351562\n",
      "epoch: 126,  batch step: 234, loss: 1.7960832118988037\n",
      "epoch: 126,  batch step: 235, loss: 5.901342391967773\n",
      "epoch: 126,  batch step: 236, loss: 14.329999923706055\n",
      "epoch: 126,  batch step: 237, loss: 1.9154233932495117\n",
      "epoch: 126,  batch step: 238, loss: 6.640872955322266\n",
      "epoch: 126,  batch step: 239, loss: 18.25969696044922\n",
      "epoch: 126,  batch step: 240, loss: 30.067340850830078\n",
      "epoch: 126,  batch step: 241, loss: 5.9032697677612305\n",
      "epoch: 126,  batch step: 242, loss: 2.3063626289367676\n",
      "epoch: 126,  batch step: 243, loss: 18.579561233520508\n",
      "epoch: 126,  batch step: 244, loss: 6.97323751449585\n",
      "epoch: 126,  batch step: 245, loss: 9.344707489013672\n",
      "epoch: 126,  batch step: 246, loss: 44.071895599365234\n",
      "epoch: 126,  batch step: 247, loss: 1.553044319152832\n",
      "epoch: 126,  batch step: 248, loss: 1.1831600666046143\n",
      "epoch: 126,  batch step: 249, loss: 2.7546229362487793\n",
      "epoch: 126,  batch step: 250, loss: 2.786545515060425\n",
      "epoch: 126,  batch step: 251, loss: 83.63062286376953\n",
      "validation error epoch  126:    tensor(69.4973, device='cuda:0')\n",
      "316\n",
      "epoch: 127,  batch step: 0, loss: 3.065519094467163\n",
      "epoch: 127,  batch step: 1, loss: 6.494668960571289\n",
      "epoch: 127,  batch step: 2, loss: 17.683210372924805\n",
      "epoch: 127,  batch step: 3, loss: 12.398894309997559\n",
      "epoch: 127,  batch step: 4, loss: 9.41120433807373\n",
      "epoch: 127,  batch step: 5, loss: 3.5406901836395264\n",
      "epoch: 127,  batch step: 6, loss: 8.594429016113281\n",
      "epoch: 127,  batch step: 7, loss: 9.011879920959473\n",
      "epoch: 127,  batch step: 8, loss: 3.7373874187469482\n",
      "epoch: 127,  batch step: 9, loss: 3.498014450073242\n",
      "epoch: 127,  batch step: 10, loss: 6.799568176269531\n",
      "epoch: 127,  batch step: 11, loss: 6.041618347167969\n",
      "epoch: 127,  batch step: 12, loss: 5.476407527923584\n",
      "epoch: 127,  batch step: 13, loss: 2.906233787536621\n",
      "epoch: 127,  batch step: 14, loss: 24.51165771484375\n",
      "epoch: 127,  batch step: 15, loss: 22.82985496520996\n",
      "epoch: 127,  batch step: 16, loss: 4.334565162658691\n",
      "epoch: 127,  batch step: 17, loss: 4.670570373535156\n",
      "epoch: 127,  batch step: 18, loss: 7.296773433685303\n",
      "epoch: 127,  batch step: 19, loss: 8.813689231872559\n",
      "epoch: 127,  batch step: 20, loss: 53.161705017089844\n",
      "epoch: 127,  batch step: 21, loss: 63.76493453979492\n",
      "epoch: 127,  batch step: 22, loss: 6.074435710906982\n",
      "epoch: 127,  batch step: 23, loss: 22.961318969726562\n",
      "epoch: 127,  batch step: 24, loss: 18.702320098876953\n",
      "epoch: 127,  batch step: 25, loss: 3.9884910583496094\n",
      "epoch: 127,  batch step: 26, loss: 14.115431785583496\n",
      "epoch: 127,  batch step: 27, loss: 3.1343953609466553\n",
      "epoch: 127,  batch step: 28, loss: 139.54177856445312\n",
      "epoch: 127,  batch step: 29, loss: 3.541795253753662\n",
      "epoch: 127,  batch step: 30, loss: 4.099154472351074\n",
      "epoch: 127,  batch step: 31, loss: 4.061580181121826\n",
      "epoch: 127,  batch step: 32, loss: 31.715824127197266\n",
      "epoch: 127,  batch step: 33, loss: 2.9121198654174805\n",
      "epoch: 127,  batch step: 34, loss: 27.10153579711914\n",
      "epoch: 127,  batch step: 35, loss: 35.30891036987305\n",
      "epoch: 127,  batch step: 36, loss: 4.417390823364258\n",
      "epoch: 127,  batch step: 37, loss: 3.304713010787964\n",
      "epoch: 127,  batch step: 38, loss: 29.130592346191406\n",
      "epoch: 127,  batch step: 39, loss: 16.46881103515625\n",
      "epoch: 127,  batch step: 40, loss: 17.683242797851562\n",
      "epoch: 127,  batch step: 41, loss: 3.81837797164917\n",
      "epoch: 127,  batch step: 42, loss: 9.942227363586426\n",
      "epoch: 127,  batch step: 43, loss: 35.103111267089844\n",
      "epoch: 127,  batch step: 44, loss: 53.67338180541992\n",
      "epoch: 127,  batch step: 45, loss: 5.287634372711182\n",
      "epoch: 127,  batch step: 46, loss: 27.598072052001953\n",
      "epoch: 127,  batch step: 47, loss: 7.4836626052856445\n",
      "epoch: 127,  batch step: 48, loss: 14.299077987670898\n",
      "epoch: 127,  batch step: 49, loss: 4.621143817901611\n",
      "epoch: 127,  batch step: 50, loss: 9.580472946166992\n",
      "epoch: 127,  batch step: 51, loss: 31.679813385009766\n",
      "epoch: 127,  batch step: 52, loss: 7.801316261291504\n",
      "epoch: 127,  batch step: 53, loss: 19.77338981628418\n",
      "epoch: 127,  batch step: 54, loss: 3.7155251502990723\n",
      "epoch: 127,  batch step: 55, loss: 31.634061813354492\n",
      "epoch: 127,  batch step: 56, loss: 7.824113368988037\n",
      "epoch: 127,  batch step: 57, loss: 30.196043014526367\n",
      "epoch: 127,  batch step: 58, loss: 8.98823356628418\n",
      "epoch: 127,  batch step: 59, loss: 3.5145347118377686\n",
      "epoch: 127,  batch step: 60, loss: 19.933757781982422\n",
      "epoch: 127,  batch step: 61, loss: 69.21334838867188\n",
      "epoch: 127,  batch step: 62, loss: 11.55027961730957\n",
      "epoch: 127,  batch step: 63, loss: 19.421817779541016\n",
      "epoch: 127,  batch step: 64, loss: 19.025348663330078\n",
      "epoch: 127,  batch step: 65, loss: 5.221570014953613\n",
      "epoch: 127,  batch step: 66, loss: 37.91457748413086\n",
      "epoch: 127,  batch step: 67, loss: 33.22512435913086\n",
      "epoch: 127,  batch step: 68, loss: 3.124147891998291\n",
      "epoch: 127,  batch step: 69, loss: 2.6469061374664307\n",
      "epoch: 127,  batch step: 70, loss: 5.672109127044678\n",
      "epoch: 127,  batch step: 71, loss: 4.879110336303711\n",
      "epoch: 127,  batch step: 72, loss: 12.906935691833496\n",
      "epoch: 127,  batch step: 73, loss: 3.664376735687256\n",
      "epoch: 127,  batch step: 74, loss: 60.1505241394043\n",
      "epoch: 127,  batch step: 75, loss: 2.444533109664917\n",
      "epoch: 127,  batch step: 76, loss: 48.94734191894531\n",
      "epoch: 127,  batch step: 77, loss: 3.951845169067383\n",
      "epoch: 127,  batch step: 78, loss: 13.771684646606445\n",
      "epoch: 127,  batch step: 79, loss: 17.973255157470703\n",
      "epoch: 127,  batch step: 80, loss: 2.800727367401123\n",
      "epoch: 127,  batch step: 81, loss: 50.463619232177734\n",
      "epoch: 127,  batch step: 82, loss: 2.5857772827148438\n",
      "epoch: 127,  batch step: 83, loss: 5.12904167175293\n",
      "epoch: 127,  batch step: 84, loss: 3.2067055702209473\n",
      "epoch: 127,  batch step: 85, loss: 18.901424407958984\n",
      "epoch: 127,  batch step: 86, loss: 4.455844879150391\n",
      "epoch: 127,  batch step: 87, loss: 4.24627685546875\n",
      "epoch: 127,  batch step: 88, loss: 42.29145431518555\n",
      "epoch: 127,  batch step: 89, loss: 2.882553815841675\n",
      "epoch: 127,  batch step: 90, loss: 37.77273178100586\n",
      "epoch: 127,  batch step: 91, loss: 4.120843887329102\n",
      "epoch: 127,  batch step: 92, loss: 4.31587028503418\n",
      "epoch: 127,  batch step: 93, loss: 14.337614059448242\n",
      "epoch: 127,  batch step: 94, loss: 25.30306053161621\n",
      "epoch: 127,  batch step: 95, loss: 6.6493330001831055\n",
      "epoch: 127,  batch step: 96, loss: 5.680047988891602\n",
      "epoch: 127,  batch step: 97, loss: 19.84502601623535\n",
      "epoch: 127,  batch step: 98, loss: 32.27842712402344\n",
      "epoch: 127,  batch step: 99, loss: 72.31179809570312\n",
      "epoch: 127,  batch step: 100, loss: 40.6686897277832\n",
      "epoch: 127,  batch step: 101, loss: 11.870206832885742\n",
      "epoch: 127,  batch step: 102, loss: 12.98652458190918\n",
      "epoch: 127,  batch step: 103, loss: 8.722871780395508\n",
      "epoch: 127,  batch step: 104, loss: 3.2460365295410156\n",
      "epoch: 127,  batch step: 105, loss: 23.10356903076172\n",
      "epoch: 127,  batch step: 106, loss: 22.473384857177734\n",
      "epoch: 127,  batch step: 107, loss: 8.808148384094238\n",
      "epoch: 127,  batch step: 108, loss: 2.261093854904175\n",
      "epoch: 127,  batch step: 109, loss: 3.7971608638763428\n",
      "epoch: 127,  batch step: 110, loss: 35.329254150390625\n",
      "epoch: 127,  batch step: 111, loss: 4.567873954772949\n",
      "epoch: 127,  batch step: 112, loss: 2.261624336242676\n",
      "epoch: 127,  batch step: 113, loss: 95.06243133544922\n",
      "epoch: 127,  batch step: 114, loss: 4.879488468170166\n",
      "epoch: 127,  batch step: 115, loss: 10.99754524230957\n",
      "epoch: 127,  batch step: 116, loss: 6.2979559898376465\n",
      "epoch: 127,  batch step: 117, loss: 2.4289960861206055\n",
      "epoch: 127,  batch step: 118, loss: 3.0632543563842773\n",
      "epoch: 127,  batch step: 119, loss: 19.89032745361328\n",
      "epoch: 127,  batch step: 120, loss: 65.29170989990234\n",
      "epoch: 127,  batch step: 121, loss: 64.6318359375\n",
      "epoch: 127,  batch step: 122, loss: 6.898029327392578\n",
      "epoch: 127,  batch step: 123, loss: 3.2157225608825684\n",
      "epoch: 127,  batch step: 124, loss: 2.530850410461426\n",
      "epoch: 127,  batch step: 125, loss: 10.320743560791016\n",
      "epoch: 127,  batch step: 126, loss: 20.832977294921875\n",
      "epoch: 127,  batch step: 127, loss: 4.42889928817749\n",
      "epoch: 127,  batch step: 128, loss: 5.50449275970459\n",
      "epoch: 127,  batch step: 129, loss: 17.4237060546875\n",
      "epoch: 127,  batch step: 130, loss: 4.968246936798096\n",
      "epoch: 127,  batch step: 131, loss: 15.69145393371582\n",
      "epoch: 127,  batch step: 132, loss: 3.334115982055664\n",
      "epoch: 127,  batch step: 133, loss: 11.807677268981934\n",
      "epoch: 127,  batch step: 134, loss: 14.22742748260498\n",
      "epoch: 127,  batch step: 135, loss: 6.8537421226501465\n",
      "epoch: 127,  batch step: 136, loss: 32.25611114501953\n",
      "epoch: 127,  batch step: 137, loss: 54.849456787109375\n",
      "epoch: 127,  batch step: 138, loss: 27.400014877319336\n",
      "epoch: 127,  batch step: 139, loss: 22.30593490600586\n",
      "epoch: 127,  batch step: 140, loss: 20.54420280456543\n",
      "epoch: 127,  batch step: 141, loss: 14.029826164245605\n",
      "epoch: 127,  batch step: 142, loss: 24.67523193359375\n",
      "epoch: 127,  batch step: 143, loss: 3.771528720855713\n",
      "epoch: 127,  batch step: 144, loss: 7.123126029968262\n",
      "epoch: 127,  batch step: 145, loss: 32.70227813720703\n",
      "epoch: 127,  batch step: 146, loss: 30.648258209228516\n",
      "epoch: 127,  batch step: 147, loss: 2.7229487895965576\n",
      "epoch: 127,  batch step: 148, loss: 18.08172607421875\n",
      "epoch: 127,  batch step: 149, loss: 13.352842330932617\n",
      "epoch: 127,  batch step: 150, loss: 31.393505096435547\n",
      "epoch: 127,  batch step: 151, loss: 16.620817184448242\n",
      "epoch: 127,  batch step: 152, loss: 4.78894567489624\n",
      "epoch: 127,  batch step: 153, loss: 17.21902847290039\n",
      "epoch: 127,  batch step: 154, loss: 6.0376996994018555\n",
      "epoch: 127,  batch step: 155, loss: 4.037367343902588\n",
      "epoch: 127,  batch step: 156, loss: 5.824149131774902\n",
      "epoch: 127,  batch step: 157, loss: 6.84958028793335\n",
      "epoch: 127,  batch step: 158, loss: 28.49850082397461\n",
      "epoch: 127,  batch step: 159, loss: 12.58326244354248\n",
      "epoch: 127,  batch step: 160, loss: 37.764503479003906\n",
      "epoch: 127,  batch step: 161, loss: 34.63960266113281\n",
      "epoch: 127,  batch step: 162, loss: 5.483974456787109\n",
      "epoch: 127,  batch step: 163, loss: 24.45181655883789\n",
      "epoch: 127,  batch step: 164, loss: 2.922935962677002\n",
      "epoch: 127,  batch step: 165, loss: 5.9303178787231445\n",
      "epoch: 127,  batch step: 166, loss: 9.389047622680664\n",
      "epoch: 127,  batch step: 167, loss: 3.4399993419647217\n",
      "epoch: 127,  batch step: 168, loss: 44.765464782714844\n",
      "epoch: 127,  batch step: 169, loss: 11.561954498291016\n",
      "epoch: 127,  batch step: 170, loss: 19.7464599609375\n",
      "epoch: 127,  batch step: 171, loss: 2.705758571624756\n",
      "epoch: 127,  batch step: 172, loss: 4.831686973571777\n",
      "epoch: 127,  batch step: 173, loss: 2.2284553050994873\n",
      "epoch: 127,  batch step: 174, loss: 3.798604965209961\n",
      "epoch: 127,  batch step: 175, loss: 28.33839988708496\n",
      "epoch: 127,  batch step: 176, loss: 4.05085563659668\n",
      "epoch: 127,  batch step: 177, loss: 26.82394027709961\n",
      "epoch: 127,  batch step: 178, loss: 16.82843589782715\n",
      "epoch: 127,  batch step: 179, loss: 2.8818063735961914\n",
      "epoch: 127,  batch step: 180, loss: 13.884596824645996\n",
      "epoch: 127,  batch step: 181, loss: 61.346900939941406\n",
      "epoch: 127,  batch step: 182, loss: 2.3153553009033203\n",
      "epoch: 127,  batch step: 183, loss: 41.72532272338867\n",
      "epoch: 127,  batch step: 184, loss: 3.0965824127197266\n",
      "epoch: 127,  batch step: 185, loss: 12.38818359375\n",
      "epoch: 127,  batch step: 186, loss: 4.399648666381836\n",
      "epoch: 127,  batch step: 187, loss: 5.546087265014648\n",
      "epoch: 127,  batch step: 188, loss: 4.619853973388672\n",
      "epoch: 127,  batch step: 189, loss: 2.0670135021209717\n",
      "epoch: 127,  batch step: 190, loss: 3.3622751235961914\n",
      "epoch: 127,  batch step: 191, loss: 12.266741752624512\n",
      "epoch: 127,  batch step: 192, loss: 2.714315414428711\n",
      "epoch: 127,  batch step: 193, loss: 16.59691047668457\n",
      "epoch: 127,  batch step: 194, loss: 34.06597900390625\n",
      "epoch: 127,  batch step: 195, loss: 6.221037864685059\n",
      "epoch: 127,  batch step: 196, loss: 2.756166458129883\n",
      "epoch: 127,  batch step: 197, loss: 4.179023742675781\n",
      "epoch: 127,  batch step: 198, loss: 5.314610481262207\n",
      "epoch: 127,  batch step: 199, loss: 9.843549728393555\n",
      "epoch: 127,  batch step: 200, loss: 9.800521850585938\n",
      "epoch: 127,  batch step: 201, loss: 11.46769905090332\n",
      "epoch: 127,  batch step: 202, loss: 1.9312763214111328\n",
      "epoch: 127,  batch step: 203, loss: 27.077774047851562\n",
      "epoch: 127,  batch step: 204, loss: 2.1749157905578613\n",
      "epoch: 127,  batch step: 205, loss: 8.82944107055664\n",
      "epoch: 127,  batch step: 206, loss: 5.745399475097656\n",
      "epoch: 127,  batch step: 207, loss: 2.2740869522094727\n",
      "epoch: 127,  batch step: 208, loss: 11.096826553344727\n",
      "epoch: 127,  batch step: 209, loss: 24.80303955078125\n",
      "epoch: 127,  batch step: 210, loss: 7.123229503631592\n",
      "epoch: 127,  batch step: 211, loss: 14.24375057220459\n",
      "epoch: 127,  batch step: 212, loss: 2.197810649871826\n",
      "epoch: 127,  batch step: 213, loss: 2.377877950668335\n",
      "epoch: 127,  batch step: 214, loss: 51.75782775878906\n",
      "epoch: 127,  batch step: 215, loss: 19.08404541015625\n",
      "epoch: 127,  batch step: 216, loss: 10.87664794921875\n",
      "epoch: 127,  batch step: 217, loss: 29.86829948425293\n",
      "epoch: 127,  batch step: 218, loss: 19.93671417236328\n",
      "epoch: 127,  batch step: 219, loss: 23.856304168701172\n",
      "epoch: 127,  batch step: 220, loss: 11.730944633483887\n",
      "epoch: 127,  batch step: 221, loss: 4.0191473960876465\n",
      "epoch: 127,  batch step: 222, loss: 2.065293788909912\n",
      "epoch: 127,  batch step: 223, loss: 3.547595500946045\n",
      "epoch: 127,  batch step: 224, loss: 14.41727066040039\n",
      "epoch: 127,  batch step: 225, loss: 1.6125402450561523\n",
      "epoch: 127,  batch step: 226, loss: 2.3259871006011963\n",
      "epoch: 127,  batch step: 227, loss: 2.186734199523926\n",
      "epoch: 127,  batch step: 228, loss: 1.6784229278564453\n",
      "epoch: 127,  batch step: 229, loss: 3.2128829956054688\n",
      "epoch: 127,  batch step: 230, loss: 42.51765441894531\n",
      "epoch: 127,  batch step: 231, loss: 2.056090831756592\n",
      "epoch: 127,  batch step: 232, loss: 47.2080192565918\n",
      "epoch: 127,  batch step: 233, loss: 3.4587059020996094\n",
      "epoch: 127,  batch step: 234, loss: 43.47471237182617\n",
      "epoch: 127,  batch step: 235, loss: 2.314499616622925\n",
      "epoch: 127,  batch step: 236, loss: 47.589683532714844\n",
      "epoch: 127,  batch step: 237, loss: 43.5528678894043\n",
      "epoch: 127,  batch step: 238, loss: 9.31125545501709\n",
      "epoch: 127,  batch step: 239, loss: 2.735926389694214\n",
      "epoch: 127,  batch step: 240, loss: 5.912866592407227\n",
      "epoch: 127,  batch step: 241, loss: 21.528928756713867\n",
      "epoch: 127,  batch step: 242, loss: 3.003145217895508\n",
      "epoch: 127,  batch step: 243, loss: 67.89104461669922\n",
      "epoch: 127,  batch step: 244, loss: 15.630366325378418\n",
      "epoch: 127,  batch step: 245, loss: 1.985500693321228\n",
      "epoch: 127,  batch step: 246, loss: 20.009517669677734\n",
      "epoch: 127,  batch step: 247, loss: 10.09107780456543\n",
      "epoch: 127,  batch step: 248, loss: 3.0343942642211914\n",
      "epoch: 127,  batch step: 249, loss: 6.176513671875\n",
      "epoch: 127,  batch step: 250, loss: 5.549966335296631\n",
      "epoch: 127,  batch step: 251, loss: 506.85955810546875\n"
     ]
    }
   ],
   "source": [
    "fileOut=open(log_result+'log'+dataString,'a')\n",
    "fileOut.write(dataString+'Epoch:   Step:    Loss:        Val_Accu :\\n')\n",
    "fileOut.close()\n",
    "fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "fileOut2.write('kernal_size of conv_f is 2')\n",
    "fileOut2.write(dataString+'Epoch:    loss:')\n",
    "\n",
    "\n",
    "\n",
    "#fcn.load_state_dict(torch.load(model_result + 'param_all_1_99_1156'))\n",
    "for epoch in range(EPOCH):\n",
    "    fcn.train()\n",
    "    for step, (img,gt) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        \n",
    "        img = Variable(img).cuda()\n",
    "        #gt=gt.unsqueeze(1).float()# batch x\n",
    "        gt=gt.float()\n",
    "        gt = Variable(gt).cuda()\n",
    "        #b_y = Variable(y)#.cuda()   # batch y\n",
    "        # print(f\"img size: {img.size()}\")\n",
    "        #output = cnn(b_x)[0]               # cnn output\n",
    "        output = fcn(img)               # cnn output\n",
    "        # print(f\"output size:{output.size()}\")\n",
    "        # print(f\"gt size:{gt.size()}\")\n",
    "        loss = loss_func(output, gt)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        print(f\"epoch: {epoch},  batch step: {step}, loss: {loss.data.item()}\")\n",
    "        fileOut=open(log_result+'log'+dataString,'a')\n",
    "        fileOut.write(str(epoch)+'   '+str(step)+'   '+str(loss.data.item())+'\\n')\n",
    "        fileOut.close()\n",
    "    if epoch%10 == 9:\n",
    "        PATH = model_result + 'param_all_2_' + str(epoch) + '_' + str(step)\n",
    "        torch.save(fcn.state_dict(), PATH)\n",
    "        print('finished saving checkpoints')\n",
    "     \n",
    "    LOSS_VALIDATION = 0\n",
    "    fcn.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (img,gt) in enumerate(test_loader):\n",
    "\n",
    "            img = Variable(img).cuda()\n",
    "            # gt=gt.unsqueeze(1)# batch x\n",
    "            gt=gt.float()\n",
    "            gt = Variable(gt).cuda()\n",
    "            # print(f\"gt test size:{gt.size()}\")\n",
    "            output = fcn(img) \n",
    "            # print(f\"validation output size:{output.size()}\")\n",
    "            LOSS_VALIDATION += loss_func(output, gt)\n",
    "        #print(LOSS_VALIDATION.data.item())\n",
    "        LOSS_VALIDATION = LOSS_VALIDATION/step\n",
    "        fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "        fileOut2.write(str(epoch)+'   '+str(step)+'   '+str(LOSS_VALIDATION.data.item())+'\\n')\n",
    "        fileOut2.close()\n",
    "        print('validation error epoch  '+str(epoch)+':    '+str(LOSS_VALIDATION)+'\\n'+str(step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
