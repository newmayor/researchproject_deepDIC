{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.nn import init\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from torchvision import models, transforms\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "def default_loader(path):\n",
    "    return Image.open(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torch.nn import init\n",
    "\n",
    "# custom convolutional layer that accounts for transposed image planes as well as conventional 2D conv layers\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, bias=False, transposed=False):\n",
    "    if transposed:\n",
    "        layer = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=1, output_padding=1,\n",
    "                                   dilation=dilation, bias=bias)\n",
    "    else:\n",
    "        padding = (kernel_size + 2 * (dilation - 1)) // 2\n",
    "        layer = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n",
    "    if bias:\n",
    "        init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "# Returns 2D batch normalisation layer\n",
    "# the range of activation values for each layer is \"forced\" to a normalized distribution of a static mean and cov value, mu & beta\n",
    "# When the activations of a previous layer are forced to a normalized distribution, it makes training of subsequent layers much more efficient\n",
    "def bn(planes):\n",
    "    layer = nn.BatchNorm2d(planes)\n",
    "    # Use mean 0, standard deviation 1 init\n",
    "    init.constant(layer.weight, 1)\n",
    "    init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# feature extraction using pretrained residual network - this performs as the decoder component of the architecture\n",
    "# Resnet addresses the vanishing gradient problem for very deep networks, using skip connections between layers\n",
    "class FeatureResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(BasicBlock, [3, 14, 16, 3], 1000)\n",
    "        self.conv_f = conv(2,64, kernel_size=3,stride = 1)\n",
    "        self.ReLu_1 = nn.ReLU(inplace=True)\n",
    "        self.conv_pre = conv(512, 1024, stride=2, transposed=False)\n",
    "        self.bn_pre = bn(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_f(x) #upsample 8 to 64. changed from Ru's original model bc my image set has 8 feature channels for some reason?\n",
    "        x = self.bn1(x1)\n",
    "        x = self.relu(x)\n",
    "        x2 = self.maxpool(x) #maxpool with kernel size of 3 and add padding of 1\n",
    "        x = self.layer1(x2) #identity sample 64 to 64\n",
    "        x3 = self.layer2(x) #upsample 64 to 128\n",
    "        x4 = self.layer3(x3) #upsample 128 to 256\n",
    "        x5 = self.layer4(x4) #upsample 256 to 512\n",
    "        x6 = self.ReLu_1(self.bn_pre(self.conv_pre(x5))) #upsample 512 to 1024\n",
    "        return x1, x2, x3, x4, x5,x6\n",
    "\n",
    "\n",
    "class SegResNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_net):\n",
    "        super().__init__()\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = conv(1024, 512, stride=1, transposed=False)\n",
    "        self.bn3_2 = bn(512)\n",
    "        self.conv4 = conv(512,512, stride=2, transposed=True)\n",
    "        self.bn4 = bn(512)\n",
    "        self.conv5 = conv(512, 256, stride=2, transposed=True)\n",
    "        self.bn5 = bn(256)\n",
    "        self.conv6 = conv(256, 128, stride=2, transposed=True)\n",
    "        self.bn6 = bn(128)\n",
    "        self.conv7 = conv(128, 64, stride=2, transposed=True)\n",
    "        self.bn7 = bn(64)\n",
    "        self.conv8 = conv(64, 64, stride=2, transposed=True)\n",
    "        self.bn8 = bn(64)\n",
    "        self.conv9 = conv(64, 32, stride=2, transposed=True)\n",
    "        self.bn9 = bn(32)\n",
    "        self.convadd = conv(32, 16, stride=1, transposed=False)\n",
    "        self.bnadd = bn(16)\n",
    "        self.conv10 = conv(16, num_classes,stride=2, kernel_size=5)\n",
    "        init.constant(self.conv10.weight, 0)  # Zero init\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        At init, the FeatureResNet() method (aka \"decoder\") is used to extract features \\\n",
    "        from the input space and then those layer activation values are passed into the \\\n",
    "        encoder's conv layers to reduce dimensionality. Then this is done recursively \\\n",
    "        via gradient descent.\n",
    "        '''\n",
    "        x1, x2, x3, x4, x5, x6 = self.pretrained_net(x) #at init, this is used as feature extraction. Then, it's subsequently used as a decoder\n",
    "        \n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x6)))\n",
    "        \n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn6(self.conv6(x+x4 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn7(self.conv7(x+x3 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn8(self.conv8(x+x2 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn9(self.conv9(x+x1 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bnadd(self.convadd(x)))\n",
    "        x = self.conv10(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = FeatureResNet()\n",
    "fcn = SegResNet(2,fnet) #changed num_classes from 2 to 8 due to the same change in definition of self.conv_f\n",
    "fcn = fcn.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/numairahmed/deepDIC/images/image_sample_pairs/'\n",
    "filename = \"validation_dataset.txt\"\n",
    "mynumbers = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        item = line.strip().split('\\n')\n",
    "        for subitem in item:\n",
    "            mynumbers.append(subitem)\n",
    "            \n",
    "test_set = []\n",
    "for z in range(350):\n",
    "    test_set.append((dataset_path+'/imgs_nocrack/test/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'/imgs_nocrack/test/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'/gts_nocrack/test/train_image_'+str(z+1)+'_.mat'))\n",
    "\n",
    "# dataset_path = '/home/numairahmed/Deep-Dic-deep-learning-based-digital-image-correlation/image_data_numair'\n",
    "filename = \"train_dataset.txt\"\n",
    "mynumbers = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        item = line.strip().split('\\n')\n",
    "        for subitem in item:\n",
    "            mynumbers.append(subitem)\n",
    "            \n",
    "train_set = []\n",
    "for z in range(6169):\n",
    "    train_set.append((dataset_path+'/imgs_nocrack/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'/imgs_nocrack/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'/gts_nocrack/train_image_'+str(z+1)+'_.mat'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "#light_index_2 = [2,7,15,8,4,22,13,57,54,40,91,21,29,84,71,25,28,51,67,62,34,46,93,87]\n",
    "class MyDataset(data_utils.Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None, loader=default_loader):\n",
    "        '''\n",
    "        fh = open(txt, 'r')\n",
    "        imgs = []\n",
    "        for line in fh:\n",
    "            line = line.strip('\\n')\n",
    "            line = line.rstrip()\n",
    "            words = line.split()\n",
    "            imgs.append((words[0],int(words[1])))\n",
    "            \n",
    "        '''\n",
    " \n",
    "        self.imgs = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_x, label_y, label_z = self.imgs[index]\n",
    "        img1 = self.loader(label_x)\n",
    "        img_1 = ToTensor()(img1.resize((128,128)))\n",
    "        img_1 = img_1[::4,:,:]\n",
    "        img2 = self.loader(label_y)\n",
    "        img_2 = ToTensor()(img2.resize((128,128)))\n",
    "        img_2 = img_2[::4,:,:]\n",
    "        imgs = torch.cat((img_1, img_2), 0)\n",
    "        try:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_1'].astype(float)\n",
    "            \n",
    "        except KeyError:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_2'].astype(float)\n",
    "            \n",
    "        # print(f\"gt size before {gt.shape} \")\n",
    "        gt = gt[:,::2,::2]\n",
    "        # print(f\"gt size after {gt.shape} \")\n",
    "        # gt = np.moveaxis(gt, -1, 0)\n",
    "        # print(f\"img_1 {img_1.shape}\\n img_2 {img_2.shape}\\n gt {gt.shape}\\n  imgs {imgs.shape}\\n\")\n",
    "        # print(f\"img1 {img1.size}\\n img2 {img2.size}\\n\")\n",
    "        \n",
    "\n",
    "        return imgs,gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TROUBLESHOOTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 450              # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 12\n",
    "print('BATCH_SIZE = ',BATCH_SIZE)\n",
    "LR = 0.001              # learning rate\n",
    "#root = './gdrive_northwestern/My Drive/dl_encoder/data/orig/orig'\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "#optimizer = torch.optim.SGD(cnn.parameters(), lr=LR, momentum=0.9)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_data=MyDataset(dataset=train_set)\n",
    "train_loader = data_utils.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_data=MyDataset(dataset=test_set)\n",
    "test_loader = data_utils.DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TROUBLESHOOTING\n",
    "\n",
    "\n",
    "for (imgs, gt) in train_loader:\n",
    "    print(imgs[0])\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[1][0])\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[1][1])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dataString = datetime.strftime(datetime.now(), '%Y_%m_%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_result = 'experiment_numairData/'\n",
    "os.mkdir(root_result)\n",
    "model_result = root_result+'model/'\n",
    "log_result = root_result+'log/'\n",
    "os.mkdir(model_result)\n",
    "os.mkdir(log_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileOut=open(log_result+'log'+dataString,'a')\n",
    "fileOut.write(dataString+'Epoch:   Step:    Loss:        Val_Accu :\\n')\n",
    "fileOut.close()\n",
    "fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "fileOut2.write('kernal_size of conv_f is 2')\n",
    "fileOut2.write(dataString+'Epoch:    loss:')\n",
    "\n",
    "\n",
    "\n",
    "#fcn.load_state_dict(torch.load(model_result + 'param_all_1_99_1156'))\n",
    "for epoch in range(EPOCH):\n",
    "    fcn.train()\n",
    "    for step, (img,gt) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        \n",
    "        img = Variable(img).cuda()\n",
    "        #gt=gt.unsqueeze(1).float()# batch x\n",
    "        gt=gt.float()\n",
    "        gt = Variable(gt).cuda()\n",
    "        #b_y = Variable(y)#.cuda()   # batch y\n",
    "        # print(f\"img size: {img.size()}\")\n",
    "        #output = cnn(b_x)[0]               # cnn output\n",
    "        output = fcn(img)               # cnn output\n",
    "        # print(f\"output size:{output.size()}\")\n",
    "        # print(f\"gt size:{gt.size()}\")\n",
    "        loss = loss_func(output, gt)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        print(f\"epoch: {epoch},  batch step: {step}, loss: {loss.data.item()}\")\n",
    "        fileOut=open(log_result+'log'+dataString,'a')\n",
    "        fileOut.write(str(epoch)+'   '+str(step)+'   '+str(loss.data.item())+'\\n')\n",
    "        fileOut.close()\n",
    "    if epoch%10 == 9:\n",
    "        PATH = model_result + 'param_all_2_' + str(epoch) + '_' + str(step)\n",
    "        torch.save(fcn.state_dict(), PATH)\n",
    "        print('finished saving checkpoints')\n",
    "     \n",
    "    LOSS_VALIDATION = 0\n",
    "    fcn.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (img,gt) in enumerate(test_loader):\n",
    "\n",
    "            img = Variable(img).cuda()\n",
    "            # gt=gt.unsqueeze(1)# batch x\n",
    "            gt=gt.float()\n",
    "            gt = Variable(gt).cuda()\n",
    "            # print(f\"gt test size:{gt.size()}\")\n",
    "            output = fcn(img) \n",
    "            # print(f\"validation output size:{output.size()}\")\n",
    "            LOSS_VALIDATION += loss_func(output, gt)\n",
    "        #print(LOSS_VALIDATION.data.item())\n",
    "        LOSS_VALIDATION = LOSS_VALIDATION/step\n",
    "        fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "        fileOut2.write(str(epoch)+'   '+str(step)+'   '+str(LOSS_VALIDATION.data.item())+'\\n')\n",
    "        fileOut2.close()\n",
    "        print('validation error epoch  '+str(epoch)+':    '+str(LOSS_VALIDATION)+'\\n'+str(step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
